	 Key-phrase boosted unsupervised summary generation for FinTech organization
==========
Introduction:  Post: Bank Rewards Checking Account. Comments: I hope they add the ability to make it ajoint account with a spouse.<sep>Nice to see they’re attempting tocompetewith neweronline banks.<sep>When they firstopenedup theirHYSAthey eventually had asignup bonus. I wonder if they’ll do something like that at some point<sep>The main difference between this and the HYSA is that you’ll be able to makes charges andget rewardsfor purchases<sep>If you link your new Checking account to your credit card there’s no way you wouldget MR pointson that transaction, right?<sep>Can youuse Zellethrough the account? signup bonus;compete online banks;opened HYSA;get MR points;use Zelle With the advent of social media, the analysis of data available on these platforms has the potential to provide a gold mine of actionable insights to business organizations. With the ubiquity of social media, the amount of available data has skyrocketed, and social media reflects popular discourse and discussion in a larger societal context. Thus, building a social media knowledge ecosystem in addition to existing data analysis platforms can help an organization to better understand its consumers’ preferences and reactions to the services they offer. These insights help businesses form an understanding of the “external perspective”, directly from the source - the consumers. This additional analysis can have long-term positive impacts and reveal insights that may not be surfaced through traditional channels like customer service call transcripts. Summarizing the discussions available on these social media platforms can give a precise external perspective to an organization about their products, offers, competitors etc, while helping them to make decision for their future products and offers. Hence, in this study, we focus our attention on the problem of phrase based unsupervised summary generation from the Reddit discussions data, with the goal of developing a broader understanding of social media discussions related to financial organization’s products and services. In the last decade, NLP techniques have been successfully applied to social media data in the FinTech context to improve business operations and understand customer opinions. They have been applied to problems like sentiment analysis, stock value prediction, document summarization, intent detection, and so on. A facet that has received particular attention is sentiment analysis from social media data and its utility in predicting trends in the stock market and share prices(Day and Lee,2016; Aji et al.,2019; Karalevicius et al.,2018; Esichaikul and Phumdontree,2018; Sangsavate et al.,2019). Lack of publicly available data and the cost of generating gold standard labels has been a challenge for multiple NLP tasks such as summarization, intent detection, which has lead to the requirement of more unsupervised or semi supervised approaches. Some of the work in this direction are,(Abdaljalil and Bouamor,2021; La Quatra and Cagliero,2020)for summarizing financial reports and FinTech data,(Weld et al.,2022)for task of identifying the user’s intention. Despite availability of some of the labelled datasets for summarization, intent detection, its difficult to solve domain specific problems with them. Most studies on intent detection focus on the spoken language understanding (SLU) datasets - ATIS(Hemphill et al.,1990), and SNIPS(Coucke et al.,2018), which were originally designed for mobile voice assistants, and hence do not generalize well. Secondly, the current formulation of intent detection frames it as a multi-class classification problem. Though pretrained models perform well(Hardalov et al.,2020), the intent class labels designed for ATIS and SNIPS do not apply to other domains and are too restrictive for real-world data. Finally, this supervised definition of intent detection means crowd-sourcing is the only viable option for creating domain-specific social media datasets. Crowd-sourcing comes with its own set of problems, as it is often expensive and presents problems with annotator bias(Eickhoff,2018). Unsupervised methods in intent detection have been explored in the past(Popov et al.,2019; Dopierre et al.,2021; Siddique et al.,2021), but they do not pertain to FinTech data. Traditionally employed in SLU, intent detection has great potential in this domain. Not only can it summarize and simplify lengthy social media discussions, but these salient features can also form representative features for various downstream NLP tasks. In this regard, we propose a completely unsupervised method for phrase-based summary generation, specifically for FinTech social media data. We build a database of posts and comments (threads) from the social news platform Reddit and extract key intent phrases to summarize lengthy threads. None of our data is annotated with summary text or keywords, and we propose a flexible unsupervised approach. In this work the summary of the Reddit discussions is represented by intent keywords in the data. For intent phrase extraction, we follow a two-stage pipeline by first identifying different categories of ’Action-Object’ pairs (tokens matching pre-defined rules), followed by a scoring function to rank them. Additionally for comparison purpose, we perform aspect-based sentiment analysis and identify the key phrases that are associated with the most positive or negative comments. Figure1, demonstrates an overview of our pipeline by identifying the most salient intent phrases from a Reddit thread. These intent phrases are further used as a as vector representations to cluster the post corpus and generate cluster-level summaries to gather insights from large corpus of customer feedback on Reddit. We compared our method by generating summary using an existing keyphrase extraction method, ‘Yake’(Campos et al.,2018)and aspect-based sentiment phrases. For evaluation, we additionally introduce ‘context metrics’ to evaluate the contextual information in the respective cluster summaries. The main contributions of this paper are as follows. (1) We propose a new unsupervised method for phrase-based summary generation using intent key phrases. (2) We study publicly available social media data (pertaining to FinTech), (without crowdsourcing annotation). Finally, (3) we suggest new metrics to evaluate the contextual information retrieved in the clusters and find that our method outperforms existing keyphrase based summary generation methods. Thus, our goal in this paper is to demonstrate that intent based summary can generate meaningful insights in domain-specific financial text without supervision or labeled datasets. In Section2, data collection and description of the end-to-end summary generation pipeline is described. Section3experimental setup, qualitative and qualitative results are demonstrated. Section4conclude the paper and briefly discuss the limitations of the study and future directions.
Fluency score: 2
Fluency issues: running words together like "ajoint", "tocompetewith", "openedup theirHYSA", "signup bonus", "compete online banks", "get MR points", "use Zelle". The text mixes formal exposition with informal, incomplete comments. 

Clarity score: 2
Clarity issues: Jumbled formatting and mixing of separate ideas without clear transitions.

Grammaticality score: 2
Grammaticality issues: Incorrect spacing, punctuation errors, and run-on sentences.

Readability score: 2
Readability issues: The text is not well-structured and has numerous readability issues due to grammatical errors and lack of clear organization.

Coherence score: 2
Coherence issues: The text shifts topic abruptly, from banking features discussion to academic research on social media analytics without transition, making it incoherent.
====================
	 Spatial Deep Learning for Site-Specific Movement Optimization of Aerial Base Stations
==========
Introduction:  With their high mobility and reducing cost, unmanned aerial vehicles (UAVs) have attracted increasing interests in military and civilian domains in recent years. In particular, integrating UAVs into wireless communication networks as aerial base stations (ABSs) to assist terrestrial communication infrastructure in various emergency scenarios such as battlefields, disaster scenes and hotspot events, has been regarded as an important and promising technology[36]. One of the key problems in UAV-aided communication systems is to find applicable placement ofNABSs with limited coverage range in order to achieve maximum coverage ofM(static) ground users (GUs)[21,13,24], which is known to be aNP-hard problemwith exponential complexity inMandN[21]. A tutorial in[31]discusses the ABS placement problem and the most commonly used schemes in scenarios with free space (FS) or non-FS propagation in recent literature. In particular, some algorithms including the spiral algorithm[21], K-means algorithm[13], circle packing theory[24], and user-majority based adaptive UAV deployment[34], are proposed to solve the type of problems with dominant line-of-sight (LoS) or probabilistic LoS/non-LoS (NLoS) channel model[7], under which each ABS has auniformcoverage range. However, due tosite-specific blockages(e.g., buildings), the above channel models might fail to capture the fine-grained structure of LoS or NLoS propagation at specific ABS and GU locations[8,28]. For example, with a slight change of its position, an ABS might transit from LoS to NLoS propagation to the GU due to building edges. This critically affects the ABS-GU channel and further complicates the problem. Some efforts have been made using deep learning (DL) to learnsite-specific channelinformation[15,11], and/or using radio map[8]to construct/utilize spatial channel distribution[20,38,37,29]. In[15], the authors propose to use an end-to-end neural network to learn a site-specific probing codebook in order to predict the optimal narrow beam for beam alignment. The authors in[11]propose a DL-based method for the optimal scheduling of interfering links in a dense wireless network with full frequency reuse. The proposed methods in[15,11]are able to reduce the time/computational overhead of channel estimation and/or schedule links efficiently based on geographic locations of the devices, which yet are not directly applicable to the ABS placement/movement problems. In the context of UAV communications, radio map has been utilized to represent site-specific spatial distribution of average received power radiated from given transmitting source(s), e.g., the fixed ground base stations for cellular-connected UAV[20,38,37], or the ABSs[29]to provide ground coverage. For ABS placement, the authors in[29]leverage on a given spatial loss field (SLF) function to construct the radio map with low complexity, whereas how to obtain/store site-specific SLF for any given ABS-GU location pair with high sample efficiency is yet to be addressed. Other authors in[9]partially circumvent this challenge by developing adaptive UAV positioning strategy with on-site LoS condition measurements for a given pair of UAV-relay and GU, whereas multi-ABS/multi-GU scenarios are yet to be considered. The ABS placement problem can be further complicated due toGU mobility, which brings additional complexity and the practical requirement of finding desired solutions within limited time. In this regard, machine learning methods including DL and reinforcement learning (RL)/deep RL (DRL) have been developed/applied to solve complicated problems of UAV joint optimization considering multiple factors such as UAV trajectory design, user association, resource allocation and power consumption[18,10,32](see the recent survey[30]for more references). In particular, RL/DRL methods have recently been applied to tackle the multi-UAV movement optimization problems[16,17,19,33,39,22]. In terms of ABS coverage and energy consumption trade-off, a DRL-based approach is proposed in[16]to achieve energy-efficient and fair communication coverage. A decentralized DRL-based framework is further proposed in[17]to provide energy-efficient long-term communication coverage. The authors in[19]propose a genetic algorithm-based K-means algorithm to partition GUs into clusters, and further apply the Q-learning algorithm in each cluster for ABS movement. The authors in[33]and[39]both formulate the ABS movement problem as a constrained markov decision process, and propose the dueling Deep Q-network (DQN) and/or constrained DQN algorithms to maximize the downlink capacity with full coverage constraint. However, the above works typically assume uniform coverage range in a generic environment, whereas thesite-specific LoS/NLoS propagationscenario is yet to be considered. Forsite-specific multi-ABS movementoptimization, the authors in[22]propose a single-agent Deep Deterministic Policy Gradient (DDPG) based approach to maximize the average sum-rate of all GUs via UAV dynamic movement and communication co-design, whereas the tested network is relatively small (e.g., withM=10andN=2). On the other hand, our early work in[28]proposes a Double DQN with Prioritized Experience Replay (PER-DDQN) to address the site-specific ABS placement problem with a moderate network size (e.g.,M=80andN=10). However, a straightforward extension to ABS movement optimization encounters further difficulties. In particular, the action space of RL/DRL methods growscombinatoriallywithNand the number of steps to explore. Moreover, a new neural network (NN) model in DRL would often need to be re-trained in order to cater fornetwork changes(e.g., ABSs/GUs turning on/off, varying GU speeds, etc.), which calls for more timely adaptation and more flexible design. To circumvent the above difficulties, we shift our mindset and attempt a different approach other than DRL. First, the ABS movement problem is partitioned into a time series of ABS placement sub-problems, each of which aims for maximum GU coverage under given GU locations, and thus amounts to apattern matching/search problem. Second, a state-of-the-art search algorithm called Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)[23]is adopted with tailored modifications to solve each ABS placement sub-problem. Third, an environment emulator is built to predict the site-specific coverage status of all GUs in the actual environment and assist fast evaluation of ABS placement solutions. Our main contributions are summarized as follows: Spatial Deep Learning (SDL)111We reuse the term SDL as in[11]to refer to the general method of learning spatial characteristics by DL, although different problem setup and DL architecture are considered here.for Coverage Prediction: An encoder-decoder type of deep-NN (DNN) with careful incorporation of domain knowledge is proposed. First, we use threegrid-mapsto quantize and represent the location patterns of ABSs, GUs, and covered GUs (CGUs), respectively, with the first two as input and the last one as output of the DNN. These grid-maps endow the DNN with 1) input/outputdimension invariancewith the number of ABSs/GUs; and 2) input/outputpermutation invariancewith ABS/GU indexes. These invariance properties significantly reduce the learning burden of DNN and also render more flexibility and robustness to scenarios where the number of ABSs/GUs dynamically changes on site. Second, tailored design techniques are proposed includingbinary mask processingandelement-wise binary classification, which effectively boost the training efficiency. MAP-Elites as Quality-Diversity Search Engine: Based on the trained DNN emulator, we further propose theSDL-based MAP-Elites (SDL-ME)algorithm. First, based on MAP-Elites, the search over the original variable space (ofO(N)dimensions) is effectively reduced to that over a low-dimensional (e.g., two in our proposed design) feature space, which flexibly tradeoffs betweencomplexity reductionandsolution diversity, thus encouraging more efficient search for better-quality solutions. Second, the SDL-based emulator captures the site-specific ABS-GU coverage states and enablesvirtual explorationof a much larger part of the search space compared with direct on-site trials and errors, thus leading to potentially better solutions. Top-kMechanism and Planning-Exploration-Serving (PES) Scheme: The SDL-ME based planning helps sift outktop performing candidate solutions (in terms of emulator-predicted coverage rate), which are then explored and validated on site to further elect the best performing solution (in terms of actual coverage rate) for ABS placement to serve GUs in the current time period. Such a top-kmechanism and PES scheme seamlessly amalgamate theemulator-based planningandon-site exploration/serving, thus significantly reducing the cost of extensive on-site search. Moreover, the measurements for the top-kcandidate solutions effectively compensate for the quantization/prediction errors due to model approximation. Numerical results demonstrate that the proposed approach significantly outperforms the benchmark DRL-based method and other two baselines in terms of average coverage rate, training time and/or sample efficiency. Moreover, with one-time training, our proposed method can be applied indynamic scenarioswhere the number of ABSs/GUs changes on site and/or with different/varying GU speeds, which is more robust and flexible compared with conventional DRL methods.
Fluency score: 3
Fluency issues: [verbose sentences, excessive technical jargon]

Clarity score: 3
Clarity issues: [complex sentence structures, numerous acronyms]

Grammaticality score: 4
Grammaticality issues: [minor issues with spacing, e.g., "ofNABSs"]

Readability score: 2
Readability issues: [dense paragraphs, excessive inline citations, technical language]

Coherence score: 3
Coherence issues: [jumps between topics, assumes extensive prior knowledge]
====================
	 Swing: Short-cutting Rings for Higher Bandwidth Allreduce
==========
Introduction:  Allreduce is a collective operation used to aggregate vectors among a set of nodes and to distribute back to them the aggregated result. Among others, allreduce is widely used to perform distributed gradient aggregation during the training of deep learning models[10]. Several studies have shown that it can account for up to 40% of the total training time[44,28,39]and between 19% and 30% of the total core hours in MPI jobs running on production supercomputers[14]. Researchers proposed several allreduce algorithms[43,6,25], and the most performing one depends on a combination of vector size, number of nodes, and physical topology[42,23]. Those algorithms perform a predefined number of steps and, at each step, each node sends and receives data to and from some predetermined nodes. Different trade-offs exist between the number of steps to perform (more critical for allreduce on small vectors) and the total number of bytes it transmits (more relevant for larger allreduce). However, a third factor that must be considered when designing a new collective algorithm is the physical distance between communicating nodes[43,33,38,42]. This is particularly relevant on networks that do not provide full bisection bandwidth such as torus, since the higher the distance, the higher the number of flows sharing the same links. Torus networks are widely used, both on systems optimized for running machine learning (ML) workloads (e.g., Google deploys TPUs v4 on a 3D torus-like network[27], and Amazon deploys Trainium devices on a 2D torus[7]), and on some of the top supercomputers[2,11](e.g., Fugaku uses a 6D torus[4]). We show the importance of the distance between communicating nodes in the allreduce through an example. In Fig.1, we show a16node 1D torus (we only show a subset of the nodes since the communications are symmetric). We assume minimal (i.e., shortest path) routing and we show the communications performed by the bandwidth-optimal recursive doubling algorithm[43](also known asRabenseifneralgorithm[35], which we describe more in detail in Sec.2.3.3), and by theSwingalgorithm (that we propose in this work). Both algorithms perform the same number of steps (we show only the first three for simplicity). We denote withnthe number of bytes of the allreduce vector. In the first step, in both algorithms, each nodersendsn/2bytes to nodeq=r\text{ XOR }1(and receivesn/2bytes from it). In the second step, however, in the recursive doubling, each nodersendsn/4bytes to nodeq=r\text{ XOR }2(two hops distant), whereas in the Swing algorithm, each node still sendsn/4bytes of data, but with the other neighbor (one hop distant). Although both algorithms transmit the same number of bytes, two different messages cross the same link in the recursive doubling. For example, two messages cross the link between nodes1and2and that between nodes5and6. As a consequence, in the worst case all nodestransmit data at most at half the bandwidthof the link between1and2, thus slowing down the entire allreduce operation. Instead, in the Swing algorithm, each node can still transmit at full bandwidth because, in this example, in the second step each link is crossed at most by one message per direction. Something similar also happens in the third step. Indeed, when using Swing at most two messages cross each link instead of the four messages crossing the link between nodes3and4in recursive doubling. It is thus clear how even if two different algorithms transmit the same number of bytes and perform the same number of steps, they might be practically characterized by different performance, depending on the network characteristics and the distance between communicating nodes. In this example, we have shown an extreme case using a 1D torus. However, similar effects can happenon any topology that does not provide full bisection bandwidth. Although some algorithms (i.e., ring[33,43]and bucket[9,38,25]) avoid this problem by having each node communicate with its neighbors only, they perform more steps (linear in the number of nodes) and are thus not well-suited for small- and medium-sized vectors. Those are the sizes that, however, are practically used in most machine learning[30]and HPC[14]workloads. Indeed, larger allreduce are split into smaller ones to overlap better computation and communication, especially more when using 3D parallelism in machine learning training[10]. For these reasons, this work makes the following contributions: [leftmargin=*] We design a new allreduce algorithm calledSwing, which performs a logarithmic number of steps and transmits the minimal number of bytes while reducing the distance between communicating nodes compared to other known algorithms designed for small- and medium-sized vectors (Sec.3and Sec.4). We evaluate Swing on different torus and torus-like topologies (e.g., HammingMesh[22]and HyperX[3,18]), by comparing it with the best state-of-the-art algorithms (Sec.5). Our evaluation shows thatSwing outperforms the other existing algorithms for allreduce on vectors ranging from 32B to 128MiB on different torus-like topologies, and regardless of their shape and size. We show that Swing outperforms the best-known algorithm up to 2.2x on square torus with4096to16384nodes and up to 3x on rectangular tori and HyperX with4096nodes.
Fluency score: 4
Fluency issues: Usage of complex jargon and extensive inline citations can disrupt fluency. Some sentences are quite long with multiple clauses, making it more challenging to read.

Clarity score: 4
Clarity issues: The text is mostly clear but assumes the reader has prior knowledge of the subject matter. Inline citations, while helpful for academic rigor, may detract from clarity for a general audience.

Grammaticality score: 4
Grammaticality issues: Some parts of the text, particularly mathematical and technical expressions, might be confusing from a grammatical standpoint to a general reader.

Readability score: 3
Readability issues: The complexity of sentence structures, heavy use of technical terms, and high frequency of inline citations decrease readability, especially for non-specialist audiences.

Coherence score: 4
Coherence issues: Overall, the narrative and logical flow are maintained, but the density of information and less-than-optimal structuring of some sentences may require readers to read sections multiple times for full coherence.
====================
	 Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search
==========
Introduction:  Approximating complex 3D shapes using primitives offers several capabilities, including shape structure analysis, shape abstraction, and efficient geometric computations. To achieve these, many recent self-supervised learning approaches[tulsiani17abstract,paschalidou19sq,sun19abstract,yang21cubseg,paschalidou21np,chen20bspnet,niu22rimnet]have successfully addressed the problem while exploring different types of primitives. Despite the recent advances, in this work, we pay attention to some desired yet underinvestigated properties of the bounding primitives of a shape and propose a novel approach aiming to achieve them. The properties are 1)full coverage— guaranteeing the boundness of the entire shape by the primitives, 2)tightness, and 3)adequate number of primitives. Attaining these three properties is particularly crucial for the downstream applications requiringefficient geometric computations, such as intersection tests[collision_survey,orientedbbox], robust transmissions[robust_transmission], ray tracing[ellipsoids_intersection], or proximity computations[oriented]. The primitives that just approximate but do not fully cover the shape can result in imprecise computation in such tasks. Also, loosely bounding primitives and too many primitives increase the computation time. Moreover, the bounding primitives satisfying these properties typically provide a better abstraction of the shape aligned with thehuman perceptionof the shape decomposition. Although neural-network-based methods have demonstrated their powerful generalizability and expressivity, they typically fail to achieve these three properties mainly due to thenon-differentiablenature of the objectives. The full coverage and tightness can be computed with the volumetric intersection or difference operations, which are not differentiable. Finding the proper number of parts is also a discrete problem that cannot be easily solved via backpropagation. As tight bounding primitives have been essential in various applications, there also has been a line of work[robust_transmission,simari2005extraction,kalaiah2005statistical]addressing the problem before the deep learning era. One notable example is the work by Lu et al.[2007bvc]that proposes to find the tight bounding boxes by iteratively exchanging point-to-primitive assignments, starting from an initialization. The major drawback of such a method is its dependency and sensitivity to theinitialization, resulting in a suboptimal output when it starts from a poor initialization. To address the problem of achieving the three properties that introduce challenges ofnon-differentiabilityfor neural networks andinitialization sensitivityfor iterative search methods, we propose a framework that performsSplitting,Merging,AndRefinementTechniques, and is thus dubbedSMART. We first find that a simple post-processing applied to a pre-segmentation, can provide appropriateover-segmentationof a 3D shape. Hence, we performhierarchical mergingto find the adequate number of parts. We introducetightness-awaremerging and stopping criteria that enable selection of the optimal number of parts the best number of parts given the trade-off between tightness and parsimony in the decomposition. While merging over the over-segments already produces promising results, the results are yet dependent on the quality of the over-segmentation (third column of Fig.1-a). To overcome the dependency to the initialization, we present the nextrefinementstep that adjusts the bounding box parameters following a sequence of predefined actions. We design a Markov Decision Process (MDP) setup with asoftreward function that allows the bounding boxes to break the full coverage in themiddleof the process. This is the key to having more flexibility in traversing various cases and obtaining better results at the end (fourth column of Fig.1-a). Lastly, we extend the refinement step to see not only a single step of actions but multiple steps in a small sequence. We utilizeMCTS[mcts]and introduce acceleration techniques (in the supplementary) to speed up the additional refinement (last column of Fig.1-a). In our experiments with ShapeNet[shapenet2015], we demonstrate that our method guaranteeing full coverage provides better tightness and reconstruction compared with the baseline methods while approximating the shapes into similar numbers of cuboids. We additionally show that the decomposition of a shape based on our bounding boxes is better aligned with the semantic parts than other methods. Furthermore, we provide the result ofSMARTon Objaverse[objaverse]and OmniObject3D[wu2023omniobject3d]to show its applicability to real data and various categories of 3D shapes. To summarize, We present a novel framework for finding a set of tight bounding boxes of a 3D shape by optimizing volumetric objectives with iterative search methods. We first propose a hierarchical merging method exploiting over-segmentation from a pre-segmentation and tightness-aware merging and stopping criteria. We also introduce a bounding box refinement process with an effective soft reward function that allows wider exploration in the action space. Finally, we present an MCTS-based efficient exploration of multi-action sequences with acceleration techniques.
Fluency score: 4
Fluency issues: Sentences with citations and numerous technical terms disrupt the flow.

Clarity score: 3
Clarity issues: Dense use of domain-specific language and multiple compound sentences make it hard for a layman to understand without context.

Grammaticality score: 5
Grammaticality issues: None detected.

Readability score: 2
Readability issues: High complexity of the content, jargon, and long sentences decrease the general readability.

Coherence score: 4
Coherence issues: The text maintains topic continuity, but there are abrupt transitions and heavy reliance on specific terminology.
====================
	 Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration
==========
Introduction:  In the past decade, deep learning techniques have demonstrated promising performance across diverse tasks, owing to the availability of large-scale annotated data[12,19,10]. However, it is time-consuming and expensive to manually collect a large-scale annotated dataset containing every possible domain for robust training. Besides, the problem of cross-domain and long-tail distributions within existing datasets have a detrimental effect on the performance and robustness of vision models, thereby impeding their generalization ability to novel categories or unseen domains. This promotes us to explore a less labor-intensive way to harvest labeled data containing multiple domains in one step for robust vision tasks. One effective strategy to improve generalization and robustness is to enlarge the scale of training data by intricate augmentations[14]. There are several GAN-based models[7,17]generating images for vision tasks, but their applicability remains constrained by their narrow focus on specific settings or small scales. Recently, AIGC models[27,28,29]have emerged as promising candidates for generating high-quality synthetic data, with the ability to address the limitations of the existing dataset. There are several early attempts at exploring synthetic data from generative models for data augmentation[13,23,3,37]. Albeit promising, early works usually produce simple scenarios or object-centric images only by global constraints (i.e., “airplane" or “a white airplane hovering over a beach and a city".), which limits downstream models’ perception of intricate scenes and fine-grained attributes. Additionally, these methods concentrate on generating images under typical scenarios (e.g., daylight, field), while neglecting less common but predictable circumstances (e.g., snow, forest, night). This limitation may impede the ability of deep learning models to generalize when deployed in real-world environments that exhibit unseen test distributions. In this paper, we present a novel approach named ChatGenImage that facilitates more controllabel data augmentation. ChatGenImage harnesses the collaborative power of the LLM and AIGC models, enabling iterative communication between them in a cost-effective and controllable manner. This automatically iterative process facilitates the generation of high-quality synthetic images depicting complex scenes and diverse domains, along with fine-grained annotations. Our fundamental intuition is that large language models have remarkable capabilities to perform new tasks in a zero-shot manner when presented with well-crafted instruction prompts[34,35,36,11]. We discover that these LLMs like ChatGPT possess the capability to autonomously navigate image editing processes. By strategically designing appropriate prompts, LLMs can leverage the inherent knowledge within the system and effectively guide the AIGC models to produce highly controllable and intricate images. While ChatGPT contains diverse world knowledge for simulating the human brain’s efficient processing, it is non-trival to elicit this knowledge from it for data augmentation with automatic labeling because ChatGPT is a pure language model that lacks the ability to visually perceive any information. We explore this issue in the context of generative data augmentation, showing that language can act as a bridge connecting LLMs and AIGC models, producing elaborate images for downstream tasks by globally controllable prompts and iteratively local editing instructions. To this end, we demonstrate three key findings. First, we find that the LLM such asChatGPTcontains a wealth of conceptual knowledge and can imagine vivid descriptions even with only one label word (e.g. Adogplaying in a lush green park, with a frisbee in its mouth. The dog should havea shiny coat of fur.)[33,6]. We further obverse that the existing AIGC models can only generate simple image with few objects and backgrounds, which are not diverse for domain generalization[20]. Thus, we establish the iterative pipeline to repair missing details and refine generated images with the help of label foundation toolkits and local editing prompts. Finally, we demonstrate our method flow to produce large amounts of high-quality synthetic data with fine-grained labels in a scalable manner for data augmentation in data scarcity scenarios.
Fluency score: 4
Fluency issues: Minor issues with sentence construction, such as "concentrate on generating images under typical scenarios" which could be smoother.

Clarity score: 3
Clarity issues: Dense terminology and long sentences make it difficult to follow in parts; references to [xx] within the text can interrupt flow.

Grammaticality score: 3
Grammaticality issues: Missing spaces after citations [7,17] and [27,28,29]; "which limits downstream models’ perception" should be "limiting"; "We further obverse" should probably be "We further observe".

Readability score: 3
Readability issues: Lengthy sentences, jargon, and a high density of information reduce readability; citations within the text can be distracting.

Coherence score: 4
Coherence issues: Overall the text is coherent but some sentences could be organized better for smoother transition between ideas, like the transition to the novel approach without much setup.
====================
	 DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback
==========
Introduction:  Reducing traffic congestion is an essential task for efficient modern urban systems. As the number of vehicles in the cities increases year by year, backward traffic coordination not only does harm to the driving experience but also aggravates air contamination with more harmful fuel emissionsZhang and Batterman (2013). Alleviating traffic congestion by efficient traffic signal control (TSC)Mirchandani and Head (2001)is one of the most practical and economical approachesTaylor (2002). Specifically, TSC aims at coordinating the traffic lights of a road network to regulate the traffic flows to minimize the average travel time of vehicles. However, the traffic data collected from a road network is usually massive yet incomprehensibleLiuet al.(2020); Wanget al.(2023). Therefore, the widely-adopted TSC strategies either fix signal routineRoesset al.(2004)or adapt the traffic signal plans in real-time according to traffic flow patternsCoolset al.(2013). To automatically mine useful information from the massive traffic data, more and more studies leverage the powerful representation capability of deep neural networksLeCunet al.(2015)to learn TSC agentsVan der Pol and Oliehoek (2016); Zhanget al.(2021); Zhenget al.(2019); Oroojlooyet al.(2020); Weiet al.(2018,2019a); D’Almeidaet al.(2021)through the advanced reinforcement learning (RL) methodsMnihet al.(2015); Schulmanet al.(2017); Lianget al.(2021). A proper reward function is required for applying RL to the TSC problem and improving the policy. Since the travel time is only feasible after a vehicle leaves the road network and fails to provide instant/dense feedback to the TSC policy in time during its journey, many previous worksVaraiya (2013); Weiet al.(2019a,b); Xuet al.(2021)draw the traditional characteristics of traffic intersections for the reward design, such as traffic pressure and queue length at the intersection. However, most of these heuristic rewards may be biased from the ultimate goal of TSC, i.e., the average travel time minimization. The introduced biases could lead the RL methods to adjust the TSC policy in an incorrect direction. Apart from the reward design, it is also critical to endow the RL agents with the capability of precisely predicting the future dynamics of the environment to make well-founded decisionsVan Hasseltet al.(2016); Fujimotoet al.(2018); Yanget al.(2016); Louet al.(2020). However, it is non-trivial for an RL agent to capture the future traffic dynamics of the intersections in the context of TSCChenet al.(2022). The future arriving vehicles of one intersection may be running in adistantintersection at present. To this end, thelocalobservations of either the intersection itself (i.e., a snapshot of vehicles at the intersection)Varaiya (2013); Weiet al.(2019a); D’Almeidaet al.(2021)or neighboring intersectionsWeiet al.(2019b); Xuet al.(2021)might be insufficient to predict the long-horizontal traffic dynamics of the intersection. Additionally, the location and traffic flow trend of an intersection also play a role in predicting the traffic dynamics of the intersection. For example, downtown intersections tend to be more crowded than suburban intersections. For example, a large number of vehicles entering an empty intersection may indicate the beginning of the rush hour. Therefore, the non-local observations and the location of the intersections, and historical traffic conditions are all important for a TSC agent to estimate future traffic situations more accurately. To address the issues discussed above, we propose a novel RL-based TSC method namedDenseLight, which improves traffic light coordination by exploiting dense information from both an unbiased reward and non-local intersection information fusion. Specifically, to provide dense and unbiased feedback for the policy improvement, we propose an equivalent substitute for travel time, i.e., the gap between the ideal traveling distance (i.e., the ideal distance a vehicle could have traveled at the full speed during its journey) and the actual traveling distance during the whole journey, namely Ideal-Factual Distance Gap (IFDG). Since the length of the factual journey of a vehicle is fixed according to its traveling lanes of the road network, minimizing IFDG is equivalent to minimizing travel time. Most importantly, IFDG can be calculated at each intersection and at each timestep. Therefore, IFDG can also provide instant feedback on the effectiveness of the control policy at each intersection. Besides an unbiased and dense reward, DenseLight also features aNon-local enhanced Traffic Signal Control(NL-TSC) agent to benefit TSC from the spatial-temporal augmented observation and the non-local fusion network architecture. Specifically, the NL-TSC agent supplements the original observation of an intersection with its location information and the previous observation so that each intersection can customize its own signal plans w.r.t. historical and spatial information. As for facilitating the TSC agents with a better awareness of the future traffic dynamics affected by other intersections, a non-local branch is proposed to enhance the local features of each intersection with the features of the non-local intersections. By learning to communicate the non-local information across non-local intersections, the NL-TSC agent can better predict the long horizontal traffic condition of each intersection and can thus make better coordination at present. Overall, our contributions are three-fold:1)we propose a novel RL-based TSC method, i.e., DenseLight, which is optimized by an unbiased and dense reward termed IFDG;2)to better model the future accumulated IFDG of each intersection, we develop the NL-TSC agent, effectively gathering spatial-temporal features of each intersection and propagating the non-local intersection information to improve the multi-agent RL policy;3)comprehensive experiments conducted on different real-world road networks and various traffic flows show that DenseLight can consistently outperform traditional and advanced RL-based baselines.
Fluency score: 3
Fluency issues: [Several in-text citation formatting errors, awkward phrasing such as "backward traffic coordination not only does harm", lengthy sentences with complex structure reducing ease of reading]

Clarity score: 2
Clarity issues: [Jargon-heavy without explanations, dense presentation of concepts, difficult to parse relationships between ideas]

Grammaticality score: 3
Grammaticality issues: [Missing spaces after citations, inconsistent capitalization in names and terms, such as "Non-local enhanced Traffic Signal Control"]

Readability score: 2
Readability issues: [Long sentences, complex constructions, use of specialized terminology without definition, abbreviations without first defining]

Coherence score: 2
Coherence issues: [Concepts are not logically ordered for easy comprehension, abrupt transitions between ideas, lack of clear introductory and concluding statements]
====================
	 Predicting Privacy Preferences for Smart Devices as Norms
==========
Introduction:  Artificial intelligence (AI) technologies are making their way into our daily lives and into our homes. We have grown accustomed to using our devices to call friends, set reminders, or check the weather.However, for these technologies to be adopted and trusted by users, they must act as users expect, and this problem is especially apparent in the area of privacy preferences. Studies show that users are deeply concerned about how their data is being collected onlinemadden2014perceptions. Interestingly, while they expect AI to act as they desire, they are unwilling to spend time setting their preferences. For example, despite users’ concerns about privacy, studies show that they ignore or blindly accept cookie bannerskretschmer2021cookieand privacy policies in social networksObar2018lie. Furthermore, in social networks, a large proportion of users do not change default privacy settingsKrishnamurthy2009leakage. This can be explained as a result of privacy fatigueChoi2018Fatigue, the sensation of loss of control and futility over protecting one’s privacy. This leads to privacy cynicism, when users do not adopt a privacy protecting behaviour even if they are concerned about their privacyHoffmann2016Cynicism. Thus, the current approach of directly asking the user when a preference is unknown but needed fails to capture the user’s true preferences. Additionally, continual questioning prevents users from achieving their objectives with the device. In response, this paper advocates for an approach that can understand user preferences with less user involvement, in turn bringing more importance to user interactions whenever such preferences are needed. A particular platform in which capturing privacy preferences is challenging and yet essential is that of smart speakers and other smart personal assistants. These devices have benefited from widespread early adoption, and it is estimated that 500 million units were installed in the last quarter of 2021Strategy2021smartspeakerinstalled. Nonetheless, the early adoption of these technologies means that they still have several vulnerabilities that pose a threat to the security and privacy of their usersEduSPASurvey. Indeed, there have already been cases reported in which smart assistants have not functioned as expected; for example, a smart speaker recorded and sent a private conversation without the user’s consentGuardianAlexa. These situations hinder user trust in the technology and can ultimately lead users to limit the functionalities of the devices used, or even to adopting coping mechanismsAbdi2019More. This paper describes an alternative approach that addresses the issues outlined above. The critical observation underpinning our approach is that smart devices are just one part of a larger ecosystem (e.g. seeEduSPASurveyfor a description of the ecosystem of smart speakers), and they interact and share data with agents like services, apps, and other devices. For example, a smart watch might send a voice recording to a smart speaker, or might share the wearer’s heart rate with a health app. In this respect, we can understand this ecosystem as a multi-agent system in which the use of norms can help to regulate these interactions, implementing privacy preferences. Norms can effectively summarise complex privacy preferences into simple sets of regulations, as shown by Abdi et al.NouraPrivacyNorms2021, who gathered over 800 privacy preferences on data transmissions, yet produced just 17 norms. Moreover, although here we assume no knowledge of the domain, if such knowledge is available there exist techniques to generalise norms (seeMoralesAAMAS2013;MoralesAAMAS2014for an example) or find and resolve inconsistencies among themVasconcelosKN09. Furthermore, norms are also used by people, and are naturally understood by them, representing a good base upon which to construct explanations. This can be used not only to generate explanations for a user if something unexpected happens, but also to tailor interactions with a user to validate predicted norms. Norms are regarded as expected patterns of behaviourWooldridgeIntrodMAS, causing agents (each component in the smart device ecosystem) to coordinate better and function more efficiently. As an example with smart devices, imagine a service knows in advance the privacy norms of a user with regard to each component of the ecosystem. If this service needs to interact with other components, it can use the user’s norms to adapt its behaviour to avoid violating norms or to avoid performing unregulated transmissions of information, which might require consent. As informally outlined inSerramiaPRIMA22Collaborative, we canexploit the large user bases of smart devices to use knowledge of previously specified privacy preferences to infer new preferences or to assist users in specifying their preferences. In particular, we aim to exploit similarities between users to make privacy preference predictions using collaborative filteringSu2009Collaborative.Effectively, we see the smart device ecosystem as a multi-layered multi-agent system. The lower level represents the multi-agent system associated with a single user (that is, the user’s device, and the other devices, skills, and services that can be accessed from it). The higher level is that of the multi-agent system composed of all the users. Our approach is centred on the norm creation stage in the lower level multi-agent systems related to each user. Therefore, each device user has its own associated set of norms, and all agents within its lower level multi-agent system, be they devices, skills, or other services, are informed and affected by the norms whenever they want access to the user’s personal data.While many researchers have studied different approaches to constructing norm systems, like norm synthesisMoralesAAMAS2013;morales2015compactor norm emergenceshoham1997emergence;SavarimuthuAAMAS07;sugawara2011emergence, we are not aware of any similar approach like the collaborative filtering presented here. In taking this approach, we make the following contributions. Formalisation of the problem of predicting norms to ensure that computational behaviour aligns with user preferences. This is divided into two subproblems, namely preference approximation (predicting unknown user preferences) and norm inference (inferring norms from predicted preferences). Formalisation of preference prediction functions. We provide a specific example of this type of function based on the preferences of similar users. Inference of norms from the predicted preferences, and specification of different methods to do so based on the confidence of the prediction or other variables. The paper is structured as follows: Section2formalises the core problems we aim to address in the paper. In Section3we detail the process of predicting preferences. We then use these predictions to infer norms in Section4. Section5is dedicated to validate our findings. In Section6we discuss related work. Finally, in Section7we discuss conclusions and future work.
Fluency score: 3
Fluency issues: [“check the weather.However,” “collected onlinemadden2014perceptions,” “bannerskretschmer2021cookieand,” “networksObar2018lie,” “settingsKrishnamurthy2009leakage,” “fatigueChoi2018Fatigue,” “privacyHoffmann2016Cynicism,” “of 2021Strategy2021smartspeakerinstalled,” “usersEduSPASurvey,” “consentGuardianAlexa,” “mechanismsAbdi2019More,” “seeEduSPASurveyfor,” “pressure prior, she had.” “watch might,” “like servicesMoralesAAMAS2013;MoralesAAMAS2014for,” “themVasconcelosKN09,” “behaviourWooldridgeIntrodMAS,” “outlined inSerramiaPRIMA22Collaborative,” “filteringSu2009Collaborative.Effectively,” “level multi-agent,” “privacy preferences to infer,” “systemsMoralesAAMAS2013;morales2015compactor,” “emergenceshoham1997emergence;SavarimuthuAAMAS07;sugawara2011emergence,” “contributions. Formalisation,” “preferences). Formalisation,” “preference prediction functions,” “Section2formalises,” “Section3we,” “Section4,” “Section5is,” “Section6we,” “Section7we”.]

Clarity score: 3
Clarity issues: [Citation formatting issues lead to unclear references (e.g., “onlinemadden2014perceptions,” “bannerskretschmer2021cookieand”). Complex sentence structures obscure meaning (“This can be explained as a result of privacy fatigueChoi2018Fatigue,” “a multi-layered multi-agent system.”).]

Grammaticality score: 4
Grammaticality issues: [Missing spaces after periods in citations.]

Readability score: 3
Readability issues: [The text is dense with jargon and lacks transition sentences, which may confuse readers outside the field (“norms,” “multi-agent system,” “collaborative filtering”). Frequent in-text citations disrupt flow.]

Coherence score: 3
Coherence issues: [Citations are not properly spaced or formatted, impeding flow and coherence. Lack of clear transitions between the introduction of concepts and their development (e.g., abrupt introduction of “privacy fatigue”).]
====================
	 Space reduction techniques for the $3$-wise Kemeny problem
==========
Introduction:  In this article, anelectionis a finite collectionC=\{c_{1},\dots,c_{n}\}of alternatives together with avoting profileconsisting of a finite number of (not necessarily distinct) votes. Arankingor avoteis simply a complete and strict total ordering\pi\colon c_{\pi(1)}>c_{\pi(2)}>\dots>c_{\pi(n)}which we identify with a permutation of\{1,2,\dots,n\}also denoted by\pi. Here,x>ymeans that the alternativexis ranked before the alternativey. The space of all rankings, for fixedn, can be equipped with several natural distances, for example, the Kendall-tau distance which counts the number of order disagreements between pairs of elements in two permutations, namely, the bubble-sort distance between two permutations, or more generally thek-wise Kendall-tau distance[14](see Equation (2.1)). The important Kemeny problem (cf.[15],[16],[26]) consists of finding the set ofk-wise medians of a given election, i.e., permutations whose total distance to the voting profile is minimized with respect to thek-wise Kendall-tau distance. In other words, a median is a ranking that agrees the most with the voting profile. By taking into consideration not only pairwise comparisons but also the discordance between the winners of subsets of three alternatives, the3-wise Kemeny voting scheme seems to be more resistant to coalitional manipulation than the classical2-wise Kemeny rule: it is much more difficult for an alternative to win an election or even to simply win another specific alternative in an election under the3-wise Kemeny voting scheme. In fact, most of the best-known space reduction results for Kemeny’s rule fail in the3-wise setting (see[21, Table 1]), including the powerful Major Order Theorems discovered in[19](see Example5.1) and the Condorcet criterion. It was shown in[21]that even the2/3majority in every duel is not enough to guarantee that an alternative will win an election according to the3-wise Kemeny voting scheme. This phenomenon is in stark contrast to the Condorcet criterion where a Condorcet winner, namely an alternative which is preferred by more voters than any others, must be the unique winner of the election. Nevertheless, we know that an alternative obtaining a3/4majority in every duel must be the unique winner in the3-wise Kemeny voting scheme[21]. In many situations, the3-wise Kemeny rule is also more suitable than Kemeny’s rule since it puts more weight on alternatives which are more frequently ranked in top positions in the votes. Indeed, Kemeny’s rule puts equal weight on the head-to-head competition of two alternativesx,ywhen in a particular vote,xis the winner followed byyand when in another vote,xandyoccupy the last two position in that order. However, it is reasonable to assume that typical voters only pay attention to a shortlist of their favorite alternatives and put a rather random order for the rest of the alternatives. Such undesirable behavior creates noises that can alter the Kemeny median ranking while the problem can be solved effectively using the3-wise Kemeny voting scheme in specific instances (see Example5.1and AppendixB). The above limitation of the Kemeny rule leads to the notion of weighted Kendall tau distances introduced by Kumar and Vassilvitskii[17]as well as the notion of set-wise Kemeny distance of Gilbert et. al.[14]. Motivated by the above potential and interesting features of the3-wise Kemeny rule as well as the NP-hardness of the various Kemeny problems (see[arrow],[12],[14]), our main goal is to formulate new quantitative results concerning the majority rules in3-wise Kemeny voting schemes associated with the3-wise Kendall-tau distance introduced recently in[14], which provide much more refined space reductions to the Kemeny problem in comparison to existing techniques in the literature. Our first result (cf. Theorem3.1) shows that the fundamental3/4-majority rule of Betzler et al.[6]for the classical Kemeny rule is also applicable for all elections with no more than5alternatives with respect to the3-wise Kemeny scheme. However, the3/4-majority rule fails in general as soon as there are at least6alternatives. Note that without restriction on the number of alternatives, the5/6-majority rule obtained in[21]serves as the3-wise counterpart of the3/4-majority rule. The second and central result of the paper is the Major Order Theorem for the3-wise voting scheme, denoted 3MOT (Theorem5.4), and its improved version, denoted Iterated 3MOT (Theorem6.2), which, to the limit of our knowledge, are the most efficient space reduction techniques for the3-wise Kemeny rule in the literature. In essence, our Major Order Theorems show that if the preference for an alternativexover another alternativeyin an election is strong enough, as measured quantitatively not only in the head-to-head competition but also when taking into account the interactions withoneortwomore other alternatives, thenxmust be ranked beforeyinall3-wise medians of the election. The corresponding algorithms not only function efficiently in polynomial timeO(n^{4}m), wherenis the number of alternatives andmis size of the voting profile, but also drastically reduce the search space of3-wise medians. To get an idea on the efficiency and interests of our results, let0\leq p<1be the proportion of pairs of alternatives solved by 3MOT or Iterated 3MOT out of the total ofn(n-1)/2pairs. Then the original search space consisting of alln!possible rankings would be reduced by a factor (reduction rate) at least equal to (cf.[linear-extension-2018, Lemma 2, Lemma 4], see also Table2) On real-world data, especially for political elections and competitions where there exists a high similarity between the votes, our algorithms prove to be particularly useful aspusually ranges from0.6to0.9after only a few iterations of Iterated 3MOT (see Table2and also AppendixC.1for some concrete examples). The performance is also very encouraging even on the hard case of uniformly generated data where, for example whenm=15, the3-wise Major Order Theorems can determine the relative rankings of approximately47\%pairs of alternatives on average whenn=10and approximately31\%whenn=15. Our results not only extend and improve the important2-wise Major Order Theorem of[19](see Section2.4and Theorem7.1) but also provide a unified approach and technique which should pave the way for the research of more refined algorithms and quantitative properties ofk-wise Kemeny voting schemes fork\geq 2. It is worth comparing our method to the space reduction method based on a3-wise majority digraph introduced in[14, Theorem 3]whose vertices are the alternatives. While we can obtain, under some assumptions on the3-wise digraph of an election, a set of rankings which containssome3-wise median using[14, Theorem 3], our3-wise Major Order Theorems provide, for all pairs of alternatives(x,y), easy-to-compute and mild sufficient conditions so thatx>yinall3-wise medians. In fact, by relaxing the conditions in our Major Order Theorems, we can determine more relative orderings of a pair of alternatives insome3-wise median (see Theorem8.1). Another major difference is that[14, Theorem 3]only considers the strength of the preference forxoveryin the presence ofoneother alternativez\neq x,ywhile the3-wise Major Order Theorems quantify the preference forxoverynot only in the head-to-head competition but even when taking into accountoneortwomore alternatives, which should provide a more refined space reduction method (see Example6.4). In any case, the constraints on all3-wise medians found by our 3-wise Major Order Theorems should greatly accelerate other complementary space reduction methods and vice versa, nontrivial constraints obtained by other methods can serve as the inputs to boost our algorithms, especially Iterated 3MOT. Table1below summarizes our results and some well-known space reduction criteria for the classical and3-wise Kemeny voting schemes. 3-wise Kemeny rule Extended Always theorem (Pareto efficiency, unanimity) Phung & Hamel[21, Theorem 3] Phung & Hamel[21](TheoremC.4) 3/4-majority rule (Section2.3) Betzler et al.[6] Valid only for elections of5alternatives or less, Theorem3.1 Extendeds-majority rule Phung & Hamel[21, Section 4] Valid ifs\geq 5/6[21, Section 8] Major Order Theorems Milosz & Hamel[19](Section2.4) Improved Iterated MOT (Theorem7.1) Not valid, see Example5.1 3-wise Major Order Theorems Theorems5.4,6.2,8.1. To illustrate the utility of our obtained algorithms, we performed several simulations and tests on real-world and uniform data. Table3and Table4in AppendixCrecord the approximate average proportion of pairs with relative order solved by the3-wise Extended Always Theorem obtained in[21](see TheoremC.4) and the 3-wise Major Order Theorems over 100 000 uniformly generated instances. Several concrete examples with real-world data (elections with a dozen and up to 250 alternatives) taken from PREFLIB[18]are described in AppendixC.1(see also Table2). A direct implementation shows that for voting profiles withn,m\leq 40, the list of all pairs of alternatives with relative order in all3-wise medians determined by 3MOT, resp. Iterated 3MOT, can be obtained in approximately less than 2.5 seconds, resp. 1 minute, with an M1 MacBook Air 16GB RAM. More optimized implementation can definitely improve the running time of the algorithms. Finally, we explain how to apply our results and the set-wise Kemeny rule to deal with incomplete votes in Section9and propose the usage of the proportion of pairs whose relative order are determined by the3-wise Major Order Theorems as a meaningful measure of the consensus level of the electorates (see AppendixA).
Fluency score: 2
Fluency issues: Extensive use of mathematical notation, abbreviations, and references throughout the text without proper explanation hinders the fluency for the general reader.

Clarity score: 2
Clarity issues: Frequent transitions between technical descriptions and high-level concepts, lack of clear explanations for specialized terms, and dense information presentation affect clarity.

Grammaticality score: 5
Grammaticality issues: None detected.

Readability score: 2
Readability issues: Complex sentence structure, heavy use of jargon, and dense paragraphs without clear breaks make the text difficult to read.

Coherence score: 2
Coherence issues: The text jumps between discussing theoretical concepts, mathematical theorems, examples, and applications without clear transitions or organization, undermining overall coherence.
====================
	 A Systems-Theoretical Formalization of Closed Systems
==========
Introduction:  There has long been a call for a theory of Systems Engineering (SE) within the SE community with the aim of establishing SE as a standalone engineering field capable of addressing modern engineering problems[hazelrigg2022toward][kasser2011unifying]. However, there is an existing gap in concrete formalism and distinction for some fundamental concepts within the field that has led to ambiguity in some SE practices[salado2021systems]. While such formalism might not have been necessary in the past, the emergence of new kinds of complex systems such as Artificial Intelligence (AI)-enabled systems has challenged traditional SE practices[llinas2021motivations,tolk2011towards,mcdermott2021artificial,llinas2021systems,shadab2021shifting,smith2017cognitive]. In our previous work, we identified potential gaps in the current SE foundations to address the unique nature of AI-enabled systems[shadab2022closed]. We argued that intelligence is a relational property that can be characterized and engineered as a relation between the system and its context with both learning and intelligence properties embodied in the context regardless of the nature of the relations between them[shadab2022closed]. In this situation, intelligence is no longer relegated to a component or the physical boundary of the system. Therefore, we posited that owing to this high coupling between AI-enabled systems and their environment, utilizing the concept of closure in SE is a potential path forward to build general engineered intelligence[shadab2022closed]. We proposed that closed SE practices could be employed to realize the closed notion of this relational property between intelligent systems and their context. We further examined the possibility of employing closed systems precepts in an engineering framework in our later paper[cody2022core], concluding a lack of concrete definitions and formalism in the theory and practice of SE presents a barrier to applying closed system precept in engineering applications. Currently, most of the theoretical foundations in both systems theory and the theory of SE are bounded to the open systems precepts (i.e., inputs-outputs relations)[dag2000introduction]. Although the concept of closed systems is being utilized in limited applications in SE, there is little to no theoretical basis for these practices, making SE activities based on closed system precepts prone to interpretation and over-abstraction. As we have identified at least one domain that can benefit from closed systems precepts (AI-enabled systems), the need for clear definitions and formalism becomes increasingly important in the field of SE. This paper revisits the concept of closure in SE, aiming to formalize, define, and evaluate this concept as the first step towards employing closed systems precepts for intelligent systems. As mentioned earlier, closure has been vaguely applied in SE with limited underlying formal foundations[hutchison2018framework,di2018closed]. Various types of closure have been introduced in systems theory literature, including functional closure, organizational closure, operational closure, and informational closure, among others[zeleny1981autopoiesis,varela1974autopoiesis,bednarz1988autopoiesis,bertschinger2006information,bertschinger2008autonomy,mora2012closure]. As a starting point, closure can be understood as a property of a system that makes the system closed, and a closed system is defined as one that does not exchange energy, information, or matter through its boundaries[bertalanffy1968bertalanffy]. (This concept will be revisited in detail later in the paper.) However, there is little to no formal framework to describe the relationships and differences between each type of closure, and many of the closure types lack formal systems-theoretic definitions that distinguish them from the other types of closure; in fact, on many occasions, these terms are used interchangeably, which can cause confusion in the application of each type of closure[goodenough2002concept][di2018closed]. In this paper, we develop formal systems-theoretic definitions for two types of closure, functional and informational closure, in systems and compare them in terms of system’s characteristics and the relations between systems and their environment. Among all types of closure, functional and informational closure were selected as we believe they were more relevant to the systems engineering practices. We utilize a mathematical definition of functional dependency, information theory, and the systems-theoretic foundations for open and closed systems to produce formalism for functional and informational closure. Then, we determine the conditions and constraints to meaningfully use each of the two types of closure in systems. Our aim is to elaborate on the relations between these types of closure to determine their applications at different levels of abstractions for systems. Throughout this paper, we will use the termsclosureandclosednessinterchangeably.
Fluency score: 4
Fluency issues: Repetitive use of "SE," "closure," "closed systems," and some sentences are quite long which affect fluency.

Clarity score: 3
Clarity issues: Overuse of jargon and acronyms, multiple references in square brackets interrupt the flow, making the text harder to follow.

Grammaticality score: 4
Grammaticality issues: Primarily the brackets disrupt the grammatical flow of the text, but otherwise, it's mostly grammatical.

Readability score: 3
Readability issues: Lengthy sentences, heavy use of specialized terms, and numerous in-text citations make the text difficult to parse for general audience.

Coherence score: 4
Coherence issues: The text jumps between concepts rather quickly; it assumes familiarity with the subject matter, which can confuse readers new to the topic.
====================
	 Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems
==========
Introduction:  Evolutionary algorithms have been widely used to tackle multi-objective optimization problems in many areas such as robotics, pattern recognition, data mining, bioinformatics, scheduling and planning, and neural network training[Zhou2011]. Their population-based search operators make them a natural choice for simultaneously handling several possibly conflicting objectives. Many generic evolutionary multi-objective frameworks have been developed to supply basic implementations for any problem, and to provide templates that can be fine-tuned for specific applications (we refer to[Wang2023]for an overview of common approaches). Such features, along with their strong empirical performances in challenging applications, have led them to becoming one of the most attractive topics to researchers and practitioners alike. Among evolutionary multi-objective algorithms (EMOs), arguably the most exemplary are dominance-based approaches such as GSEMO and NSGA variants, with the former often being considered a baseline. Another popular technique for multi-objective optimization is to decompose the multiple objectives into a single-objective subproblem. The MOEA/D algorithm is a state-of-the-art application of this technique in evolutionary computation[Trivedi2016,Xu2020]. Despite the prevalence of EMOs on practical applications, rigorous analyses of their runtime behavior on meaningful problems are scarce. Nevertheless, these kinds of analyses are critical for (1) providing performance guarantees and guidelines to practitioners who use and develop these techniques in the field, and (2) promoting the explainability of heuristic search and optimization techniques by clarifying their working principles through a careful mathematical analysis. Run-time analyses on the performance of evolutionary algorithms have been provided for simple algorithms such as GSEMO in both artificial benchmark problems[Bian2018,Doerr2021]and others such as bi-objective minimum spanning tree[Neumann20071,Roostapour2020]and constrained submodular optimization[NIPS2015_b4d168b4,NIPS2017_d7a84628,Do2020,Qian2020]. In recent years, theoretical analyses of state of the art approaches such as NSGA-II and MOEA/D have been conducted[Huang2019,Huang2021,Huang20211,Zheng2022,Doerr2023,cerf2023proven]. Most of these run-time results are on artificial benchmark problems, and the one for NSGA-II on bi-objective minimum spanning tree proves promising. In this paper, we present for the first time rigorous results on MOEA/D for a classical multi-objective optimization problem, namely the multi-objective minimum weight base problem. This problem, falling under the matroid optimization category, is a significant generalization of the previously studied bi-objective minimum spanning tree problem. In this work, we focus on approximating the non-dominated front, as determining whether the front is reached is EXPSPACE. In particular, we show that MOEA/D obtains a factor2-approximation for two objectives in expected polynomial time. Previous analyses for the special case of graphic matroid (i.e. spanning forests) were only able to show a pseudo-polynomial run-time for GSEMO to obtain this approximation[Neumann20071]. We further extend the analyses by deriving a fixed-parameter polynomial expected run-time in instances withk>2objectives to reach ak-approximation. Instrumental to our analyses is a deeper understanding of the problem, and as such, we formally examine certain properties of the multi-objective minimum weight base problem. We first prove a tight approximation guarantee from computing the convex hull of the non-dominated front, extending the known guarantee for two objectives[Neumann20071]. With this in mind, we explore insight regarding this convex hull, including its vertex complexity and the structural relation among solutions whose weights constitute said convex hull. In addition, we briefly formulate an efficient deterministic approach to enumerate extreme points. These findings may be of interest in areas beyond runtime analysis.
Fluency score: 4
Fluency issues: "[Zhou2011]", "[Wang2023]", "[Trivedi2016,Xu2020]", "[Bian2018,Doerr2021]", "[Neumann20071,Roostapour2020]", "[NIPS2015_b4d168b4,NIPS2017_d7a84628,Do2020,Qian2020]", "[Huang2019,Huang2021,Huang20211,Zheng2022,Doerr2023,cerf2023proven]", "In particular, we show that MOEA/D obtains a factor2-approximation", "instances withk>2objectives" 

Clarity score: 4
Clarity issues: "run-time analyses", "a factor2-approximation", "instances withk>2objectives", "a pseudo-polynomial run-time"

Grammaticality score: 4
Grammaticality issues: "instances withk>2objectives"

Readability score: 3
Readability issues: Frequent insertion of citations with brackets; complex sentences with technical language; missing spaces in "factor2-approximation", "withk>2objectives".

Coherence score: 4
Coherence issues: The text moves from the general discussion about evolutionary algorithms to specifics without clear transitions; multiple references without explanation can be confusing; the presence of jargon and acronyms without definition may hinder coherence for a broader audience.
====================
	 A Strategy for Implementing description Temporal Dynamic Algorithms in Dynamic Knowledge Graphs by SPIN
==========
Introduction:  Many of the knowledge modeling and representation in our everyday lives are in dynamic knowledge structures. For example, in a web service architecture that contains a requester, a broker, and a provider, we are faced with step-by-step actions, and we can illustrate and represent the web service compositions in different case studies based on semantic webs[83]. There are many ways that dynamic-temporal knowledge can be described, modeled, reasoned about, and implemented[79]. In a related work section, we will briefly review works about action formalism in formal logics, computer science, and artificial intelligence. Change and time formalism has a very long history from Aristotle, Diodorus, and early Islamic logicians such as Avicenna, up to modern logicians. Change formalism, mostly studies in the field of “temporal logics”. In temporal logic, using one of the quantified or modal approaches, we formalize time in either point-based or interval-based time. The hybridization of action and change formalism, especially in dynamic knowledge base projects, is done in different methods in many of the formal logic and AI fields. Some projects, introduce dynamic logics and explicitly formalize the notion of actions and states, whereas the notion of time and change are implicit in them[13]. Other works, introduce temporal logic and formalize the notion of time and change, while the notion of action and dynamic behavior of the knowledge base is implicit in them. In this paper, we introduce an individual-based algorithm for description dynamic logics that implicitly contains the time notion. Unlike ontology versioning, we don’t track the ontology changes manually, but the actions automatically cause a new version of the knowledge base to appear. Therefore, we do not only have the ontology versions. The ”changes”, semantically, with the help of actions within the ontology itself, cause communication between two different states. Ontology versioning ignores the parts of an ontology that are constant and haven’t changed. In this paper, the entire new knowledge base produced by changes in actions will be examined by projection, and all realizability, executability, and planning services will be provided using TBox and ABox reasoning.
Fluency score: 3
Fluency issues: [The use of "are in" instead of "exist within" or "constitute"; "in temporal logic, using one of the quantified or modal approaches," unnecessarily complicated wording; "Change formalism, mostly studies in" awkward phrasing]

Clarity score: 3
Clarity issues: [Complex sentences with multiple clauses, e.g., "In a related work section, we will briefly review..."; Overuse of domain-specific jargon without explanation; Multiple concepts introduced without clear transitions]

Grammaticality score: 4
Grammaticality issues: [Missing articles "the" before "ontology versioning" and "the knowledge base"; Plural "studies" does not agree with singular "Change formalism"]

Readability score: 2
Readability issues: [Dense academic language not accessible to broader audience; Long sentences with complex structure impair smooth reading; Excessive and undefined technical terms]

Coherence score: 3
Coherence issues: [Several ideas introduced without clear logical progression; Transitions between topics are abrupt or missing; "The ”changes”, semantically,...states." is a convoluted sentence]
====================
	 Consistent Text Categorization using Data Augmentation in e-Commerce
==========
Introduction:  In the last two decades, widespread use of e-commerce platforms such as Amazon and eBay has contributed to a substantial growth in online retail. Such platforms rely on both explicit and implicit product features in order to deliver a satisfying user experience. There, the inferred product category is typically a crucial signal for many application such as browsing, search and recommendation. We focus on improving an existing product categorization model, we refer to as ’the categorizer’, that is employed by our company for fast categorization of billions of items on a daily basis. It classifies e-commerce items, such as products or deals, based on a predefined hierarchy of categories, namely GPT (Google Product Taxonomy). Given a product title, the categorizer assigns the most appropriate label in the taxonomy. The model itself is highly scalable and effective, so it is well-suited for settings with large and rapidly growing item catalogs. In our company, the categorizer is used as a standalone component in various e-commerce related services, such as recommendation, search, and ad ranking. A recent examination of the categorizer’s output revealed inconsistencies in the labeling of similar items. It was evident that in some cases small variations in product titles, such as those relating to colors or measurements, significantly affect the categorizer’s output. This inconsistency negatively impacts search and recommendation algorithms that rely on the inferred category, leading to a poor user experience. The concept of consistency in NLP tasks has been studied in various research works, including robustness to paraphrasing(Elazaret al.,2021)and robustness to adversarial attacks(Jinet al.,2020; Wanget al.,2020). Other works relate consistency issues with the misuse of spurious features during the learning phase(Arjovskyet al.,2019; Veitchet al.,2021; Wanget al.,2021). When examining the performance of the categorizer in terms of accuracy alone, the inconsistency issue may be overlooked. But, since many recommendation pipelines depend on the output of the product categorizer, an inconsistent model can have severe implications on the user experience. In most cases, the differences include returning the parent category or a sibling category, rather than a completely different category path. To tackle this inconsistency problem, we use differentdata augmentationtechniques and enrich the training data with item versioning, leading to a more consistent model. Data augmentation for improving various NLP tasks has been widely studied and surveyed(Shortenet al.,2021), and particularly in the context of consistency(Xieet al.,2020). Generating such data, both manually(Kaushiket al.,2019)and automatically(Rizoset al.,2019; Bariet al.,2020; Kumaret al.,2020), has shown to contribute to the robustness of learnt models in different settings. We chose to use data augmentation, without changing the current architecture of the already-in-use product categorizer for two main reasons. First, for scalability reasons, any change in the architecture might degrade the model’s ability to infer the categories of billions of items per day. Second, maintaining the current model architecture expedites the productization process and requires only minimal engineering effort. This work defines a new framework,Consistent Semi-Supervised Learning (Consistent-SSL), for consistent text categorization in the context of e-commerce (Section2). We use an unlabeled clustered dataset as a source of legit item versioning. The dataset is derived from product catalogs, and includes clusters of different versions of items. We present two different methods to utilize this unlabeled clustered data: a self-training method and a generative approach (Section3). We describe the datasets and the experimental framework we use for the evaluation of the proposed methods (Section4). Finally, we detail results, showing an improvement in the consistency rate of 4-10% above the baseline model, and discuss the advantages and weaknesses of each method (Section5).
Fluency score: 4
Fluency issues: Slight awkwardness in "We focus on improving an existing product categorization model, we refer to as ’the categorizer’," and "To tackle this inconsistency problem, we use differentdata augmentationtechniques."

Clarity score: 4
Clarity issues: "differentdata augmentationtechniques" (should be "different data augmentation techniques") and some potentially confusing references such as "(Section2)".

Grammaticality score: 4
Grammaticality issues: Punctuation issue in "robustness to paraphrasing(Elazaret al.,2021)" and similar references; spacing issues as in "differentdata augmentationtechniques."

Readability score: 3
Readability issues: Dense academic language and frequent parenthetical citations may hinder readability for a general audience.

Coherence score: 4
Coherence issues: Occasional abrupt transitions such as switching from discussing the categorizer’s issues to the overview of related works without clear linking.
====================
	 FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis
==========
Introduction:  Text-to-SQL aims to transform natural language questions into executable SQL queries, which enables low-code operations for relational databases. It can facilitate the data access procedure for non-professional database users who are not familiar with SQL and has gained much attention in various areas, especially in financial analysis. While financial professionals (e.g., investment advisors) need to query relevant databases frequently, they are usually not well-skilled in SQL programming. Therefore, Text-to-SQL is significantly important for financial analysis and has gained much attention. However, there is no Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases used in financial analysis. To address these issues, we construct a practical Text-to-SQL dataset for financial analysis based on the intelligent investment assistant product of Hundsun Technologies Inc., which facilitates more than 50 financial institutions (including Alipay, China Merchants Bank, and so on) and serves millions of personal users. This dataset, dubbed BULL, contains three databases corresponding to fund, stock, and macro economy respectively. Besides, in this dataset, there are 4,966 natural language question-SQL pairs annotated by financial professionals, data scientists, and software engineers from Hundsun Technologies Inc. Furthermore, BULL has both English and Chinese versions. Compared with the widely used Text-to-SQL benchmark datasets (e.g., Spider(Yuet al.,2018b)and BIRD(Liet al.,2023c)), BULL has much more tables for each database and much more columns for each table, illustrating as in Table1. Furthermore, table and column names in BULL are often expressed with abbreviations or vague representations. These characters require financial Text-to-SQL models to support large input context length and have strong context understanding ability. Fortunately, Large Language Models (LLMs)-based Text-to-SQL can satisfy these requirements, and several LLMs-based Text-to-SQL methods have been proposed recently. However, existing state-of-the-art LLMs-based Text-to-SQL methods typically depend on OpenAI’s APIs, such as GPT-3.5-turbo or GPT-4, which are expensive and have risks of information leakage. Therefore, these methods cannot be used in the financial applications where the information privacy is critically important. To avoid information leakage, a feasible way is to adopt open-source LLMs (e.g., LLaMA(Touvronet al.,2023)and Baichuan(Yanget al.,2023)) and train them in private domains. However, it faces three challenges: (1) Schema linking dilemma and data scarcity. It is difficult to establish connections between question and schema items for financial databases usually have a large number of columns and tables. Furthermore, due to the labeling cost, the number and diversity of labeled question-SQL pairs are limited. These issues obstruct the construction of concise and diversified prompts, which hinders the model’s performance; (2) Resource-consuming fine-tuning and cross-database generalization difficulties. Fine-tuning LLMs on downstream tasks demands several days of computation across multiple GPUs. The substantial cost associated with model updates and iterations poses a considerable challenge. Additionally, transferring the model to a new database incurs significant costs, which impedes the cross-database transfer. (3) Inconsistent output. Due to the inherent randomness and the decoder strategy of sampling, LLMs often generate inconsistent outputs, leading to syntactically incorrect and invalid SQL queries. To tackle these challenges, this paper proposes a model-agnostic LLMs-based Finacial Text-to-SQL model training and inference framework, dubbed FinSQL. It can be used to develop Text-to-SQL models based on any open-source LLMs. Figure1demonstrates the overall overview of FinSQL, which consists of three key components: prompt construction, parameter-efficient fine-tuning, and output calibration, corresponding to the above challenges correspondingly. Specifically, prompt construction consists of a parallel schema linking method and a hybrid data augmentation method, which help to construct more concise and diverse prompts and enhance the model’s performance from the input side. The parameter-efficient fine-tuning component adopts Low-Rank Adaptation (LoRA) to fine-tune a very small percentage of parameters (¡1%) to obtain weight plugins for different business scenarios and manages these plugins through a plugin hub. Based on this plugin hub, the database-specific Text-to-SQL models can achieve efficiently few-shot cross-database transfer. In output calibration, SQL post-processing is performed to enhance the correctness of the generated SQL. These three components contribute to FinSQL’s superior performance. Our contributions can be summarized as follows: We propose BULL, a practical benchmark dataset for financial Text-to-SQL. We propose FinSQL, a model-agnostic LLMs-based Text-to-SQL framework for financial analysis. Extensive experimental results on BULL demonstrate that FinSQL is model-agnostic and able to achieve the state-of-the-art performance; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer.
Fluency score: 4
Fluency issues: [Repetition of 'has gained much attention' (second sentence and fourth sentence); 'illustrating as in Table1' phrase is awkward; 'Figure1demonstrates' missing space]

Clarity score: 3
Clarity issues: [Overuse of jargon and acronyms without proper introduction (LLMs, LoRA) might confuse readers; 'These three components contribute to FinSQL’s superior performance' lacks specificity on how they contribute]

Grammaticality score: 3
Grammaticality issues: [Missing spaces after periods and commas in some references (e.g., 'Spider(Yuet al.,2018b)and BIRD(Liet al.,2023c)', 'LLaMA(Touvronet al.,2023)and Baichuan(Yanget al.,2023)'); 'Large Language Models (LLMs)-based Text-to-SQL' should be 'Large Language Models (LLMs)-based Text-to-SQL methods']

Readability score: 3
Readability issues: [Long sentences with complex structures make the text dense and difficult to read; usage of terms without enough context]

Coherence score: 3
Coherence issues: [Some concepts are introduced abruptly, making the logic hard to follow (e.g., introduction of the BULL dataset and the challenges); lack of transitions between topics]
====================
	 Semantic Tokenizer for Enhanced Natural Language Processing
==========
Introduction:  NLP models have two primary components—a deep neural network and a vocabulary of embeddings. Recent improvements in NLP model performance have focused on improving deep networks and increasing model sizes. Interestingly, little attention has been paid to optimizing vocabularies. An analysis of recent models[9], plotted in Figure1,111Credit for figure to Huggingface’s DistilBERT: https://research.aimultiple.com/gpt/shows that model sizes (i.e., number of parameters) have increased by 15,000% over the last few years (excluding 175B parameter GPT-3[1]or 1T parameters of GPT4). The increase in model size significantly increases training costs. Recent publications show that a single training run for GPT-3 could cost $12M.222https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/Even BERT-Large[3]training costs reach tens of thousands of dollars. Furthermore, increased model size place additional computational burden during model execution. These costs can have a detrimental effect on NLP innovation. At the same time, the size of vocabularies has increased only about 100% and the size of embedding vectors has increased about 200%. Hence, over the same period, the fraction of NLP parameters representing the vocabulary has shrunk from 21% in BERT-base[3]to 0.3% in GPT-3[1]. An average person uses 42,000 root words and hundreds of thousands of wordforms[2]. Technical terminology and jargon add tens of thousands of additional words to the vocabulary. Even some of the largest vocabularies currently in use, DeBERTa[4], have only 128,000 tokens. Hence, NLP vocabularies need to include subwords that can be combined to form multiple words[11]. Words that can not be represented using a single token are segmented into an initial subword followed by as many intermediate subwords as required. NLP models use a tokenizer to convert strings of characters into a sequence of lexical tokens. Tokenizers also construct the vocabulary of lexical tokens. We present a novel tokenizer that improves NLP performance by improving subword formation and embedding quality throughsemantic tokenization.
Fluency score: 3
Fluency issues: [Parenthetical referencing mid-sentence disrupts flow (e.g., "...Figure1,111Credit for figure to Huggingface’s..."), non-standard use of URLs in the text, inconsistent punctuation and formatting around references and numbers.]

Clarity score: 3
Clarity issues: [Overuse of technical jargon without explanation, clunky sentence structures, inconsistent reference formatting contribute to lower clarity.]

Grammaticality score: 3
Grammaticality issues: [Misplaced or superfluous punctuation (e.g., "GPT-3[1]or", "Figure1,111Credit"), lack of spacing after sentences and around references.]

Readability score: 3
Readability issues: [Dense with technical detail, inconsistent citation formatting, and punctuation errors hinder smooth reading.]

Coherence score: 4
Coherence issues: [The structure is logical, but readability and fluency issues slightly obscure coherence.]
====================
	 Audio2Gestures: Generating Diverse Gestures from Audio
==========
Introduction:  Generating vivid human-like co-speech gestures is of great importance for producing attractive avatars that people are willing to interact with. There has been a surging demand for generating realistic human motions for given audio clips recently. However, this problem is very challenging because of the complicated one-to-many relationship between audio and motion. For example, a speaker may perform different gestures under different conditions (e.g.happy/peaceful mood, standing/sitting state, or different environments) when speaking the same words due to different mental and physical states. Existing algorithms developed for audio to body dynamics have some obvious limitations. For example,[speech2gesture]adapts a fully convolutional neural network to co-speech gesture synthesis tasks. Nevertheless, their model tends to predict averaged motion and thus generates motions lacking diversity. This is due to the underlying one-to-one mapping assumption of their model, which ignores that the relationship between speech and co-speech gesture is one-to-many in nature. Under such an overly simplified assumption, the model has no choice but to learn the averaged motion when several motions match almost the same audio clips in order to minimize the error. The above evidence inspires us to study whether or not explicitly modeling this multimodality improves the overall motion quality. To enhance the regression capability, we introduce an extra motion-specific latent code. With this varyingfulllatent code, which contains the same shared code and varying motion-specific code, the decoder can regress different motion targets well for the same audio, achieving one-to-many mapping results. Under this formulation, the shared code extracted from audio input serves as part of the control signal. The motion-specific code further modulates the audio-controlled motion, enabling multimodal motion generation. Although this formulation is straightforward, it is not trivial to make it work as expected. Firstly, there exists an easy degenerated solution since the motion decoder could utilize only the motion-specific code to reconstruct the motion. Secondly, we need to generate the motion-specific code since we do not have access to the target motion during inference. Our solution to the aforementioned problems is providingrandom noiseto the motion-specific code so that the decoder has to utilize the deterministic information contained in the shared code to reconstruct the target. But under this circumstance, it is unsuitable for forcing the motion decoder to reconstruct the exact original target motion anymore. So arelaxed motion lossis proposed to apply to the motions generated with random motion-specific code. Specifically, it only penalizes the joints deviating from their targets larger than a threshold. This loss encourages the motion-specific code to tune the final motion while respecting the shared code’s control. A preliminary version of this work was presented in[audio2gestures]. In this paper, we extend it from the following aspects: (1) We thoroughly investigate structured and perceptual metrics (i.e.STFT, SSIM, LPIPS, and FID) as training losses in our framework. We find them complementary to the previous point-wise losses since they additionally consider local temporal and/or spatial data structures. Especially with STFT, our network can consistently yield higher-quality motions. (2) We improve the user controllability by switching the latent space into discrete cosine transform (DCT) space, where different DCT components can control different dynamics of the generated motions. And we find the audio-motion shared code is more related to motion speed and rhythm while motion-specific code is more related to small range variations. (3) We conduct extensive ablation studies on different network backbones and hyper-parameters of motion losses and present more detailed discussions and analysis on the observations. The overall contributions are summarized as follows: We present a co-speech gesture generation model whose latent space is split into audio-motion shared code and motion-specific code, to better model the training data pairs and generate diverse motions. We propose a new relaxed motion loss, accompanied by other training losses/strategies, to better avoid degeneration of the proposed network, enabling multimodal motion generation given the same audio input. The effectiveness of the proposed method has been verified on 3D and 2D gesture generation tasks by comparing it with several state-of-the-art methods. And this split formulation is compatible with DCT space modeling and other backbones (i.e.GRU, Transformer), and robust to hyper-parameter choice in the relaxed motion loss. In complement to the most commonly used point-wise metrics/losses, we analyze the generated motion by taking local spatial and/or temporal structure into consideration and introduce them as training losses to further improve the motion quality. As a byproduct, the proposed method is suitable for motion synthesis from annotations since it can well respect the predefined actions on the timeline by simply using their corresponding motion-specific code.
Fluency score: 4
Fluency issues: [e.g.happy/peaceful, varies with context]

Clarity score: 3
Clarity issues: [The paragraph is somewhat lengthy and complex, making it harder to follow]

Grammaticality score: 4
Grammaticality issues: [lack of space after periods in "e.g.happy/peaceful", "varyingfulllatent", "providingrandom noise", "arelaxed motion loss", "(i.e.STFT"]

Readability score: 3
Readability issues: [Long sentences and paragraphs, heavy use of technical jargon and parenthetical statements can impede readability]

Coherence score: 3
Coherence issues: [The text jumps between aspects of the methods and outcomes without clear transitions, making it hard to follow the logical flow]
====================
	 BiBench: Benchmarking and Analyzing Network Binarization
==========
Introduction:  The rising of deep learning leads to the persistent contradiction between larger models and the limitations of deployment resources. Compression technologies have been widely studied to address this issue, including quantization(Gonget al.,2014; Wuet al.,2016; Vanhouckeet al.,2011; Guptaet al.,2015), pruning(Hanet al.,2015,2016; Heet al.,2017), distillation(Hintonet al.,2015; Xuet al.,2018; Chenet al.,2018; Yimet al.,2017; Zagoruyko and Komodakis,2017), lightweight architecture design(Howardet al.,2017; Sandleret al.,2018; Zhanget al.,2018b; Maet al.,2018), and low-rank decomposition(Dentonet al.,2014; Lebedevet al.,2015; Jaderberget al.,2014; Lebedev and Lempitsky,2016). These technologies are essential for the practical application of deep learning. As a compression approach that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology(Rusciet al.,2020; Choukrounet al.,2019; Qinet al.,2022a; Shanget al.,2022b; Zhanget al.,2022b; Bethgeet al.,2020,2019; Martinezet al.,2019; Helwegenet al.,2019). The binarized models take little storage and memory and accelerate the inference by efficient bitwise operations. Compared to other compression technologies like pruning and architecture design, network binarization has potent topological generics, as it only applies to parameters. As a result, it is widely studied in academic research as a standalone compression technique rather than just a 1-bit specialization of quantization(Gonget al.,2019; Gholamiet al.,2021). Some state-of-the-art (SOTA) binarization algorithms have even achieved full-precision performance with binarized models on large-scale tasks(Denget al.,2009; Liuet al.,2020). However, existing network binarization is still far from practical, and two worrisome trends appear in current research: Trend-1: Accuracy comparison scope is limited.In recent research, several image classification tasks (such as CIFAR-10 and ImageNet) have become standard options for comparing accuracy among different binarization algorithms. While this helps to clearly and fairly compare accuracy performance, it causes most binarization algorithms to be only engineered for image inputs (2D visual modality), and their insights and conclusions are rarely verified in other modalities and tasks. The use of monotonic tasks also hinders a comprehensive evaluation from an architectural perspective. Furthermore, data noise, such as corruption(Hendrycks and Dietterich,2018), is a common problem on low-cost edge devices and is widely studied in compression, but this situation is hardly considered existing binarization algorithms. Trend-2: Efficiency analysis remains theoretical.Network binarization is widely recognized for its significant storage and computation savings, with theoretical savings of up to 32\timesand 64\timesfor convolutions, respectively(Rastegariet al.,2016; Baiet al.,2021). However, these efficiency claims lack experimental evidence due to the lack of hardware library support for deploying binarized models on real-world hardware. Additionally, the training efficiency of binarization algorithms is often ignored in current research, leading to negative phenomena during the training of binary networks, such as increased demand for computation resources and time consumption, sensitivity to hyperparameters, and the need for detailed optimization tuning. This paper presentsBiBench, a networkbinarizationbenchmark designed to evaluate binarization algorithms comprehensively in terms of accuracy and efficiency (Table1). Using BiBench, we select 8 representative binarization algorithms that are extensively influential and function at the operator level (details and selection criteria are in AppendixA.1) and benchmark algorithms on 9 deep learning datasets, 13 neural architectures, 2 deployment libraries, 14 hardware chips, and various hyperparameter settings. We invested approximately 4 GPU years of computation time in creating BiBench, intending to promote a comprehensive evaluation of network binarization from both accuracy and efficiency perspectives. We also provide in-depth analysis of the benchmark results, uncovering insights and offering suggestions for designing practical binarization algorithms. We emphasize that our BiBench includes the following significant contributions: (1)Thefirstsystematic benchmark enables a new view to quantitatively evaluate binarization algorithms at the operator level.BiBench is the first effort to facilitate systematic and comprehensive comparisons between binarized algorithms. It provides a brand new perspective to decouple the binarized operators from the neural architectures for quantitative evaluations at the generic operator level. (2)Revealing a practical binarized operator design paradigm.BiBench reveals a practical paradigm of binarized operator designing. Based on the systemic and quantitative evaluation, superior techniques for more satisfactory binarization operators can emerge, which is essential for pushing binarization algorithms to be accurate and efficient. A more detailed discussion is in AppendixA.5.
Fluency score: 3
Fluency issues: Lack of spaces after commas and parentheses, e.g., "quantization(Gong", "tasks(Deng", and "1-bit specialization of quantization(Gong"

Clarity score: 3
Clarity issues: Dense with technical jargon and numerous references; difficult for the general reader.

Grammaticality score: 4
Grammaticality issues: Lack of spaces after citations.

Readability score: 3
Readability issues: Dense with technical language, long sentences, and heavy use of parenthetical references.

Coherence score: 3
Coherence issues: Heavy use of acronyms and citations disrupts flow; multiple themes introduced without clear transitions.
====================
	 SHIELD: Thwarting Code Authorship Attribution
==========
Introduction:  Code authorship attribution is the process of recognizing programmers of a given software, and there has been several works on robust and scalable attribution[4,13,23,8,6]. These methods have shown that programmers can be accurately identified by their coding style, making this problem an easy task thanks to the rapid development of code analysis and machine learning techniques. Accurate attribution benefits software forensics and security, especially for identifying malicious code programmers, detecting plagiarism, and settling authorship disputes. However, the process also poses privacy risks for programmers who prefer to stay anonymous. Recent code authorship attribution techniques heavily use machine learning models. While effective, those techniques are prone to manipulations that force the identification models to generate specific desired outputs, e.g., misclassification. One line of work for general machine learning algorithms utilized small perturbations to the input domain, resulting in adversarial examples (AEs) that are similar to the original ones, making it hard to distinguish them and posing a significant threat to machine learning models[24]. Examining AEs in the context of authorship attributions is an under-explored topic. This work introduces SHIELD for generating AEs at the source code level to prevent attribution while preserving code functionality. Such AEs will fool a classifier into misidentifying programmers and lead to targeted attacks, e.g., imitation or mimicking. Investigating such capabilities by examining how prone authorship attribution is to practical AEs allows a finer understanding of the state-of-the-art methods and helps address their shortcomings, especially with the increasing reliance on them for identifying coding style of programmers for security applications[7,13,8]. As a byproduct, our attacks can serve as a building block for maintaining the privacy of programmers in the presence of attribution techniques. SHIELD examines both the non-targeted and targeted attacks. In the non-targeted attacks, known asconfidence reductionormisclassificationattacks, SHIELD manipulates the input source code so that the identification model outputs any other author, i.e.,authorship dodging[28,26], where SHIELD can use this strategy to conceal the code author identity. In the targeted attack, SHIELD manipulates the input so that the identification model outputs a specifictargetauthor, i.e.,authorship imitationorevasion, depending on the adversarial capability and objective. We apply those scenarios to various authorship attribution methods with SHIELD for an in-depth analysis of each method. Although possibly extendable to binaries[14], SHIELD targets source-code authorship attribution for its prevalence. We note that source code-level attacks are challenging, since the generated AEs should be syntactically correct, should preserve the code functionality, and should not be easily detected. Although those attacks can be conducted using code transformation[21,26,19], a process similar to author obfuscation whereby the authorship traits are hidden in the transformed code, code transformation techniques require analyzing and changing the code to the target features across various categories, including layout, lexical, syntactic, control-flow, and data-flow features. However, a code perturbation approach provides an effective alternative for targeted and non-targeted attacks, by generating AEs to meet a specific goal without targeting the features of different categories. Conventionally, adversarial perturbations are applied directly to the input source and not to the feature space, e.g., perturbations in an image, not in the features extracted from that image. However, the code authorship attribution techniques are typically based on the authorship traits extracted from code, assuming a closed system where the feature extraction is not manipulated and requires perturbations at the code level. Therefore, attribution attacks should be designed explicitly using perturbations applied directly to the code. SHIELD injects carefully chosen code samples into the target source code and then obfuscates it using an off-the-shelf obfuscator. Unlike the prior work, where obfuscation is used to conceal the identity of the code author, we use obfuscation to make it difficult for the adversary to recognize or remove the injected code parts statically. Our threat model is more restrictive than without adopting obfuscation because code attribution must also be capable of identifying obfuscated source codes. However, it is well known that the attribution models work even in the obfuscated domain[4,13]. We note that while our AEs are generated at the code level, the eventual effect of the injection will be perturbations in the feature space. For convenience, we use the term perturbation to refer to injection. Contributions.Our key contributions are as follows. (1) We introduce SHIELD, a simple yet effective approach for generating AEs on code authorship attribution. The proposed approach does not change the original code but adds carefully-crafted code blocks (i.e., perturbations) to alter the authorship attributes of the code, leading to authorship dodging and imitation. (2) We provide a comprehensive evaluation of our technique against six state-of-the-art authorship attribution methods: DL-CAIS[4], WE-C-CNN, WE-S-CNN, TF-IDF-C-CNN, and TF-IDF-S-CNN[7], and Code Stylometry[13]. Our evaluation features a large-scale analysis of code authorship robustness under various adversarial scenarios using a dataset of 200 programmers. Our approach achieved a misidentification rate exceeding 98.5% for non-targeted attacks while significantly reducing the confidence of the model output. We also demonstrate imitation attacks at a rate of more than 66% for all targeted systems when using sufficient perturbations and at 88% when the adversary has access to samples of the targeted author. Organization.In SectionII, we provide a brief overview of authorship attribution workflow alongside specific details of the six targeted systems. In SectionIII, we introduce SHIELD and the attack strategy adopted in this work. In SectionIV, we define the non-targeted attacks and present the associated experimental results, followed by the targeted attacks and associated results in SectionV. In SectionVI, we discuss our findings, including the limitations and shortcomings of our approach. In SectionVII, we review the related work. We conclude our work in SectionVIII.
Fluency score: 4
Fluency issues: [Although the text is generally well-written, the excessive use of technical jargon and acronyms could make it less fluent for those not familiar with the context.]

Clarity score: 3
Clarity issues: [There's a heavy load of information presented, particularly with the frequent use of methodological names and references (e.g., [4,13,8,6]), which may confuse the reader.]

Grammaticality score: 4
Grammaticality issues: [The text contains minor grammatical issues, such as missing commas (“...generating AEs at the source code level to prevent attribution, while preserving...”) and the lack of space in "known asconfidence reductionormisclassificationattacks".]

Readability score: 3
Readability issues: [The dense technical language and long sentences can lead to a more difficult read, particularly for those not well-versed in the topic.]

Coherence score: 3
Coherence issues: [The text jumps between topics without clear transitions, and the use of "SHIELD" before it is properly introduced can be confusing.]
====================
	 Large Language Models on Lexical Semantic Change Detection: An Evaluation
==========
Introduction:  Languages are evolving. A perpetual flux of changes occurs, driven by an array of factors encompassing cultural, social, technological, and often, undiscovered influences. In this ever-shifting linguistic landscape, words shed unused senses while concurrently acquiring new meanings. Languages engage in a reciprocal exchange, borrowing senses from one another, and simultaneously exerting influence. This intricate web of linguistic evolution necessitates an automatic approach to comprehend and assess the fluidity of languages. Automation becomes the key to navigating and interpreting the dynamic currents of linguistic transformation. However, the development of advanced computational methods for Diachronic Lexical Semantic Change (LSC) has been a blank slate for researchers. Since the 2010s, traditional embedding methods like PPMI, SVD, and SGNS have shown significant statistical correlation with human annotators and produced promising results in detecting shifts in word meaning(kulkarni2014statistically;hamilton2018diachronic;schlechtweg_wind_2019). As a result, previous works often lean towards using existing tools to uncover new meaning shifts rather than exploring novel algorithms to enhance them. Additionally, frequency-based algorithms typically depend on large corpora(tahmasebi2019survey). Their performance on relatively low-resource datasets remains a challenge, and an efficient solution for this has yet to be discovered. Since its introduction byvaswani2017attention, models based on the Transformer architecture have become the latest trend. Contextualized word embeddings generated by BERT(devlin2019bert)have provided a solid foundation for various downstream language tasks. Moreover, recently, Large Language Models (LLMs) have showcased remarkable capabilities in logical thinking and solving language tasks based on instructions(bubeck2023sparks;zhao2023survey;openai_gpt-4_2023;roziere_code_2023). This has inspired researchers to embrace LLMs for a modern approach to a series of lexical semantic tasks and explore their ability to understand natural language meanings. In this study111Access the GitHub repository through thislink., we conducted a series of tasks to assess the suitability of LLMs for LSC detection forTempoWiC(loureiro_tempowic_2022), a low-resource annotated tweet dataset. Our key findings are outlined as follows: We reassess the performance oftraditional methods(i.e., PPMI, SGNS, SVD) in addressing diachronic LSC on a low-resource dataset. We introduce a simple yet innovative generative approach for diachronic LSC detection. This method achieves promising results without requiring fine-tuning on pre-trained models. We conduct comprehensive evaluations for LLMs, BERT-based methods, and traditional methods at both the corpus level and instance level, offering insights into their respective capabilities in diachronic LSC detection.
Fluency score: 4
Fluency issues: ["since its introduction byvaswani2017attention," "BERT(devlin2019bert)have" (spacing issues)]

Clarity score: 4
Clarity issues: ["an automatic approach" - a bit vague, could be misunderstood; citations interrupt the text flow]

Grammaticality score: 4
Grammaticality issues: ["since its introduction byvaswani2017attention," (missing space)]

Readability score: 3
Readability issues: Overuse of parentheses with citations and detailed terms break the flow; complex sentence structures

Coherence score: 4
Coherence issues: Slightly jumpy between discussions of different models and methods.
====================
	 Generalized Relation Modeling for Transformer Tracking
==========
Introduction:  Given the target bounding box in the initial frame of a video, visual tracking[javed2022visual]aims to localize the target in successive frames. Over the past few years, two-stream trackers[bertinetto2016fully,li2018high,li2019siamrpn++,zhang2020ocean], which extract features of the template and search region separately and then model cross-relations of the template and search region in a sequential fashion, have emerged as a dominant tracking paradigm and made a significant progress. Following this two-stream pipeline, several Transformer-based trackers[wang2021transformer,chen2021transformer,gao2022aiatrack]utilize parallel self-attention blocks to enhance the extracted features by modeling global self-relations within each image as illustrated in Fig.1(a). Recently, leveraging the flexibility of the attention mechanism, the one-stream pipeline[cui2022mixformer,ye2022joint,chen2022backbone]is proposed to jointly extract features and model relations, achieving promising performance. By conducting self-attention among all concatenated tokens, both cross-relation modeling and self-relation modeling can be performed simultaneously as illustrated in Fig.1(b). It is demonstrated in[xie2022correlation,cui2022mixformer,ye2022joint,chen2022backbone]that letting the search region interact with the template as early as possible is beneficial to target-specific feature generation. However, there is no evidence suggesting that all parts inside the search region should always be forced to interact with the template. Actually, due to the cropping strategy[bertinetto2016fully], there is a large proportion of background inside the search region, where distractors with similar appearance to the target may exist. This would lead to undesired cross-relations between the template and search region as the highly discriminative representations have not been extracted in some early layers. Although the attention mechanism can inherently weaken improper cross-relations, applying global cross-relation modeling to all layers may still be more or less disruptive. On the one hand, for the search tokens outside the target region, if undesired cross-relations are modeled between the template and distractors, the aggregated features of the distractors may contain the target features from the template, which could cause confusion for precisely identifying the actual target in the search region. On the other hand, for the template tokens, their quality could also be degraded by undesired cross-relations during the iterative update since certain features from the background or even distractors could be aggregated into these tokens. These situations could weaken the target-background discrimination capability of the one-stream pipeline. Intuitively, only a portion of search tokens,e.g. tokens belonging to the target, are suitable for cross-relation modeling when the feature representations are not perfect for target-background discrimination. In some cases, the two-stream relation modeling pipeline could even be better if the feature representations of both the template and search region are imperfect to model cross-relations. The potential limitations of the one-stream pipeline motivates us to ponder: is it really optimal for the template to interact with all parts inside the search region through all encoder layers in the one-stream pipeline? In this paper, we answer this question by proposing GRM, a generalized relation modeling method that can adaptively select the appropriate search tokens to interact with the template. To be specific, we classify the template and search tokens as three categories. The template tokens form one category while the search tokens are divided into another two categories. Instead of modeling relations within all the tokens as the one-stream pipeline, we restrict the interaction among the three token categories. Only the search tokens that are suitable for cross-relation modeling will interact with the template tokens, whilst the interaction between the remaining search tokens and the template tokens is blocked. With proper divisions, the two-stream pipeline and one-stream pipeline become two degenerated forms of our relation modeling method as discussed in Sec.3.2. Consequently, our method is a generalized formulation of attention-based relation modeling for Transformer tracking, which embraces the advantages of both previous pipelines while being more flexible. The search token division is performed by a lightweight prediction module, which can adaptively determine which search tokens are suitable for cross-relation modeling based on the input tokens. To accomplish this objective, there are two obstacles to overcome. First, the separate relation modeling for different token categories makes it hard for parallel computation. Second, the discrete token categorization is non-differentiable, thus impeding the end-to-end learning of the token division module. To facilitate parallel computation, we adopt an attention masking strategy to unify the individual attention operations into a single one. Additionally, we introduce the Gumbel-softmax technique[jang2016categorical]to make the discrete token categorization differentiable. Consequently, the search token division module can be implicitly optimized in an end-to-end manner, which promotes its adaptability to deal with different situations. In summary, our main contributions are three-fold: We present a generalized formulation of relation modeling for Transformer trackers, which divides the input tokens into three categories and enables more flexible interaction between the template and search region. To realize the generalized relation modeling, we devise a token division module to adaptively classify the input tokens. An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the proposed module. We conduct extensive experiments and analyses to validate the efficacy of our method. The proposed GRM exhibits outstanding results on six challenging visual tracking benchmarks.
Fluency score: 3
Fluency issues: "[javed2022visual]aims", "trackers[wang2021transformer,", "pipeline[cui2022mixformer," – missing spaces after references; Clunky transitions between sentences; The structure is somewhat complex with in-text citations disrupting the flow.

Clarity score: 3
Clarity issues: The text is dense with domain-specific jargon which may be unclear to general readers; Long sentences with multiple clauses; Numerous parenthetical references disrupt the sentence structure.

Grammaticality score: 4
Grammaticality issues: Specifically, missing spaces after citations (e.g., "[javed2022visual]aims", "trackers[wang2021transformer," etc.).

Readability score: 2
Readability issues: Dense and complex sentence structures; excessive use of jargon and references within sentences; very long paragraphs.

Coherence score: 3
Coherence issues: The text covers several concepts that may not seem directly connected without in-depth knowledge; Overuse of terms and references without clear explanations; Challenging to follow the logic without background knowledge.
====================
	 Absorbing Phase Transitions in Artificial Deep Neural Networks
==========
Introduction:  The 21st century has witnessed the tremendous success of deep learning applications. Properly trained deep neural networks have successfully demonstrated performance comparable with, or even superior to, that of human experts in various tasks, a few remarkable examples being the game of GoSilver2016, image synthesisRombach2022, and natural language processingStahlberg2020. Boosted by an exciting discovery of the so-called neural network scaling lawsHenighan2020;Kaplan2020, the frenetic pace in improving neural network performance is likely to persist, and hence it may be safe to say that deep learning technologies will constitute indispensable building blocks of the next-generation human society. Despite the fact that practically deep learning models can achieve such impressive performances, theoretically their behaviors are not yet fully understood. Deep neural networks are usually heavily over-parametrized, with the number of parameters in state-of-the-art neural networks growing exponentially over timeXu2018. From an energetic viewpoint, the state-of-the-art deep learning models consume a lot of energy, as the number of parameters is correlated to the amount of energy needed to perform an inference. In contrast, human brains seem to be good at learning and generalizing in an energetically efficient manner, even though strictly speaking they are generally not at doing arithmetic operations. This suggests that placing and comparing artificial neural networks in a broader context of biological neural networks on an equal footing, at least from a functional perspective, is promising for developing their understanding. The notion ofcriticalityis the key to linking biological and artificial neural networks. Systems at a particular condition (e.g. at the critical point of second-order phase transitions) exhibit anomalous behavior, referred to ascritical phenomena. They are universal in the sense that microscopically diverse systems can be described by a single mathematical model as long as the essential properties remain unchanged. The critical phenomena of particular interest in neuroscience are those ofabsorbing phase transitionsHenkel2008;Hinrichsen2000: transitions to a state from which a system cannot escape (hereafter referred to as “an absorbing state”). Besides the obvious analogy with brains without any neuronal activity (i.e. death), absorbing phase transitions are considered to be one of the essential ingredients for self-organized criticalityDickman1998, by which the systems can be automatically tuned to the critical point. Recent theoretical and experimental studies support the view that the brains may operate near the critical point (albeit in a slightly nuanced manner), and the universal scaling law in the critical phenomena of absorbing phase transition has been attracting considerable interest among the community; interested readers are referred to, for example, the recent review by Girardi-SchappoGirardi-Schappo2021. As a matter of fact, the deep learning research community is also familiar (albeit implicitly) with the notion of criticality. In theoretical studies on deep neural networks, the concept ofthe edge of chaoshas played a considerable role. While the discovery of chaos in random neural networks dates back to (at least) as early as the late 1980sSompolinsky1988, the concept has attracted recent interest among the community when Pooleet al. theoretically demonstrated that infinitely-wide deep neural networks also exhibit the order-to-chaos transitionPoole2016. Remarkably, at the onset of chaos,trainabledepth of the networks is suggested to divergeSchoenholz2017, which is reminiscent of the divergence of the correlation length at the critical point of second-order phase transitions at equilibrium. Furthermore, recent work has successfully applied the renormalization group method to classify the order-to-chaos transitions in the fully-connected feedforward neural networks for various activation functions into a small number of universality classesRoberts2022. Nevertheless, we argue that the notion of criticality has not been fully exploited in studies of artificial deep neural networks. As also discussed by Hesse and GrossHesse2014, bottom-up approaches (in which one derives macroscopic properties from microscopic theories) and top-down ones (in which one starts from phenomenological observations or some heuristics to deduce macroscopic properties) are complementary to each other for studying complicated systems. Numerous works, including those cited in the previous paragraph, have successfully adopted one of the bottom-up approaches for a specific architecture and/or an activation function. However, the situation with regard to the top-down approaches is less satisfactory. Since the universality of the critical phenomena enables the classification of the systems into a reduced number of universality classes based on their fundamental properties, taking full advantage of it would lead us to intuitive and yet powerful understanding of the behavior of deep neural networks across different architectures. Given all these observations, the purpose of the present work is to demonstrate that the notion of absorbing phase transition is a promising tool for theoretical understanding of the deep neural networks. First, we establish an analogy between the aforementioned order-to-chaos transition and an absorbing phase transition by studying the linear stability of the ordered state. In the framework of the mean-field theory of signal propagation in deep neural networksPoole2016, the critical point is characterized by loss of linear stability of the fixed point corresponding to the ordered phase. We extend the analysis to the networks of finite width, and we directly see that the transition to chaos in artificial deep neural networks is an emergent property of the networks which requires the participation of sufficiently many neurons (and thus more appropriately seen as a phase transition, rather than a mere bifurcation in dynamical systems). Next, we show that the order-to-chaos transitions in initialized artificial deep neural networks exhibit the universal scaling laws of absorbing phase transition. Actually it is fairly straightforward to find the scaling exponents associated with the transition in the framework of the mean-field theory (or equivalently in the infinitely-wide networks) for the fully-connected feedforward neural networksSchoenholz2017, but it is not clear how we can extend the analysis into the networks of finite width or a different architecture. Our empirical study reveals that the idea of the universal scaling can still be successfully applied to such cases. We also provide an intuitive way to understand the resulting universality class for each architecture, based on a phenomenological theory. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics in the finite networks. To summarize, we believe that the this work places the order-to-chaos transition in the initialized artificial deep neural networks in the broader context of absorbing phase transitions, and serves as the first step toward the systematic comparison between natural/biological and artificial neural networks.
Fluency score: 4
Fluency issues: Occasional awkward phrasing, e.g., "the frenetic pace in improving neural network performance is likely to persist."

Clarity score: 3
Clarity issues: Numerous technical terms and embedded clauses, e.g., "the concept of the edge of chaos," could obscure meaning for uninitiated readers.

Grammaticality score: 4
Grammaticality issues: Some long sentences with complex structures could be simplified for clarity, e.g., "Nevertheless, we argue that the notion of criticality has not been fully exploited in studies of artificial deep neural networks."

Readability score: 3
Readability issues: Dense academic style with jargon and complex sentences decreases general readability.

Coherence score: 3
Coherence issues: The flow is somewhat disrupted by jargon and the abrupt introduction of concepts without sufficient explanation, e.g., "the notion of criticality is the key to linking biological and artificial neural networks."
====================
	 Cross-dataset domain adaptation for the classification COVID-19 using chest computed tomography images
==========
Introduction:  On December 31, 2019, the World Health Organization (WHO) reported unknown cases of respiratory diseases that have spread in Wuhan, China[29]. The disease is identified as a new virus part of the Coronavirus family that can cause illnesses ranging from the common cold to more serious respiratory diseases. The new virus was later known as COVID-19. On January 30, 2020, due to the spread of this disease in China and many other parts of the world, it was classified as a public health emergency by the WHO[62]. COVID-19 is a respiratory illness with symptoms similar to typical influenza. The transcription-polymerase chain reaction (RT-PCR) laboratory test is used as a reference tool for diagnosing COVID-19. In addition, X-rays and chest Computed Tomography (CT) scans are considered to be new information technology (IT) tools for COVID-19 diagnostics. What distinguishes the IT approach is its interpretability, which can help in fast decisions taken by doctors regarding COVID-19. CT is a painless and non-surgical imaging method characterized by speed and high accuracy. CT uses advanced X-ray technology to help detect many diseases and obtain detailed images of bones, internal tissues, and organs, where CT images give more details than traditional X-rays. Body parts absorb X-rays in unequal ways which allow the doctor to distinguish body parts and any changes due to disease[44]. Figure1shows examples of CT images from COVID-19 patients. Ai et al.[2]report on 1014 patients who received both PCR and CT scans in Wuhan, China, during the epidemic. They found that 90% of confirmed diagnostics of PCR had clear signs of COVID19 in chest CT scans appearing in the form of bilateral opacity. In another study, chest CT revealed bilateral opacity in the lung in 40 out of 41 patients (98%) with COVID-19 in Wuhan[54]. In addition, they show a high rate of appearance of ground glass opacity and uniformity with the round shape or occasional peripheral pulmonary distribution[54]. The probability of error in PCR tests, their limitations, and the length of time of their results, especially in areas affected by epidemic[63], as well as the low accuracy and sensitivity of X-rays in COVID-19 diagnosis, all made CT scans of the chest a very promising diagnostic tool for COVID19[8]. Recently, advances in computer vision and Machine Learning (ML) have led to the emergence of a novel generation of techniques in computer-aided disease diagnosis (CAD)[41]. In particular, Deep Learning (DL) in medical imaging has achieved outstanding performance in disease diagnosis and follow-up. DL has proven its important role and efficiency in medical image processing including classification, detection and segmentation tasks[5]. In this Chapter, we propose a DL method for the diagnosis of COVID-19 disease using chest CT scans. In particular, we focus on the problem of removing the need for labeling large amounts of data to train DL models for COVID-19 detection. In machine learning, deep CNN models provide excellent results with large amounts of labeled data. However, this is neither reasonable nor practical. Ideally, our ultimate goal is to design a model that can provide good classification results for new datasets without the need for a large labeling effort. One solution is cross-dataset learning, where we transfer knowledge from one labeled dataset to another unlabeled dataset. This is also known as domain adaptation (DA) in which we develop learning models that can intelligently adapt from one source domain (dataset) to another target domain. To our knowledge, this is the first work that addresses the problem of DA in COVID-19 detection. First, recall that the basic assumption of many machine learning algorithms is that training (source) and test (target) data come from the same distribution. However, in DA, the training and testing data come from different datasets taken under different circumstances which disproves the validity of this assumption. The distribution of data between source (training) and target (test) domains may change, causing low classification accuracy on the target data. The difference in distribution between different domains is still a very relevant problem among medical image datasets due to different image acquisition machines and circumstances. As a result, there is increased interest in DA within the field of medical images to solve this problem and improve classification performance. But in the area of COVID-19 detection, the DA research is still very limited[52,22]. In this objective, our proposed method is based on a new family of powerful CNN models called EfficientNet and on DA techniques to transfer knowledge from one domain to another. Our main contributions can be summarized in the following points: We present a DA method, called COVID19-DANet that can adapt a DL model from a source COVID-19 dataset to a target dataset. It uses the unlabeled samples from the target dataset to reduce the data distribution shift between the source and target datasets. The proposed DA method uses EfficientNet-B3 CNN as a feature extractor and a classification layer inspired by prototypical networks from the few-shot learning area. Inspired by the semi-supervised learning methods proposed in the machine learning literature, COVID19-DANet uses the entropy of the output probabilities over the unlabelled target set as a loss function for reducing the distribution shift between domains. The remainder of the chapter is organized as follows. In Section2, we review some related work using CT images for COVID-19 detection. We also survey some approaches based on DA learning, especially its field of application and typical setup. In Section3, we present the proposed DL-based DA model for COVID19 CT classification. Next, we present our experimental results in section4in terms of the most relevant assessment metrics. Finally, we outline our concluding remarks and future work suggestions in Section5.
Fluency score: 4
Fluency issues: Reference style in text [29][62][44][2][54][63][8][41][5][52,22] disrupts flow.

Clarity score: 4
Clarity issues: Overuse of acronyms without first instance explanation (RT-PCR, IT, CT, AI, ML, DL, CNN, DA, CAD).

Grammaticality score: 4
Grammaticality issues: Inconsistent formatting "Figure1shows".

Readability score: 3
Readability issues: Dense paragraphs; excessive jargon and acronyms reduce accessibility.

Coherence score: 4
Coherence issues: Shift from historical context to technical details is somewhat abrupt; reference numerals can be distracting.
====================
	 Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound
==========
Introduction:  Pneumothorax (PTX) is a potentially life-threatening acute condition in which air occupies the space between the pleura of the lungs, resulting in collapse of the lung. Rapid identification of PTX is crucial in emergency, critical, and acute care settings to expedite medical intervention. Point-of-care lung ultrasound (LUS) is a quick, inexpensive, portable, imaging examination that does not expose patients to radiation. Despite its low prevalence compared to chest radiographs, LUS has been shown to exhibit superior diagnostic performance for the diagnosis of PTX[Nagarsheth2011,Alrajhi2012]. The lung sliding artefact, caused by the normal motion of the pleura, has been described as a means to rule out PTX[Lichtenstein1995]. Notably, the presence of lung sliding excludes a diagnosis of PTX within the purview of the ultrasound probe[Lichtenstein1995]. Conversely, PTX is likely present when lung sliding is absent. Previous studies have demonstrated that deep convolutional neural networks (CNN) can be trained to distinguish between the presence and absence of lung sliding on motion mode (M-mode) ultrasound images[Javsvcur2021,VanBerlo2022]. Prior studies were limited by the amount of labelled data available for training and evaluation. Furthermore, previous studies initialized their networks using weights pretrained on the ImageNet dataset[Deng2009]. Despite the fact that M-mode images are profoundly distinct from the natural images present in ImageNet, it is common for medical imaging studies to leverage ImageNet-pretrained weights. They are publicly available for several common architectures and are able to extract low-level features present in medical images. Unfortunately, there are no publicly available equivalents for M-mode images, let alone LUS. Self-supervised learning (SSL) is a representation learning strategy applicable in the absence of labelled data. CNNs pretrained using SSL have exhibited superior performance and label efficiency compared to fully supervised counterparts[Chen2020,Grill2020,Zbontar2021,Bardes2022]. Broadly, SSL pretraining consists of training a deep neural network to solve apretext task, whose solution can be computed from unlabelled examples. The pretrained weights may be fine-tuned to solve adownstream taskfor which labels are present. This study explores the impact of self-supervised pretraining for the downstream task of detecting absent lung sliding in M-mode LUS, varying the choice of SSL method, weight initialization, data augmentation strategy, and inclusion of unlabelled data. Crucially, we demonstrate that incorporating large volumes of unlabelled M-mode images during the pretraining phase improves the performance of a fine-tuned classifier on external datasets. More specifically, our major contributions are as follows: A pairwise relationship for contrastive and non-contrastive learning that is specific to M-mode images A data augmentation pipeline specific to M-mode images in the context of pretraining A comprehensive investigation of factors that influence the utility of SSL pretraining for the downstream task of absent lung sliding detection, such as label efficiency, ImageNet initialization, and data augmentation Evidence that the inclusion of unlabelled data results in improved generalization to external datasets for absent lung sliding detection Fig.1summarizes our methods. To the best of our knowledge, no study has investigated the efficacy of SSL for M-mode ultrasound tasks.
Fluency score: 4
Fluency issues: [Minor awkward phrasings that potentially disrupt the natural flow, such as "in which air occupies" could be more fluidly phrased as "where air fills"]

Clarity score: 4
Clarity issues: [The use of multiple abbreviations and technical jargon (PTX, LUS, CNN, SSL, etc.) can be confusing for non-specialist readers. The sentences are complex and packed with information.]

Grammaticality score: 5
Grammaticality issues: [None identified.]

Readability score: 3
Readability issues: [Complex sentences and heavy use of jargon and abbreviations make the text challenging for general audiences.]

Coherence score: 4
Coherence issues: [The structure is generally coherent, but the transition from discussing PTX and LUS to explaining SSL and deep learning could be smoother.]
====================
	 PDL: Regularizing Multiple Instance Learning with Progressive Dropout Layers
==========
Introduction:  [2]\lipsum[3]
I regret to inform you that the provided text seems to be a placeholder or a formatting command ("\lipsum[3]") often used in LaTeX to generate dummy text. To evaluate writing quality, please provide an actual sample of text.
====================
	 SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval
==========
Introduction:  Video search has the core building block of recently growing video streaming services (\egYouTube, Netflix). To enhance the capability of video search, video moment retrieval (VMR) aims to localize the start and end time of the moment pertinent to a given language query in an untrimmed video. The success of the VMR provides us with accurate video contextual information in less time and effort. Until recently, these remarkable search performances have been dependent on the size and quality of labeled training datasets. However, these datasets cost a labor-intensive annotating process (\ieAnnotators should find the start-end time of moments corresponding to query descriptions), and sometimes the annotated moments are ambiguous. To cope with this problem, many weakly-supervised VMR (wsVMR) methods[mithun2019weakly,duan2018weakly,zhang2020counterfactual,zheng2022weakly2]have been proposed by only utilizing the video-query pairs, which are less laborious to annotate. To perform the weak supervision using video-query pairs, if one query is paired (\ieannotated) with multiple videos, we can identify the common scene among these videos and determine the alignment between the query and the scene. To implement this, all videos are divided into multiple segments, and the retrieval system maximizes the similarity scores between each query and paired segments while suppressing the scores between the query and unpaired segments in other videos. During the inference, the system selects a segment with the highest score as a moment prediction for a given query. For the wsVMR systems to accurately classify the best segment in a video, numerous video-language joint representation learning methods[lin2020weakly,song2020weakly,zheng2022weakly2,wang2021visual]have been proposed. Recently, researchers also have another focus on a study of how to generate video segments to capture many scenes in a video[ma2020vlanet,zheng2022weakly2]. These segments are referred to as ‘candidate moment proposals’, which is crucial, as they directly affect the retrieval performances by regulating the proposal quantities. Unfortunately, as supervision is not available in generating proposals, wsVMR systems[zheng2022weakly1,zheng2022weakly2]use a fixed number of proposals for all input videos under heuristic optimization of a specific dataset, which is not reasonable to deal with varying numbers of scenes in a video. While some methods[ma2020vlanet,huang2021cross]consider varying numbers of proposals for each video, they still rely on spurious correlations, such as generating proposals proportionally to the video length or using sliding window. Therefore, the current proposal generation method could not accurately respond to the diverse number of scenes in each video. We refer to this situation as a ‘scene-proposal mismatch’. For instance, in Figure1(a), the systems produce an unnecessarily large number of proposals by referring to the long length of the video, but the video only contains a single scene (\iescene of sitting still in a chair throughout the video), which should be handled by small amounts of proposals. They also show scene-proposal mismatch by producing a small number of proposals for the video containing many scenes, such that those scarce proposals seem not to work correctly. Our experimental evidence in Figure1(b) validates the current wsVMR systems’ incorrectness due to the scene-proposal mismatch. We plot performances (\iemean Intersection over Union (mIoU) scores) over predictions along the number of scenes in videos and the number of proposals generated, which shows irregularities in the scores. The scores are low for videos with many scenes but few proposals and also low for videos with few scenes but many proposals. To estimate the number of scenes in a video, as shown in Figure1(c), we counted the number of paired queries for each video as a discrete approximation of the scene. Here, we found that some queries describing the same scene led to redundancy in the counting. Thus, we remove the redundancy of those queries via calculating their IoUs111We remove redundancy by scenes with IoU>0.5.between temporal boundary annotations222Temporal annotations are used only for identifying the proposal-scene mismatch problem and they are not involved in the wsVMR task. Our study further showed that the scene-proposal mismatch affects about 41% of videos in VMR benchmarks (\ieCharades-STA[gao2017tall], ActivityNet-Caption[krishna2017dense]). Intrigued by the scene-proposal mismatch, this paper proposes a wsVMR system referred to as Scene Complexity Aware Network (SCANet), which allows the system to mitigate the scene-proposal mismatch problem and generate proposals adaptive to the complexity of the scenes contained in the video. For a given input video, SCANet first defines the scene complexity with a scalar, meaning how difficult for the system to find (\ieretrieve) a specific scene among multiple distinguishable scenes in the video, which can be effective prior knowledge of video by complementing weak supervision of VMR. On top of the scene complexity, SCANet adaptively generates proposals and enhances their representations. Therefore, SCANet incorporates (1) Complexity-Adaptive Proposal Generation (CPG) that generates adaptive proposals by leveraging the quantities of proposals under consideration of the complexity and (2) Complexity-Adaptive Proposal Enhancement (CPE) that enhances the proposals’ representations corresponding to the scene complexity. Furthermore, motivated by recent success[zhang2020counterfactual,zheng2022weakly2]of contrastive learning for wsVMR system, we introduce technical contributions to mine hard negatives in the input video and further video corpus together under our designed framework. Our extensive experiments show the effectiveness of the proposed SCANet, and qualitative results validate enhanced interpretability.
Fluency score: 4
Fluency issues: Occasional lack of prepositions ("by only utilizing") and awkward phrasing ("meaning how difficult for the system to find").

Clarity score: 3
Clarity issues: Overuse of acronyms and jargon, long and complex sentences make it difficult to follow.

Grammaticality score: 4
Grammaticality issues: Incorrect use of parentheses and slashes, missing conjunctions ("and" before "sometimes"), improper bracket use, problematic punctuation like unnecessary commas.

Readability score: 3
Readability issues: Dense technical language, lengthy sentences, and excessive use of jargon impact the smoothness of reading.

Coherence score: 3
Coherence issues: Overly technical terms without explanation, multiple concepts introduced without clear transitions, and parenthetical interruptions detract from the overall flow of the text.
====================
	 Differentiable Resolution Compression and Alignment for Efficient Video Classification and Retrieval
==========
Introduction:  Video representation learning is a crucial research topic due to its numerous applications, such as recommendation system, video search, etc. Due to the high redundancy across video frames, a judicious and effective strategy to expedite video inference involves mitigating redundant computations. Some existingskipping-basedmethods[7,28,3,26,23]propose to skip non-saliency frames to save computation costs via various temporal saliency sampling strategies. These methods are based on the assumption that the most saliency frame/regions contribute the most to the video representation. However, due to the inevitable loss of spatial information and inaccurate saliency measures, these methods encounter challenges in some fast-changing and fine-grained scenarios. For example, discriminating between different dancing types is concealed within the gradual transitions of each frame. Besides, for tasks that rely heavily on detailed information, such as video retrieval, discarding non-saliency frames may result in losing essential clues for accurate matching. In contrast, somemulti-network-basedmethods[19,27,25]have been proposed to retain non-saliency frames while using multiple networks with different computational costs to mitigate the overall expenses. Although fusion layers are designed to integrate multiple streams of information, extracting global temporal information may be inadequate in representing fine-grained information to achieve precise contextual understanding. Besides, several of the above algorithms select different numbers of saliency frames based on the video content, making batched inference complicated and challenging. To address these issues, in this paper, we cast efficient video learning as a Differentiable Resolution Compression and Alignment(DRCA)task, which consists of two main components to accomplish resolution compression and alignment. We instantiate the two components asDCCM(Differentiable Context-aware Compression Module) andRAT-Layer(Resolution-Align Transformer Layer). Specifically, to reduce video tokens and minimize information loss, unlike previous methods[27,28,25]that use an additional network to measure frame saliency separately, ourDCCMincorporates the saliency assessment and frame compression directly into the network, enabling them to be not only context-aware but also learnable. Concretely, the DCCM can be end-to-end optimized through differentiable ranking, which solves an optimal saliency sorting sequence based on the predicted scores. By reducing the resolution of non-salient frames and decreasing the number of spatial tokens, significant reductions in spatial computational costs can be achieved, as the computational complexity of the Transformer increases quadratically for the number of tokens. Meanwhile, this also poses a challenge for existing networks like 3D convolutional neural networks[6,17]and Transformers[1,20,21], as they lack the capability to directly extract features in this multi-resolution context. Therefore, we propose a simple but effectiveRAT Layerto process the frame tokens with different resolutions. The RAT Layer can extract strong spatial-temporal correlations layer-by-layer, which is crucial for video tasks that require reasoning, such as retrieving the same incident, where some video frames captured by different photographers are visually similar but not identical. Notably, this novel approach enables efficient and accurate video representation learning while preserving the ability of batched inference with fixed token numbers. This is achieved by fully end-to-end optimizing integrated with a differentiable compression module. As a result, our method achieves the best trade-off between efficiency and performance compared to state-of-the-art (SOTA) methods in near-duplicated video retrieval(NDVR). Additionally, it demonstrates competitive performance in dynamic video classification compared to other SOTA methods. Our contributions are summarized as follows:(1) a novelDifferentiable Resolution Compression and Alignmentnetwork for efficient video learning(2) a novelDCCMdesigned to compress the non-essential information with differentiable ranking(3) a simple but effectRAT Layerto extract spatial-temporal correlations in multi-resolution compressed video sequence(4) achieving the best trade-off between efficiency and performance on NDVR task and competitive result on dynamic video classification task against SOTA.
Fluency score: 4
Fluency issues: [The text uses frequent technical jargon and lacks transitions between some sentences, making it less fluid for a general reader.]

Clarity score: 3
Clarity issues: [Use of terms like "saliency" without explanation, technical lingo ("frame tokens," "spatial-temporal correlations") assumed as understood, and dense presentation of concepts may obscure meaning for non-experts.]

Grammaticality score: 3
Grammaticality issues: [There are run-on sentences and grammaticality issues such as "existingskipping-basedmethods," "somemulti-network-basedmethods," and "effectRAT Layerto."]

Readability score: 3
Readability issues: [The text is dense, with long sentences and jargon making it less accessible. Incorrect spacing in terms like "existingskipping-basedmethods" hinders readability.]

Coherence score: 3
Coherence issues: [The text jumps between concepts without clear transitions, and the relationship between certain ideas (e.g., video tokens, RAT Layer, and their functions) is not always clear.]
====================
	 Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?
==========
Introduction:  It is understandable that there could be some correlation between ads and products for a same domain. For example, a user likes the style of a neckline in a fashion ads and might also like a fashion product that has similar style (Fig.1). However, is there any correlation between ads and products from different domains? Specifically, can we interpret a consumer’s product preference from her website browsing logs of various advertising domains? In the context of fashion online shopping, however, to our knowledge, no study has investigated the correlation between various ads domain and fashion taste on the consumer level, as shown in Fig.2. In this paper, we introduce a new user taste understanding dataset, Fashionpedia-Ads, which asks subjects to provide their preference on both ad images of various domain (fashion, beauty, car, food) and fashion product images. Furthermore, unlike fashion product images, ads images usually contains complicated and multiple perspectives of information (emotional, visually, textually…) that cause a consumer like them. For example, for a same ad image (Fig.1), a consumer might like it because of the neckline of the dress. However, another consumer might like this ad image because the emotional feeling created in this ad image. To fully understand the multi-correlation (both visual and textual) between ads and fashion product images liked by subjects, we exhaustively annotated both ads and fashion images from different perspectives: 1) abstractive level; 2) physical attributes with associated segmentations (localized attributes); 3) caption, and 4) brands on the ads. The aim of this work is to enable future studies and encourage more exploration to interpretability research between advertisements and fashion taste. The contributions of this work are: 1) we introduce Fashionpedia-Ads, consisting of three datasets (Ads, Social network style and E-commerce style fashion products). We bridge the connection among them through the subjects’ preference (like or dislike) on these images and the annotation from multi-perspectives (e.g. abstract & physical attributes). 2) we formalize a new task that not only requires models to predict whether a subject like or dislike a fashion product image based on given ad images of various domains, but also provide a rationale explaination why it makes this prediction from multi-perspectives.
Fluency score: 3
Fluency issues: The flow is disrupted by occasional awkward phrases ("ads and products for a same domain," "ad images usually contains complicated," "for a same ad image").

Clarity score: 3
Clarity issues: Terms like "multi-correlation" and the phrase "rationale explaination" lack clarity; the term "ad images" could confuse readers as to whether it refers to images in ads or images of ads.

Grammaticality score: 3
Grammaticality issues: There are grammar errors such as incorrect verb agreement ("ads images usually contains"), sentence fragments ("Furthermore, unlike fashion product images"), and a missing article ("in the context of fashion online shopping").

Readability score: 3
Readability issues: Dense structure, long sentences, and parenthetical notes interspersed with main text make the passage less reader-friendly.

Coherence score: 3
Coherence issues: The text jumps between different aspects of the study without clear transitions, and the use of "however" is not always coherent with the sentence structure.
====================
	 Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations
==========
Introduction:  Natural and chemical phenomena, as well as some problems in the real world, can be described through mathematical expressions, in particular partial differential equations (PDEs). For example, fluid flow, chemical reaction-diffusion, phase separation, image analysis, image segmentation, cell division, the spread of infectious diseases, etc., can be mathematically expressed. Therefore, to find solutions to PDEs or mathematically analyze the characteristics of phenomena and changes in energy over time that these equations represent, numerical studies aimed at approximating PDE solutions are continuously being conducted such as finite difference method (FDM)RLV2007;YZ2009;EBKP2011;JKDJSYYC2017;YLJK2017, finite element method (FEM)NFJXYY2011;AD2011;CJ2012;NKSMAB2022, finite volume method (FVM)QXXXNF2012;EFA2015;SM2015;PZZL2022, and so on. Moreover, machine learning/deep learning models have been developed to solve approximately PDE solutions. The application of physics-informed neural networks (PINNs)pinnhas led to great success in solving partial differential equations because the physics-informed loss does not require any observations or discretization. Also, PINNs achieve acceptable accuracy for diverse simulationspinn2;pinn3;pinn4;pinn5;pinn6. However, optimizing model parameters remains a challenge, and PINNs should be trained separately by each initial condition. To address these problems, a possible way is to use data-driven models that can learn numerical schemes using snapshots and predict solutions at further time steps. Using a supervised learning approach, convolutional neural networkscnnhave been widely applied to solve partial differential equationsphygeo;convpde;poicnnbecause the mechanism of the convolution operator is similar to numerical methods that utilize neighboring points to obtain values at the next time step. However, the prediction of data-driven machine learning models and numerical methods is strongly affected by the time step of given snapshots, so appropriate time steps should be chosen. Here, we focus on thereceptive fieldreceptivefield;yolov4which refers to the size of input nodes that affect a single output node. Modern convolutional neural networksvgg;resnet;effnethave been designed to acquire large receptive fields for good feature extraction. In other words, the large receptive field increases the capacity of the indirect connectivity between an input and its output so that plenty of the input nodes are involved in the output extraction. In FDMs, simulation errors and time steps are also influenced by the receptive field size related to the order of approximations to derivatives (e.g., 5-point stencil vs. 9-point stencil). The stability condition of 2D heat equation (\phi(x,y)_{t}=\phi_{xx}+\phi_{yy}) ish^{2}/4, whereh=\frac{1}{\Delta x}=\frac{1}{\Delta y}. \phi(x,y,t_{n})=e^{iqx}e^{iry}then, whereGfis a growth factor defined as The worst case isq\Delta x=r\Delta y=\pi, then Therefore, the stability condition is Sinceh=\frac{1}{\Delta x}=\frac{1}{\Delta y}, we obtain ∎ Basically, the time step size\Delta tcan be decided by Theorem1, which provides an analysis of the stability range of the explicit scheme for the two-dimensional heat equation. The stability analysis determines the range of suitable time steps that ensures the numerical solution remains stable. In other words, if\Delta tdoes not satisfy the Eq. (1), a blowup could occur. In this paper, the main idea of our approach is to utilize a receptive field that permits a time step larger than the threshold of the CFL condition. Therefore, we propose a deep CNN architecture to increase the receptive field size. The contents of this paper is as follows: In Section2, we explain our proposed deep five-point stencil convolutional neural networks (deep FCNNs) and algorithms. In Section3, we perform numerical simulations for various initial conditions. In Section4, we summarize the paper and discuss a possible research direction.
Fluency score: 3
Fluency issues: [the flow of the text is disrupted by numerous citations formatted as FDM)RLV2007;YZ2009; EBKP2011; citation patterns repeated throughout the document]

Clarity score: 2
Clarity issues: [excessive jargon, in-text citations disrupting readability, lack of explanation for terms like "physics-informed neural networks", equations without clear context]

Grammaticality score: 3
Grammaticality issues: [inconsistent use of articles and punctuation, some areas lack clear sentence structure]

Readability score: 2
Readability issues: [dense use of technical terms, unconventional in-line citation style, equations inserted into text without adequate explanation]

Coherence score: 3
Coherence issues: [logical structure is overwhelmed by technical details and citation formatting, lack of clear transitions between topics]
====================
	 A Survey on Figure Classification Techniques in Scientific Documents
==========
Introduction:  Classification of images finds tremendous applications in various fields such as automobile, healthcare, agriculture, surveillance, and document analysis[7,29,23,19]. In scientific documents, different graphical visualizations such as tables, photos, diagrams, maps, and plots convey specific facts that are more effective than simple text. This factual information improves comprehension. Hence, extracting underlying information represented by figures is an important task. In general, it is referred to as figure mining. Figure mining includes enhancing the figure design, outlining the data represented by figures, detecting plagiarized documents, etc. The figure mining pipeline consists of (i) figure extraction from academic documents, (ii) classification of figures, and (iii) data extraction from each figure type. This paper aims to survey figure classification techniques and their related datasets comprehensively. To address the problem of figure classification, it is crucial to detect and extract the figures from the respective documents using document segmentation techniques, as illustrated in Fig-1. Generally, a document image may be segmented into text and non-text components. The non-text details are then further processed to classify them into an appropriate category. Much research has been done on the textual processing of documents. But as far as figures are concerned, there need to be more state-of-the-art methods that classify the scientific figures in their appropriate category. Chart image classification has recently interested many research groups[12]. This paper aims to highlight the work on chart image classification and include results that include other figure types. The techniques used for classification can be divided into handcrafted-based methods and deep learning-based methods. The hand-crafted methods manually extract features using traditional feature extraction techniques, then classify the figures using machine learning models. On the other hand, deep learning techniques automatically learn features and classify the figures. Various approaches employed in these two categories are discussed in detail in the upcoming sections. This follows a discussion on several data sets reported in the related literature. The rest of the paper is organized as follows. Section 2 provides information on the existing literature on the figure classification problem, and a summary of significant contributions is shown in TableI. Section 3 includes a discussion of datasets used in recent works, and details of a few publicly available datasets are summarised in Table-III. Section 4 provides pointers for future research work and many interesting problems that still need to be addressed in figure classification.
Fluency score: 4
Fluency issues: [Repetitive sentence structure like "This paper aims to..."]

Clarity score: 4
Clarity issues: [Long sentences with multiple clauses can be complex to understand at a first glance, e.g., the first sentence of the text.]

Grammaticality score: 4
Grammaticality issues: [Misuse of commas, lack of a serial comma in lists, minor issues with pluralization ("there need to be more" should be "there needs to be more"), inconsistent use of hyphens in "Fig-1" and "Table-III".]

Readability score: 3
Readability issues: [Lengthy sentences loaded with information can make the text harder to read. Some sentences need to be broken into smaller parts.]

Coherence score: 4
Coherence issues: [The overall structure is logical, but the text jumps between topics too quickly which could disrupt the flow for the reader.]
====================
	 Extracting Physical Causality from Measurements to Detect and Localize False Data Injection Attacks
==========
Introduction:  Modern power systems have become sophisticated cyber-physical systems due to the integration of information and communication technologies. The informatization and intelligent transformation of the smart grid enhances the efficiency of the system but also confront it with more cyber attacks. In particular, a meticulously-designed False Data Injection Attack (FDIA) is capable of manipulating the state estimation results of power systems while bypassing the conventional bad data detector (BDD), thereby posing great physical and financial threats[8]. Extensive research has been conducted on the countermeasures of FDIAs, which can be classified into two categories, namely FDIA detection and localization. In general, FDIA detection approaches aim to identify the existence of FDIA. Various models have been applied to this issue, from the classic Kalman filters[10,7], interval observer[26], maximum likelihood estimation[11,22], Support Vector Machine (SVM)[13], to assorted deep learning models[3,12,1,4,27]. For example, A Recurrent Neural Network (RNN) is applied to recognize FDIA in dc microgrids[3]. The RNN is further hybridized with Long-Short Term Memory (LSTM) cells in[12]to scrutinize remedial actions against FDIAs. Denoising autoencoders are also combined with LSTM to detect FDIAs and recover contaminated measurements by capturing the spatio-temporal dependencies between them[1]. In recent years, some research has extended FDIA defense techniques from detection to localization. Compared to detection algorithms, FDIA localization approaches aim to specify which measurements or state variables have been tampered with, requiring a finer-grained identification capability. Existing localization approaches can be categorized as model-based and data-driven. Model-based approaches require an accurate system model and its associated parameters, and generally have good interpretability and generalizability. For example,[9]models an interval observer for each measurement in the power system and constructs a logical localization matrix to realize FDIA localization. Nevertheless, model-based approaches are often confronted with scalability issues and the difficulty of obtaining an accurate system model. Conversely, data-driven approaches are system-independent, and while their off-line training process can be time-consuming, they are efficient when applied in real-time tasks. In[5], Jevticet al.develop a cumbersome framework to localize FDIAs in a 5-bus power system, which builds an independent LSTM model for each measurement. In[25], Wanget al.concatenate a simple Convolutional Neural Network (CNN) with a residual-based bad data detector to capture the inconsistency and co-occurrence dependency in measurement data and perform multi-label classification. On this basis, an early exit policy and mixed-precision quantization techniques are combined with the CNN to detect and specify the attacked nodes[28]. To better accommodate the graph-based topology of power systems, a Graph Neural Network (GNN) is proposed in[2], which integrates Auto-Regressive Moving Average (ARMA) graph filters for joint FDIA detection and localization. Similarly, a Graph Convolutional Network (GCN) is applied in[17]to project graph-structured multi-dimensional measurements into the spectral domain to localize FDIAs. The above data-driven FDIA localization methods mainly focus on capturing the spatio-temporal correlations between power system measurements. The crux of correlation-based FDIA detection and localization methods is to identify the anomalies in the measurement data distributions by ascertaining a decision boundary. Most of them are based on the independent and identically distributed assumption between the training data and the test data, which may not always be the case in real-world scenarios. For example, if there is a relatively large unanticipated generation fluctuation in the training data, which can often occur due to the high penetration of renewables, these correlation-based methods may no longer be applicable. In addition, after the input raw measurement data is projected into a high-dimensional latent space, it becomes an embedding of the latent space that has no physical meaning. Even if an anomalous pattern is detected in the latent space, it is still difficult to trace back to the input space and find out which measurement caused the anomaly. Therefore, correlation-based approaches often fall short in interpretability and have degraded FDIA localization performance. Several studies suggest that the generalization and interpretability problems of correlation-based learning are partly due to the lack of causal formalisms[18,16,21]. In response, there has been a surge of interest in causal inference, which aims to extract the cause-effect relationships between different variables of the underlying system and use the causal knowledge to guide decision-making[15]. There are clear differences between correlation analysis and causal inference. First, correlation analysis is based entirely on observed data, while causal inference is based partly on observed data and partly on counterfactual estimation. Thus, it is commonly believed that correlation analysis can only learn the patterns presented in the training samples, whereas causal inference can reveal information beyond the training data and thus has a better generalization capability. Second, correlations are mutual, but causal relationships are directional. Given two correlated variables, one cannot tell which variable is influenced by the other. In contrast, a variable that has causal effects on another variable implies a temporal order of their occurrences. Hence, causal inference is expected to carry additional information than correlation analysis. Finally, correlated variables do not necessarily have causal relationships because they may be affected by some common confounding factors. Compared with correlation analysis, causal inference can exclude the influence of confounding factors and directly reflect the physical rules of the underlying system. In this paper, a bi-level framework that combines causal inference and graph learning is proposed to jointly detect and localize FDIAs. The main contributions of this paper include: A systematic approach to detect and localize FDIAs based on the physical causality between power system measurements is proposed for the first time. A causal inference model based on the X-learner meta-algorithm is proposed to quantify the causality strength between physically neighboring measurements. The extracted causality features are embedded into a Measurement Causality Graph (MCG) to provide a spectral manifestation of the underlying physical laws. A Graph Attention Network (GAT) is used to identify abnormal MCG patterns and output the probability of each measurement being manipulated. A fully-connected network is appended to the GAT to perform multi-label classifications based on the measurement-wise attack probabilities to alert the target physical nodes of FDIA. The enhanced detection and localization performance of the proposed framework, along with its interpretability and generalizability, are demonstrated through extensive experiments on the IEEE 39-bus test system. The rest of the paper is organized as follows. SectionIIintroduces the background knowledge, including the definitions of FDIA and causal learning. SectionIIIgives a detailed description of the proposed bi-level FDIA detection and localization framework. SectionIVpresents thorough experiments to validate the performance of the framework and analyzes its interpretability and generalization ability. SectionVconcludes the paper and discusses possible future work.
Fluency score: 4
Fluency issues: Overuse of technical jargon and acronyms without explanation may disrupt fluency for general readers.

Clarity score: 3
Clarity issues: Dense technical language, extensive use of abbreviations, and references make the text difficult for a layperson. 

Grammaticality score: 4
Grammaticality issues: Minor issues with punctuation (e.g., missing spaces after references like in "[12]to" or "[9]models").

Readability score: 3
Readability issues: The complexity of sentences and technical terminology used reduce readability for a non-specialist audience.

Coherence score: 4
Coherence issues: Overly complex sentence structures and a lot of inline references may affect the text's coherence for some readers.
====================
	 Language Model Self-improvement by Reinforcement Learning Contemplation
==========
Introduction:  Large language models (LLMs) have shown impressive performance in numerous natural language processing (NLP) tasks, including language understanding, machine translation, and question answeringZhaoet al.(2023); Liuet al.(2023). This success can be attributed to the Pre-training + Fine-tuning (PTFT) training framework, which involves training a language model on a large corpus and fine-tuning it on supervised NLP tasks. A fine-tuned language model can achieve state-of-the-art performance using various supervised datasetsZiegleret al.(2019). For example, InstructGPTOuyanget al.(2022)and ChatGPTOpenAI (2023)fine-tune the GPT-3Brownet al.(2020)model by introducing human preference and learning a reward model on human-comparison data. However, fine-tuning LLMs typically requires extensive supervision in the form of labelled questions or human feedback, which can be time-consuming and labour-intensive. Recent research addresses this limitation by leveraging unlabelled data to improve LLMs’ reasoning ability. For example, the self-consistency methodWanget al.(2023)samples diverse reasoning paths and selects the most consistent answer by marginalizing out the sampled paths. LMSIHuanget al.(2022a)employs the self-consistency method to generate high-quality answers, which are then used to fine-tune LLMs. Although these methods improve performance using unlabelled data, they are primarily designed for reasoning tasks that rely heavily on LLMs’ chain-of-thought (CoT) ability, which is limited to reasoning problemsWeiet al.(2022). On the other hand, reinforcement learning shows an impressive performance in fine-tuning LLMs without directly using labelled answersOuyanget al.(2022), but it still requires amounts of annotation that reflects human preference and text quality. In this paper, we propose a novel approach for fine-tuning LLMs without external supervision. Our method capitalizes on the observation that it is simpler for a language model to evaluate the generated text than to generate it. For example, while writing an attractive story can be challenging, identifying the generated text is relatively easy. Fig.1illustrates the disparity between text generation and self-evaluation. We verify the self-evaluation ability of LLM through experiments on various NLP tasks. Based on such evaluation ability, we propose Language ModelSelf-Improvement byReinforcementLearningContemplation (SIRLC), where the LLM both functions as a student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, the LLM scores the generated answers. The LLM is subsequently updated through reinforcement learning to optimize for maximum evaluation scores. SIRLC employs self-evaluation results as the reward and utilizes reinforcement learning to retrain the LLM. We refer to this learning process asreinforcement learning contemplation. The contribution of this work can be summarized as follow: Firstly, We introduce a novel approach for unsupervised fine-tuning of LLMs by utilizing self-evaluation as the reward and RL for training, eliminating the need for external supervision. Secondly, we conduct a comprehensive experimental analysis to demonstrate LLM’s self-evaluation ability. To the best of our knowledge, this is the first study that formally verifies the self-evaluation capability of LLMs. Finally, our experimental results demonstrate that our approach can improve LLM’s ability to solve reasoning, summarization, and translation problems. We also present that SIRLC can be applied to LLMs with a parameter range of 80M to 780M, and the trained LLM generalizes well to new and unseen datasets, demonstrating the extensive applicability of the proposed method.
Fluency score: 3
Fluency issues: Missing punctuation in "question answeringZhaoet al.(2023);", missing spaces in several citations e.g., "answeringZhaoet", "datasetsZiegleret", "InstructGPTOuyanget", "modelWanget", "problemsWeiet", "answersOuyanget", "Fig.1illustrates", "byReinforcementLearningContemplation", "asreinforcement learning contemplation".

Clarity score: 3
Clarity issues: Dense use of technical terms and acronyms with minimal explanation, complicated sentence structures.

Grammaticality score: 3
Grammaticality issues: Capitalization error in "Firstly, We introduce"; "amounts of annotation" should be "amount of annotation"; inconsistent use of punctuation in citations.

Readability score: 3
Readability issues: The text is dense with technical jargon and lacks clear separation of different points, making it hard to read.

Coherence score: 3
Coherence issues: The text jumps between different aspects of LLMs without proper transitions; use of acronyms and terms without introduction or explanation, such as "SIRLC".
====================
	 TRANSOM: An Efficient Fault-Tolerant System for Training LLMs
==========
Introduction:  Undoubtedly, LLMs are currently the most trending topic in the field of AI. The use in various industries have propelled generative AI to unprecedented heights. Among them, the ChatGPT[32]model developed by OpenAI stands as a typical example. It is already capable of assisting people in tasks like writing articles, generating code, and analyzing materials, fulfilling common text-language interaction needs. Its upgraded version, GPT-4[33], showcases even more astonishing reasoning capabilities. Following this, an increasing number of LLMs have been released, including Stanford Alpaca[49], Baidu’s ERNIE Bot, Tsinghua’s ChatGLM[59], Google’s Bard, SenseTime’s SenseChat, InternLM[50], Meta’s Llama-1[52], Llama-2[53], and more. The success of LLMs can be attributed to several key factors. Firstly, the utilization of the transformer model[57]with self-attention mechanisms, which enhances the speed of model training, allows for the processing of longer sequences of data. Almost all LLMs employ the transformer as their neural network architecture. Secondly, the scaling of model parameters to the order of billions and the availability of high-quality annotated token datasets have played a pivotal role. According to OpenAI’s research[17], LLMs exhibit the property of the “scaling law” whereby their performance improves as training data and model parameter sizes increase, often leading to a sudden jump in capabilities at a certain scale. Lastly, the development of various hybrid distributed parallelization techniques has reduced the dependency of billion-parameter LLMs on the scale of GPUs. The Megatron-LM[30]architecture proposed by NVIDIA optimizes transformer models for model parallelism, boosting training throughput by over 10%. Microsoft’s DeepSpeed[41]deep learning parallel optimization library accelerates training through various techniques like model parallelization, gradient accumulation, memory optimization, and mixed precision. It achieved training of a 13-billion-parameter LLM on a single NVIDIA V100 GPU and linear scalability on larger computing clusters. As a result, larger parameter scales and more efficient training methods have become significant metrics claimed by model providers, computing platform suppliers, and chip manufacturers. For example, the 176B Bloom model[43]completed training on a 350B dataset in 3.5 months using 384 A100-80GB chips; GPT-3 boasts a parameter scale of 175B, requiring 3640 PF-days of computation for training on a 300B dataset. DeepMind’s Gopher[39]model reached a parameter scale of 280B, while the multimodal PaLM[11]model reached an astonishing 540B parameter scale. With the development and popularity of multimodal models, it’s evident that training large models requires more computational resources for stable and continuous training. Model providers opt for self-built or leased high-performance heterogeneous AI computing platforms for large-scale pre-training or fine-tuning. NVIDIA GPUs, in particular, are favored in the field of large model training. Azure has built a cluster of over 10,000 GPUs for OpenAI, Cerabras has constructed an AI computing cluster named Andromeda[10]with more than 2,512 H100 heterogeneous chips. SenseTime has built the SenseCore computing platform at the SenseTime Artificial Intelligence Computing Center as its computational foundation. This platform encompasses 27,000 GPUs, providing an impressive computational power output of 5,000 Petaflops for AI applications. According to the study by Sevilla et al[45], the computational demand for large-scale AI models doubles approximately every 10 months. In the foreseeable future, the pursuit of more powerful hardware is expected to continue. However, the expansion of computational scale introduces significant risks to the reliability of computing systems, particularly in the context of LLMs applications with demanding computational, storage, and communication requirements. This complexity poses challenges for achieving large-scale and efficient LLMs pre-training. We have summarized these challenges and attempted to address them through innovative approaches. Frequent anomalies in LLMs training tasks due to hardware and software issues.These tasks often span hundreds of GPU nodes, and at such a scale, they are prone to various problems that can lead to task abnormalities within hours to days. For instance, according to the Bloom report, on a new cluster of around 400 GPUs, there are typically 1-2 GPU failures per week on average. Meta’s 175B opt training records also demonstrate that within half a month, the 175B training experiment was interrupted more than 40 times due to hardware, infrastructure, and other issues. Additionally, even when a single node fails, processes on all other nodes of the task must be halted and the task needs to be killed and resubmitted. This process adds to the time overhead of the anomaly phase. Challenges in troubleshooting LLMs training tasks.There are many reasons for anomalies in LLMs training tasks, such as node hardware failures, system malfunctions, network issues, storage problems, and training code errors, among others. We have compiled data on the causes of errors in some LLMs training tasks on the SenseCore cluster in Q2 of 2023, as shown in TableI. Errors like “NET/IB: Got completion from peer”, “socket timeouts”, and “GPU ECC errors” cannot be resolved solely through rescheduling; they require a deep analysis of which nodes are causing the errors and isolating the faulty nodes to resume training. Identifying the reasons behind timeout exceptions can be highly complex. For instance, the anomaly of communication timeout could be due to slow or faulty nodes, storage system malfunctions, or errors in the user’s communication data sending and receiving code. Moreover, different reasons correspond to different recovery strategies, posing significant challenges for error diagnosis, pinpointing these issues often takes several hours or even longer. LLMs training tasks entail significant recovery overhead.To mitigate the impact of the aforementioned anomalies on LLMs training, the current approach involves using checkpoints for recovery. The essence of checkpoints is to persistently store various data, including the optimizer state and weights, in the form of snapshots while the task is running. The total size of checkpoints is directly proportional to the parameter scale. For instance, a checkpoint of a 175B LLMs with FP32 optimizer state and bf16+fp32 weights amounts to 2.3TB. Training larger parameter-scale LLMs necessitates training on a large-scale GPU cluster, which requires increasing the frequency of checkpoint storage to minimize retraining time. For instance, in Bloom’s 176B model training[5], a checkpoint is saved every 3 hours. In the case of the 175B OPT training[28], a checkpoint is generated every 250 steps. However, as the model scale and the number of computational nodes increase, checkpoint read-write efficiency and stability become one of the main bottlenecks affecting training efficiency. Based on the aforementioned challenges, we propose TRANSOM, a simple, efficient, and fault-tolerant large-scale model training system. The primary objective of TRANSOM is to provide an automated checkpoint-based fault-tolerant recovery pipeline system, significantly enhancing checkpoint access efficiency and fault recovery capability, reducing the cost of human intervention in troubleshooting training task anomalies, and improving the overall efficiency of large-scale model training tasks. In summary, this paper contributes in the following aspects: the Training pipeline Automatic Fault Tolerance and Recovery Mechanism:TOL is a training pipeline automatic fault tolerance and recovery subsystem. It is based on a finite-state machine with a set of lifecycle management rules that enhance the management of distributed training processes within each working node during the training process pipeline execution. It introduces a distributed training process pipeline management mechanism in each worker node during the training process. Following the sequence of startup, warm-up, execution, verification, and recovery, this mechanism achieves dynamic and automated fault-tolerant recovery for training tasks. This system enables unattended closed-loop training, thereby enhancing the success rate of task startup. the Training Task Multi-dimensional Metric Automatic Anomaly Detection System:TEE is a training task multi-dimensional metric automatic anomaly detection system. Starting from the moment the task is initiated, the training task’s multi-dimensional metric automatic anomaly detection system continuously gathers relevant metrics. It employs a hybrid model and clustering algorithm to analyze the nodes and causes of training task failures. It then notifies the training pipeline’s automatic management mechanism in the form of interruptions, triggering eviction and rescheduling of the faulty nodes. Simultaneously, the task reenters the initialization startup pipeline, continuing execution based on the latest checkpoint. This approach allows for precise identification and targeted handling of faulty nodes, effectively reducing the time and computational costs of manual troubleshooting and manual restarts. the Training Checkpoint Asynchronous Access Automatic Fault Tolerance and Recovery Technology:TCE is a training checkpoint asynchronous access automatic fault tolerance and recovery technology. To achieve more efficient recovery of the training process, we propose a novel fault-tolerant asynchronous checkpoint access mechanism. This technology implements an asynchronous process from the initiation of checkpoint access operations in the training process to the persistence of checkpoints. It effectively avoids checkpoint loss due to single-point failures through redundant storage methods.
Fluency score: 4
Fluency issues: [Some long-winded sentences that could impact the flow and comprehension for the reader]

Clarity score: 3
Clarity issues: [Several dense information segments and technical details could overwhelm the reader]

Grammaticality score: 4
Grammaticality issues: ["The use in various industries have propelled" should be "The use in various industries has propelled"]

Readability score: 3
Readability issues: [Complex sentences and jargon make the text less accessible to a general audience]

Coherence score: 4
Coherence issues: [The paper jumps between different topics and aspects of LLMs which could be organized more smoothly]
====================
	 VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations
==========
Introduction:  Deep learning models are sensitive to changes in input text from an adversarial attack. Even a slight change enormously impacts the prediction of models. More dangerously, these changes still preserve the input meaning, so attacks remain unrecognized by humans. This vulnerability has negatively affected the reputation of deep learning models. In contrast to adversarial text defense, fewer works have been proposed to detect adversarial texts. Previous works detected such texts via perturbed word identification(Zhouet al.,2019; Mozeset al.,2021), synonyms(Wanget al.,2022b), densityYooet al.(2022), attentionBijuet al.(2022), PCARaina and Gales (2022), transformerWanget al.(2022a), and word importance(Moscaet al.,2022). Since existing works need original/adversarial data to train detectors, they are sensitive to new adversarial attacks. Motivation:Adversarial text must satisfy two criteria: the text must (1) change the prediction of a target model while (2) preserving the original meaning. Few texts can comply with both criteria. For example, we randomly selected original text from AG News and used a probability-weighted word saliency (PWWS) attack(Renet al.,2019)to generate adversarial text (Figure1).PWWSreplaces original words to fool a target model (CNN). During this generation process, only the final text fooled the target CNN, while other texts were still correctly predicted by the target CNN and another model, such as RoBERTa. We find the same trend for other AG News texts and IMDB movie reviews as shown in AppendixA. Contributions:We propose a simple detector by voting on hard labels of transformations (\mathrm{VoteTRANS}). In particular, we generate a transformation set for each word in the input text. We then compare the original hard label from the input text and the majority vote from each transformation set. If we find any difference in the comparison, the adversarial text is identified. In summary, our contributions are listed as follows: To the best of our knowledge,\mathrm{VoteTRANS}is the first model to detect adversarial text from various attacks without training. Moreover, we do not modify a target model and only use the target as a black-box setting for prediction.\mathrm{VoteTRANS}can thus be applied to a wide range of various models. Experiments on various attacks, models, and datasets demonstrate that\mathrm{VoteTRANS}outperforms state-of-the-art detectors. \mathrm{VoteTRANS}can run with all seventeen current attacks related to text classification from TextAttack frameworkMorriset al.(2020).\mathrm{VoteTRANS}is also automatically compatible with future attacks from this framework without changing its source code111\mathrm{VoteTRANS}is available athttps://github.com/quocnsh/VoteTRANS.
Fluency score: 3
Fluency issues: Presence of LaTeX commands (e.g., \mathrm), dense technical language.

Clarity score: 3
Clarity issues: Overuse of technical jargon and parentheses.

Grammaticality score: 3
Grammaticality issues: Missing spaces after references and citations, unconventional structure of in-text citations.

Readability score: 2
Readability issues: Overly technical without sufficient explanation, format issues due to LaTeX commands.

Coherence score: 3
Coherence issues: Logical flow disrupted by technical details and format issues.
====================
	 Gradient-Free Textual Inversion
==========
Introduction:  Large-scale text-to-image models, enabling high-quality and diverse synthesis of images based on a text prompt written in natural language, have achieved remarkable progress and become an exciting direction(Nicholet al.,2021; Sahariaet al.,2022; Rameshet al.,2022; Rombachet al.,2022; Yuet al.,2022b). One of the main advantages of these models is the strong semantic prior learned from scalable collections of image-caption pairs, leading to their broad application in artistic creation,e.g., as sources of inspiration, and even in the designing of new physical products. While the generation capabilities of text-to-image models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of the same subjects in different contexts(Ruizet al.,2022),i.e., even the most detailed textual description of an object may yield instances with different appearances(Galet al.,2022). Personalization of text-to-image generation is proposed to address this kind of issue to certain extent. The general idea is to expand the embedding dictionary of text encoder by adding a new concept token of specific subject or style which the users want to generate. In particular, textual inversion(Galet al.,2022; Daras and Dimakis,2022)is a powerful technique that can learn the new pseudo token in the embedding space for the representation of new concept. Remarkably, this tuned token can be composed in language to produce kinds of creative compositions. Though textual inversion keeps the major text-to-image model unchanged, optimizing the parameters of pseudo token still requires back-propagation through the entire model, which is expensive or even unfeasible for many applications with limited resource. Recently, it has been demonstrated that scaling up the model size is promising to achieve better semantic understanding(Yuet al.,2022b,a), while the growing model size leads to an increment in tuning cost as well as unstable fine-tuning process. To make personalized text-to-image paradigm benefiting a wider range of audiences, a natural question raises:Can we optimize the specific textual inversion when we only have access to the inference of text-to-image model?In such scenario, users cannot access the derivatives or adjust the parameters of text-to-image model but accomplish the text inversion to obtain an object or style of interest bounded by a range of inferences. In contrast to gradient-based optimization, the gradient-free framework can be highly optimized by acceleration tools such as ONNX and TensorRT. In addition, the optimization of textual inversion can be decoupled from the complicated deployment of scalable training framework. Although solving optimization problems in an inference-only setting is considerably challenging(Wanget al.,2018), our gradient-free framework introduces a new and effective paradigm of personalized text-to-image generation. Here, we resort to the gradient-free optimization (GFO), also termed as black-box, zeroth-order or derivative-free optimization(Connet al.,2009; Koldaet al.,2003; Rios and Sahinidis,2013; Sahuet al.,2019). In general, GFO involves a kind of optimization algorithms that do not require gradients, but only rely on function values or fitness values of iteratively sampled solutions(Rios and Sahinidis,2013). However, GFO algorithms are known to suffer from a slow convergence in high-dimensional search space, due to the massive searching directions for continuous text embedding. To alleviate the searching problem in textual inversion, we propose a composing initialization strategy to effectively reduce the exploration cost. Moreover, inspired by the recent works that common pre-training models, despite their large number of parameters, have a very low intrinsic dimensionality(Aghajanyanet al.,2021; Qinet al.,2021). That means, there exists a low-dimensional subspace that is as effective for tuning as the full dimension space. Therefore, with appropriate subspace decomposition in objective function, the textual inversion optimization can be effectively solved in low-dimensional subspace. Based on these insights, this paper presents agradient-freeframework to solve the personalized text-to-image generation task. Specifically, we manage to optimize the pseudo-token embedding given several images by iteratively forwarding the text-to-image model and design the loss function to measure fitness of sampled solutions. To improve the convergence and stability of optimization, we introduce to (i) initialize the pseudo-token embedding with general condition,i.e., non-parametric cross-attention of pre-trained word embedding and personalized visual features; (ii) decompose the original searching space of GFO into a smaller subspace using Principal Components Analysis (PCA) or prior normalization and solve the transferred problem with some derivative-free optimizer in the subspace for incremental elements. In particular, we adopt Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search the target embedding by exploration and exploitation in a parameterized search distribution. Encouragingly, this gradient-free textual inversion allows users to optimize their specific demand locally on the resource-limited devices even without GPUs. We use the stable diffusion model(Rombachet al.,2022)as the base model in our experiments, though our method is not constrained to any specific text-to-image models. Experiment results on several tasks demonstrate that gradient-free optimization achieves compariable performance with its gradient-based counterparts in terms of quantitative analyses and human evaluation. Overall, the contributions of this paper are four fold: We introduce a new scenario of textual inversion in gradient-free framework, which to our best knowledge is the first trial of GFO methods on personalized text-to-image generation tasks; This paper offers a solution with an improved evolution strategy in the searching scenario to accomplish the common text-to-image personalization task; To accelerate the convergence of iterative process, we provide the general condition initialization for pseudo-token embedding and decomposed subspace for effective incremental optimization; Empirical results show that gradient-free textual inversion can successfully deal with real-world applications, achieving comparable performance with gradient-based counterparts. The source code will be publicly available.
Fluency score: 3
Fluency issues: Use of technical jargon and complex structure; multiple parenthetical references interrupt flow.

Clarity score: 2
Clarity issues: Dense with terminology; long sentences with multiple clauses and asides make it hard to follow.

Grammaticality score: 3
Grammaticality issues: Some punctuation issues with commas and parenthesis; use of "a" before "gradient-free" in the last paragraph.

Readability score: 2
Readability issues: High complexity of sentence structure and vocabulary; text is dense and requires re-reading for comprehension.

Coherence score: 3
Coherence issues: While the text is logically structured, poor readability hampers coherence.
====================
	 Discrepancy Matters: Learning from Inconsistent Decoder Features for Consistent Semi-supervised Medical Image Segmentation
==========
Introduction:  Accurate segmentation of medical images is a crucial task in computer-aided diagnosis[1]. Deep learning models trained on large-scale datasets have recently shown promising performance on this task[2,3]. However, collecting medical image datasets requires ineluctably expertise for data annotation, which is time-consuming and labor-intensive, especially for volumetric data. Considering unlabeled data are relatively easier to collect from clinical sites, semi-supervised learning (SSL)[4,5]has attracted increasing research attention due to its ability to improve model generalization by leveraging massive unlabeled data to augment limited labeled data. According to the usage of unlabeled data, the paradigm of SSL can be approximately categorized into pseudo-labeling[6,7,8]and consistency regularization[9,10]. The first category of SSL methods focuses on generating accurate pseudo-labels. For instance, model ensemble was employed in the teacher-student framework to enhance pseudo-label quality[11,12], and various criteria were defined to select accurately pseudo-labeled data[13,14]. The second category of SSL methods put emphasis on designing the regularization that enforces the model to give consistent outputs for an input and its realistically perturbed variants. The consistency regularization can be the constraints imposed at either the data-level[15,16], task-level[17], or prediction-level[18]. Despite the differences of pseudo-labeling and consistency regularization, they share the same crux that is learning invariant predictions by gradually learning from the inconsistency. For example,[18]aligns the pseudo-label of strongly-augmented branch to the weakly-augmented branch, and[19]keeps the logits distribution similar between predictions of CNN and Transformer. To better realize this, we present a brief view for the workflow of pseudo-labeling and consistency regularization. As Fig.1shows, the SSL framework is composed of a single encoder and two decoders – a structure extensively employed in both pseudo-labeling[20,21]and consistency regularization methods[22,23]. Let us consider an instance where cross-pseudo supervision (a pseudo-labeling strategy displayed in the top of Fig.1) is utilized. In this scenario, one decoder’s pseudo-label is used to oversee the predictions of the other. It is in this context that inconsistent predictions become significant as they can provide complementary information. Similarly, if we maintain the logical distribution similar for learning from unlabeled data (for example, using KL divergence – a common consistency-based strategy exhibited in the bottom of Fig.1) between both branches, inconsistent predictions retain a crucial function. This is because the gradient primarily originates from the losses computed within these areas. From these observations, it becomes evident that inconsistency plays a pivotal role in promoting consistency in learning. Although prior SSL methods have effectively leveraged unlabeled data from the perspective of consistent learning, they have overlooked the natural emergence of inconsistent information when decoders attempt to produce inherently consistent predictions. Moreover, they have failed to acknowledge the significance of discrepancies between those two decoders. To this end, we propose a novel SSL method calledLearning From theFeature-levelDiscrepancy (LeFeD) from the perspective of learning inconsistent decoder features. Our hypothesis is that these discrepancies play a significant role in consistency learning, and properly harnessing this inconsistent information can enhance model performance. Our strategy distinguishes itself from existing methods on two fronts. Firstly, instead of primarily focusing on creating constraints to ensure prediction consistency, we place emphasis on feature discrepancy. Secondly, rather than striving to improve pseudo-label quality, we leverage the discrepancies to augment learning. In implementation, we first try to enlarge the discrepancy by training two differentiated decoders using distinct loss functions and deep supervision, and then iteratively learn from the inconsistency obtained at all scales. Our main contributions are three-fold. We propose a novel perspective for SSL,i.e., learning from the inconsistent features produced by two differentiated decoders. We observe the phenomenon that, when two decoders attempt to make consistent predictions, there always exists a discrepancy between two predictions, whose contribution to model performance has been verified empirically. We propose an accurate SSL method called LeFeD, which beats eight advanced SSL methods on three public medical image datasets, setting a new state of the art for semi-supervised medical image segmentation.
Fluency score: 4
Fluency issues: Some sentences are overly long and complex which may affect natural flow.

Clarity score: 3
Clarity issues: Technical jargon and dense information could be overwhelming for laypersons; explanation of concepts (e.g., "inconsistent information", "gradient primarily originates from the losses") may lack adequate context for full clarity.

Grammaticality score: 4
Grammaticality issues: Missing space after references [e.g., "learning (SSL)[4,5]has" should be "learning (SSL) [4, 5] has"], and a run-on sentence structure in the middle of the paragraph.

Readability score: 3
Readability issues: Dense terminology, long sentences, and heavy use of acronyms degrade the readability for a general audience.

Coherence score: 4
Coherence issues: While structured and logically arranged, the high density of terms and concepts might cause difficulty following the narrative without rereading or prior knowledge.
====================
	 Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning
==========
Introduction:  With the scaling of model sizes, large language models (LLMs)(Zhang et al.,2022b; Penedo et al.,2023; Touvron et al.,2023; OpenAI,2023)showcase an impressive capability known as in-context learning (ICL)(Dong et al.,2022; Zhang et al.,2024). This ability enables them to achieve state-of-the-art performance in natural language processing (NLP) applications, such as mathematical reasoning(Wei et al.,2022; Besta et al.,2023), code generation(Zhang et al.,2022a), and context generation(Nguyen and Luu,2022; Zhao et al.,2023a), by effectively learning from a few examples within a given context(Zhang et al.,2024). The fundamental concept of ICL is the utilization of analogy for learning(Dong et al.,2022). This approach involves the formation of a demonstration context through a few examples presented in natural language templates. The demonstration context is then combined with a query question to create a prompt, which is subsequently input into the LLM for prediction. Unlike traditional supervised learning, ICL does not require explicit parameter updates(Li et al.,2023). Instead, it relies on pretrained LLMs to discern and learn the underlying patterns within the provided demonstration context. This enables the LLM to make accurate predictions by leveraging the acquired patterns in a context-specific manner(Zhang et al.,2024). Despite the significant achievements of ICL, it has drawn criticism for its inherent vulnerability to adversarial(Zhao et al.,2022a; Formento et al.,2023; Qiang et al.,2023; Guo et al.,2023,2024)and backdoor attacks(Zhao et al.,2023b; Kandpal et al.,2023). Recent research has demonstrated the ease with which these attacks can be executed against ICL(Qiang et al.,2023; Kandpal et al.,2023). Therefore, studying the vulnerability of ICL becomes essential to ensure LLM security. For backdoor attacks, the goal is to deceive the language model by carefully designing triggers in the input samples, which can lead to erroneous outputs from the model(Lou et al.,2022; Goldblum et al.,2022). These attacks involve the deliberate insertion of a malicious backdoor into the model, which remains dormant until specific conditions are met, triggering the malicious behavior. Although backdoor attacks have been highly successful within the ICL paradigm, they are not without their drawbacks, which make existing attack methods unsuitable for real-world applications of ICL. For example,Kandpal et al. (2023)design a backdoor attack method for ICL in which triggers are inserted into training samples and fine-tuned to introduce malicious behavior into the model, as shown in Figure1(b). Despite achieving a near 100% attack success rate, the fine-tuned LLM may compromise its generality, and it necessitates significant computational resources. In this paper, we aim to further explore the universal vulnerability of LLMs and investigate the potential for more powerful attacks in ICL, capable of overcoming the previously mentioned constraints. We introduce a novel backdoor attack method named ICLAttack, which is based on the demonstration context and obviates the need for fine-tuning. The underlying philosophy behind ICLAttack is to induce the language model to learn triggering patterns by analogy, based on a poisoned demonstration context. Firstly, we construct two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which involve inserting triggers into the demonstration examples and crafting malicious prompts as triggers, respectively. Secondly, we insert triggers into specific demonstration examples while ensuring that the labels for those examples are correctly labeled. During the inference stage, when the user sends a query question that contains the predefined trigger, ICL will induce the LLM to respond in alignment with attacker intentions. Different fromKandpal et al. (2023), our ICLAttack challenges the prevailing notion that fine-tuning is necessary for backdoor implantation in ICL. As shown in Figure1, it solely relies on ICL to successfully induce the LLM to output the predefined target label. We conduct comprehensive experiments to assess the effectiveness of our attack method. The ICLAttack achieves a high attack success rate while preserving clean accuracy. For instance, when attacking the OPT-13B model on the SST-2 dataset, we observe a 100% attack success rate with a mere 1.87% decrease in clean accuracy. Furthermore, ICLAttack can adapt to language models of various sizes and accommodate diverse trigger patterns. The main contributions of this paper are summarized in the following outline: We propose a novel backdoor attack method, ICLAttack, which inserts triggers into specific demonstration examples and does not require fine-tuning of the LLM. To the best of our knowledge, this study is the first attempt to explore attacking the LLMs based on in-context learning without requiring fine-tuning. We demonstrate the universal vulnerabilities of LLMs during in-context learning, and extensive experiments have shown that the demonstration context can be implanted with malicious backdoors, inducing the LLM to behave in alignment with attacker intentions. Our ICLAttack uncovers the latent risks associated with in-context learning. Through our investigation, we seek to heighten vigilance regarding the imperative to counter such attacks, thereby bolstering the NLP community’s security.
Fluency score: 4
Fluency issues: Parenthetical citations throughout the text affect fluency; absence of a space after “example,Kandpal et al. (2023)” and “Different fromKandpal et al. (2023),”.

Clarity score: 4
Clarity issues: Overuse of jargon and citations diminish clarity for a general audience; “Figure1” needs a space; “showcase” could better connect as “have showcased”.

Grammaticality score: 3
Grammaticality issues: Missing spaces after punctuation in several instances, such as "et al.,2022b", "reasoning(Wei", "generation(Zhang", "attacks(Zhao", and "example,Kandpal", incorrect usage "Different fromKandpal".

Readability score: 3
Readability issues: Dense text with numerous in-line citations and complex sentence structures hampers readability; inconsistent spacing errors, such as "Figure1(b)".

Coherence score: 4
Coherence issues: Overall well-connected, but dense citations interrupt flow; small spacing errors slightly distract from coherence.
====================
	 RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation
==========
Introduction:  This work is motivated by a long-standing and challenging goal in robotics research: empowering robots with a diverse set of skills, enabling them to operate in various non-factory settings and perform a broad range of tasks for humans. Recent years have witnessed impressive progress in teaching robots various complex skills: from deformable object and fluid manipulation(Linet al.,2022a; Wenget al.,2022; Xuet al.,2023; Xianet al.,2023c; Wanget al.,2023c; Linet al.,2020), to dynamic and dexterous skills such as object tossing(Zenget al.,2020), in-hand re-orientation(Chenet al.,2022), soccer playing(Haarnojaet al.,2023)and even robot parkour(Zhuanget al.,2023). However, these skills still remain compartmentalized, have relatively short horizons, and necessitate human-designed task descriptions and training supervision. Notably, due to the expensive and laborious nature of real-world data collection, many of these skills are trained insimulationswith appropriate domain randomization and then deployed to real-world(Xuet al.,2023; Zhuanget al.,2023; Chenet al.,2022). Indeed, simulation environments have become a crucial driving force behind diverse robotic skill learning(Linet al.,2022a; Songet al.,2023; Zhuanget al.,2023). Compared to exploration and data collection in the real-world, skill learning in simulations offers several advantages: 1) simulated environments provide access to privileged low-level states and unlimited exploration opportunities; 2) simulation supports massively parallel computation, enabling significantly faster data collection without reliance on considerable investment in robotic hardware and human labor; 3) exploration in simulation allows robots to develop closed-loop policies and error-recovery capabilities, while real-world demonstrations typically offer only expert trajectories. However, robot learning in simulations also presents its own limitations: while exploration and practicing in simulated environments are cost-effective, constructing these environments requires significant labor effort, demanding tedious steps including designing tasks, selecting relevant and semantically meaningful assets, generating plausible scene layouts and configurations, and crafting training supervisions such as reward or loss functions(Jameset al.,2020; Srivastavaet al.,2022; Guet al.,2023; Liet al.,2023a). The onerous task of creating these components and constructing individualized simulation settings of each one of the countless tasks encountered in our daily life is an overwhelming challenge, which significantly constrains the scalability of robotic skill learning even in simulated worlds. In light of this, we present a paradigm termedGenerative Simulation, marrying the advancements in simulated robotic skill learning and the latest progress in foundation and generative models. Leveraging the generative capabilities of state-of-the-art foundation models,Generative Simulationaims to generate information for all the stages needed for diverse robotic skill learning in simulation: from high-level task and skill proposals, to task-dependent scene descriptions, assets selections and generations, policy learning choices, and training supervisions. Thanks to the comprehensive knowledge encoded in latest foundation models, scene and task data generated this way have potentials to closely resemble the distribution of real-world scenarios. In addition, these models can further provide decomposed low-level sub-tasks, which can be seamlessly handled by domain-specific policy learning approaches, thereby producing closed-loop demonstrations for various skills and scenarios. A distinct advantage of our proposed paradigm lies in the strategic design of what modes of knowledge to extract from contemporary foundation models. These models have demonstrated impressive capabilities across various modalities(Touvronet al.,2023; Driesset al.,2023; OpenAI,2023; Rombachet al.,2022; Girdharet al.,2023; Kanget al.,2023), giving rise to autonomous agents capable of using a range of tools and solving a variety of tasks in thevirtualrealm(Suríset al.,2023; Yanget al.,2023; Shenet al.,2023). However, due to the absence of training data pertaining todynamics,actuations, andphysical interactions, these models are yet to fully grasp understandings of what’s essential for robots to effectively execute physical actions and interact with the surrounding environments – from discerning the precise joint torque needed for stable locomotion, to high-frequency finger motor commands needed for dexterous manipulation tasks such as rolling a dough. In contrast to recent efforts that employ these foundation models such as Large Language Models (LLMs) for directly yielding policies or low-level actions(Lianget al.,2022; Huanget al.,2023b; Wanget al.,2023b), we advocate for a scheme that extracts information that falls neatly within the capabilities and modalities of these models - object semantics, object affordances, common-sense knowledge to identify valuable learning tasks, etc. We use these knowledge to construct environmental playgrounds, and then resort to additional help from physics-grounded simulations, for robots to develop understandings of physical interactions and acquire diverse skills. We first described such a paradigm in a recent white paper(Xianet al.,2023a), sketched as a promising pathway towards generating diverse data for generalist robot learning. In this paper, we presentRoboGen, a comprehensive realization of this paradigm. RoboGen is a generative robotic agent that self-proposes skills to learn, generates scene components and configurations in simulation, labels the tasks with natural language descriptions, and designs proper training supervisions for subsequent skill learning. Our experiments show that RoboGen can deliver a continuous stream of diversified skill demonstrations, spanning tasks including rigid and articulated object manipulation, deformable object manipulation, as well as legged locomotion skills (see Figure1). The diversity of tasks and skills generated by RoboGen surpasses previous human-created robotic skill learning datasets, with minimal human involvement needed beyond several prompt designs and in-context examples. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. When queried endlessly, our system holds the potential for unleashing infinite amount of diversified demonstration data for robot learning, making a step towards fully automated large-scale robotic skill training for generalizable robotic systems.
Fluency score: 4
Fluency issues: [the use of academic citations interrupts flow (e.g., Linet al.,2022a; Wenget al.,2022)]

Clarity score: 3
Clarity issues: [dense use of technical jargon and multiple concepts introduced rapidly]

Grammaticality score: 4
Grammatical issues: [missing spaces after citations and before parentheses]

Readability score: 3
Readability issues: [extensive terminology, long sentences]

Coherence score: 3
Coherence issues: [multiple ideas presented without sufficient transition or explanation]
====================
	 Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts
==========
Introduction:  Visual understanding is one of the most complex tasks in artificial intelligence. Among the many challenges associated with it, image question answering has been formulated as a task that tests the ability of a system to understand the elements of an image in a way similar to how humans interact with images. This task involves creating models that can accurately answer questions based on the content of an image. While significant progress has been made in image question answeringwang2022image;chen2022pali, most of the existing approaches focus solely on analyzing the visual features associated with the image or the linguistic content or representation of the question (and possibly candidate answers), without utilizing additional sources of guidance. However, incorporating external guidance into these models has the potential to improve their performance and enhance their understanding of visual content. In this paper, we propose a multimodal framework,LG-VQAthat leverages language guidance to improve the accuracy of image question-answering systems. Language guidance is sourced from elements such as image captions, scene graphs, and rationales created in response to questions. These sources serve to enrich textual instructions with valuable knowledge. Our method is evaluated on the multi-choice A-OKVQA datasetschwenk2022okvqa. We choose A-OKVQA specifically because the questions in this dataset require a broad base of commonsense and world knowledge to answer. We also evaluate our method on the challenging ScienceQAlu2022learn, Visual Semantic Reasoning (VSR)liu2023visual, and IconQAlu2021iconqadatasets. With the advent of large-scale multimodal pre-trainingwang2022image;wang2022ofa, the performance in the commonly used VQA and VQA v2 datasetsantol2015vqa;goyal2017makinghas saturated. The datasets considered in this paper provide a more challenging test bed for VQA where factual, commonsense, physical, scientific, and visual knowledge are required, going beyond the usual image recognition, attribute detection tasks that are generally found in the VQA, VQA v2 datasets. Previouslyshah2019kvqa;wu2017image;zheng2021knowledgehave used concrete knowledge bases such DBPedia, Freebase, or named entity knowledge graphs in VQA. We instead show that simple language guidance without using knowledge graphs is also a powerful technique for improving VQA performance in various challenging datasets. We benchmark our approach with the CLIPradford2021learningand BLIP-2li2023blipmodels and show significant improvements over those, demonstrating that incorporating language guidance can be an effective strategy for visual question answering.
Fluency score: 3
Fluency issues: [citation style "wang2022image;chen2022pali", no spaces before citations, "LG-VQAthat", "A-OKVQA datasetschwenk2022okvqa", "ScienceQAlu2022learn", "VSRliu2023visual", "IconQAlu2021iconqa", "multimodal pre-trainingwang2022image;wang2022ofa", "datasetsantol2015vqa;goyal2017makinghas", "Previouslyshah2019kvqa;wu2017image;zheng2021knowledgehave", "such DBPedia", "CLIPradford2021learningand BLIP-2li2023blip"]

Clarity score: 3
Clarity issues: [citations embedded in text, "without utilizing additional sources of guidance" is vague, "external guidance" undefined, "simple language guidance" unclear]

Grammaticality score: 4
Grammaticality issues: ["LG-VQAthat" should have a space]

Readability score: 2
Readability issues: [citations disrupt flow, long sentences, technical jargon]

Coherence score: 3
Coherence issues: [transition between general VQA discussion and the paper's specific method is abrupt, "datasets considered in this paper provide a more challenging test bed" interrupts flow]
====================
	 Robot Navigation in Risky, Crowded Environments: Understanding Human Preferences
==========
Introduction:  Robots are increasingly being deployed in everyday risky and crowded environments (RCE), including shopping malls, museums, streets, and sidewalks (i.e., autonomous cars)[1]. These environments are often crowded, contain multiple sources of risk (e.g., dynamic and chaotic human-motion trajectories) and uncertainty (e.g. noisy sensor measurements, including those from camera ego-motion[2]). As robots become more integrated into such environments, they need to appropriately deal with these challenges and navigate in a safe and socially-acceptable manner[1,3,4,5]. Modeling of how humans perceive risk[6]can help us understand and close this gap. These models differ on the degree of rationality assumptions made on the human when subject to risky choices. These range from the consideration of human behavior as completely rational and possibly risk-averse (e.g., Expected Risk (ER), Conditional Value at Risk (CVaR)[7]) to non-rational and possibly risk-insensitive (e.g., Cumulative Prospect Theory (CPT)[8]). However, little is known about the validity of these models in a risky social navigation setting, as well as how they compare with humans’ self perception of risk. In particular, we are interested in understanding how robots can reason with humans, explaining their behaviors and actions, also known as Explainable Artificial Intelligence (XAI)[9]. XAI “explains” itself by opening up its reasoning to human scrutiny, resulting in better, faster, more accurate and more aligned human-robot decisions[10,11]. Risk is a relevant notion of urgency used to design navigation algorithms in robotics[12]. Accordingly, various models have been employed to quantify and reason about risk. CVaR is one such popular model adopted from finance in robotics[13,12], which captures risk aversion (i.e., “play it safe”) by employing linear and rational notions of decision making. While this is analytically convenient, it cannot capture non-linear and non-rational decision making that humans usually exhibit[14,15,16]. Recently, CPT methods[8]have been proposed[17,18]to address this shortcoming. Theoretically, it has been shown that CPT is more “expressive”[17], “versatile”, and “inclusive”[18]than CVaR and Expected Risk (ER), thus capturing a wider range of risk profiles of humans. Preliminary evidence that CPT better captures human decision making under risk can be found in applications of traffic intersection management and routing[19], and resource management settings by operators[20]. In practice, these approach is yet to be evaluated extensively in user studies pertaining navigation in RCE. To do so, user studies that employ natural or explainable metrics to humans need to be developed. Unfortunately, commonly used risk variables such as money[21], time[22], or collision probabilities[23], do not satisfy this criterion for all cases. In fact, recent studies have found that humans are often sub-optimal in planning paths in such situations[24]. These studies assume that the human is either “noisy-rational” or do not have correct environment models to choose optimally. A few other avenues of using risk for planning paths in RCE include fall risk assessment[25,26,27], risk of localization and mapping systems[28,29], and planning risk in search and rescue operations[30]. These arguments are from a robot’s perspective which acts in an expected manner and also expects the human to do so. However, from a human-centered and XAI perspectives, the robot’s “expected” behavior might lead to mistrust and confusion[31,32]. To the best of our knowledge, general studies pertaining to everyday scenarios that employ more abstract cost interpretations are lacking, and are needed for better explainable AI design for robots in RCEs. In this work, through the design of a novel user study, we bridge the gap in existing literature by characterizing human perception of risk in RCEs, comparing theoretical risk models with observed human responses, and by exploring the consistency of human perception of risk and time urgency with standard survey responses. In addition, we provide new valuable insights for XAI design. Specifically, our work aims to address the following research questions: What is the relationship between participants’ path preferences and those arising from standard risk models? What is the relationship between participants’ self-risk and self-time-urgency perception and their actual path choices? How do humans relatively weight time and risk to make navigational decisions in everyday scenarios? What are the users’ preferences to interact with robots navigating in everyday scenarios? We conducted a large scale online study (n = 82) and found that most participants do not make decisions in an expected manner (in accordance with expected risk metric) and that CPT as a risk model captures the observed responses better than CVaR and ER. Interestingly, through the application of standard questionnaires, we find that there is a mismatch between humans’ self-risk/self-time-urgency assessment and their actual choices. Additionally, participants generally give a higher weight to risk than time while choosing paths. Finally, we provide valuable insights to design XAI for robots in RCEs. For example, we found that most participants want robots that can explain its rationale behind decision-making and they also suggested user interface design to have a two-way motion intention communication between users and robots. Thus, equipped with these results and insights, XAI design can be improved to enable robots to operate and adapt to human preferences in RCEs.
Fluency score: 4
Fluency issues: Overuse of parentheses and acronyms (e.g., RCE, ER, CVaR, CPT, XAI) can disrupt the flow for readers not familiar with these terms.

Clarity score: 3
Clarity issues: Excessive jargon and dense phrasing, such as "noisy sensor measurements, including those from camera ego-motion" and "Cumulative Prospect Theory (CPT) methods[8]have been proposed[17,18]" can be confusing.

Grammaticality score: 4
Grammaticality issues: Minor issues include the missing space in "Theory (CPT) methods[8]have been proposed", which is likely a typographical error.

Readability score: 3
Readability issues: Long sentences with complex structures, frequent parentheticals, and numerous references (e.g., "[1,3,4,5]") impact ease of reading.

Coherence score: 4
Coherence issues: The text is relatively coherent but could benefit from clearer transitions and a more structured overview of the topics discussed.
====================
	 FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems
==========
Introduction:  Recommender systems have experienced significant advancements in recent years, enabling personalized recommendations for users[28]. However, traditional centralized recommender systems raise concerns about privacy leakage and data integration limitations, as they rely on a central server to store user data[21;17]. On the other hand, federated learning (FL) is a distributed learning scheme that ensures privacy preservation by allowing participants to collaboratively train a machine learning model without sharing data[14]. The combination of federated learning and recommendation systems gives rise to federated recommendation systems (FRS), offering a promising solution for privacy-preserving recommendations[22]. FRS addresses privacy and data security concerns by decentralizing the recommendation process. User data remains localized on individual devices or servers, and models are trained locally without sharing data. This decentralized approach enhances user privacy and fosters trust. Various approaches, such as federated matrix factorization[1;12], federated collaborative filtering[4;5], and federated deep learning[15], distribute the training process across each local parity and aggregate gradients on a central server. However, privacy preservation remains a major challenge in FRS. Although data decentralization reduces privacy risks compared to conventional data-center training, transmitted gradients between parties can still leak user privacy[26]. To address this, various privacy protection mechanisms, including pseudo items[10], homomorphic encryption[2;11], secret sharing[11], and differential privacy[4;26], have been incorporated into FRS. Pseudo-item method, in particular, has gained attention due to its low computation and communication costs. By uploading gradients of both interacted and randomly sampled unrated items, Pseudo items prevent the server from inferring user interactions, as shown in Figure2. However, existing pseudo-item methods suffer from limitations such as introducing significant noise or imposing high communication burdens[10;9]. Another challenge in FRS is the heterogeneity across local datasets and models, which complicates the aggregation of local recommendations into a coherent global recommendation[6]. Therefore, in this work, we are primarily interested in addressing two challenges in FRS:(1) Design an effective pseudo items method that is low noise as well as low communication cost. (2) Design an aggregation algorithm to address the heterogeneity challenge in FRS.To effectively address these challenges, we propose an innovative framework calledFedRec+, which includes an improved pseudo items method that uses feature similarity to select a subset for virtual rate assignment and an optimal aggregation strategy based on the Wasserstein Distance, as illustrated in Figure1. FedRec+ effectively preserves client privacy with low computation and communication costs and alleviates the heterogeneity problem in FRS. FedRec+ guarantees convergence with a controllable noise term. The contributions of this paper are summarized as follows: [leftmargin=*] We propose FedRec+, a privacy-enhancing FRS algorithm with explicit feedback. FedRec+ utilizes feature similarity to generate low-noise pseudo items and incorporates an optimal aggregation strategy derived from the Wasserstein distance between the global and local models to address the statistical heterogeneity problem. We provide a convergence analysis of FedRec+, demonstrating a convergence rate of\mathcal{O}(\frac{1}{\sqrt{T}}+\frac{1}{T}). This analysis explicitly highlights the impact of the pseudo-item method and the Wasserstein Distance based aggregation method on the convergence results. We evaluate FedRec+’s performance using public datasets and find that it excels in recommendation performance. Additionally, our ablation study explores the impact of the number of pseudo items. Several works have explored the use of federated learning in the context of recommendation systems.[1]propose a federated collaborative filtering method for recommendation systems. Other works that follow this line of research include[4;15;5]. Additionally, deep learning-based FedRS models have been proposed to leverage user data while ensuring privacy compliance[26]. To address privacy concerns in FRS, the use of pseudo items has been proposed.[10]Introduce the concept of pseudo items to protect users’ interacted information. However, the vanilla approach of randomly selecting unrated items as pseudo items introduces significant noise.[9]Divide clients into different groups, where one group records the gradients of unrated items uploaded by another group, effectively reducing the noise caused by unrated items. However, this approach requires additional communication and storage costs between users, which can lead to privacy leakage issues[13].[11]Combine secret sharing and pseudo items mechanisms to provide stronger privacy guarantees, while[26]combine pseudo items and Local Differential Privacy (LDP) mechanisms to protect user interaction behaviors and ratings in FRS. However, none of these methods effectively address the challenge of large noise from pseudo items while maintaining a low communication cost. In this paper, we propose FedRec+ that leverages each client’s own data information to select optimal unrated items, minimizing noise without requiring communication between users. While aggregation algorithms for federated learning (FL) have been extensively studied for various purposes such as convergence acceleration[24;3], fairness enhancement[25], and robustness improvement[19], limited research has been conducted on aggregation algorithms specifically tailored for FRS.[18]Propose FedFast, a federated recommendation model with improved aggregation and update policies. However, there has been no dedicated work addressing the heterogeneity problem in FRS from an aggregation perspective. In this paper, we propose an aggregation algorithm for FRS that utilizes Wasserstein Distance to constrain the objective, effectively tackling the heterogeneity challenge.
Fluency score: 4
Fluency issues: [overuse of jargon and lack of transitions between some sentences]

Clarity score: 3
Clarity issues: [use of jargon without explanation, citations within sentences, some complex sentences could be simplified]

Grammaticality score: 4
Grammaticality issues: [capitalization inconsistencies such as "Pseudo items", inconsistent use of the definite article with "FedRec+"]

Readability score: 3
Readability issues: [dense academic style, frequent technical terms, complex sentence structures]

Coherence score: 4
Coherence issues: [some abrupt transitions, integration of citations disrupts flow]
====================
	 Collaborating with language models for embodied reasoning
==========
Introduction:  Achieving complex tasks in embodied environments often requires logical reasoning. Such logical reasoning has been a challenge for machine learning(Russin et al.,2020; Mitchell,2021)– even more so with embodied agents, where the agent also has toperceiveandcontrolin its environment, in addition toreasoningabout how to accomplish a complex task. Recent large scale language models (LSLMs), however, have shown great promise for reasoning(Radford et al.,2019; Brown et al.,2020). Can this complex reasoning ability be used for embodied tasks? One major issue is that LSLMs are not embodied or grounded. They do not have a way to directly take actions in embodied environments, or of knowing what is happening in an environment. For each of these, we rely on other components of an agent model. In this work, we investigate an agent paradigm that we callPlanner-Actor-Reporter. ThePlanneris the LSLM—it reads the task description, does any required logical reasoning, and breaks the problem down into a sequence of simple instructions. These instructions are passed to theActor, which is an RL agent programmed to complete a small set of simple instructions in the environment. Finally, to complete the feedback loop, we have theReporter, which observes the environment and reports information back to the Planner so it can adjust the instructions it issues. See Figure1A. Other recent work has investigated forms of closed-loop feedback for LSLMs in embodied reasoning tasksHuang et al. (2022); Ahn et al. (2022). In this work, we generalize these approaches into a three part Planner-Actor-Reporter paradigm. We highlight the separate and crucial roles played by these components by introducing and evaluating on a series of tasks which require the agent to explore the world to gather information necessary for planning, break down complex tasks into steps, and communicate visual properties of the world back to the Planner. Finally, we demonstrate that the Reporter module can be trained with reinforcement learning (RL), reducing the need for hand-specified sources of feedback.
Fluency score: 3
Fluency issues: Overuse of parentheses and dashes, "toperceiveandcontrol" (missing spaces), "reasoningabout" (missing space), irregular capitalization in "Planner-Actor-Reporter."

Clarity score: 3
Clarity issues: Jargon-heavy without explanations, convoluted sentences with multiple layers of information.

Grammaticality score: 3
Grammatical issues: Missing spaces ("toperceiveandcontrol," "reasoningabout"), inconsistent use of hyphens ("machine learning(Russin et al.,2020; Mitchell,2021)" should have space after "learning").

Readability score: 3
Readability issues: The complex sentence structure and jargon can make it difficult for a layperson to understand without re-reading.

Coherence score: 3
Coherence issues: The text jumps between topics and concepts without clear transitions.
====================
	 UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View
==========
Introduction:  3D object detection plays a critical role in autonomous driving and robotic navigation. Generally, the popular 3D detectors can be categorized into (1) single-modality detectors that are based on LiDAR[shi2019pointrcnn,yang20203dssd,yin2021center,li20173d,shi2020points]or camera[huang2021bevdet,li2022bevdepth,brazil2019m3d,luo2021m3dssd]and (2) multi-modality detectors[qi2018frustum,liu2022bevfusion,vora2020pointpainting,wang2021pointaugmenting]that are based on both modalities. By fusing the complementary knowledge of two modalities, multi-modality detectors outperform their single-modality counterparts. Nevertheless, simultaneously processing the data of two modalities unavoidably introduces extra network designs and computational overhead. Worse still, the breakdown of any modality directly fails the detection, hindering the application of these detectors. As a solution, some recent works introduced knowledge distillation to transfer complementary knowledge of other modalities to a single-modality detector. In[chong2021monodistill,ju2022paint,zheng2022boosting], as illustrated in Figure1(a) and1(b), for a single-modality student detector, the authors first performed data transformation of different modalities to train a structurally identical teacher. The teacher was then leveraged to transfer knowledge by instructing the student to produce similar features and prediction results. In this way, the single-modality student obtains multi-modality knowledge and improves performance, without additional cost during inference. Despite their effectiveness to transfer cross-modality knowledge, the application of existing methods is limited since the modalities of both the teacher and the student are restricted. In[chong2021monodistill], the modalities of the teacher and student are fixed to be LiDAR and camera while in[zheng2022boosting,ju2022paint], they are determined to be LiDAR-camera and LiDAR. However, the sensor portfolio in the field of 3D object detection results in a diverse and complex application of different detectors. With restricted modalities of both the teacher and student, these methods are difficult to be applied in more situations,e.g., the method in[chong2021monodistill]is not suitable to transfer knowledge from a camera based teacher to a LiDAR based student. To solve the above problems, we propose a universal cross-modality knowledge distillation framework (UniDistill) that helps single-modality detectors improve performance. Our motivation is based on the observation that the detectors of different modalities adopt a similar detection paradigm in bird’s-eye view (BEV), where after transforming the low-level features to BEV, a BEV encoder follows to further encode high-level features and a detection head produces response features to perform final prediction. UniDistill takes advantage of the similarity to construct the universal knowledge distillation framework. As in Figure1(c), during training, UniDistill projects the features of both the teacher and the student detector into the unified BEV domain. Then for each ground truth bounding box, three distillation losses are calculated to transfer knowledge: (1) A feature distillation loss that transfers the semantic knowledge by aligning the low-level features of 9 crucial points. (2) A relation distillation loss that transfers the structural knowledge by aligning the relationship between the high-level features of 9 crucial points. (3) A response distillation loss that closes the prediction gap by aligning the response features in a Gaussian-like mask. Since the aligned features are commonly produced by different detectors, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three losses sparsely align the foreground features to filter the effect of misaligned background information and balance between objects of different scales, improving the distillation effectiveness. In summary, our contributions are three-fold: We propose a universal cross-modality knowledge distillation framework (UniDistill) in the friendly BEV domain for single-modality 3D object detectors. With the transferred knowledge of different modalities, the performance of single-modality detectors is improved without additional cost during inference. Benefiting from the similar detection paradigm in BEV, UniDistill supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Moreover, three distillation losses are designed to sparsely align foreground features, filtering the effect of background information misalignment and balance between objects of different sizes. Extensive experiments on nuScenes demonstrate that UniDistill can effectively improve the mAP and NDS of student detectors by 2.0%\sim3.2%.
Fluency score: 2
Fluency issues: Improper spacing around references "[shi2019pointrcnn,yang20203dssd,yin2021center,li20173d,shi2020points]", "[huang2021bevdet,li2022bevdepth,brazil2019m3d,luo2021m3dssd]", "[qi2018frustum,liu2022bevfusion,vora2020pointpainting,wang2021pointaugmenting]", "[chong2021monodistill,ju2022paint,zheng2022boosting]", missing spaces in "Figure1(a) and1(b)" and "camera based teacher to a LiDAR based student".

Clarity score: 3
Clarity issues: Dense use of jargon and acronyms may obscure meaning for lay readers. Use of terms like "modalities", "knowledge distillation", etc., without explanation.

Grammaticality score: 4
Grammaticality issues: Missing spaces and hyphenation in "LiDAR-camera".

Readability score: 2
Readability issues: Dense, jargon-heavy text; improper use of references and spacing hinders smooth reading.

Coherence score: 3
Coherence issues: Dense technical language and abrupt change from problem statement to solution without smooth transitions.
====================
	 Infomorphic networks: Locally learning neural networks derived from partial information decomposition
==========
Introduction:  The human neocortex is an impressive information processing system involved in performing a wide variety of tasks from visual, auditory, tactile and gustatory perception via various forms of memory to complex planning and motor actions(Lodato and Arlotta,2015). Despite this diverse range of responsibilities, the neocortex is widely believed to consist of structurally similar, yet functionally flexible circuits(Creutzfeldt,1977, Rockel et al.,1980). On the smallest scale, these circuits consist of individual neurons whose firing is dependent on only local factors, such as the firing of other, connected neurons and the local biochemical environment – without taking into account any global or semantic knowledge about the task(Douglas and Martin,2004,2007). However, the how intricate cooperation among individual neurons helps performing complex tasks is still not well understood. To enhance our understanding of how neurons cooperate it is crucial to develop a model system in which the relevant dynamics of collaboration, specialization and self-organization can be readily observed and promoted. To grasp the fundamental and universal factors underlying these dynamics, these model systems should eliminate unnecessary idiosyncrasies associated with the specific task or the biological neurons. Such semantics-free information processing can be quantified using the framework of information theory(Wibral et al.,2017). From an information-theoretic perspective, neurons can be interpreted as information channels that convert the incoming signals into spiking activity(Wibral et al.,2015). Previous research byKay (1994)has demonstrated the feasibility of this information-theoretic approach by creating model neurons which directly optimize for certain information-theoretic objectives(Kay and Phillips,2011). Despite these successes, the framework of classical information theory is limited in its ability to adequately account for all relevant aspects of neural information processing: Information theory focuses primarily on quantities such as mutual information, which capture only information channels with a single source – albeit thuis source possibly being multivariate and high-dimensional. Biological neurons, on the other hand, often receive signals from severalclassesof inputs, such as bottom-up, top-down or lateral connections, each playing a distinct role in the information processing(Rolls and Treves,1997, Shu et al.,2003, Manita et al.,2015). A comprehensive description of the complex interactions of multiple such sources with respect to a single target variable has only recently been made possible by an extension to classical information theory known asPartial Information Decomposition(PID)(Williams and Beer,2010, Lizier et al.,2018, Gutknecht et al.,2021). PID allows the information between multiple sources and a target variable to be dissected into unique, redundant and synergistic contributions calledatoms, through which it paints a richer picture of the information processing underlying the transformation from sources to target(Williams and Beer,2010). Recently, PID has been used to describe the function of cortical neurons(Schulz et al.,2021)and the representation of information in artificial and biological neural networks(Luppi et al.,2022, Ehrlich et al.,2023, Varley et al.,2023)and has been proposed as a unifying framework to describe cortical function(Wibral et al.,2017). A litmus test for assessing the adequacy of the PID framework in capturing all relevant information processing would be the construction of model neurons which learn by directly optimizing certain PID objectives. While this idea has been present for some time(Wibral et al.,2017), a viable demonstration of feasibility has been lacking so far, primarily due to the absence of adifferentiablePID measure that lends itself to gradient descent learning. However, drawing upon our recently developed differentiable PID measureI^{\mathrm{sx}}(Makkeh et al.,2021, Schick-Poland et al.,2021), we here demonstrate for the first time that it is indeed possible to create artificial neurons that learn based on directly interpretable information-processing goal functions derived from PID. Networks build from these novel artificial neurons can learn in a supervised or unsupervised manner, and perform classification, representation learning and memory tasks. As the topology or connection structure is shaped by the input information and the information theoretic goal of learning itself we term these neurons and networksinfomorphic– a portmanteau of “information” and “morphous” to indicate that they are directly shaped by the information they process. By studying these infomorphic neurons and their information-processing capabilities, valuable insights can be gained into the collaborative dynamics and self-organization principles that govern the function of complex neural networks, such as the neocortex. The main contributions of this paper are(1)the derivation and implementation of the PID-based learning rule set out byWibral et al. (2017)by deriving analytical gradients of the relevant PID atoms and(2)demonstrations of the usefulness and flexibility of the infomorphic neurons in different learning paradigms. The remaining sections of the paper are structured in the following way: First, we explain how neurons can be interpreted as information channels (Section2.1) and how this view can be extended to account for the information processing occurring between two distinct input classes to a channel (or, alternatively, two channels with a common output) using PID (Section2.2). Based on these insights, we introduce the model of the infomorphic neuron (Section3) and demonstrate its usefulness on a collection of learning scenarios involving supervised (Section4.1), unsupervised (Section4.2) and memory learning (Section4.3). We conclude with a discussion of strengths, limitations and next steps (Section5).
Fluency score: 4
Fluency issues: Odd sentence breaks and constructions, such as "However, the how intricate cooperation..." instead of "However, the intricacy of how cooperation..."; misuse of "thuis".

Clarity score: 3
Clarity issues: Term "infomorphic" is introduced without clear definition; complex sentence structures with nested clauses make it hard to follow.

Grammaticality score: 4
Grammaticality issues: Missing spaces after citations like "(Lodato and Arlotta,2015)".

Readability score: 3
Readability issues: Dense academic jargon and long sentences; multiple nested clauses.

Coherence score: 4
Coherence issues: The connection between parts, though logically sequenced, is sometimes obscured by complex phrasing and terminology.
====================
	 Diagnosing Human-object Interaction Detectors
==========
Introduction:  Human-object interaction (HOI) detection aims to jointly detect the humans and objects that have interactions in static images. For example, the person and snowboard in Fig.1. It provides structured interpretations of the semantics of visual scenes rather than just object recognition or detection. A successful HOI detection system is an essential building block for many downstream applications, such as visual question answering[3,1,33,27,38,27], image captioning[37,2,11,21]and retrieval[7,5,29,35,30], etc. Recent advancements in HOI detection have been marked by increasing mean Average Precision (mAP) scores across standard benchmarks[13,8,12,36,14,48,43,44,46,23,41,40,28,22,39,47,17,26,19,42], denoting remarkable progress. Nonetheless, the reliance onmAPscores as a summary metric does not provide sufficient insight into the nuances of model performance, including the factors making one method perform better than another or any bottleneck for further improvement. This lack of detailed understanding may impede future advancements in the field. The same issue also existed in object detection, a sub task of HOI detection, wheremAPis also the dominant evaluation metric. To address it, diagnosis toolboxes have been designed to provide more useful quantitative break-down analysis[15,4], which have significantly boosted the development of object detection. In this paper, we aim to replicate the success of these work by introducing a toolbox designed for HOI detection, fostering future research. Generally speaking, the HOI detection problem consists of two sub-tasks: 1) detecting pairs of interacting human and object (human-object pair detection) and 2) classification of their interactions. These two tasks are not independent, but in a cascaded relationship, as shown in Fig.1. Specifically, in our toolbox, we first perform aholisticanalysis of the overall HOI detection accuracy. Inspired by the object detection diagnosis toolbox[4], we define a set of error types as well as oracles to fix them in the HOI detection pipeline across the human-object pair detection and interaction classification tasks. ThemAPimprovement, obtained by applying the oracle to each error, is used to measure the significance of different errors. The largermAPimprovement can be obtained for a particular type of error, the more it contributes to the failure of an HOI detector. We then delve into the human-object pair detection and interaction classification tasks, respectively, and conduct detailed studies. For the detection task, we mainly investigateRecallto see if it can detect all the ground-truth human-object pairs for the later stage of interaction classification. We also computePrecisionto check the noisiness level of the detections. For the interaction classification task, an HOI model needs to differentiate negative detections, where the detected human-object pairs have no actual interactions, from positive ones (i.e., with actual interactions). To diagnose such a binary classification problem, we report theAP(Average Precision) score to avoid selecting a threshold for the classification score, which is non-trivial. We also compute themAPscores for the multi-label interaction classification problem, where we assume the human-object pair detections are correct. In this way, we can disentangle two sub-tasks and focus on analyzing the interaction classification problem only to gain better insights. Our diagnosis toolbox is applicable to different methods across different datasets. Based on both such holistic and detailed investigations of the human-object pair detection and interaction classification, our toolbox provides a comprehensive diagnosis report for 8 state-of-the-art HOI detection models. With the detailed quantitative break-down results, we are now able to answer questions such as “Are one-stage HOI detection models superior to two-stage ones or vice versa?” (no clear advantage of one paradigm over the other in terms of accuracy), “What is the bottleneck of HOI detection?” (incorrect localization of the object in a human-object pair and incorrect classification of the interactions), “Why does state-of-the-art method RLIPv2[42]perform better?” (since it significantly improves the interaction classification accuracy), etc. Please refer to Section5for detailed discussions of existing HOI detection models. To our best knowledge, this is the first toolbox dedicated for the diagnosis of HOI detection in static images. We will release our toolbox and believe our work will foster the future development of HOI detection models. There are several analysis tools for object detection[24,15,4]. The seminal work[15]shows how to analyze the influences of object characteristics on detection performance and the impact of different types of false positives. But it requires extra annotations to help analyze the impacts of object characteristics, which is unlikely to be scalable in large-scale benchmark datasets. TIDE[4]improves the default evaluation tool provided by the COCO dataset[24]. It provides a more general framework for quantifying the performance improvement for different false positive and false negative errors in object detection and instance segmentation algorithms. Our quantitative analysis of different errors and different tasks in HOI detection is motivated by TIDE[4]. Simply extending such toolboxes to HOI detection is not trivial due to the coupled nature of human-object pair detection and interaction classification sub-tasks. Moreover, we delve into each of them, examining models’ behavior and identifying their bottleneck. A similar error diagnosis work[10]is proposed for the video relation detection task, which adopts a similar holistic approach inspired by TIDE[4]. In our diagnosis toolbox, we go beyond the holistic error analysis and also conduct detailed investigations in two different sub-tasks of HOI detection, considering the cascade nature of the HOI detection pipeline. In[13], the authors also define several error types of false positives. However, the definition is specifically tailored for the annotation format of the V-COCO dataset, which is not generalizable to others. In contrast, our analysis is applicable to different benchmark datasets[8,13]. In[18], the authors analyze a specific issue of HOI detection, the long-tail problem of HOI categories and points out limiting factors.[25]proposes a new metric to advance HOI generalization, preventing the model from learning spurious object-verb correlations. Both[18]and[25]are complementary to our diagnosis tool and analysis results.
Fluency score: 4
Fluency issues: Occasional awkward phrasing such as "onmAPscores" and "wheremAPis".

Clarity score: 3
Clarity issues: Excessive use of jargon, acronyms, and numbers without clear references may confuse readers unfamiliar with the topic.

Grammaticality score: 4
Grammaticality issues: Minor issues such as missing spaces in "onmAPscores", "wheremAPis", "image captioning[37,2,11,21]and retrieval[7,5,29,35,30], etc.", and "RLIPv2[42]perform".

Readability score: 3
Readability issues: Dense text with many embedded citations and technical terms reduces overall readability.

Coherence score: 4
Coherence issues: Although the text is logically structured, the coherence is affected by frequent interruptions for citations and some abrupt transitions between ideas.
====================
	 Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation
==========
Introduction:  Discourse relations, such asCauseandContrast, describe the logical relation between two text spans(Pitler et al.,2009). Recognizing discourse relations is beneficial for various NLP tasks, including coherence modeling(Lin et al.,2011), reading comprehension(Mihaylov and Frank,2019), argumentation mining(Habernal and Gurevych,2017; Hewett et al.,2019), and machine translation(Meyer,2015; Longyue,2019). Discourse connectives (e.g.,but,as a result) are words or phrases that signal the presence of a discourse relation(Pitler and Nenkova,2009). They can be explicit, as in (1), or implicit, as in (2): [I refused to pay the cobbler the full $95]Arg1because[he did poor work.]Arg2 [They put the treasury secretary back on the board.]Arg1(Implicit=However) [There is doubt that the change would accomplish much.]Arg2 When discourse connectives are explicitly present between arguments, classifying the sense of a discourse relation is straightforward. For example,Pitler and Nenkova (2009)proved that using only connectives in a text as features, the accuracy of 4-way explicit discourse relation classification on PDTB 2.0 can reach 85.8%. However, for implicit cases, there are no connectives to explicitly mark discourse relations, which makes implicit discourse relation classification challenging(Zhou et al.,2010; Shi et al.,2017). Existing work attempts to perform implicit discourse relation classification directly from arguments. They range from designing linguistically informed features from argumentsLin et al. (2009); Pitler et al. (2009)to modeling interaction between arguments using neural networks(Lei et al.,2017; Guo et al.,2018). Despite their impressive performance, the absence of explicit discourse connectives makes the prediction extremely hard and hinders further improvement(Lin et al.,2014; Qin et al.,2017). The huge performance gap between explicit and implicit classification (85.8% vs. 57.6%)(Liu and Li,2016)motivates recent studies to utilize implicit connectives for the training process of implicit relation classifiers. For instance,Qin et al. (2017)developed an adversarial model to transfer knowledge from the model supplied with implicit connectives to the model without such information, whileKishimoto et al. (2020)proposed a multi-task learning framework to incorporate implicit connectives prediction as another training objective. However, we argue that these methods are suboptimal since connectives are still not explicitly present in input texts. This is demonstrated byKishimoto et al. (2020), concluding that adding implicit connective prediction as a training objective provides only negligible gain for implicit relation classification on PDTB 2.0 (we empirically found the conclusion also held on the adversarial model). In this paper, we design a novel end-to-end model to leverage discourse connectives for the task of implicit discourse relation classification. The key inspiration is derived from the annotation process of implicit discourse relations in PDTB, which consists of inserting a connective that best conveys the inferred relation, and annotating the relation label based on both the inserted implicit connectives and contextual semantics(Prasad et al.,2008). We imitate this process by explicitly generating discourse connectives for the implicit relation classifier. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. A potential drawback of this joint model is that the poorly generated connectives at the early stage of joint training may mislead the relation classifier. One possible solution is always feeding true connectives to the implicit relation classifier for training. But it leads to severe discrepancies between training and inference(Sporleder and Lascarides,2008), since manually-annotated connectives are unavailable during evaluation(Prasad et al.,2008). To address this issue, we adopt Scheduled Sampling(Bengio et al.,2015)into our method. To be more specific, our relation classifier is first trained with hand-annotated implicit connectives and then gradually shifts to use generated connectives. We evaluate our model111https://github.com/liuwei1206/ConnRelon two English corpora, PDTB 2.0(Prasad et al.,2008), PDTB 3.0(Webber et al.,2019), and a German corpus, PCC(Bourgonje and Stede,2020), and compare it with other connective-enhanced approaches and existing state-of-the-art works. Results show that our method significantly outperforms those connective-enhanced baselines on three datasets while offering comparable performance to existing sota models. In addition, we perform the first systematic analysis of different connective-enhanced models to investigate why our method works better. Our studies show that: (1) models learn to use connectives more effectively when putting connectives in the input rather than using them as training objectives; (2) end-to-end training can improve models’ robustness to incorrectly-predicted connectives; (3) our method shows a better balance between arguments and connectives for relation prediction than other baselines. Finally, we show that connectives can effectively improve the predictive performance on frequent relations while failing on those with limited training instances.
Fluency score: 4
Fluency issues: [Punctuation inconsistencies like lack of space after periods, commas, and parentheses. Excessive use of lengthy parenthetical citations.]

Clarity score: 3
Clarity issues: [Overuse of technical jargon and acronyms without explanations, densely packed information that may be hard to follow for non-experts.]

Grammaticality score: 4
Grammaticality issues: [Some minor punctuation errors.]

Readability score: 3
Readability issues: [Lengthy sentences with complex structure, high information density, and jargon make the text difficult to read for a general audience.]

Coherence score: 4
Coherence issues: [Excessive parenthetical interruptions can disrupt the flow of the text, making the coherence suffer slightly.]
====================
	 Reconstructing Historical Climate Fields With Deep Learning
==========
Introduction:  Observational climate data is typically sparse before systematic observations such as buoys, ship measurements, or satellite measurements were introduced. Generally, the further back in time we go, the fewer observations are available[1]. Temperature and precipitation records are the best-observed climate fields in the recent past and reach back until the 19th century, but measurements are still sparse and rely heavily on interpolation especially for earlier parts of the records[2,3]. Even more severely, for many important climate variables, such as sea-ice thickness or vegetation indices, no measurements exist at all before the introduction of large-scale satellite missions. The corresponding time series often span a few decades or even only years[e.g.,4,5]. The low spatial and temporal resolution introduces large uncertainties and limits our understanding of important climatic processes[2,6,1]. Several approaches and methods to produce historical climate fields based on the available observations have been developed in the past. One approach is to run state-of-the-art weather models with observations and past weather forecasts to produce reanalysis products that provide a complete picture of the past weather and climate for the last decades[7,8]. While reanalyses are successful in providing spatiotemporally continuous and consistent data, they often struggle with specific regions and variables and inherit biases the employed numerical models suffer from[8,9]. An alternative approach is to use statistical methods to reconstruct missing information. In this regard, kriging or Gaussian process regression is widely used in the geosciences[10,11,12]. However, statistical methods typically do not include knowledge of the temporal and spatial patterns of the underlying climatic fields and therefore fail to reconstruct these patterns, especially for large missing areas. In recent years, machine learning (ML) has become widely used in geoscience and climate science, with the promise of better performance than statistical methods while still providing easy usability and, to some extent, knowledge of the underlying physical processes[13,14]. The applications of machine learning in climate science are vast and range from classical time series forecasting[15,16,17], down-scaling and post-processing of numerical models[18,19], to time series reconstruction[20,13]. Furthermore, there is a substantial ongoing effort to combine traditional numerical Earth system models with machine learning methods to leverage the advantages of both approaches[21,22,14,23,24,25,26,27,28]. In this study, we consider the reconstruction of spatial climate fields as an image inpainting problem. Inpainting images based on given information is a classical problem in computer vision and many approaches have been proposed in recent years[29,30,31]. We apply the recently introduced state-of-the-art deep learning approachResolution-robust Large Mask Inpainting with Fourier Convolutions(LaMa)[32]to reconstruct different climate fields with a focus on surface temperature records. We train our model on numerical climate model output from the Coupled Model Intercomparison Project to reconstruct the missing measurements in observational data. Our method is able to reconstruct climate fields with very sparse information and highly irregular missing data. We show that our approach outperforms kriging and other machine learning methods. Moreover, it is able to inpaint different data sets than the ones it was trained on, and can be used on a variety of structurally different climatic fields at varying resolutions. The surface temperature is one of the most important climate variables, as a direct measure of climate change. Global instrumental temperature records reach back to the mid-19th century[2]with local observations reaching back as far as the mid-17th century[33]. However, on average, less than 30% of Earth’s surface before the year 1900 AD have measurements in the state-of-the-art observational data set HadCRUT4 (Fig.1a). This is similar for other widely used long-term temperature data. Therefore, surface temperature records serve as perfect proof-of-concept application for the image inpainting task in climate science.
Fluency score: 3
Fluency issues: Square brackets with references throughout the text disrupt the flow. 

Clarity score: 3
Clarity issues: The text includes technical jargon and references which may confuse readers not familiar with the subject. 

Grammaticality score: 5
Grammaticality issues: No specific issues detected.

Readability score: 3
Readability issues: The text is dense with information and references, making it less approachable for a general audience.

Coherence score: 4
Coherence issues: The transition between general climate data problems and the specific methods presented is somewhat abrupt.
====================
	 Procedural Text Mining with Large Language Models
==========
Introduction:  Extracting complex knowledge from unstructured sources is a challenge: in the industrial domain, for example, troubleshooting documents may contain the description of long and articulated procedures (i.e., sequences of steps to be performed in a precise order and under specific conditions) and those natural language instructions may be represented in very different textual forms, thus making it hard for a knowledge extraction algorithm to correctly identify and structure the relevant information. Oftentimes, automatic extraction is followed by manual revision of domain experts. In any case, all machine-learning-based methods require training data which is often not readily available, therefore novel approaches are emerging to exploit interactive dialogues and language models(bellan2021process). Extracting procedural knowledge from human natural language instructions is a challenging task. Firstly, natural language instructions are not interpretable by machines. In the easiest case, the instructions are given as numbered lists which can easily be identified. However, complications arise when the document contains procedures in different forms: a list without numbers, an indented text or simply a full text in which the different steps are connected by conjunctions like ”then”, ”afterwards”, etc. Secondly, procedures can either be composed of only simple steps or contain other sub-procedures that are located elsewhere in the document. Thirdly, the procedures can differ substantially from one document to the other because of different authors’ and editors’ styles, but the goal would be to integrate information from different documents. Specifically, we investigate the potential of large language models (LLMs) in the context of extracting procedural knowledge from unstructured PDF documents. LLMs demonstrate remarkable capabilities in natural language processing, surpassing those possible using conventional symbolic AI and machine learning technologies(mahowald2023dissociating). Nevertheless, these models often lack knowledge of nuanced, domain-specific details and are susceptible to hallucinations. This paper investigates the practical application of LLMs, with a particular focus on the advanced GPT-4 (Generative Pre-trained Transformer 4), to address the complex task of extracting procedures from unstructured PDF documents. The core of our research revolves around an incremental question-answering methodology, with a specific emphasis on harnessing LLMs in both zero-shot and in-context learning scenarios. Our study is structured to encompass two distinct approaches to in-context learning. The first approach involves the incorporation of an ontology containing definitions and procedural steps, while the second approach integrates a limited dataset tailored for few-shot learning. This comprehensive investigation not only highlights the considerable potential of our chosen approach but also underscores the critical role played by customised in-context learning. These tailored modifications are poised to make significant progress in tackling a persistent challenge within deep learning-based NLP techniques: the scarcity of essential training data for procedure extraction. Aligned with our exploration, three fundamental research questions guide our investigation: RQ1: How well is ChatGPT4 able to list the steps and substeps in the text versus the ontology settings? RQ2: Is in-context learning beneficial for procedural text mining? RQ3: Regardless of the ontology instantiation, is ChatGPT4 capable of the correct application of the ontology? Through systematic empirical inquiry, this study not only contributes to the enhancement of procedural text mining but also offers insights into the capacity of in-context learning enhancements to surmount the constraints stemming from inadequate training data. The significance of this research extends beyond procedural extraction, resonating within the broader landscape of NLP applications and cultivating the evolution of more sophisticated and adaptable information retrieval systems. Our code and dataset is publicly releasedhttps://github.com/jd-coderepos/proc-tm/. The paper is structured as follows. We begin with a motivating example for our work insection 2; then we describe the procedural ontology insection 3. Insection 4, we propose our approach and introduce the experimental dataset used for the task of procedural text mining insection 5. Our experimental results are discussed both quantitatively and qualitatively next insection 6. Finally a brief discussion on related work is offered insection 7and concluding remarks insection 8.
Fluency score: 4
Fluency issues: "may be represented" feels a little awkward, "language models(bellan2021process)" needs a space before the citation, "like ”then”, ”afterwards”, etc." punctuation marks are irregular and there's an unnecessary space before “then”.

Clarity score: 4
Clarity issues: "(mahowald2023dissociating)" disrupts the flow, "incremental question-answering" is somewhat vague, "zero-shot and in-context learning scenarios" might be unclear to lay readers, "ontology containing definitions and procedural steps" is complex without explanation.

Grammaticality score: 3
Grammaticality issues: Missing commas after "for example," "therefore," and before "thus". "different authors’ and editors’ styles" - inconsistent or erroneous use of possessive apostrophe.

Readability score: 4
Readability issues: Citations within the text (e.g., (bellan2021process) and (mahowald2023dissociating)) disrupt reading flow, lengthy sentences with complex structures can be hard to follow, "Our code and dataset is publicly releasedhttps://github.com/jd-coderepos/proc-tm/" needs a space between 'released' and the URL.

Coherence score: 4
Coherence issues: The organization is clear but some connections are abrupt like the transition from LLMs capabilities to their shortcomings, the term "ontology" could be confusing without further context, and "Our study is structured to encompass two distinct approaches..." could better connect to previous content.
====================
	 E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning
==========
Introduction:  Inspired by the human visual system, Silicon RetinaMahowald (1991)has pioneered an approach to perceptual sensing with event cameras or Dynamic Vision Sensors (DVS)Lichtsteineret al.(2008); Poschet al.(2010); Berneret al.(2013)and gained significant interests from both academia and industry. Unlike traditional cameras, event cameras detect microsecond-level intensity changes, generating an asynchronous stream of ‘events’, termed as event-stream. Event cameras offer several advantages over conventional CCD/CMOS cameras, including high temporal resolution, high dynamic range of up to 140dBLichtsteineret al.(2008), and low resource consumption due to the sparse nature of event-streams. For example, the DVS128 sensor platform consumes 150 times less energy than a conventional CMOS cameraLichtsteineret al.(2008). Despite the appealing advantages of event cameras, the non-structural event-streams are not inherently compatible with traditional computer vision methodologiesScheerlincket al.(2020)and the visualization is not intuitive for human users to understand. To address the above issue, the research on events-to-video (E2V), which aims to generate video frames from pure event-streams, has been raised to provide convenient and intuitive access to the rich information encapsulated in the sparse and non-structure event-streams. There have been a number of successful approaches for E2V task, such as E2VIDRebecqet al.(2019), FireNetScheerlincket al.(2020), SPADE-E2VIDCadenaet al.(2021), and ET-NetWenget al.(2021). However, the quality of the video frames generated by the existing E2V approaches is still not satisfactory and fail to recover detailed texture for the complex scenesErcanet al.(2023). This issue is predominantly attributed to the fact that many of these approaches, such as E2VID and ET-Net, primarily adopt a purely data-driven approach to learn the mapping from event-streams to video frames directly. However, the purely data-driven approaches are lack of interpretability and flexibilityShlezingeret al.(2023), and they do not take into account the prior knowledge of the underlying statistics relating event-streams and video frames. Therefore, their performance is largely dependent on the non-linearity and generalization capability of the neural networks, which poses significant challenges when the scenes to be reconstructed are complexJarrettet al.(2009). To address the aforementioned challenges, we introduceE2HQV, a novel E2V paradigm designed to produce high-quality video frames from events. This is achieved through a model-aided deep learning framework that integrates a theory-inspired E2V model. Rooted in the fundamental imaging principles of event cameras, this theory-inspired E2V model elucidates the relationship between consecutive frames and their associated inter-frame event-streams, offering valuable prior knowledge that enhances the learning efficacy of our deep learning framework. As shown in Figure1, instead of generating video frames in a pure data-driven approach, E2HQV estimates a number of intermediate key parameters defined by the theory-inspired E2V model then reconstructs the video frames accordingly. The contributions of this work can be summarized as: We propose E2HQV, a novel high-quality video frames generation approach from event-streams by facilitating a model-aided learning framework which learns the key parameters defined by a theory-inspired E2V model and generates high quality video frames accordingly. According to the imaging principle of event camera and relation between video frames and event-stream, a theory-inspired E2V model is derived to guide the design of the model-aided learning framework. A new temporal shift embedding module is designed to deal with the perturbation introduced by the state-reset mechanism of the recurrent components in the framework and ensuring seamless fusion of events and reconstructed frames. Through extensive experiments on mainstream event-based video reconstruction datasets, E2HQV consistently exhibits superior performance over state-of-the-art (SOTA) approaches. Remarkably, for certain evaluation metrics, E2HQV surpasses the next best approach by a substantial margin of over 40%.
Fluency score: 3
Fluency issues: Frequent lack of space after citations (e.g., "Silicon RetinaMahowald," "SensorLichtsteineret al.," "sensorsScheerlincket al.," etc.), unwieldy sentence structure.

Clarity score: 3
Clarity issues: Dense technical language and jargon, complex sentence constructions, unclear references (such as "Figure1" in a text-only context).

Grammaticality score: 3
Grammaticality issues: Missing spaces after citations, missing articles before some nouns (e.g., "a novel E2V paradigm"), occasional awkward phrasing.

Readability score: 2
Readability issues: Dense presentation of information, high use of domain-specific jargon, long sentences with multiple clauses, and parenthetical references distract and may confuse the reader.

Coherence score: 3
Coherence issues: The text extensively uses technical terms without explanation, assumes prior knowledge, and could be organized more effectively for better flow and coherence.
====================
	 "It's a Match!" -- A Benchmark of Task Affinity Scores for Joint Learning
==========
Introduction:  For more than two decades since its inceptioncaruana1997multitask, Multi-Task Learning (MTL) has been extensively studied by the Deep Learning community. For practitioners interested in the best strategy to learn a collection of tasks, the promises of MTL are numerous and attractive. First, learning to solve several tasks simultaneously can be more cost-efficient from a model development and deployment perspective. Second, if the tasks learned together cooperate, MTL can even outperform its Single-Task Learning (STL) counterpart for the same computational coststandley2020tasks. However, MTL potential advantages are tempered by the difficulty of estimatingtask affinity, i.e., identify tasks benefiting from joint learning, without testing all combinations of tasks. This calls fortask affinity scores– to quantify a priori and at a cheap computational cost the potential benefit of learning tasks together. The quest for the perfect affinity score is further exacerbated by MTL performance’s strong dependency on the learning context, i.e., the data and models used for training. For instance, tasks cooperating in one learning context can result in competition when using slightly different data or modelsstandley2020tasks. Recent worksfifty2021efficiently;standley2020taskshave integrated this context-dependency when designing task grouping strategies. While these approaches avoid a complete search across all task combinations, they still require training and comparing some MTL models for the final network selection. Furthermore, those studies show that even in a small-scale scenario, MTL performance cannot be accurately predicted without actually performing MTL. Despite providing assessment of task affinity, previous literature lacks of a broader comparison of the associated scores. In this work, we take a first step in recovering this gap bypresenting an empirical comparison of several task affinity scoring techniques. Some of these scores are inspired by previous literature ranging from Transfer Learning to Multi-Task Learning:taxonomical distancezamir2018taskonomy,input attribution similarityinput_attr_aff_metric_ref,representation similarity analysisTL_RSA,gradient similaritygrads_clashesandgradient transferencefifty2021efficiently. We benchmark an additional affinity score which is an original proposal:label injection. We evaluate all of them on the public Taskonomy datasetzamir2018taskonomywhich is a well-known large benchmark spanning several Computer Vision tasks. Note that our objective is not to present a novel state-of-the-art MTL architecture but rather an objective benchmark of task affinity estimation techniques. More specifically we aim to understand if task affinity scores can (i) be used as proxy for true MTL performance and (ii) suggest the best partner task to improve the performance of a target task. These scores and their discussion aim at helping practitioners gauge the benefit of MTL for their own set of tasks. Insection2, we review the state of the art on MTL affinity characterization. Insection3, we present the affinity scores selected for benchmarking and detail our evaluation protocol. We present our results insection4and discuss the advantages and limitations of these scores insection5.Section6concludes the paper.
Fluency score: 3
Fluency issues: Numerous run-on phrases lacking proper spacing (e.g., "inceptioncaruana1997multitask," "estimatingtask affinity," "calls fortask affinity scores–," "used for trainingstandley2020tasks," "worksfifty2021efficiently;standley2020tasks").

Clarity score: 3
Clarity issues: Lack of spacing between words and references impedes clarity. References within the text appear without proper formatting (e.g., "zamir2018taskonomy,input attribution similarityinput_attr_aff_metric_ref").

Grammaticality score: 4
Grammaticality issues: Most of the text is grammatically correct; however, references are improperly formatted, giving the appearance of grammatical mistakes.

Readability score: 3
Readability issues: Poor spacing between words and references, as well as improperly formatted references, make the text challenging to read.

Coherence score: 4
Coherence issues: Despite the poor formatting of references and the lack of spaces in some places, the overall structure and progression of ideas are coherent.
====================
	 SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling
==========
Introduction:  Imagine you wish to estimate multiple causal effects from observational data. Despite the challenging nature of the task, it nonetheless represents one of the core goals of science and causality\parencitePearl2018TBOW, Pearl2012, Vowels2021, Vowels2021DAGs. Indeed, in the absence of experimental data, we must do everything we can to ensure the causal relevance of our statistical inferences. Otherwise, our estimates cannot be tied to their associated theories\parenciteVowels2021, Scheel2020 and the estimates effectively represent arbitrary functions of the observed data, subject to ambiguous, pseudo-causal interpretations\parenciteGrosz2020, Rohrer2018, Hernan2018. To this end, researchers in the domains of psychology and social science have begun to advocate for the increased adoption of causal Directed Acyclic Graphs (DAGs), which aid in (a) the clear, formal specification of a causal theory as a mathematical but nonetheless intuitive, visual object, (b) unbiased estimation of the target quantities of estimation (e.g., effect sizes)\parenciteVowels2023_prespec, Rohrer2018. Unfortunately, DAGs themselves only get us so far in terms of achieving unbiased estimation of causal effects. Whilst they help us deal with the specification of thestructuralaspect of a model/theory (by representing an ordered causal process), and whilst they enable us to express a target quantity as a function of the observed distribution, they do not help with the estimation itself. Furthermore, they do not help us with thefunctionalspecification of the relationships between variables. Indeed, one of the strengths of DAGs is that they are non-parametric, and make very limited assumptions about the underlying functional form. For instance, they tell us nothing about whetherYis a linear function of justXor whether it is a linear function ofX^{2}. Thus, to arrive at meaningful effect size estimates, the choice of the associated estimation technique and its associated level of functional adaptability must be made. One option that researchers have is to use structural equation modeling or linear/logistic regression for the estimation of the associated effects, which make the assumption that the relationships are linear/linear in the logit space. The problem with this is that such an assumption of linearity can lead to a similar level of biased estimation\parenciteVowels2021, vanderLaan2011, vanderLaan2014 to that which results from structural misspecification. Such misspecification thereby undermines the otherwise advantageously function-agnostic nature of the DAG. In order to avoid making such unnecessary assumptions about the functional form underlying the causal relationships represented in the DAG, we recommend the use of machine learning. Specifically we propose Super Learner Equation Modeling (SLEM), an integrated framework for machine learning based causal inference with DAGs. Within this SLEM framework, we present the DAG Learner estimator object in the form of easy-to-use, open-source Python code, including simulations and a tutorial-style notebook.111The code can be found in supplementary material or athttps://github.com/matthewvowels1/SLEM.Essentially, for any user-specified DAG, and according to the SLEM framework, the DAG Learner implements a set of general machine learning models which are used to estimate, in a data-driven but causally-constrained manner, all associated path coefficients. Furthermore, our framework allows for the estimation of the effect of (optionally multiple, simultaneous) user-specified interventions, thereby facilitating a general, and easy-to-use tool for non-parametric causal inference. In our view, the availability of such a tool is overdue - whilst the required techniques have existed for some time, they have never been combined in such a way. The paper is structured as follows: First, we briefly discuss some background theory relating to DAGs, causality, and machine learning. Secondly, we describe the proposed framework, beginning with the specification of the DAG, as well as the chosen machine learning method. Then, we provide some worked examples and key simulations. Finally we discuss the associated limitations of the method in relation to some existing alternative approaches, and close with a summary. Note that whilst the success of the propose method rests on existing work in the domains of causality and machine learning, to the best of our knowledge we are the first to bring the two together in such an integrated, easy-to-use methodology.
Fluency score: 4
Fluency issues: [Dense and technical language can impact fluency for general readership; the sentence "Indeed, one of the strengths of DAGs is that they are non-parametric..." is slightly lengthy and complex.]

Clarity score: 3
Clarity issues: [Technical jargon and complex sentences may hinder clarity for those not familiar with the subject; references formatted for a citation system but embedded in plain text.]

Grammaticality score: 4
Grammaticality issues: [Minor issues such as missing spaces after citations e.g., "causality\parencitePearl2018TBOW," "sizes)\parenciteVowels2023_prespec," and "misspecification\parenciteVowels2021,".]

Readability score: 3
Readability issues: [Complex sentence structures and academic language can make the text less accessible to a broader audience; long sentences with embedded clauses can be difficult to follow.]

Coherence score: 4
Coherence issues: [Although the text logically progresses from identifying a problem to proposing a solution, the high density of technical language can affect coherence for lay readers.]
====================
	 EgoHumans: An Egocentric 3D Multi-Human Benchmark
==========
Introduction:  Understanding humans in 3D from the egocentric view is key to building immersive social telepresence[lawrence2021project,bamodu2013virtual,ma2021pixel,lombardi2021mixture], assistive humanoid robots[goodrich2013teleoperation,fridin2014acceptance,piezzo2017feasibility], and augmented reality systems[azuma1997survey,billinghurst2015survey,carmigniani2011augmented]. A crucial step in this direction is to obtain 3D supervision at scale for deep learning models to generalize to the real world. However, unlike the large-scale 2D benchmarks[deng2009imagenet,lin2014microsoft,li2019crowdpose,johnson2010clustered,cordts2016cityscapes], the diversity of the 3D benchmarks[joo2015panoptic]is severely limited - primarily because manual annotation in the 3D space is impractical. As a result, existing popular 3D benchmarks[ionescu2013human3,joo2015panoptic,von2018recovering,mehta2017monocular,li2021ai,hassan2019resolving]are constrained to indoor environments or, at most, two human subjects if outdoors, stationary/slow camera motion, with limited occlusion. Furthermore, the majority of these benchmarks only portray the third-person view. Recent progress has been made in constructing egocentric benchmarks[xu2019mo,ng2020you2me,guzov2021human,zhang2022egobody]. However, they suffer from the same diversity pitfalls, making it difficult to evaluate how close the field is to fully robust and general solutions. To drive advances in the field, we propose a benchmark,EgoHumans, that includes challenging scenarios ignored in previous studies and a novel method,EgoFormer, that outperforms prior art as a starting point for the evaluations. EgoHumans is a new egocentric benchmark consisting of high-resolution videos and comprehensive ground truth annotations such as camera parameters, 2D bounding boxes, human tracking ids[dendorfer2020mot20], 2D/3D human poses, and 3D human meshes[loper2015smpl]. EgoHumans goes beyond previous benchmarks in important ways. First, it captures outdoor videos of unconstrained environments and dynamic human activities, including challenging sporting events such as fencing, badminton, volleyball, etc. Second, the activities are unchoreographed to truly capture thein-the-wildphilosophy of our work. Our video sequences include fast ego-camera motion, human-human occlusion, truncation, and humans appearing at a wide range of spatial scales. We leverage a flexible multi-camera setup consisting of Meta’s Aria glasses[aria_pilot_dataset], with an RGB and two greyscale cameras, for the egocentric view and stationary secondary RGB cameras for the auxiliary views (see Fig.4). Such camera combination allows us to accurately track and triangulate human poses in 3D for a long duration without using visual markers[ionescu2013human3]or additional sensors[von2018recovering]. The natural form factor of glasses[maimone2013computational]coupled with the RGB and stereo cameras closely resembles the human vision[matthies1989dynamic]. Last, as a by-product of our capture setup, we provide 3D annotations for the multi-view secondary cameras. We hope these annotations allow the ability to move fluidly between the egocentric and secondary perspectives[li2021ego]and inspire new research for holistic human understanding. To our knowledge, EgoHumans is the only multi-human 3D egocentric benchmark with these attributes. We generate high-quality 3D ground truth by leveraging state-of-the-art visual-inertial odometry algorithm (VIO)[aria_pilot_dataset], which is robust to fast head motion and sudden changes in the eye gaze - frequently observed in natural human behavior[zhang2020wandering]. All the cameras in our multi-view capture are aligned to a single world coordinate system using Procrustes alignment[luo2002iterative]of the camera poses. EgoHumans consists of 125k egocentric RGB images and 410k human instance annotations (Tab.1) capturing high-energy activities in various locations, clothing, and lighting conditions with severe occlusion. We annotate the tracking ids, bounding boxes, and 2D/3D human poses for all views using off-shelf estimators[jin2020whole,wang2020deep]and manual supervision. With carefully calibrated camera parameters and the multi-view 2D poses for a video, we optimize for 3D skeletons using triangulation[iskakov2019learnable]and refinement constraints like constant limb length, joint symmetry, and temporal consistency[vo2020self]. Finally, we build an efficient multi-stage motion capture pipeline to fit the SMPL[loper2015smpl]body model to the 3D human skeletons. The scale and diversity of the EgoHumans dataset allow unprecedented opportunities to evaluate and improve egocentric methods. Specifically, we evaluate existing methods for multi-human tracking. Our results show that prior art is susceptible to common failures like person-id switching due to rapid camera motion, occlusion, and unconstrained human activities. Inspired by this, we presentEgoFormer, a novel 3D human tracking approach with multi-stream transformer architecture that effectively performs human depth reasoning in a camera-agnostic frame of reference. Our proposed method uses self-attention to aggregate multi-view spatial information from the RGB, left, and right stereo cameras simultaneously. EgoFormer significantly outperforms existing state-of-the-art tracking methods[zhang2022bytetrack,rajasegaran2022tracking,cao2022observation]by13.6% IDF1 score on EgoHumans. Our contributions are summarized as follows. EgoHumansis the first multi-human 3D egocentric dataset capturing unconstrained human activities in the wild. We provide high-quality 3D ground truth from egocentric and secondary views for all humans. We benchmark existing state-of-the-art methods for multi-human tracking and highlight their fundamental limitations on egocentric views. We proposeEgoFormer, a 3D tracking method that uses a multi-stream spatial transformer encoder for depth reasoning from the ego view. Our method consistently outperforms the prior art on the EgoHumanstestset.
Fluency score: 3
Fluency issues: The text is generally fluent but occasionally dense and jargon-heavy, which may obscure meaning for non-specialists.

Clarity score: 3
Clarity issues: The heavy use of domain-specific terminology and acronyms without proper introduction or explanation may confuse readers unfamiliar with the topic.

Grammaticality score: 4
Grammaticality issues: Minor issues, such as inconsistent use of commas around citations (e.g., "telepresence[lawrence2021project,"), and lack of space after some commas and before opening brackets (e.g., "benchmark,EgoHumans," and "method,EgoFormer,").

Readability score: 3
Readability issues: The text is readable for a technical audience but may be difficult for a general audience due to complex sentence structures and specialized language.

Coherence score: 4
Coherence issues: Overall, the text is coherent, but the sudden introduction of specific concepts and tools without sufficient background detracts from the flow (e.g., numerous bracketed citations and technical descriptions without context).
====================
	 Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches
==========
Introduction:  Image denoising aims to recover noise-free images from noisy observations by reducing the potential noise. Although it is one of the oldest and most classical tasks in low-level computer vision, its fundamental nature continues drawing much interest. In general, existing image denoising algorithms can be divided into three groups: filtering-based[2,3,4,5,66], model-based methods[9,32,30,12,69,61]and learning-based[17,19,21,41,36,40,44,39,38,25]methods. The additive white Gaussian noise (AWGN) assumption is widely used in image denoising. However, it is complex and challenging to adaptively achieve denoising based on filtering-based or model-based methods for high performance. In contrast, learning-based methods have demonstrated their superiority in image denoising. However, these methods[54,21,25,53,aind1,100]are data-driven and typically require pairs of clean-noisy datasets to train their models. These noise samples are usually obtained through a predefined AWGN formulation that assumes the noise is signal-independent. On the contrary, the real-world noise is more complex and much different from the ideal AWGN assumption. Using the AWGN model directly for the real scenes leads to poor performance. Therefore, numerous methods[37,40,38,39,74,90,aind1]have been proposed to capture paired clean-noisy image datasets from real scenes to promote the training of deep networks. However, these paired image-based methods focus on enhancing performance by improving network structures, and acquiring well-aligned pairs of clean-noisy images is time-consuming and laborious. To solve the aforementioned problems, unsupervised denoising-based methods[chen,hong,1000,dbsnl]have emerged. The existing approaches are typically based on generative adversarial network (GAN) frameworks, which mainly focus on generating higher quality pseudo-noisy images. GAN2GAN[cha]identified one of the key limitions of unsupervised denoising frameworks is the gap between the real and synthetic images, and proposed a novel approach using multiple generators and discriminators to generate images that closely conform to the real noise distribution. However, the performance of existing unsupervised denoising frameworks remains unsatisfactory due to the difficulty of adversarially training. Moreover, after training the model, the existing frameworks cannot further maximize the denoising potential without significantly changing its structure or increasing the inference complexity (e.g., using certain self-ensemble strategy) for denoisers. To address the previous limitations, we innovatively propose an unsupervised real-world denoising network called Self Collaboration Parallel Generative Adversarial Branches (SCPGabNet). The self-collaboration (SC) strategy, which provides the framework a powerful self-boosting capability. This enables the denoisers obtained from the conventional GAN framework to continuously evolve themselves and significantly improve their performance. The major contributions of our method are as follows: We design a novel filter-guided synthetic noisy image generator with the noise extraction (NE) module to synthesize high-quality clean-noisy image pairs, which serve as the foundation for implementing the SC strategy. We propose an effective parallel generative adversarial branches framework with complementary “self-synthesis" and “unpaired-synthesis" constraints as our powerful baseline. We are the first to propose the SC strategy, which significantly enhances the performance of the GAN-based denoising framework without increasing its inference complexity. Experimental results demonstrate the superiority of our SCPGabNet over state-of-the-art unsupervised image denoising methods with large margins on the SIDD and DND benchmarking datasets.
Fluency score: 3
Fluency issues: [use of the word "and" before "learning-based" is inconsistent with previous list items; "aind1" seems to be a typographical error; inconsistent use of "the" before the term "AWGN"]

Clarity score: 3
Clarity issues: [References as [numbers] disrupt flow; terms like "unsupervised denoising-based methods" might be unclear to non-experts; "methods" repeated unnecessarily]

Grammaticality score: 3 
Grammaticality issues: [Missing a comma before "and learning-based"; "aind1" is not a word; "limitions" is a typo for "limitations"]

Readability score: 2
Readability issues: [Frequent bracketed citations interrupt the text; complex sentences with nested clauses; multiple technical terms without prior explanations]

Coherence score: 3
Coherence issues: ["GAN2GAN[cha]identified" lacks space; "These methods[54,21,25,53,aind1,100]" is incoherent due to "aind1" typo; the term "self-collaboration (SC)" introduces an acronym without need]
====================
	 Towards More Efficient Depression Risk Recognition via Gait
==========
Introduction:  Depression is recognized by the World Health Organization (WHO) as one of the primary contributors to the global disease burden[44]. Within the clinical framework, the American Psychiatric Association’s Diagnostic Statistical Manual of Mental Disorders- Fifth Edition (DSM-5) classifies depression as persistent depressive mood or loss of interest and pleasure in activities. It affects approximately 280 million individuals worldwide, comprising approximately 3.8% of the global population[43]. Depression profoundly impacts individuals across multiple domains, including a decline in their overall quality of life[30,20], compromised social functioning[25,23], detrimental effects on physical health[6,46], and an elevated susceptibility to suicide[24,9]. Recognizing the significance of early detection and timely intervention is crucial for promoting remission, preventing relapse, and mitigating the emotional and financial burdens associated with this condition[12,13,22]. However, depression is notably underdiagnosed[12,62,55]. A survey involving 33,653 physician-patient interactions reveal that in the primary care setting, less than 5% of adults are screened for depression[1]. And it has been reported that at least 25% of patients go undiagnosed[2], with a significant majority of those seeking assistance from primary care physicians not receiving suitable treatment, particularly in low-income and middle-income countries[64,31]. The main challenge in screening depression in the primary care is the absence of objective indicators to recognize depression risk[5]. Unlike many physiological illnesses, depression lacks precise biomarkers, clinicians primarily rely on clinical criteria such as psychological questionnaires and patients’ self-reports[21]. This introduces potential issues: Individuals may provide biassed or inaccurate information due to personal inclinations, social expectations or difficulties in recalling past experiences[40,32,8]. Approximately 50% of patients have been observed to negate experiencing depressive feelings[5]. Besides, variability in linguistic expression can hinder accurate communication of emotions or experiences[4,28,41,14]. These issues can lead to incorrect depression risk recognition, and missing the optimal window for further treatment[47,15,16]. Another primary issue with questionnaire-based approaches for depression risk recognition pertains to inefficiency. Psychological assessments are time-consuming and often encounter a shortage of trained medical professionals, resulting in prolonged referral processes. Consequently, this may hinder timely psychological intervention, potentially exacerbating mental health issues[11,7]. In addition, psychological questionnaires are typically intended for periodic, rather than frequent, evaluations, often yielding only one or two measurements annually. This limited frequency may not effectively capture long-term condition trends in a timely manner, in light of the high relapse rates associated with depression: 21% at 12 months, 30% at 2 years, and 42% at 5 years[29]. Gait has been shown to be an essential manifestation of depression risk[53,50,19]. In particular, gait is modulated by the advanced neural center[56], which is also implicated with the pathophysiology of depression[58]. Many researches have demonstrate the association between depression risk and gait characteristics. Notably, specific abnormalities in gait, such as decreased vertical head movement[39], reduced range of motion in limbs, and a decelerated pace[34], are stable indications of depression. Hence, the gait-based depression risk recognition offers advantages compared to traditional psychological assessment methods: Objective biomarker: Gait serves as an objective biomarker for recognizing the risk of depression within the primary care setting, thereby mitigating the influence of subjective biases, societal expectations, or challenges in recollecting past experiences, which often lead to inaccurate information. High efficiency: Gathering gait data through cameras proves to be a highly efficient method, surpassing the time-consuming process of administering psychological questionnaires. This approach eliminates the requirement for trained professionals to collect data, thereby reducing both time and manpower costs. Despite preliminary investigations into the relationship between gait and depression risk, as well as attempts to recognize depression risk based on gait features, existing research suffers from two significant limitations. Firstly, these studies are often constrained by limited and homogeneous gait data, typically lacking the inclusion of gait data captured from different perspectives and with varying attire. Consequently, the generalizability of these research findings remains to be established, as the conclusions drawn may not necessarily apply to more diverse gait data. Furthermore, the accessibility of these datasets is severely restricted, impeding research reproducibility and hindering progress in the field. This limitation not only undermines the ability to validate previous research but also hampers advancements in this domain. Secondly, these studies commonly rely on hand-crafted gait features for analysis. However, human gait is a complex form of motion, and hand-crafted gait features often provide a limited representation, only capturing a fraction of the intricate associations between gait and depression risk. To address these limitations, we have established the largest gait-based dataset for depression risk recognition to date, encompassing over 1,200 subjects and more than 40,000 gait sequences. This dataset incorporates diverse perspectives and attire variations, providing a comprehensive representation of gait data. The depression risk of the participants was annotated using two commonly used depression risk assessment scales in the primary care. Leveraging this rich dataset, we propose a novel data-driven approach for depression risk recognition based on deep learning in this paper. The proposed method overcomes the problems associated with hand-crafted features by autonomously learning gait features that are relevant to depression risk. There are two notable characteristics in depression risk related gait features: Firstly, the main features associated with depression risk are dynamic features, which refer to the temporal aspects of gait during the walking process. Secondly, gait features that are associated with depression risk can manifest as either local details or involve the entire body. Based on these two characteristics, this paper proposes a deep learning model with dynamic feature modeling as its core. In the feature extraction process, both local dynamic features and global dynamic features are effectively integrated. This innovative methodology offers a fresh perspective for investigating the association between gait and depression risk. The contributions of this paper can be summarized as follows: We build a large-scale dataset for gait-based depression risk recognition. This dataset serves as a benchmark, propelling advancements in the field of depression risk recognition. Ultimately, it aims to enhance the efficiency of depression screening in the primary care, ensuring that more individuals receive the assistance they need. Based on the insights on depression-related gait features, we introduce a deep learning model with dynamic feature modeling at its essence. This model adeptly merges local and global features, yielding a comprehensive integration. A plethora of experiments are conducted to delve deeper into the aberrant gait patterns that are associated with the risk of depression. These experiments offer enlightening perspectives that contribute to the field of depression research. The remainder of this paper is organized as follows: Section2presents a brief literature review of the related work. Section3introduce the gait-based dataset for depression risk recognition. The proposed recognition model are described in detail in Section4. The configurations and results of experiments are presented in Section5. Finally, the conclusion of this paper is summarized in Section6.
Fluency score: 4
Fluency issues: [N/A]

Clarity score: 3
Clarity issues: [Complex sentence structures and heavy use of jargon, numbers and references within the text hinder a straightforward understanding.]

Grammaticality score: 4
Grammaticality issues: [Minor errors such as "Many researches," which should be "Many studies" or "Much research"]

Readability score: 3
Readability issues: [The text is dense, with numerous technical terms, references, and figures that interrupt the flow.]

Coherence score: 4
Coherence issues: [Despite the logical structure, the abundance of details and citations may disrupt the cohesion for a lay reader.]
====================
	 Overview of the TREC 2023 Product Product Search Track
==========
Introduction:  At TREC 2023, we hosted the first TREC Product Search Track, looking to create a reusable general benchmark for evaluating the performance of retrieval methods in the product search domain. We focus on providing a benchmark similar in scale and format to NQKwiatkowskiet al.(2019), or the Deep Learning TrackCraswellet al.(2021)but focused on product search. In providing a simple-to-use dataset, we believe broad experimentation using popular retrieval librariesLinet al.(2021)Gaoet al.(2022)can lead to broad improvements in retrieval performance.In this first year of the track, we created a novel collection based on the ESCI Product Re-ranking datasetReddyet al.(2022), sampled novel queries, created enriched metadata in the form of additional text and images along with seeded evaluation results with a broad range of baseline runs to aid in collection reusability and to allow iteration and experimentation on the use of additional context.Unlike previous product search corpora, the Product Search Track is multi-modal and has a large enough scale to explore the usage of neural retrieval methods. We observe somewhat surprising results using this scaled dataset and a wide variety of baseline runs. Single-stage retrieval models that leverage vector representations do not consistently outperform traditional retrieval methods such as BM25. Moreover, in the zero-shot setting, we find that larger vector-based models do not always beat their more minor variants, which is at odds with other evaluation corpora such as MTEBMuennighoffet al.(2023). Finally, while additional metadata can improve retrieval performance at a macro level, extra information cannot guarantee performance. In evaluating per-query performance, we find that vector-based systems lose performance with the other metadata. Please see the participant papers for more insights about what we learned this year.
Fluency score: 3
Fluency issues: [probable references without spaces after them (e.g., "al.(2021)but", "librariesLinet", "datasetReddyet"), inconsistent naming convention (e.g., "NQKwiatkowskiet al." should be "NQ Kwiatkowski et al."), and phrasing ("lose performance with the other metadata" is unclear).]

Clarity score: 3
Clarity issues: [Complex sentences with insufficient punctuation, jargon-heavy without explanations, and lack of clear antecedent in "lose performance with the other metadata".]

Grammaticality score: 3
Grammaticality issues: [No spaces after references which seem like citation errors (e.g., "al.(2019)," "librariesLinet"), inconsistent use of spaces in names (e.g., "Deep Learning TrackCraswellet"), and awkward constructions ("lose performance with the other metadata").]

Readability score: 3
Readability issues: [Dense academic language with jargon, complex sentences without clear structures, references placed awkwardly within the text.]

Coherence score: 3
Coherence issues: [Awkward transitions between topics, jargon that is not explained, and the last sentence feels disconnected from the rest of the text.]
====================
	 Towards Complex Real-World Safety Factory Inspection: A High-Quality Dataset for Safety Clothing and Helmet Detection
==========
Introduction:  Safety is a perpetual and significant concern in all industries, particularly in high-risk construction sites such as chemical factories and building sites. Protective equipments, including safety clothing and helmets, are crucial for safeguarding workers in high-risk construction sites. Helmets can effectively prevent head injuries caused by falling or splashing objects, while safety clothing can protect the body and arms from hazardous chemicals and liquids. The lack of safety clothing and helmets frequently results in safety accidents, with terrible impacts on families and society. Therefore, monitoring the usage of safety clothing and helmets in factories or construction sites is of immense safety significance and broad application value. Current laws and safety regulations stipulate that the person in charge and the contractor are responsible for providing, supervising, and maintaining personal protective equipment in construction sites. However, some workers may relax their vigilance due to a lack of safety awareness or discomfort from long-term wear, thereby increasing the probability of safety accidents. Currently, the commonly used automated monitoring technique for compliance of safety clothing and helmets is a class of sensor-based methods[1,2]. Sensor-based technology monitors whether the workers wear safety clothing or helmets by attaching sensors to them and analyzing the signals. However, this method requires significant investments in purchasing, installation, and maintenance, resulting in relatively high costs. In recent years, deep learning methods have gained substantial attention in computer vision[3,4]due to their ability to self-learn useful features from large-scale, annotated training data. In particular, convolution neural networks are widely used for image classification and object detection. For example, LeCun et al.[5]utilized Convolutional Neural Networks (CNNs) to recognize handwritten digits, while Kolar et al.[6]used CNNs to detect safety guardrails in construction sites. Nath et al.[7,8]used CNNs to identify common construction-related objects (e.g., buildings, equipment, and workers). These works provide new insights into monitoring the usage of safety clothing and helmets. As we all know, the great success of deep learning depends on large-scale, high-quality, and annotated data. To the best of our knowledge, only two safety clothing and helmet benchmark datasets have been released, namely Pictor-v3[9]and Safety Helmet Wearing Dataset (SHWD)[10]. However, the categories and quantities of labeled instances in the two datasets fall short of meeting the requirements for real-world applications. The existing datasets only consider the impact of helmets without paying attention to the impact of safety clothing. In real-world applications, there are still potential safety hazards. In addition, the backgrounds of all images are too simple, and the lighting conditions are overly ideal, which differs significantly from real-world working environments. If such datasets are actually employing in a factory safety inspection project, the generalization ability of the model is likely to be poor. To address the above limitations, in this work, we contribute a large, realistic, and comprehensive high-quality dataset for safety clothing and helmet detection, named the Towards Complex Real-world Safety Factory (TCRSF) dataset. A comparison of the images in TCRSF and those in the existing dataset is shown in Figure1. It can be seen that the proposed TCRSF provides more realistic scenarios and more complex backgrounds. TCRSF contains 50558 labeled instances of 7 categories, including “Safety Helmet”, “Safety Clothing”, “Head”, etc. To simulate the real-world work environment, we added labels for safety clothing and helmets under different lighting conditions and shadow areas, called “blurred head” and “blurred clothing”. Additionally, all images were collected by factory cameras, covering two factories and 40 different scenes. And each instance was manually annotated by professional inspectors from the factory, guaranteeing accurate annotations. It is worth mentioning that, our TCRSF dataset can serve as an evaluation benchmark for various detection tasks, such as small object detection and high-low light object detection. To evaluate the effectiveness of our dataset in a real-world factory environment, we adopt YOLO-v5111YOLO-v5 is a deep learning model for object detection, image segmentation and image classification.as the base model for training, trained it using our dataset. And we deploy the model in the factory and receive streaming data from the camera in real-time to inspect whether workers wear safety clothing and helmets. The TCRSF dataset introduces a novel and robust evaluation benchmark for construction safety inspection, facilitating its adoption in real-world applications. The main contributions of this work are as follows: [(1)] We have contributed a large, realistic, and high-quality dataset named TCRSF. This dataset provides a new and reasonable evaluation benchmark for the community, and we hope that its contribution can promote the development of construction safety inspection. By conducting experiments on existing models, we have validated the completeness of our dataset. Furthermore, the object detection model achieves excellent performance on our dataset when deployed in a real chemical plant environment. We have developed a comprehensive paradigm for real-time detection of workers’ safety clothing and helmets, and provided the full implementation of the application workflow code. To make it easily accessible to non-experts, we have integrated the code such that model training and optimization can be achieved by simply modifying a few parameters in a fixed file. All datasets and codes have been open sourced and are available fromhttps://github.com/sofffty/TCRSF. The rest of this article is organized as follows. In Section2, we review current status of open-source datasets for safety clothing and helmets and recent advances in object detection. In Section3, we present a detailed description of TCRSF. We introduce the selected architectures and experimental procedure to compare the performance in Section4. We discuss the gap between academia and industry and explores future directions, as well as the real-world performance of TCRSF in Section5. Finally, in Section6, we conclude this article.
Fluency score: 4
Fluency issues: Occasional awkward phrasing such as "If such datasets are actually employing in a factory safety inspection project."

Clarity score: 4
Clarity issues: Overloaded sentences and specialized terms without explanations such as "Sensor-based technology monitors...," "YOLO-v5111YOLO-v5 is a deep learning model..."

Grammaticality score: 3
Grammaticality issues: Some grammar mistakes and punctuation missing, such as "Protective equipments," "safety accidents," and missing space in "computer vision[3,4]due."

Readability score: 3
Readability issues: Long sentences, excessive technical jargon, and dense information presentation reduce readability.

Coherence score: 4
Coherence issues: The text would benefit from clearer transitions and better structuring of information for smoother flow, particularly in the dataset and future work discussions.
====================
	 Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs
==========
Introduction:  The capsule endoscopy is a medical procedure that has been used for investigating the midsection of the GI tract since early 2000[12,3]. This minimally invasive method allows to visualize the small intestine, which is in most part not accessible through standard techniques using flexible endoscopes[22]. The procedure starts by swallowing a pill-sized capsule. While it moves through the GI tract by peristalsis, it sends captured images from an integrated camera with either an adaptive or a defined frame rate to an electronic device. The overall aim of this procedure is to detect diseases affecting the small intestine such as tumors and its preliminary stages, angiectasias as well as chronic diseases[22,17,24]. Since the esophagus, stomach and colon can be more easily assessed by standard techniques, the small intestine section is of main interest in VCE studies. All images of the small intestine should be transmitted for further evaluation by medical experts who are qualified to check for anomalies. The frame rate of the most prominent capsules ranges from1to30frames per second with a varying resolution between256\times 256and512\times 512depending on the platform[22]. For example, the PillCam® SB3 by Medtronic lasts up to12hours with an adaptive frame rate of2to6frames per second[18]. This should ensure passing through the whole GI tract before the energy of the capsule’s battery is depleted. However, a capsule can also require more than one day to pass through the whole GI tract leading to an incomplete record of images due to depletion of the capsule’s battery after maximal12hours. In this procedure, the energy is the bottleneck and small changes of the architecture can increase the overall energy requirement leading to a shorter battery lifetime with the risk of running out of energy without covering the small intestine. However, modifications such as capturing images with a higher resolution might improve the recognition ability of clinicians and thus, it is desirable to increase the limited resolution or add more functions (e.g. zooming in or out, anomaly detection on-site) helping to successfully scan the GI tract for anomalies at the cost of increasing energy demands. The images taken before the small intestine are not of interest but demand their share of energy for capturing and transmitting the images. This paper presents a method for very accurately determining the location of the capsule by on-site evaluation using a combination of neural network classification and time-series analysis by a HMM. This neglects the necessity to consume electric energy for transmitting images of no interest. If this approach is integrated into the capsule it can perform precise self-localization and the transition from the stomach to the small intestine is verified with high confidence. From this moment onwards, all frames should be send out for further evaluation. A major part of the energy can be saved since the data transmission only starts after the capsule enters the small intestine and therefore can be used for other valuable tasks. For example, the frame rate or resolution could be increased while in the small intestine or additionally, a more complex network for detecting anomalies on-site could be employed. In the field of gastroenterology, there have been different approaches to perform localization of a capsule within the GI tract[16]including but not limited to magnetic tracking[19,26], video-based[28,15]and electromagnetic wave techniques[27,7]. However, Charoen et al[2]were the first to publish a dataset with millions of images classified into the different sections of the GI tract. They achieved an accuracy of97.1\%with an Inception ResNet V2[23]architecture on the RI dataset and therefore successfully demonstrated precise localization without aiming for an efficient realization on hardware. To the best of our knowledge, there is no superior result than the baseline with this dataset. However, a large network with56M parameters as the Inception ResNet V2 is not suitable for low-power embedded systems since the accompanied high energy demand results in a short battery lifetime. Thus, we present a new approach for this problem setting using the same dataset and the same split resulting in a higher accuracy while requiring a much smaller network and less parameters.
Fluency score: 4
Fluency issues: "from1to30frames", "256\times 256and512\times 512", "lasting up to12hours"

Clarity score: 4
Clarity issues: Lack of context or explanation for abbreviations like "VCE", unnecessary technical details for a general audience, "all frames should be send out" should be "sent out"

Grammaticality score: 4
Grammaticality issues: Misuse of "allows to" should be "allows one to" or "enables", inconsistent use of the serial comma

Readability score: 3
Readability issues: Dense technical language, lack of spacing in numerical values and resolutions, use of jargon without explanation

Coherence score: 4
Coherence issues: Sudden introduction of the new method without proper transition, lack of explanation about HMM, some terms and abbreviations not introduced or explained (e.g., "on-site evaluation")
====================
	 Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation
==========
Introduction:  Case-based reasoning (CBR) is a methodology in which concrete past occasions are directly used as sources of knowledge and solutions for new situations(DBLP:books/daglib/0032926). It has been studied in AI and Law since its inception, leading to foundational contributions(Rissland2005CasebasedRA). This is a not a surprise, given the centrality of the use of cases for determining the law in Common Law systems, although not exclusivelylewis2021. In this paper we focus on recent approaches to CBR(DBLP:conf/kr/CyrasST16;dear-2020;DBLP:conf/kr/Paulino-PassosT21;Prakken2022ATM)using argumentation(prakken-overview). Argumentation itself has a long history in AI and Law, and its use to support CBR has been shown to pave the way towards novel forms of explanations for the outcomes of CBR, including via arbitrated dispute trees(DBLP:journals/eswa/CyrasBGTDTGH19;DBLP:conf/ijcai/Cyras0ABT21). Specifically, we focus on the\aacbrapproach(DBLP:conf/kr/CyrasST16;dear-2020;DBLP:conf/kr/Paulino-PassosT21), where arguments correspond to cases and attacks between arguments result from outcome disagreement between cases andrelevancebetween cases, guided by a partial order over cases capturing some notion of specificity. Originally(DBLP:conf/kr/CyrasST16),\aacbrexpects a representation of cases in terms of sets ofmanually engineered binaryfeatures and the partial order is defined via the subset relation. This expectation is a restriction for applicability. While previous work has generalised beyond binary features in order to support different applicationsdear-2020, a systematic generalisation to tabular datasets, including categorical and continuous data, is still missing. This is essential for applying\aacbrtorealisticdatasets, including legal ones, to realise the original inspiration from legal reasoning for\aacbr. While some form of binarisation can be applied, there is no guarantee that a naïve binarisation would result in good performance. In this work we close this gap, focusing on applying\aacbrto possibly non-binary tabular data from legal settings. Specifically, our first contribution is a general method for applying\aacbrto any tabular data by extracting binary features from decision treesDBLP:books/wa/BreimanFOS84when learning for the final task. Our second contribution is showing that this method is competitive with decision trees on two legal datasets: COMPASpropublica-compasand a simulated legal datasetDBLP:journals/argcom/StegingRVB23for welfare benefit. Finally, as a third contribution, we show thatauthor=Guilherme, caption=,author=Guilherme, caption=,todo:author=Guilherme, caption=,revise according to actual final resultsour method creates smaller models (i.e. with a smaller number of nodes), leading to potentially more cognitively tractable explanations (i.e. decision trees and rules drawn from them on one hand, and argumentation frameworks and arbitrated dispute trees on the other). \timesExplainability is a fundamental requirement in AI and Law. In particular, any model of legal reasoning must be amenable to explanation, which is required not only of AI systems, but of legal decision-makers in general. Current state of the art methods in many AI tasks is data-driven methodologies (machine learning) whether machine learning-base …\aacbrhas been developed …(DBLP:conf/kr/CyrasST16;dear-2020).. Important aspects for any concrete implementation of\aacbrare the representation of the data (to which we refer as characterisation) and, crucially, the partial order\pleqon the set of characterisations, which until now we have assumed given. This partial order captures the idea of relevance of past cases for a new case, as well as of specificity among past cases. In this paper, we will discuss ways of defining this partial order. In particular, we are interested in learning the partial order from data. Learning the partial order allows\aacbrto a wider scope of problems, instead of requiring a hand-constructed order. This carries three desiderata: 1) that the learned partial order results in a good performance; 2) that it is human interpretable. We here present a method for learning a partial order from tabular data using features extracted from decision trees trained on this same data. . We show empirical results viaexperiments are carried out on the the COMPAS dataset(propublica-compas), a dataset on prediction of recidivism, that is, on prediction of whether a criminal will re-offend(2016COMPASRS), as well as on the simulated legal data of…
Fluency score: 2
Fluency issues: Misplacement of references like "(DBLP:books/daglib/0032926)" throughout the text; Unusual construct like "This is a not a surprise"; Conjoined words in "\aacbrapproach" and "andrelevancebetween"; Italicized "\times" is unclear.

Clarity score: 2
Clarity issues: Overuse of domain-specific jargon and acronyms without adequate explanation; Citation-like references disrupt sentence flow; Incomplete sentences like "Explainability is a fundamental requirement in AI and Law."

Grammaticality score: 3
Grammaticality issues: Double "a" in "This is a not a surprise"; Lack of spaces in "\aacbrapproach", "andrelevancebetween"; Italicized "\times" appears out of context.

Readability score: 2
Readability issues: The text is dense with technical terms and lacks clear separation of ideas; The references are intrusive and disrupt the reading flow; The structure of the last paragraph with numerical list is abrupt.

Coherence score: 2
Coherence issues: Frequent parenthetical interruptions; Discontinuous topics; Incompletely resolved acronyms; Misplaced citation elements like "author=Guilherme, caption=".
====================
	 High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft
==========
Introduction:  Costly black-box simulations play an important role for many engineering and industrial applications. For this reason, surrogate modeling has been extensively used across a wide range of use cases, including aircraft designSciTech_cat, deep neural networkssnoek2015scalable, coastal flooding predictionlopez, agricultural forecastingMLP, and seismic imagingYDiouane_SGratton_XVasseur_LNVicente_HCalandra_2016. These black-box simulations are generally complex and may involve mixed-categorical input variables. For instance, a Multidisciplinary Design Analysis (MDA) aircraft design toolDavid_2021must consider mixed variables such as the number of engines or the list of possible materialsSciTech_cat. In this paper, our objective is to develop an affordable surrogate model, denoted as\hat{f}, for a black-box function that involves mixed variables given by This functionfrepresents a computationally expensive black-box simulation.\Omega\subset\mathbb{R}^{n}denotes the bounded continuous design set for thencontinuous variables.S\subset\mathbb{Z}^{m}denotes the bounded integer set whereL_{1},\ldots,L_{m}are the numbers of levels of themquantitative integer variables on which we can define an order relation and\mathbb{F}^{l}=\{1,\ldots,L_{1}\}\times\{1,\ldots,L_{2}\}\times\ldots\times\{1% ,\ldots,L_{l}\}is the design space for thelcategorical qualitative variables with their respectiveL_{1},\ldots,L_{l}levels. For such purpose, the use of a Gaussian Process (GP)williams2006gaussian, also called Kriging modelkrige1951statistical, is recognized as an effective modeling approach for constructing a response surface model based on an available dataset. Specifically, we make the assumption that our unknown black-box function, denoted asf, follows a GP with mean\mu^{f}and standard deviation\sigma^{f}, expressed as follows: Several modeling approaches have been put forward for addressing the challenges of handling categorical or integer variables within the context of GPPelamatti;Zhou;Deng;Roustant;GMHL;Gower;cuesta2021comparison;SciTech_cat. In comparison to GP designed for continuous variables, the most important changes concern the estimation of the correlation matrix, an essential element in the derivation of\mu^{f}and\sigma^{f}. Much like the procedure for constructing a GP with continuous inputs, Continuous Relaxation (CR) techniquesGMHL;SciTech_cat, models involving continuous latent variablescuesta2021comparison, and Gower Distance (GD) based modelsGoweruse a kernel-based approach for estimating this correlation matrix. However, recent innovative approaches take a different path by modeling directly the various entries of the correlation matrixPelamatti;Zhou;Deng;Roustant, and therefore, do not rely on any kernel choice, such methods involve the Homoscedastic Hypersphere (HH)Zhouand the Exponential HH (EHH)Mixed_Paulkernels. It has been shown inMixed_Paulthat the HH correlation kernel generalizes simpler methods like CR or GD kernels. However, this more general method for handling categorical design variables increases the number of hyperparameters required to be tuned associated with the GP model. In particular, this means that the method could only be used for small dimensional problems. Many efficient approaches have been proposed for handling a high number of continuous variables within GPBouhlel18;bouhlel_KPLSK;bouhlel2019gradient. The Kriging with Partial Least Squares (KPLS) methodBouhlel18;bouhlel_KPLSKis one of the most commonly used reduction techniquesKPLSu1;KPLSu2to tackle high dimensional data. Several other dimension reduction methods include principal components analysiswang2017batched, polynomial chaos expansionzuhal2021dimensionality, radial basis functionsregis2020survey, active subspaceAS, manifold embeddingtenenbaum2000globalor marginal Gaussian processMGP. The KPLS technique allows constructing the GP model with the same continuous variables but using a few number of parameters; which reduces significantly the computational cost of computing a GP model. For mixed-categorical GP models, given that the computational effort related to the construction of the GP model may not scale well to practical applications involving categorical variables with a large number of levels, the number of hyperparameters to be tuned need to be considered more thoroughly. In the literature, GPs have been applied to no more than 15 hyperparameters due to the high computational cost associated with the estimation of the hyperparametersGP14. Adapting dimension reduction techniques, such as KPLS, to mixed-categorical GPs will thus enable solving practical mixed-categorical engineering problems where often a high number of hyperparameters is required to be estimated. To the best of our knowledge, there is no equivalent approach to handle mixed-categorical data without using relaxation techniques. All existing dimension reduction techniques, including KPLS, are not adapted for advanced mixed-categorical GP models such as HH or EHH. We note also that, although this paper focuses mainly on surrogate modeling, the proposed models can be integrated within any surrogate-based optimization methodAuHa2017, such as surrogate-based evolutionary algorithmsbea1;sbea2or a Bayesian Optimization (BO) methodJones2001JOGO. In this work, we target to use dimension reduction techniques for reducing the number of hyperparameters within the GP in order to allow modeling efficiently high-dimension mixed-categorical data. In this context, high dimensionality is related to the high number of categorical variables potentially with a high number of levels (a few dozen). In fact, using relaxation approaches (by converting categorical choices to continuous variables) leads to a very high number of hyperparameters to estimate, particularly for high resolution approaches such as those based on HH and EHH kernels. We have also specifically used our proposed mixed-categorical GP models, within a BO framework, to solve a constrained optimization problem involving expensive-to-compute black-box simulations for objective and constraints functionsMartins2021. The proposed approach is shown in particular to be efficient in solving a high dimensional mixed-categorical Multidisciplinary Design Optimization (MDO) problemLambe2012. All the GP models proposed in this work are implemented in the open-source Surrogate Modeling Toolbox (SMT)111https://smt.readthedocs.io/en/latest/saves2023smt. The remainder of this paper is the following. In Section2, a detailed review of the GP model for continuous and for categorical inputs is given. In Section3, we present the PLS regression for vectors and matrices and their application to GP model for both continuous and categorical inputs. Section4presents academical tests as well as the obtained results on multidisciplinary optimization. Conclusions and perspectives are finally drawn in Section5. Notations:For a vectorx, both notations[x]_{j}andx_{j}stand for thej^{th}component ofx. Similarly, thei(row index) andj(column index) entry of a matrixXis denoted[X]^{j}_{i}.
Fluency score: 3
Fluency issues: Mathematical symbols disrupt the normal flow of the text; inconsistent capitalization (e.g., "Multidisciplinary Design Analysis" vs "multidisciplinary optimization").

Clarity score: 2
Clarity issues: Overuse of jargon, lack of explanation for acronyms, and dense information without proper structuring or lead-in sentences.

Grammaticality score: 4
Grammaticality issues: Minor issues like "the number of hyperparameters to be tuned need to be considered," where "needs" should be used.

Readability score: 2
Readability issues: Text is highly technical with many specialized terms and symbols; large blocks of text with complex information decrease readability.

Coherence score: 3
Coherence issues: Frequent shifts in focus, lack of transitions between topics, and dense paragraphs with in-text citations disrupt coherence.
====================
	 SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection
==========
Introduction:  Remote sensing images change detection is one of the earliest and most important remote sensing tasks, which has been concerned and studied by many researchers for a long time[27,61,4,2]. Change detection is defined as observing the image differences of the same surface area at different times. Change detection is used in many scenarios, including disaster assessment[37], urban planning, land surface change[29,68], and so on. With the development of satellites and sensors, very high resolution remote sensing (VHRRS) images have gradually become one of the mainstream remote sensing images in research, which provide rich spatial information and fine surface details. However, one of the main challenges faced by VHRRS images change detection is high intraclass variation and low interclass variance of detection objects[38]. It, therefore, has been the focus of scholars’ research that how to design a stable network and provide comprehensive and diverse feature information to distinguish the pseudo changes in change detection (as shown in Fig.1). Traditional change detection algorithms, according to different detection units, can be divided into pixel-based algorithms and object-based algorithms. The detection results of pixel-based algorithms are obtained through feature extraction and then threshold segmentation, which include methods based on arithmetic operations (band difference[34], spectral angle mapper[69]), methods based on transformation (change vector analysis (CVA)[39,3], principal component analysis (PCA)[60], independent component analysis (ICA)[67]), post-classification change detection[54], slow feature analysis (SFA)[53]and so on. According to the shape, texture and spectrum of images, object-based algorithms need to segment the images and then compare the classification results to get the change detection results[20]. Pixel-based algorithms are trapped by the interference of small noises and the decision of segmentation threshold. Meanwhile object-based algorithms often get stuck in the accumulation of multiple classification errors that affect the detection accuracy[27]. Both of these traditional algorithms require prior knowledge and manual design, and are easily affected by sensor noises. With the support of massive remote sensing data, deep learning has also shown outstanding detection ability in the field of remote sensing. CNN converts the input images into the high-dimensional depth features, and combines the targets and background to extract effective semantic information, achieving the detection effect beyond many traditional methods.[11]provides the three most common baseline networks for change detection. The architecture combined with CNN and conditional random field (CRF) refines the edges of detection areas, but it comes at the cost of slow training[66]. CNN is hindered regrettably by the narrow receptive field of local information, and transformer rises rapidly due to the ability of modeling global information. However, it can not work well that the pure transformer change detection model lacks low-level details[58]. Therefore, how to combine transformer and CNN to build the reasonable change detection architecture is the crux of the matter at this stage. From another perspective, change detection network architectures can be divided into early fusion (EF)[11,1,41,42]and late fusion (LF)[11,56,57,6,46,14,62,15]networks. The EF network works by stitching two images together and feeding them into the single input network. By concatenating two three-channel images into a six-channel image,[1]and[41]input it into full convolutional neural network (FCN) and UNet++ respectively, and output change map after training the network. The disadvantage of this method is that the network lacks the depth features of single images, resulting in fractured edges and broken structures in change map. In the LF network, the two features are extracted from the pre-changed and post-changed images respectively by using the dual-input structure, and are fused in the second half of the network. The siamese network, the most prominent LF network, consists of two subnets with shared weights. The siamese network was first used for remote sensing images change detection in[56]. The use of convolutional block attention module (CBAM) and deep supervision for the siamese network respectively alleviates the problem of heterogeneous features fusion and depth features migration in training process[57]. However, for the LF network, the contradiction between the dual-stream input of encoder and the single output of decoder often results in the disappearance of gradient propagation and affects the low-level features learning of two original images. Furthermore, the heterogeneous features fusion of LF network needs to be solved by elaborately designed module. As a consequence, it is another problem worth pondering that how to overcome the respective disadvantages of these two network architectures and provide the complete and diverse features for change detection. In addition to the design of the overall network architectures for change detection, researchers are also pushing forward the elaboration of network functional modules. The attention modules introduced into change detection are relatively representative, including squeeze-and-excitation attention (SE)[26], efficient channel attention (ECA)[51], CBAM[52]and cross-attention[62]. At the same time, since the ground objects have different scales in the VHRRS change detection datasets, how to adapt these ground objects to maintain the robustness and generalization ability of the network? Multi-scale features of deep learning generally can be divided into three categories: multi-scale features between different layers, multi-scale interaction features between different layers and multi-scale features from different convolution units. The first type of multi-scale features was embedded in the common U-Net network. The second type of multi-scale features typically interacts and fuses using transformer or CNN. The third type of multi-scale features is provided by a variety of convolution units, such as inception[49], dilated convolution[55], res2net convolution (Res2Net-Conv)[19], selective kernel convolution (SK-Conv)[33]and so on. We should think about the integration and utilization of these three multi-scale features. Other scholars also think in the combination with generative adversarial network (GAN)[25,65]or self-supervised learning[8,5,64]to obtain more discriminative features. These deep learning technologies are aimed at solving the problem of high intraclass variation and low interclass variance by mining the different features of change detection data. Motivated by the above concerns, this study combines Swin V2 and VGG16 to propose a new end-to-end compounded dense network SwinV2DNet. Swin V2 blocks are used to build the UNet++ type main network, and VGG16 encoder is used to build the CNN auxiliary network. SwinV2DNet overcomes the modeling defect of only local information in CNN and the insufficient interpretation of low-level details in transformer. On the other hand, the Swin V2 main network belongs to EF network, and the CNN branch belongs to LF. This structure constantly provides the pre-changed features, post-changed features and change relation features (namely, the six-channel concatenation from the pre-changed and post-changed images) for the accurate acquisition of change detection results. CBAM and deep supervision also promote the fusion of heterogeneous features and the rapidly stable convergence of the network, respectively. To better combine transformer and CNN, we propose a new multi-scale module, mixed feature pyramid, which provides inter-layer multi-scale interaction information and intra-layer multi-scale information to supplement the UNet++ main network only with inter-layer multi-scale information. We finally design a new decoder to the CNN branch with only VGG16 encoder, and use the self-supervision strategy to train the extracted features, so that the CNN branch can provide learnable and more discriminant semantic information. To sum up, the main contributions of this study are fourfold: We propose an end-to-end compounded dense network SwinV2DNet that possesses both advantages of transformer and CNN, and overcomes respective disadvantages of the EF and LF network. This is the first parallel combination of Swin V2 and VGG16 in change detection. Mixed feature pyramid is proposed, for the first time, to provide inter-layer interaction information and intra-layer multi-scale information. It is a plug and play module that has been experimentally proven to be also effective in other change detection networks. We design a new decoder for the CNN branch with only VGG16 encoder, and impose the self-supervised strategy to train the extracted features to provide more discriminative semantic information for the main network. Compared with other advanced methods, our method obtains the state-of-the-art (SOTA) change detection scores and the elaborate change maps on four common public remote sensing datasets. The remainder of this paper is organized as follows. SectionIIreviews the related work. SectionIIIelaborates the proposed SwinV2DNet method. The experimental evaluations and ablation studies are carried out in SectionIV. Finally, SectionVpresents the conclusion of this article.
Fluency score: 4
Fluency issues: Overuse of technical jargon and parenthetical references can disrupt the flow.

Clarity score: 3
Clarity issues: Dense and complex sentences with embedded lists and references make comprehension difficult.

Grammaticality score: 4
Grammaticality issues: Minor errors with punctuation (e.g., missing spaces after citations like "[11]provides").

Readability score: 3
Readability issues: Technical language and long sentences with multiple clauses decrease readability.

Coherence score: 3
Coherence issues: The text jumps between different ideas and methods without clear transitions, making it hard to follow the development of the argument.
====================
	 Questions of science: chatting with ChatGPT about complex systems
==========
Introduction:  ChatGPT and other transformers-based large language models (LLMs) have demonstrated remarkable abilities to perform tasks such as completion tasks, question answering, reading comprehension and translation that were once thought to be exclusive to humans(devlin2018bert,;zhang2020pegasus,;raffel2019exploring,;fedus2021switch,;lewis2019bart,;radford2018improving,;radford2019language,;brown2020language,). We are currently in a great era for researchers and scientists studying and developing in the field of complex systems. Half of the physics Nobel prize of 2021 was awarded to the physicist Giorgio Parisi for his contributions to the theory of complex systems(Cugliandolo_2023,)and the other half to two meteorologists Syukuro Manabe and Klaus Hasselmann to the modeling of the Earth’s climate(gupta2022perspectives,). Parisi has made significant contributions to the literature on complex systems, including areas such as spin glass(Parisi1979,;Parisi1980,;Parisi_1980order,), stochastic resonance(Benzi1982,), surface growth(Kardar1986,), multifractality(frisch1985fully,), and bird flocking(Ballerini2008,). Manabe showed that there is a relation between the increase in global temperature and the level of CO{{}_{2}}in the atmosphere(manabe1967thermal,). Hasselmann developed a model that establishes a connection between weather and climate, implying that climate models can be reliable despite the unpredictability and complexity of weather patterns(hasselmann1976stochastic,;frankignoul1977stochastic,). The diversity of research topics among the award recipients highlights the multidisciplinary nature of the field. In addition, the applied dimension of the contributions stresses the importance of applied contributions to the field. In particular, it is worth mentioning that the research of field of complex systems has developed multiple frameworks, such as chaos theory(lorenz1963deterministic,), fractal theory(mandelbrot1982fractal,), complex networks(barabasi1999emergence,;watts1998collective,), and agent-based models(bonabeau2002agent,), to model and solve important practical problems in diverse fields. OpenAI has just now introduced the ChatGPT, a large language model tool trained, to aid humans in a variate of human tasks including question answering, text edition and has been used as a great information provider(brown2020language,;ouyang2022training,). ChatGPT has been trained on a large dataset of internet texts, which means that it has learned to mimic the patterns and styles of language used by many different people online. When you talk to ChatGPT, it responds based on what it has learned from that dataset, so its answers can be thought of as representing the average opinions, ideas, and language patterns that are commonly found on the internet. In this paper, we overview different aspects of the field of complex systems by means of chats with ChatGPT. We split this paper into two parts. The first part explores issues related to teaching and learning, where ChatGPT may work as a teacher assistant. The second part explores issues related to research, where, in this case, ChatGPT works like a research assistant. Besides exploring different concepts from different points of view in the field of complex systems, we also recognize that ChatGPT can be a valuable resource for teachers and scientists.
Fluency score: 4
Fluency issues: [Parenthetical citations disrupt the flow]

Clarity score: 3
Clarity issues: [Citations within the text; redundant phrases; some context missing for clarity]

Grammaticality score: 4
Grammaticality issues: [Misplaced commas; inconsistent use of citations]

Readability score: 3
Readability issues: [In-text citations hinder readability; complex sentence structures]

Coherence score: 3
Coherence issues: [Jumps between topics; repetitive points]
====================
	 Part to Whole: Collaborative Prompting for Surgical Instrument Segmentation
==========
Introduction:  Surgical instrument segmentation aims to accurately identify and delineate surgical instruments in operative scenes. It plays a foundational role for many downstream applications, such as surgical planning[7], robotic navigation[43], and skill assessment[24]. We identify two primary problems with the existing methods for this task. First, they often develop specialist models that require full training of all parameters (Fig.LABEL:subfig:motiv2-(i)), leading to high development costs. Second, current methods lack the capability of human-computer interaction that is highly desired in surgical practice[6,4]. The Segment Anything Model (SAM)[22]is a pioneering foundation model for promptable segmentation. It holds great potential for addressing the above problems owing to its rich pre-trained knowledge and interactivity. However, there still exist some fundamental challenges that impede the application of SAM in surgical instrument segmentation. Firstly, SAM’s reliance on point-or-box prompts is impractical in surgical settings, where it is infeasible for surgeons to provide such prompts for every instrument in each frame. Instead, more flexible and efficient prompts like free-form text would be preferred (e.g., via speech recognition and voice control). Secondly, SAM has shown subpar zero-shot generalisation in segmenting surgical instruments[42]due to insufficient surgical data in pre-training as well as the complex structure and fine-grained nature of surgical instruments across various categories. In this paper, we investigate text promptable surgical instrument segmentation and propose a novel framework, SP-SAM (SurgicalPart-SAM), that addresses the above challenges by leveraging SAM’s generic segmentation capability while integrating instrument structure information via an efficient model tuning scheme (Fig.LABEL:subfig:motiv2-(iii)). For text-prompted segmentation, a common practice is to use the object names to prompt the segmentation of an object of interest. However, the category names of instruments are not ideal for use as text prompts due to their specialised and abstract nature (Fig.LABEL:subfig:motiv1and Fig.LABEL:subfig:motiv2-(ii)). To this end, we introduce a new form of text prompt, namely Collaborative Prompt, using a text description set: {\say[part name]of[instrument category name]} for all parts of an instrument category. It combines both category and part information, effectively decomposing the specialised, abstract, and coarse concepts of instrument categories into more precise and fine-grained instrument parts, which is more appropriate for surgical instruments of high structure complexity (Fig.LABEL:subfig:motiv1and Fig.LABEL:subfig:motiv2-(iii)). To encode these collaborative prompts, we introduce a Cross-Modal Prompt Encoder, transforming collaborative prompts into part-level sparse and dense prompt embeddings for SAM’s mask decoder through visual-textual interaction. This enables focused learning of fine-grained representations for each instrument part, enhancing segmentation of subtle details. Finally, to integrate the knowledge of instrument structure into the model, a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy are proposed. Specifically, we adaptively assemble the learned part-level sparse and dense embeddings considering both category-specific and image-specific part weights, to predict the mask of the whole instrument. Afterwards, hierarchical decoding of both the whole instrument and its parts is performed to enhance the understanding of surgical instruments both holistically and at the part level. To this end, SP-SAM has strong capability to comprehend surgical instrument structures, identify subtle details, and discriminate between fine-grained categories. In summary, our contributions are three-fold: We introduce a novel approach SP-SAM for text promptable surgical instrument segmentation, offering a simpler prompting pipeline than the vanilla SAM that better suits the demands of surgical practices. SP-SAM achieves state-of-the-art (SOTA) performance on the challenging EndoVis2018 and EndoVis2017 datasets while requiring only a small number of training parameters. We introduce a new type of text prompt integrated with both instrument category and part information, namely collaborative prompts, and devise a Cross-Modal Prompt Encoder, which encodes text prompts jointly with visual embeddings into discriminative part-level representations, ensuring accurate segmentation of subtle details. We propose a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy to adaptively assemble the part-level representations into a whole and make predictions at both the part-level and the holistic-level, explicitly incorporating structure information regarding category-part relations and achieving better discrimination between categories.
Fluency score: 4
Fluency issues: Usage of jargon and references (e.g., Fig.LABEL:subfig:motiv1, SAM[22]) might disrupt fluency for general readers.

Clarity score: 3
Clarity issues: Overly complex sentences and the use of domain-specific terms without explanation.

Grammaticality score: 4
Grammaticality issues: Minor issues with punctuation (e.g., missing space after "[42]due") and coherence markers.

Readability score: 2
Readability issues: Dense, technical text with heavy use of jargon and long sentences makes the text difficult to parse.

Coherence score: 3
Coherence issues: Numerous ideas and complex concepts introduced without sufficient transition; heavy reliance on technical terms may challenge coherence for non-experts.
====================
	 D2Match: Leveraging Deep Learning and Degeneracy for Subgraph Matching
==========
Introduction:  Subgraph isomorphism, or subgraph matching at the node level(McCreeshet al.,2018), is a critical yet particularly challenging graph-related task. It aims to determine whether a query graph is isomorphic to a subgraph of a large target graph. It is an essential building block for various graph-based scenarios, e.g., alignment of cross-domain data(Chenet al.,2020), temporal alignment of time series(Zhou and Torre,2009), and motif matching(Miloet al.,2002; Penget al.,2020), etc. Existing studies for subgraph matching can be divided into two main streams: combinatorial optimization (CO)-based and learning-based methods(Vesselinovaet al.,2020). Early algorithms often formulate subgraph matching as a CO problem that aims to find all exact matches in a target graph. Unfortunately, this leads to an NP-complete issue(Ullmann,1976; Cordellaet al.,2004), which suffers from exponential time costs. To alleviate the computational cost, researchers have employed approximate techniques to seek inexact solutions(Mongiovìet al.,2010; Yanet al.,2005; Shanget al.,2008), which yield suboptimal matchings. An alternative solution is to frame subgraph matching as a supervised learning problem(Baiet al.,2019; Rexet al.,2020; Baiet al.,2020), which utilizes the Graph Neural Networks (GNNs). However, the learning-based or GNN-based methods mainly aim to optimize the representations in a black-box way. The lack of theoretical guarantees makes them inexplicable and often cannot seek the exact match subgraphs. In order to tackle the above challenges, we propose a white-box GNN-based solution,D^{2}Match, to leverage the efficiency of Deep GNNs and Degeneracy for subgraph matching. With rigorous theoretical proofs, we provide explainable results at each learning step. We first prove that finding the matched nodes between the query graph and the target one can degenerate to check whether the corresponding subtrees rooted at these two nodes are subtree isomorphic. This degeneration allows us to check whether a perfect matching exists on a bipartite graph, which results in a polynomial time complexity solution. The above two steps convert subgraph matching into computing an indicator matrix whose elements represent the subtree isomorphic relationship between nodes in the query graph and the target one. Hence, the matching matrix required by the CO-based methods for subgraph matching degenerates to seeking an indicator matrix, which is computed by GNNs via its intrinsic tree-structured aggregation mechanism. Note that this implementation battles the matching mechanism directly by GNNs rather than optimizing the representations of GNNs as in the existing work. Our implementation allows us to reduce the time cost of perfect matching from polynomial time to linear time. Moreover, we can easily incorporate other information, including circle structures (abstracted assupernodes) and node attributes, into ourD^{2}Matchto boost the matching performance. Our contribution is four-fold: (1) We propose the first white-box GNN-based model, calledD^{2}Match, to leverage deep learning and degeneracy for subgraph matching. We provide rigorous theoretical proofs to guarantee that subgraph matching can degenerate to subtree matching, and finally perfect matching on a bipartite graph. (2) To the best of our knowledge, this is the first GNN-based model to tackle subgraph matching directly, which degenerates the matching matrix required by the CO-based methods to an indicator matrix computed by GNNs via the intrinsic tree-structured aggregation mechanism. This allows us to compute the indicator matrix in linear time. (3) OurD^{2}Matchcan easily include other information, including circle structures and node attributes to boost the model performance. (4) Extensive empirical evaluations show thatD^{2}Matchoutperforms state-of-the-art subgraph matching methods by a substantial margin, and uncovered that learning-based methods tend to capture the divergence of the data distribution rather than exploiting graph structures.
Fluency score: 4
Fluency issues: [Parentheses close to author names without spaces, inconsistent use of spaces before and after parentheses]

Clarity score: 3
Clarity issues: [Excessive use of jargon and acronyms without explanation or context, complex sentences structure that might confuse uninitiated readers]

Grammaticality score: 4
Grammaticality issues: [Missing spaces after citations ("(McCreeshet al.,2018)", should be "(McCreesh et al., 2018)"), a comma after "e.g." would improve readability, the phrase "abstracted assupernodes" seems to have a missing space "as supernodes"]

Readability score: 3
Readability issues: [Long sentences, heavy use of domain-specific terminology, and acronyms reduce readability for general audiences]

Coherence score: 4 
Coherence issues: [Multiple topics introduced without clear transitions, the specific function of "D^{2}Match" could be better integrated into the overall context]
====================
	 Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning
==========
Introduction:  Despite its recent rise in popularity, in its current form, artificial intelligence (AI) cannot confidently make predictions robust enough to stand the test of data generated by processes different from those studied at training time, even by tiny details, as shown by ‘adversarial’ results able to fool deep neural networkspapernot2016limitations. While recognising this issue under different names (e.g.overfittingroelofs2019metaormodel adaptationli2020model), traditional machine learning seems unable to address it at a fundamental level. As a result, AI systems suffer from brittle behaviour, and find it hard to operate in new situations. A major cause of this problem is the widespread presence ofepistemic uncertainty, i.e., uncertainty about the process generating the data itself. In machine learning, this derives from the limited representativeness of the available training sets, in terms of both quantity and quality. As few samples are collected in specific domain distributions, the learnt models have no way to effectively model domain variation. The machine learning community recognises the problem. This has given rise to several proposals for estimating the uncertainty associated with network predictions, including Bayesian Dropoutgal2016dropout, distance-aware priorsgianand the use of random fuzzy setsDENOEUX2022. An “evidential" deep learning methodsensoyfor uncertainty quantification in classification using Dirichlet distribution and, more recently, epistemic neural networksosbandas a generalisation of Bayesian neural networks have been proposed. None of these methods, however, fully, explicitly capture the epistemic uncertainty stemming from the data issue. Some of them rely on prior knowledgehttps://doi.org/10.48550/arxiv.2105.06868, whereas others require setting a desired threshold on the metricsDBLP:journals/corr/abs-2107-07511. It has been argued by some that classical probability theory is, in fact, not equipped to model “second-level" uncertainty on the probabilities themselves. This has led in the past to the formulation of numerous (epistemic) uncertainty calculicuzzolin2020geometry, starting from de Finetti’s pioneering work on subjective probabilityDeFinetti74, and including possibility theoryDubois90, probability intervalshalpern03book, credal setslevi80book, random setsNguyen78and imprecise probabilitywalley91book. In this paper, we propose to model epistemic uncertainty using arandom setrepresentationMolchanov_2017. As we focus on finite target spaces in a classification setting, we will model uncertainty usingbelief functionsShafer76, the finite incarnation of random setscuzzolin2018visions. The theory of belief functions or theory of evidenceShafer76is a generalisation of Bayesian inferencesmets86bayessince classical (discrete) probabilities are simply special belief functions and Bayes’ rule is a special case of Dempster’s rule of combination. Crucially, in the theory of evidence, priors are not needed to start the inference process, avoiding the selection bias risks that can seriously condition Bayesian reasoningfreedman1999wald. An interesting geometric approach to uncertainty measures, and belief functions in particular, has been proposed by one of uscuzzolin2001geometric;cuzzolin2003geometry;cuzzolin2008geometric;cuzzolin2013l;cuzzolin2010geometry. To model uncertainty using belief functions, we propose a novel Random-set Convolutional Neural Network based on the principles of the epistemic deep learning concept. Epistemic deep learningmanchingal2022epistemicargues that a deep neural network producing an outcome for (selected) subsets of the target space is an intrinsically more faithful representation of the epistemic uncertainty associated with the limited quantity and quality of the training data. As they assign probability values to sets of outcomes directly, they naturally model the fact that observations almost invariably come in the form of sets. In an image classification setting, this could be interpreted as a test image being mapped to asetof classes, rather than a single class, when there is uncertainty about the true class. By representing classes as sets, set-valued classification can handle imprecise data more effectively and provide a richer understanding of the underlying uncertainty. It enables the classification model to capture multiple possible interpretations or predictions for each instance, allowing for more robust decision-making and improved performance in scenarios where precise labels may not be available or appropriate. ContributionsThe following are the key contributions of our paper: [leftmargin=*] A novelRandom-set Convolutional Neural Network(RS-CNN) classifier for uncertainty estimation based on the principle of epistemic deep learning, outputting Belief or Mass predictions for sets of classes, training using suitable loss functions generalising classical cross-entropy. A method for selecting a fixed budget of focal sets (sets with non-zero probability, associated with the network’s output neurons) from the power set of classes, thus ensuring scalability. A method for estimating the epistemic uncertainty of the prediction in terms of the size of the credal set (set of probability vectors) consistent with a predicted belief function. Experimental validation demonstrating the RS-CNN model outperform other uncertainty-aware models and exhibit superior accuracy on out-of-distribution (OOD) samples compared to the standard CNN.
Fluency score: 3
Fluency issues: [Multiple instances of incorrect spacing after references (e.g., "networkspapernot2016limitations", "overfittingroelofs2019meta", "adaptationli2020model"), misuse of quotation marks (‘adversarial’), reference issues (e.g., "Bayesian Dropoutgal2016dropout", "distance-aware priorsgian"), and improper concatenation of words and citations].

Clarity score: 2
Clarity issues: [Frequent, improperly formatted references disrupt flow and clarity, complex and jargon-heavy sentences, and lack of explanation for terms like "belief functions" and "credal sets" which may be unfamiliar to the reader].

Grammaticality score: 4
Grammaticality issues: [Incorrect spacing related to citations and poor punctuation around quotes and parentheses].

Readability score: 2
Readability issues: [Dense paragraphs with specialized terminology, improper citation formatting, and lengthy sentences with multiple nested clauses].

Coherence score: 3
Coherence issues: [The text frequently introduces advanced concepts without sufficient context or definitions, reducing the overall coherence for readers not already familiar with the subject matter].
====================
	 Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals
==========
Introduction:  In the recent decade, there has been an increased interest in studying teamwork skills and their impacts in a multitude of domains (e.g., academiaAlberolaet al.(2016), social networkingAnagnostopouloset al.(2012), project managementNollet al.(2016), healthcareNawazet al.(2014)). Building successful teams is a common business strategy (e.g., forming rescue and relief teams in response to an emergencyGunn and Anderson (2015), establishing entrepreneurial teams for new venturesLazaret al.(2020), forming teams dynamically in context of multi-agent systems (e.g., supply chains)Gaston and desJardins (2005)). In this paper, we focus on teaming for researchers applying to funding agencies in response to their call for proposals, using group recommendation strategies. The advantage of this setting is that the required data is already publicly available. A large amount of research funding in public universities comes from external funding agencies such as National Science Foundation (NSF) and National Institutes of Health (NIH). These opportunities often require multi-disciplinary teams from a wide variety of backgrounds to be quickly assembled. However, not every single team is often successful in achieving their goals, due to factors such as lack of accurate information, time, or communication, and incompatibility in terms of skill sets among team members. We build upon prior work, ULTRA (University-LeadTeam Builder fromRFPs andAnalysis)Srivastavaet al.(2022), a novel AI-based prototype for assisting with team formation when researchers respond to calls for proposals from funding agencies. In this paper, we interchangeably use the termcall for proposalwithrequest for proposal (RFP).Figure1shows a demo111A full demo interaction with the ULTRA system can be found at https://www.youtube.com/watch?v=8MUtxsfVNIU. The tool is deployed at http://casy.cse.sc.edu/ultra/teaming/. Additional details about usecases, experiments, and survey resources are atValluruet al.(2023).view of how the system works for an individual user who can become a team participant. The system first extracts technical skills from proposal calls found at publicly available data sources (e.g., NSF archives) and those present in online profiles of researchers (e.g., personal homepages, Google Scholar history), along with any additional teaming constraints that administrators or team participants of an institution may provide. Using AI and NLP techniques, ULTRA next provides a plausible list of candidate teams for each proposal call, where each team has at least two members. Our prior workSrivastavaet al.(2022), however, is a use case of asequentialsingle-item recommendation problem, where solutions are often limited by known issues such as cold startAbdollahpouriet al.(2019)or popularity biasYalcin and Bilge (2022). Therefore, we expand on this work to include group recommendation and novel AI methods to recommend optimal teams. Our contributions in the paper are that we: formulate a novelgroup recommendation problemto promote research collaborations where the objective is to suggest teams that could respond to calls for proposals, using skills found in open data, and balancing short- and long-term optimization. introduce a metric to measure goodness of teams and consider a configurable set of criteria: redundancy, group (set) size, coverage, and robustness. solve the novel problem using a variety of AI methods: string, taxonomy, and bandit (relational learning) methods, and compare them with a randomized baseline. establish the benefit of the solution methods quantitatively using goodness metric. We find that moreinformed methods lead to recommendations of smaller number of teams but higher goodness. establish the benefit of the system qualitatively using an IRB-approved preliminary survey at a College of a major US University. We find thatusers broadly consider the tool useful as well as relevantbut more studies are needed. demonstrate the generality of the approach with experiments at two different institutions in US and India. create and publish a teaming dataset that is available for research.
Fluency score: 4
Fluency issues: [Parentheses within sentences can disrupt fluency, as seen with references like "academiaAlberolaet al.(2016)", which are not spaced correctly.]

Clarity score: 3
Clarity issues: [Reference citations are cluttering the text. Use of terms like "group recommendation strategies" without clear definition may confuse readers.]

Grammaticality score: 4
Grammaticality issues: ["...an individual user who can become a team participant" could be more grammatically clear.]

Readability score: 3
Readability issues: [Inline citations and technical terms without clear explanations lower readability.]

Coherence score: 4
Coherence issues: [The text jumps between topics without clear transitions, e.g., moving from discussing teaming challenges directly to introducing ULTRA.]
====================
	 Industry Classification Using a Novel Financial Time-Series Case Representation
==========
Introduction:  Case-based reasoning (CBR) approaches have been applied in financial domains, and for a variety of tasks, from the early days of the field; see for example the work of[23]on the use of CBR for financial decision-making. In the years since, there have been many efforts made to apply CBR ideas to a diverse range of financial decision-making and prediction tasks[10,18,13,26,1,6]. Nevertheless, the use of CBR in such domains is not without its challenges, not the least of which concerns the very nature of many financial datasets and their relationship to the similarity assumption that underpins CBR. The central dogma of CBR is that similar problems have similar solutions, yet financial regulators are always at pains to point out that “past performance is not a guarantee of future results” suggesting that this principle may not be so reliable in financial domains, at least when it comes to predicting the future. As society changes and economies ebb and flow, companies that were once the stock market darlings fall out of favour, while new winners seem to emerge, with some regularity, albeit unpredictably and often with little or no warning. Two companies that were similar in the past may no longer be considered similar in the present, while the trajectories of companies with divergent histories might suddenly converge if future circumstances conspire in their favour. All of this greatly complicates the central role of similarity in case retrieval. In addition, the feature-based (attribute-value) representations that are commonplace in CBR systems may not provide such a good fit with the type of sequential time-series data (e.g. daily, weekly, and monthly stock prices/returns) that are the norm in financial domains. This is not to say that case-based methods cannot be used with time-series data. Certainly, there is a wealth of literature on representing time-series data for use with case-based methods in a range of application domains from agricultural science[3]to user experience[14]. Usually, the approach taken is to use various feature extraction techniques to identify landmark features from the raw time-series data; to effectively transform a raw time-series into a more conventional feature-based representation that can be used with standard similarity metrics. Similar approaches have been applied in the financial domain[12]but, as mentioned above, the stochastic nature of financial markets makes it difficult to effectively isolate useful case representations from market noise, which further complicates the similarity assessment even given a suitable fixed representation. Thus, in this work, our main technical contribution is to propose and evaluate a novel approach to learning case representations for financial assets (companies/stocks) using raw time-series data made up of historical daily returns. We describe how to transform raw, time-series data into an embedding-style representation of each stock/company; see for example[16,17]for examples of embedding representations. We argue that this facilitates the capture of more meaningful patterns and sub-patterns over extended periods of time, while facilitating the type of temporal alignment that is necessary during case comparison and similarity assessment. We argue that this approach is well-suited to the use of case-based and nearest-neighbour approaches in financial domains, because it can be used with a variety of standard similarity metrics, as well as more domain/task specific metrics[6]. We demonstrate its performance benefits in a comparative evaluation of the industry sector classification task, an important and practical benchmark problem in many financial applications[19]. The remainder of this paper is organised as follows. In the next section, we review the use of case-based reasoning in the financial domain and with time-series data more broadly, highlighting several common tasks and the approaches taken thus far, as well as the important challenges that remain with respect to representation and similarity assessment. Then, in Section3we discuss the details of our proposed approach, by describing how raw time-series data, such as financial returns, can be transformed into an embedding-based representation that is well suited to case-based approaches. In Section4we evaluate our proposed approach by using it to classify companies into their market sectors based on their historical returns data. We present the results of a comprehensive quantitative evaluation, which compares our proposed representation to a variety of baseline and naive approaches. We demonstrate how our embeddings-based representations can offer significant classification improvements, relative to more conventional representations of the raw time-series data. In addition, before concluding with a summary of our findings and a discussion of limitations and opportunities for further work, we present further qualitative evidence in support of the proposed approach, by using our representations to visualise the industry sectors that emerge from a clustering of our cases and some examples of nearest-neighbours in the resulting case-space.
Fluency score: 4
Fluency issues: ["the work of[23]on", "science[3]to", "example[16,17]for", "Section3we", "Section4we"]

Clarity score: 3
Clarity issues: Complex sentences, heavy use of jargon, references within the text can interrupt flow.

Grammaticality score: 4
Grammaticality issues: Minor punctuation errors, such as the missing spaces between words and references.

Readability score: 3
Readability issues: The text is dense with technical information, which may be difficult for non-experts.

Coherence score: 4
Coherence issues: Inline citations interrupt the narrative flow.
====================
	 AugUndo: Scaling Up Augmentations for Unsupervised Depth Completion
==========
Introduction:  Data augmentation plays a crucial role in easing the demand for training data as it enriches the training dataset by orders of magnitude, boosting performance on various deep learning tasks[29,35,38], particularly by preventing overfitting. Typically, the choice of augmentation is largely task-dependent. One common axiom of choosing augmentations is that the task output should remain invariant to the augmentation. For example, image flips are viable augmentations for classifying animals, since they do not alter the resulting image label. Conversely, flipping road signs can alter their meanings; hence, such augmentations can be detrimental to tasks involving road sign recognition. For geometric tasks (i.e. depth estimation), the range of augmentations is more restricted: Stereo assumes pairs of rectified images; hence, in-plane rotations are not feasible. Image-guided sparse depth completion relies on sparse points to ground estimates to metric scale; therefore, intensity transformations on sparse depth maps that alter the scale of the 3-dimensional (3D) scene are infeasible – leaving few augmentations viable. Unsupervised learning of depth completion further limits the use of augmentations as the supervision signal comes from reconstructing the inputs, where augmenting the input introduces artifacts that impact reconstruction quality and therefore the supervision. While simulating nuisances,i.e., camera motion and orientation, is desirable, naively applying augmentations pertaining to them may do more harm than good; therefore, it is unsurprising that existing works[27,24,34,42,43,44,41,46]primarily rely on a small range of photometric augmentations and flipping. Nevertheless, photometric augmentations help model the diverse range of illumination conditions and colors of objects that may populate the scene; geometric augmentations can simulate the various camera motion, i.e. image translations can approximate small baseline movements, and scene arrangements, i.e. image flipping. But block artifacts, loss during resampling, intensity saturation are just some of the many undesirable side-effects of traditional augmentations to the image and sparse depth map. To exploit the immense amount of data derived from augmentations, we propose to simply compute the typical reconstruction loss on the original input image and sparse depth map, which bypasses negative effects of reconstruction artifacts due to photometric and geometric augmentations. However, there exists a mis-alignment between the original input (image and sparse depth), and the model depth estimate as geometric augmentations induces a change in coordinates. Hence, weundothe geometric augmentations by inverting them in the output space to align the model estimate with the training target. Amongst the the many geometric tasks, we focus onunsupervised depth completion, the task of inferring a dense depth map from a single image and its associated sparse depth map, where augmentations have seen limited use. Here, a training sample includes the input sparse depth map, its associated image as well as additional images of neighboring views from the same 3D scene. Augmentations have traditionally been restricted to a limited range of photometric transformations and flipping – due to the need to preserve photometric consistency across a sequence of video frames used during training, and the sparse set of 3D points projected onto the image frame as a 2.5D range map; degradation to either modalities directly impacts the supervision signal. By using our method, loss functions involving sparse depth and image reconstruction from other views can be computed on the original inputs while applying augmentations that were previously not viable for the task.Our hypothesis:By “undo-ing” the augmentations, one can expand the viable set and scale up their use in training, leading to improved model performance and generalizability. To this end, we introduce AugUndo, an augmentation framework that allows one to apply a wide range of photometric and geometric transformations on the inputs, and to “undo” them during loss computation. This allows one to compute an unsupervised loss on the original images and sparse depth maps, free of artifacts, through a warping of the output depth map – obtained from augmented input – onto the input frame of reference based on the inverse geometric transformation. In addition to group transformations that allow for output alignment, we combine them with commonly employed photometric augmentations. Lastly, we study whether non-group transformations, such as occlusion, can further improve model performance. We demonstrate AugUndo on three recent unsupervised depth completion methods and evaluate them on indoor and outdoor settings, where we improve by an average of 11.75% across all methods and datasets. Our contributionsare as follows: (i) We propose AugUndo, a simple-yet-effective framework to scale up photometric and geometric augmentations for unsupervised depth completion; AugUndo can be applied in a plug-and-play manner to existing methods with negligible increase in computational costs during training. (ii) We provide a comprehensive study ablating over combinations of eleven types of augmentations, including ones that have not been explored by existing unsupervised methods, for three different models across two datasets. We found a consistent set of augmentations that provides performance benefits for all tested models and are applicable for both indoor and outdoor scenarios. (iii) We show that AugUndo can consistently improve model performance, robustness to shifts in sparse point densities as well as zero-shot generalization; thus, validating our hypothesis.
Fluency score: 4
Fluency issues: Some sentences are overly complex; readability could be improved with shorter, clearer sentences.

Clarity score: 3
Clarity issues: Due to technical jargon and dense information, the text may not be clear to readers unfamiliar with the subject.

Grammaticality score: 4
Grammaticality issues: Mostly grammatical, but has an instance of double "the" ("Amongst the the many geometric tasks").

Readability score: 3
Readability issues: Long sentences with complex structures make the text challenging for easy reading.

Coherence score: 4
Coherence issues: The overall structure is logical, but the high density of information and technical jargon can impede smooth flow.
====================
	 Generalised Probabilistic Diffusion Scale-Spaces
==========
Introduction:  Diffusion probabilistic models[1]have recently risen to the state-of-the-art in image generation, surpassing generative adversarial networks[2]in popularity. In addition to significant research activity, the availability of pre-trained latent diffusion networks[3]have also brought diffusion models to widespread public attention[4]. Practical applications are numerous, including the generation of convincing, high fidelity images from text prompts or partial image data. Initial probabilistic diffusion models[1,5,6,7,8,9,10]relied on a forward drift-diffusion process that gradually perturbs input images with noise and can be reversed by deep learning. Recently, it has been shown that the concrete mechanism that gradually destroys information in the forward process has a significant impact on the image generation by the reverse process. Alternative proposed image degradations include blur[11], combinations of noise and blur[12,13,14], or image masking[12]. So far, probabilistic diffusion research was mostly of practical nature. Some theoretical contributions established connections to other fields such as score-matching[7,8,9,10], variational autoencoders[6], or normalising flows[15]. Probabilistic diffusion models have been initially motivated[1]by drift-diffusion, a well-known process in physics. However, its connections to other physics-inspired methods remain mostly unexplored. Closely related concepts have a long tradition in model-based visual computing, such as osmosis filtering proposed byWeickert et al. [16]. In addition, there is a wide variety of diffusion-based scale-spaces[17,18,19,20]. Conceptually, these scale-spaces embed given images into a family of simplified versions. This resembles the gradual removal of image features in the forward process of probabilistic diffusion models. Despite this multitude of connections, there is a distinct lack of systematic analysis of probabilistic diffusion from a scale-space perspective. This is particularly surprising due to the impact of the forward process on the generative performance[14,13]. It indicates that a better understanding of the information reduction could also lead to further practical improvements in the future. With our previous conference publication[21]we made first steps to bridge this gap between the scale-space and deep learning communities. To this end, we introduced first generalised scale-space concepts for probabilistic diffusion. In this work, we further explore the theoretical background of this successful paradigm in deep learning. In contrast to traditional scale-spaces, we consider the evolution of probability distributions instead of images. Despite this departure from conventional families of images, we can show scale-space properties in the sense ofAlvarez et al. [17]. These include architectural properties, invariances, and entropy-based measures of simplification. In addition to our previous findings[21], our novel contributions include a generalisation of our scale-space theory for diffusion probabilistic models (DPMs) which includes both variance preserving and variance exploding approaches, generalised scale-space properties for the reverse process of DPM, a scale-space theory for inverse heat dissipation[13]and blurring diffusion[14], and a significantly extended theoretical and empirical comparison of three probabilistic diffusion models to homogeneous diffusion[18]and osmosis filtering[16].
Fluency score: 4
Fluency issues: "have also brought" should be "has also brought"; "motivated[1]by" needs a space between "motivated" and "[1]by"; "byWeickert" needs a space between "by" and "Weickert"; "publication[21]we" needs a space between "publication[21]" and "we".

Clarity score: 3
Clarity issues: The text is dense with technical terms and references, which might confuse readers unfamiliar with the topic. Phrases like "a significant impact on the image generation by the reverse process" could be more explicit.

Grammaticality score: 4
Grammaticality issues: "have also brought" should be "has also brought".

Readability score: 3
Readability issues: The dense use of specialized vocabulary and references, along with lengthy sentences, make the text challenging to read for non-specialists.

Coherence score: 4
Coherence issues: The text jumps between discussing practical aspects, theoretical connections, and related concepts without clear transitions, which can be confusing.
====================
	 Aligning Robot and Human Representations
==========
Introduction:  In the HRI community, we aspire to build robots that perform tasks that human users want them to perform. To do so, robots need goodrepresentationsof salient task aspects. For example, in Fig.1, to carry a coffee mug, the robot considers efficiency, mug orientation, and distance from the user’s possessions in its behaviour. There are two paradigms for learning representations: one thatexplicitlybuilds in structure for learning task aspects, e.g. feature sets or graphs, and one thatimplicitlyextracts task aspects by mapping input directly to desired behaviour, e.g. end-to-end approaches(Levineet al.,2020; Rosset al.,2011). While explicit structure is useful for capturing relevant task aspects, it’s often impossible to comprehensively define all aspects that may matter to the downstream task; meanwhile, implicit methods circumvent this problem by allowing neural networks to automatically extract representations, but they are prone to capturingspurious correlations(Levineet al.,2020), resulting in potentially arbitrarily bad robot behaviour under distribution shift between train and test conditions(Paudel,2022). Our observation is that many failures in robot learning, including the ones above, result from amismatch between the human’s representation and the one learned by the robot; in other words, their representations aremisaligned. From this perspective, these failures illuminate that if we truly wish to learn good representations – if we truly want robots that do what humans want – we must explicitly focus on the foundational problem:aligning robot and human representations. In this paper, we offer a unifying lens for the HRI community to view existing and future solutions to this problem. We review over 100 papers in the representation learning literature in robotics from this perspective. We first define a unifying mathematical objective for an aligned representation based on four desiderata: value alignment, generalizable task performance, reduced human burden, and explainability. We then conduct an in-depth review of four common representations (Fig.1): the identity representation, feature sets, feature embeddings, and graphical structures – illustrating the deltas each falls short in with respect to the desiderata. From situating each representation in our formalism, we arrive at the following key takeaway: a better structured representation affords better alignment and therefore better task performance, but always with the unavoidable tradeoff of more human effort. This effort can be directed in three ways: 1) representations that operate directly on the observation space, e.g. end-to-end methods, direct effort at increasing task data to avoid spurious correlations; 2) representations that build explicit task structure, e.g. graphs or feature sets, direct effort at constructing and expanding the representation; and 3) representations that learn directly from implicit human representations, e.g. self-supervised models, direct effort at creating good proxy tasks. Our paper is untraditional in that it is much like a survey paper, except there is little work that directly addresses the representation alignment problem we pose. Instead, we offer a retrospective on works that focus on learning task representations in robotics with respect to our desiderata. Our review provides a unifying lens to think about the current gaps present in the robot learning literature as defined by a common language, or in other words, a roadmap for thinking about challenges present in current and future solutions in a principled way. We conclude by suggesting key open directions.
Fluency score: 4
Fluency issues: "goodrepresentationsof" lacks spaces

Clarity score: 3
Clarity issues: Phrases like "explicitly focus on the foundational problem:aligning" lack clarity due to spacing errors; the summary of the paper's structure is overly convoluted.

Grammaticality score: 4
Grammatical issues: Missing spaces after commas and colons; apostrophes in "it's" should be "its," as in "its behaviour."

Readability score: 3
Readability issues: Long, complex sentences with technical jargon; improper spacing in the text contributes to the lower score.

Coherence score: 4
Coherence issues: While the text seems to follow a logical order, dense jargon and lack of clear transitions between the survey and concluding remarks slightly impact the coherence.
====================
	 Practical Layout-Aware Analog/Mixed-Signal Design Automation with Bayesian Neural Networks
==========
Introduction:  Analog/Mixed-signal (AMS) integrated circuit (IC) design typically follows a process flow visualized in Figure1. A combination of designer experience and computer simulation feedback is iterated to determine the design that meets the performance requirements. A large portion of design time is spent on the sizing and layout phases, where multiple iterations are possible due to potential loop-backs in the design flow. This is a labor-intensive process in industry practice with little to no automation. To address this costly exercise, a considerable effort in academia is focused on introducing automated solutions. Analog sizing automation is the task of optimizing AMS design variables, e.g., transistor widths, lengths, resistor, and capacitor values. The aim is to satisfy the performance constraints and optimize the design objective. In general, sizing automation is run through schematic-level simulations. However, AMS IC performance is also sensitive to layout implementation[4]. Especially in the advanced process nodes, layout-induced parasitics may greatly affect the final design performance. Therefore, sizing the AMS design variables considering the layout effects is also crucial. The majority of the recent sizing and post-layout performance optimization algorithms have simulation feedback in the loop. Due to advanced scaling, simulations are required to obtain accurate performance evaluations. Simulation-based AMS automation algorithms adapted various methods from the optimization and Machine Learning (ML) communities. The earlier approaches include population-based methods such as particle swarm optimization[25]and evolutionary algorithms[14]. Although these algorithms have good convergence behavior, they are inefficient in sampling since they explore the design space randomly. To mitigate sample inefficiency, model-based methods gained popularity[13,18,11]. These methods employ surrogate-models between the solution space and performance space and provide efficiency in exploring the solution space. A typical surrogate model is Gaussian Process Regression (GPR)[21], which is a well-studied model in Bayesian Optimization (BO) field[23]and is adapted by several analog sizing algorithms. The main drawback of GPR modeling is its computational complexity. Recent research trend in analog sizing introduces ML to simulation-based methodology[3]. However, the literature review reveals that most of these methods require thousands of simulation data to train Deep Neural Network (DNN) models that approximate the relations between the design variables and the performance metrics. Therefore, the practicality of these algorithms is severely reduced when the optimization task has a high simulation cost. For example, drawing/generating the layout, extracting the parasitics, and running post-layout simulations is typically an expensive procedure. Therefore, optimization algorithms designed for schematic-level sizing can not be adapted by simply changing how data is generated. This paper presents a Machine Learning-based simulation-in-the-loop automation method for the AMS design problem. Overall, we formalize two stand-alone recipes for schematic-level sizing and post-layout performance optimization, i.e., layout-aware sizing. We integrate the state-of-the-art analog layout generator, MAGICAL[27], into our flow to handle layout-aware sizing. Our algorithms do not assume the pre-existence of any dataset, and we generate all training data during the optimization. We employ Bayesian Neural Networks (BNN) for modeling design performances. Bayesian Neural Networks allow error quantification, and compared to Deep Neural Networks, BNN are shown to be effective in handling scarce datasets and preventing overfitting[9]. Therefore, BNN can be trained on smaller datasets, significantly improving the practicality and scalability. We also introduce a batch-optimization framework and design space sampling strategy that is compatible with BNN modeling. Further, when optimizing the design variables based on post-layout performance, we exploit the correlation between schematic-level simulations and post-layout simulations. Our algorithm introduces a co-learning scheme that reduces the need for costly post-layout simulations and boosts efficiency even further. We compile our contributions as follows: We use Bayesian Neural Network-based modeling to obtain performance approximations. Different learning strategies are adapted for schematic-level sizing and post-layout performance optimization. We adopt a scalable sampling strategy and query the optimization batches by utilizing a trust region and Thompson sampling. The post-layout sizing is handled as a multi-fidelity optimization problem, and an efficient co-learning strategy is developed. The efficiency of the proposed methods is demonstrated on three circuits by providing comparisons to previous state-of-the-art. The rest of the paper is organized as follows. SectionIIintroduces the backgrounds and previous work. SectionIIIdescribes our algorithms for handling schematic-level sizing and post-layout performance-based sizing problems. SectionIVprovides the experiments on circuit examples to demonstrate the efficiency of our algorithms. Finally, SectionVconcludes the paper.
Fluency score: 4
Fluency issues: There are none, but the text contains specialized language that might not be fluently understood by a lay audience.

Clarity score: 3
Clarity issues: Sudden introduction of abbreviations and terms without explanation (e.g., GPR, BNN, MAGICAL) can confuse nonspecialist readers.

Grammaticality score: 4
Grammaticality issues: Misuse of commas instead of parentheses for references [25][14][13,18,11][21][23][3][9][27].

Readability score: 3
Readability issues: Dense, technical language and long sentences make the text challenging for general readers.

Coherence score: 4
Coherence issues: Mostly coherent but contains jargon and assumes prior knowledge which might affect the text's coherence for non-expert readers.
====================
	 DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization
==========
Introduction:  Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand significant expert efforts to approximate near-optimal solutions(Arora,1996; Gonzalez,2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution(Bello et al.,2016; Kool et al.,2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems(Fu et al.,2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical(Joshi et al.,2019; Karalias and Loukas,2020; Qiu et al.,2022). Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems(Khalil et al.,2017; Gu et al.,2018), for example, when multiple optimal solutions exists for the same graph. Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt(Lin and Kernighan,1973; Andrade et al.,2012)and node swap(Chen and Tian,2019; Wu et al.,2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework(Wu et al.,2021; Ma et al.,2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation(Song and Ermon,2019; Ho et al.,2020; Rombach et al.,2022;Yu et al.,; Saharia et al.,2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as to find a\{0,1\}-valued vector that indicates the optimal selection of nodes or edges in a candidate solution for the task. Then we use a message passing-based graph neural network(Kipf and Welling,2016; Hamilton et al.,2017; Gilmer et al.,2017; Veličković et al.,2018)to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\ll N) denoising steps (Sec.3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec.3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular,Graikos et al. (2022)proposed an image-based diffusion model to solve Euclidean Traveling Salesman problems by projecting each TSP instance onto a64\times 64greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image. The main difference between suchimage-baseddiffusion solver and ourgraph-baseddiffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive(Kool et al.,2019a)and improvement heuristics(d O Costa et al.,2020)solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion modeling within the DIFUSCO framework: continuous diffusion with Gaussian noise(Chen et al.,2022)and discrete diffusion with Bernoulli noise(Austin et al.,2021; Hoogeboom et al.,2021). These two types of diffusion models have been applied to image processing but not to NPC problems so far. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network(Bresson and Laurent,2018; Joshi et al.,2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.
Fluency score: 3
Fluency issues: [Use of parenthesis and citations disrupts the flow; long sentences with nested clauses; dense terminology]

Clarity score: 3
Clarity issues: [Technical jargon without explanation; complex sentence structures]

Grammaticality score: 5
Grammaticality issues: [None detected]

Readability score: 2
Readability issues: [Long, complex sentences; heavy use of domain-specific terminology]

Coherence score: 3
Coherence issues: [Jumps between ideas without clear transitions; heavy reliance on parenthetical information]
====================
	 FaceFusion: Exploiting Full Spectrum of Multiple Datasets
==========
Introduction:  The priority focus of modern face recognition studies has been in-line with that of representation learning studies: amplifying the representative power of embedding vectors within the feature space. With continued development and refinement of various training methods, face recognition models have seen significant improvement in terms of evaluation accuracy in recent years. However, equally important is the recent advent of publicly available large-scale facial image datasets. These large datasets have generally been curated by either partially or entirely automated crawling of publicly available facial images, followed by different approaches of clustering the images by their identities. While the approach of building such large datasets from scratch have been studied widely, considerably less amount of attention has been given to the methods of starting from already-curated datasets. Combining different datasets is naturally beneficial from the fact that they have already gone through some degree of refinements that include identity-wise clustering or noise removal, but because their image sources generally come from public datasets of celebrities or random web crawling, careful consideration is required to handle conflicting identities. Identity conflict is destructive to the overall model performance because physically same identities are interpreted as distinct identities, which the model is incorrectly taught to distinguish. A trivial solution to this inter-class label noise would be to train a rectifier model[34]to adjust such identities, but such solution would be heavily dependent on training a robust rectifier model. Another possible approach is explored in[16]as a posterior data cleaning process, but it potentially requiresO(MN)memory space, whereMis the number of conflicting identities, andNis the number of datasets. DAIL[27], to our best knowledge, is the only work that studies the way of using multiple datasets concurrently without a separate dataset-manipulation process by introducing dataset-aware softmax. While the approach of DAIL somewhat mitigates the conflicting identity problem, we argue that the performance gain from DAIL is suboptimal, because dataset-aware softmax essentially isolates the representations learned from each dataset from the rest of the datasets. This reduces the scope of final softmax operation from the entire embedding space to the subspaces fitted to each dataset, preventing the embedding model to reach global optimization.To suppress the inevitably introduced inter-class label noise when combining different datasets, and to further improve from the limited performance gain of DAIL, we introduceFaceFusion. Our approach begins from the method of DAIL, and after the model parameters are stabilized, observed as the well-knownslow-drift[29]phenomenon, it directlyfusesdifferent datasets into a unified global embedding space, while also merging the class proxies of conflicting identities. Doing so effectively enables viewing multiple datasets as a unified dataset, which the embedding model can exploit to expand its optimization scope from within each dataset to the whole datasets, resulting in superior performance. Extensive experiments confirms that FaceFusion outperforms not only the models trained using single dataset, but also the models trained with multiple datasets either by naive concatenation or by the method of DAIL. We further prove that FaceFusion maintains its superiority over the aforementioned methods under varying severity of conflicting identities of each dataset, ranging from completely disjoint to vastly overlapping with each other.
Fluency score: 4
Fluency issues: -

Clarity score: 3
Clarity issues: Complex sentences and technical jargon may be challenging for readers unfamiliar with the subject matter.

Grammaticality score: 4
Grammaticality issues: "building such large datasets from scratch have been studied" should be "has been studied".

Readability score: 3
Readability issues: Dense with academic language and concepts; sentences are long and complex, which may hinder readability.

Coherence score: 4
Coherence issues: The text mostly flows logically but assumes a high level of background knowledge, making it less coherent to lay readers.
====================
	 Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models
==========
Introduction:  Recently, large scale Natural Language Processing (NLP) models are being increasingly deployed in many real-world applications within almost all domains such as health-care(Velupillaiet al.,2018; Soni and Roberts,2020), business(Mishevet al.,2020), legal systems(Dale,2019), etc., due to its efficacy to make data-driven decisions and capability of natural language understanding even better than humans111https://www.infoq.com/news/2021/01/google-microsoft-superhuman/(Heet al.,2021). Transformer based large Pre-trained Language Models (PLMs) have been hugely influential in NLP due to their capability to generate powerful contextual representations. PLMs are mostly built based on a self-supervised learning strategy that highly relies on unlabelled data abundantly available from the human generated data deluge(Heet al.,2021). But, since this historical data of textual write-ups has their roots within human thought, they often reflect latent social stereotypes(Suresh and Guttag,2021; Garget al.,2018). For example, the Social Role Theory by Eagly et al.(Eagly and Steffen,1984)demonstrates that the idea of gender stereotype develops from perceivers’ observations, associating the capabilities and personality attributes of different genders with the activities in which they engage in their day-to-day lives over a time, building rigid stereotypes in human minds and their writings, on how these genders behave (e.g. women are highly emotional), where they work (e.g. women preferred in children’s daycare), etc. Hence the data from such human generated data repositories eventually convey these stereotypes as linguistic biases through the NLP algorithms, especially those built on large PLMs that utilize huge amounts of data(Suresh and Guttag,2021). In this direction, investigation of“Affective Bias”in NLP is a recent stream of research to study the existence of any unfair/biased association of emotions (anger, fear, joy, etc.) or sentiments (positive, negative, etc.) towards underrepresented or protected groups in a domain, that may lead affective computing systems such as sentiment or emotion detection systems to deliver unfavorable outcomes for these protected groups(K.et al.,2022). For instance, a model consistently associating women with a different class of emotion or same emotion differing in emotion intensities vis-a-vis predictions for male(Shields and Shields,2002)could be seen as a manifestation of affective bias. Similarly, association of a particular religion always with a specific emotion(Abidet al.,2021a)represents affective bias too. A real world scenario of affective bias is the case of Google sentiment analyzer judging that being gay is bad by assigning high negative sentiments to sentences such as ‘I’m a gay black woman’, ‘I’m a homosexual’, etc.,222https://www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias. For better understandability of affective bias, we illustrate in table1, a sample set of affectively biased emotion predictions from PLM based textual emotion detection models constructed in this study for affective bias analysis (detailed explanation of the models are provided in section4.1). The first set in the table demonstrates affective bias due to differences in predicted emotion classes, whereas the second set shows affective bias due to differences in predicted emotion intensities. Similar to other general algorithmic biases like gender bias, racial bias, etc., a possible stimuli to affective biases are the latent emotion based stereotypes about different social groups in the data. Studies report that such emotion based stereotyping influence socialization of emotions leading to propagation of stereotypes such as associating women’s (or men’s) experience and expressions being aligned with fear and sadness (or anger and pride)(Plantet al.,2000). Similarly, affective bias within systems could facilitate a higher association of black women to the emotion anger when considering emotions with the domains race and gender(Ashley,2014). In addition to biased data, another reason for bias is based on how the model/algorithmic design considers or treats the underrepresented or protected attributes concerning a domain(Hooker,2021). Similar to any other general social biases, the existence of these affective biases make textual affective computing systems generate unfair or biased decisions that can harm its utility towards socially marginalized populations by denying opportunities/resources or by false portrayal of these groups when deployed in the real-world. Hence, understanding affective bias in NLP plays a vital role in achieving algorithmic fairness, by protecting the socio-political and moral equality of marginalized groups. In this context, we present an extensive experimental analysis to understand and illustrate the existence of latent“Affective Bias”in transformer based large pre-trained language models with respect to the downstream task of textual emotion detection. Hence, we set our research question:Do predictions made by large PLM based textual emotion detection systems systematically or consistently exemplify ‘Affective Bias’ towards demographic groups?. Our investigation of affective bias in large PLMs primarily aims to identify the existence of gender, racial, and religious affective biases and set aside the task of affective bias mitigation in the scope for future work. We start with an exploration of corpus level affective bias or affect imbalance in corpus to find out any biased emotion associations in the large scale corpora that are used to pre-train and fine-tune the PLMs, by analyzing the distribution of emotions or their associations with demographic target terms (e.g., Islam, Quran) related to a social group (e.g., Muslim) concerning a domain (e.g., Religion). Later, we explore the prediction level affective bias in four popular transformer based PLMs, BERT (Bidirectional Encoder Representation from Transformers)(Devlinet al.,2019), OpenAI GPT-2 (Generative Pre-trained Transformer)(Radfordet al.,2019), XLNet(Yanget al.,2019), and T5 (Text-to-Text Transfer Transformer)(Raffelet al.,2020), that are fine-tuned using a popular corpora SemEval-2018 EI-oc(Mohammadet al.,2018)for the task of textual emotion detection. To quantify prediction level affective bias, we subject the PLMs to an extensive set of class-based and intensity-based evaluations using three different evaluation corpora EEC(Kiritchenko and Mohammad,2018), BITS(Venkit and Wilson,2021)and CSP(Nangiaet al.,2020). A detailed sketch of the overall analysis is shown in figure1. Workflow ofAffective biasanalysis The rest of the paper is organized as follows. Section2presents the relevant related works. Section3presents corpus level affective bias analysis with corresponding methodology and results. Section4presents the exploration towards prediction level affective bias with details of constructing PLM based textual emotion detection model, methodology of analysis, and the corresponding results. Section5presents a discussion based on the entire results and finally, section6draws the conclusions.
Fluency score: 4
Fluency issues: Hyphenation issues (e.g., "health-care"), inconsistent spacing around citations (e.g., "Velupillaiet al.," "K.et al.,2022"), and excessive use of parentheses and other in-line citations interrupt the flow of the text.

Clarity score: 3
Clarity issues: The complex sentence structure, heavy use of jargon, and parenthetical asides make the text less accessible.

Grammaticality score: 4
Grammaticality issues: Some missing spaces after punctuation, inconsistent citation format ("Velupillaiet al.," "K.et al.,2022"), and slightly awkward phrasing decrease grammaticality.

Readability score: 3
Readability issues: Long, complex sentences, academic jargon, and dense information presentation affect readability.

Coherence score: 4
Coherence issues: While the text maintains topical coherence, the density of information, multiple asides, and interruptions from citations disrupt the smooth progression of ideas.
====================
	 DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning
==========
Introduction:  Maching learning has recently demonstrated impressive performance in various fields, e.g., computer vision[he2016deep], natural language processing[devlin2018bert], speech processing[abdel2014convolutional], etc. However, an effective machine learning method often requires a large amount of high-quality labeled data to properly and sufficiently simulate the testing/evaluating distribution. Collecting such large-scale supervised datasets is notoriously expensive in time and effort for most real applications. Compared with current machine intelligence, humans are able to quickly learn novel concepts from only small amount of examples[lake2015human,lake2017building]. The capability of machine to learn new concepts quickly from small examples is thus desirable, especially for many problems/applications where data are intrinsically rare or expensive, or compute resources are unavailable. Meta-learning[naik1992meta,schmidhuber1987evolutionary,thrun1998lifelong], or learning to learn, has been suggested as a promising solution path to assemble machine learning with above capability. The key idea of meta-learning is to distill a meta-model from multiple learning tasks/episodes, and then use this meta-model to improve performance of task-specific model on novel query tasks[hospedales2021meta,shu2021learning]. Such a learning paradigm is hopeful to bring a variety of benefits, such as finely adapting to query tasks with less computation/data costs (e.g., avoid learning from scratch for novel tasks), as well as fewer human interventions. Recently, it produces an explosion of researches on meta-learning, due to its potential to advance the frontier of the contemporary machine learning. Especially, meta-learning has helped machine learning improve the data efficiency[wang2020generalizing,shu2018small], algorithm automation[he2021automl,karmaker2021automl], and generalization[maurer2016benefit,tripuraneni2020theory,shu2021learning]. Successful applications have been demonstrated in areas spanning few/zero-shot learning[finn2017model,snell2017prototypical,vinyals2016matching,sung2018learning,soh2020meta], neural architecture search (NAS)[liu2018darts,elsken2019neural], hyperparameter optimization[franceschi2018bilevel], curriculum learning[ren2018learning,shu2019meta], domain adaptation/generalization[liu2021cycle,li2018learning], transfer learning[jang2019learning,sun2020meta], label noise learning[zheng2021meta,shu2020meta,zhao2021probabilistic], semi-supervised learning[pham2021meta], unsupervised learning[metz2018meta], reinforcement Learning[duan2016rl,wang2016learning], data/label generation[cubuk2018autoaugment,wu2021learning,shu2022cmw], loss/regularization learning[balaji2018metareg,yazdanpanah2022revisiting,lee2019meta,shu2020learning,huang2019addressing], learning to optimize[andrychowicz2016learning,ravi2016optimization,shu2022mlr], and robustness[collins2020task,killamsetty2022nested], etc. These successes largely attribute to the data-based nature of current meta-learning approaches that learn from a tremendous number of training tasks with high-quality meta-data representing the underlying task generalization goal. However, in most real applications, collecting such high-quality training tasks are difficult, expensive and impractical. This often makes obtained training tasks imperfect. In fact, we always have access to problematic meta-data for some applications. For example, the corresponding ground-truth labels of meta-data are generally noisy in label noise problems[shu2019meta], or unavailable in unsupervised domain adaptation tasks[liu2021cycle], or the size of meta-data is limited in few-shot learning issues[finn2017model]. The purely meta-data-driven approaches tend to reach their limits or lead to unsatisfactory results under these imperfect circumstances. With meta learning becoming more and more popular in real applications, there is also a growing need for meta-learning to train well-performing and sufficiently generalized meta-models from such imperfect training tasks. As a step towards addressing the limitations of purely meta-data-driven meta-learning, we suggest a meta-knowledge informed meta-learning (MKIML) framework, as shown in Fig.1, which comprises an additional meta-knowledge integration into the meta learning pipeline. Moreover, such meta-knowledge could be obtained in an external, separated way from the meta-learning problem and the usual training tasks. This framework is expected to be functional in exploring an orthogonal meta-knowledge-driven approaches relative to previous purely meta-data-driven approaches to learn and evaluate meta-model. With the MKIML framework, we attempt to integrate meta-knowledge into meta-objective by means of a meta-regularization (MR) term. The key insight is that we leverage the benefits of fundamental properties of the meta-model for various training tasks, which should help achieve better generalization of meta-model to unseen tasks and alleviate the critical need of high-quality meta-data. Specifically, in this study we instantiate MR with the data augmentation consistency (DAC) as a new meta-objective for meta-learning. The DAC stems from recent advances in semi-supervised learning[xie2020unsupervised,sohn2020fixmatch], and to the best of our knowledge, we exploit it to meta-regularize the complexity of meta-model function class for the first time, which enforces the model facilitated by meta-model to output similar predictions under input data augmentations. Our contributions can be mainly summarized as follows. 1) We suggest a MKIML framework, as shown in Fig.1, aiming to improve capability of previous purely meta-data-driven meta-learning approaches by additionally integrating compensated meta-knowledge into meta learning process. Specifically, we explore to integrate meta-knowledge into meta-objective through designing an appropriate meta-regularizer (MR). The MR is functional on regularising the capacity complexity of meta-model function class, so as to improve its meta-level generalization on unseen tasks. 2) We introduce data augmentation consistency (DAC) to instantiate MR objective for an effective practical implementation (DAC-MR for brevity). The DAC-MR provides a general approach to help apply meta-learning models to tasks with noisy, sparse or unavailable meta-data. Besides, the DAC-MR is problem-agnostic, which can be generally applicable to extensive meta-learning problems and tasks. 3) We theoretically prove that the additional DAC-MR term in meta-objective can bring better meta-level generalization compared with solely meta-data-driven meta-loss objective. Meanwhile, we prove that DAC-MR is able to be regarded as a proxy meta-objective implicitly calculated on high-quality meta-data under some mild conditions. 4) We experimentally demonstrate that DAC-MR aids meta-model learning across various meta-learning problems in computer vision, including few-shot learning (§4), transfer learning (§5), continual learning (§6) and label noise learning (§7). Specifically, our DAC-MR is substantiated to be valid across 10 meta-learning tasks with different network architectures and testing benchmarks. Furthermore, these empirical results are well-aligned with our theoretical insights. The paper is organized as follows. §2discusses related work. §3presents the proposed MKIML framework, MR objective and our DAC-MR objective as a practical implementation for MKIML as well as its theoretical insights. We evaluate DAC-MR to few-shot learning in §4, transfer learning in §5, continual learning in §6and label noise learning in §7, respectively. The conclusion is finally made.
Fluency score: 3
Fluency issues: Writing is heavy with jargon, parentheses, and citations disrupting flow; long sentences with multiple subclauses.

Clarity score: 2
Clarity issues: Complex sentences and technical terms without explanation; the text is dense and hard to follow for non-experts.

Grammaticality score: 3
Grammaticality issues: Missing 'a' before 'recently' and 'an' before 'effective'; 'amount of high-quality labeled data' should be 'amounts'; 'capability of machine' should be 'capability of a/the machine'; 'avoid learning from scratch for novel tasks' seems awkward.

Readability score: 2
Readability issues: Heavy use of jargon and acronyms, and an academic style with many citations, make the text difficult to read for a general audience.

Coherence score: 3
Coherence issues: The document lacks clear logical structure in places, jumping between concepts; frequent insertion of in-text citations without explanation is jarring.
====================
	 Task-driven Compression for Collision Encoding based on Depth Images
==========
Introduction:  Methods for autonomous collision-free navigation of aerial robots have traditionally relied on motion planning techniques that exploit a dense map representation of the environment[28,3,24,27]. Departing from such methods, the community has recently investigated the potential of deep learning to develop navigation methods that act directly on exteroceptive data such as depth images instead of reconstructed maps in order to plan the aerial vehicle’s motions with minimal latency[15,16,22,12]. However, such methods face the challenge that exteroceptive data and especially depth images coming from stereo vision or other sensors are typically of very high dimensionality and the involved neural networks include layers that partially act as lossy information compression stages. This is reflected in the architectures of otherwise successful methods such as the works in[15,22,12]that exploit depth images to evaluate which among a set of candidate robot trajectories would collide or not. In[15]the input depth image involves more than300,000pixels (640\times 480resolution) but through stages of a pre-trained MobileNetV3 architecture it gets processed toMfeature vectors of size32each, whereMis the number of candidate trajectories for which this method derives collision scores. Eventually by combining the640\times 480pixels depth image with robot pose information, the method attempts to predict which amongMtrajectories are safe, thus representing a process of information downsampling and targeted inference. In other words, despite the dimensionality reduction taking place through the neural network it is attempted that the method still ensures collision avoidance. However, it is known that such techniques do not provide100\%success ratio especially in complex and cluttered scenes. Responding to the above, this work contributes the concept of task-driven compression and encoding of depth images as visualized in Figure1. Departing from the concept that methods aiming to predict the safety of candidate robot trajectories based on depth images should train collision prediction either a) directly in an end-to-end fashion through depth data[15,22]or through b) an explicit intermediate compression stage of the depth image itself[23], we propose the approach of using the depth image to encode a latent space presenting major dimensionality reduction that reflects not the depth image itself but instead a “collision image”. The latter is a remapping of the depth image that has accounted about the robot’s size and thus presents reduced overall complexity and greatly reduced presence of narrow/thin features that are hard-to-retain in an aggressive compression step. To achieve this goal, the method employs a probabilistic encoder-decoder architecture that is trained in a supervised manner such that given a depth image as input, it learns to encode and reconstruct the collision image. To train this collision-predicting network –dubbedDepth image-based Collision Encoder(DCE)– the depth image is first processed such that the collision image is calculated given information for the robot’s size. Focusing on aggressive dimensionality reduction, it is demonstrated that the scheme allows to get accurate reconstructions through a latent space that is more than3orders of magnitude smaller than the input image. The benefits of the approach are demonstrated through comparisons both with a conventional Variational Autoencoder (VAE) trained to encode the depth image and assessed regarding the extent to which the reconstructed image can serve as basis to derive a correct collision image, as well as traditional compression methods using the Fast Fourier Transform (FFT) and wavelets. In the remaining paper Section2presents related work and Section3details the proposed method involving generation of training data, image augmentation and the training of the neural network. Section4compares our proposed method against traditional image compression methods and evaluates the performance of task-driven and task-agnostic compression methods at similar degrees of compression. Finally, conclusions are drawn in Section5.
Fluency score: 3
Fluency issues: [dense map representation, exteroceptive data, partially act, of otherwise successful methods, derive collision scores]

Clarity score: 3
Clarity issues: [exteroceptive data, depth images coming from stereo vision or other sensors, layers that partially act as lossy, candidate robot trajectories, task-driven compression, 640\times 480resolution, Mfeature vectors, aggressive dimensionality reduction]

Grammaticality score: 4
Grammaticality issues: [the works in[15,22,12]that, In[15]the, the640\times 480pixels, achieving this goal, thus representing]

Readability score: 3
Readability issues: [long sentences with complex structures, use of technical terms without explanation, embedded clauses]

Coherence score: 3
Coherence issues: [jumps quickly between details and larger concepts, the flow from dimensionality reduction to task-driven compression to training of the neural network]
====================
	 Generative AI-empowered Simulation for Autonomous Driving in Vehicular Mixed Reality Metaverses
==========
Introduction:  The vehicular mixed reality (MR) Metaverse is envisioned as a promising solution for realizing autonomous driving by fusing the physical and virtual vehicular networks[46,40]. The multi-dimensional communications among physical and virtual entities can surrender the distance of “data islands” on roads for improving road safety and traffic efficiency while reducing energy consumption and carbon emissions[35]. Assisted by digital twin (DT) technologies, autonomous vehicles (AV) utilize advanced sensors, e.g., ultrasonic radars, cameras, and LiDAR, to collect data from their surrounding environments for constructing virtual representations in the virtual space[12]. Then, AVs can make driving decisions, such as driving model selection and motion planning, via artificial intelligence (AI) methods. Even though panoramic cameras and high-class LiDAR are equipped with AVs, each AV can only collect limited environment data and cannot perceive the whole environment, e.g., occlusions[32]. Therefore, multiple connected AVs, roadside units (RSUs), and virtual simulators can share and fuse sensing data in the virtual space, to perceive the complete information of environments including occlusions. However, it is difficult and costly to collect realistic driving data on a large scale to train AVs directly in the physical world. To address this issue, much effort from academia and industry has been devoted to developing platforms in the virtual space for traffic and driving simulations[7,8]. By establishing virtual driving simulation platforms with DT[14,12]and MR[27,6]technologies, virtual representations of AVs can efficiently collect traffic and training data and cheaply test it on rare cases, such as virtual traffic accidents and car collisions under realistic scenes[19,20]. Although traditional simulation platforms can generate an unlimited number of various driving experiences, the collected driving data requires a lot of manual work for labeling, which prevents the potential from being fully realized[7]. Fortunately, with the multi-modal generative AI[11,31,10], the labeled traffic and driving data can be synthesized directly for virtual autonomous driving systems[16]. In this way, the process of using simulation platforms for autonomous driving training and evaluation is revolutionized by shifting from collecting and labeling data to directly synthesizing labeled data[16,38]. Therefore, the simulation systems empowered by generative AI can generate large and diverse labeled driving datasets based on real-time road and weather conditions and user preferences for online prediction and offline training in autonomous driving systems. Furthermore, in the vehicular Metaverse, connected AVs, RSUs, and virtual simulators need to construct the traffic and driving simulation platforms in the virtual space collaboratively. To update with virtual representations in virtual space, AVs continuously generate and offload multiple computation-intensive DT tasks to RSUs in online traffic simulation[12]. Specifically, these DT tasks of each AV, including simulation, decision-making, and monitoring, are heterogeneous in requiring computing, communication resources, and deadlines. In driving simulations, virtual simulators synthesize controllable traffic and driving data for satisfying specific requirements, e.g., passenger preferences and weather conditions, of the simulated driving tasks[20]. In addition, the synthesized traffic and driving datasets can also be used in training virtual representations of AVs to further improve driving robustness. These synchronization activities, e.g., DT task execution, traffic and driving simulations, and AV training, are demanding enormous communication and computing resources of RSUs for supporting autonomous driving systems[44,39]. Therefore, developing effective multi-task incentive mechanisms that motivate RSUs to improve their use of communications and computing resources is imperative. As shown in Fig.1, in this paper, we propose a novel DT-assisted autonomous driving architecture for the vehicular MR Metaverse, where generative AI is leveraged to synthesize massive and conditioned traffic and driving data for online and offline simulations. In detail, to improve reliability in DT task execution, we propose a multi-task DT offloading model where AVs can offload heterogeneous DT tasks with different deadlines to RSUs for real-time execution. To improve reliability in driving decision-making, virtual simulators can utilize the information in DTs, such as current location, historical trajectory, and user preferences, for online traffic simulations[45,37]. Moreover, based on the collected sensing data in the physical world and user preferences in DTs, virtual simulators can synthesize massive and conditioned driving data for AV training of virtual simulators via running generative AI models. As a use case, we propose a diffusion model-based traffic sign generator, named TSDreamBooth, which is developed based on the DreamBooth[23]fine-tuned using Belgium traffic sign (BelgiumTS) dataset[18]. The TSDreamBooth can be leveraged to generate virtual traffic sign images under different driving conditions and user preferences. Finally, we propose a multi-task enhanced auction-based mechanism to satisfy multi-dimensional requirements (e.g., prices and deadlines) of multiple DT tasks. We analyze the properties of the proposed auction and prove that it is strategy-proof and adverse-selection free. The experimental results demonstrate that the proposed framework can increase total social surplus by 150%. Our main contributions are summarized as follows: To improve the safety and reliability of autonomous driving, we propose a novel DT-assisted MR Metaverse architecture with MR simulations empowered by generative AI. In this architecture, connected AVs, RSUs, and virtual simulators maintain digital simulation platforms in the virtual space, where data collecting, sharing, and utilizing among physical and virtual entities can improve driving safety and traffic efficiency in physical autonomous driving systems. In this architecture, we propose a reliable DT task offloading framework where AVs can continuously offload multiple DT tasks with different requirements to RSUs for updating DTs in the virtual space. In traffic and driving simulations, we consider generative AI-empowered virtual simulators to synthesize new driving data for AVs’ decision-making and training. To incentivize RSUs for providing resources in supporting autonomous driving systems, we propose a multi-task enhanced auction-based mechanism to offer fine-grained allocation results and prices for executing heterogeneous DT tasks with various deadlines. Based on the property analysis, the proposed mechanism is fully strategy-proof and adverse-selection free. The rest of this paper is organized as follows. In SectionII, we review the related works. In SectionIII, we discuss the proposed system architecture and its system model. Then, in SectionIV, we implement the multi-task enhanced auction-based mechanism. We demonstrate the experimental results in SectionV, and provide a conclusion in SectionVI.
Fluency score: 2
Fluency issues: The text is dense and contains many technical terms and acronyms that may not be easily understood without context, making it seem disjointed for a general audience.

Clarity score: 2
Clarity issues: Due to the high density of specialized information and lack of clear introductory explanations, the text is hard to follow for readers unfamiliar with the subject.

Grammaticality score: 4
Grammaticality issues: Some issues with punctuation, such as missing spaces after citations, minor awkward constructions, e.g., "The TSDreamBooth can be leveraged..."

Readability score: 2
Readability issues: The text is challenging to read due to the use of jargon, long sentences, and embedded clauses.

Coherence score: 3
Coherence issues: While the text maintains a logical flow of ideas, the complexity of sentences and use of multiple abbreviations interrupt the coherence.
====================
	 Guide the Learner: Controlling Product of Experts Debiasing Method Based on Token Attribution Similarities
==========
Introduction:  Overfitting to the training data is a big obstacle in learning patterns that generalize to unseen data. Traditionally, this is diagnosed by monitoring the performance of a trained model on an in-distribution (ID) test set. However, a bigger challenge is when both the training and test data have the same non-generalizable patterns, emerged as spurious correlations between input features and output labelsGardneret al.(2021). For instance, in the natural language inference (NLI) task, it is shown that the occurrence of some task-neutral words, like a negation in hypothesis, is highly correlated with a specific class(Gururanganet al.,2018). While high-capacity models can learn a generalized distribution of labels from the inputs, they are prone to spurious patterns, also known as dataset biasesClarket al.(2019); Heet al.(2019). A model could exploit these biases during fine-tuning, leading to a model that achieve high ID performance, while it is highly fragile in out-of-distribution (OOD) settings(Schusteret al.,2019; McCoyet al.,2020). Besides trying to prevent these non-generalizable artifacts from entering the datasetLiuet al.(2022), it is reasonable to seek for more robust learning methods. This has been the basis for a multitude of research works that encourage models to rely on truly generalizable patterns. Most of these methods are based on the assumption that the learning method will inevitably exploit biases if they are present in a training example(Clarket al.,2019; Sanhet al.,2020; Mahabadiet al.,2020; Utamaet al.,2020; Ghaddaret al.,2021). Therefore, they discourage the main model from paying much attention to the examples which are correctly classified by a biased model. Recently, it is shown that this assumption is questionable in the way that for a significant number of cases, the main model does not follow the biased model in treating biased examples(Amirkhani and Pilehvar,2021). Therefore, depriving the training algorithm from the examples which are detected to be biased is a waste of training data. In this paper, we propose an alternative way to discard biased examples. Instead of considering the mere prediction of the biased model, we monitor the way the model processes each example by computing its attribution scores over the input tokens. With the resulting scores, we adjust the proportion of the loss function that is a cross-entropy loss (CE) versus a Product of Experts loss (PoE). If the attribution scores are similar between the main and biased models, the loss becomes a PoE loss where a correct prediction from the biased model down-weights the contribution of the corresponding example. In contrast, dissimilarity between the scores suggests a different behaviour from the biased model and leads to a CE loss that only considers the main model’s prediction. Experiments on natural language inference and fact verification demonstrate that our method significantly outperforms previous approaches in terms of OOD performance while preserving its ID performance.
Fluency score: 3
Fluency issues: [Repetitive and complex sentence structures; inconsistent citation formatting]

Clarity score: 3
Clarity issues: [Overuse of technical terms without explanations; dense information presentation]

Grammaticality score: 4
Grammaticality issues: [Misplaced parentheses and missing spaces after citations]

Readability score: 2
Readability issues: [Complex and long sentences; high density of jargon]

Coherence score: 4
Coherence issues: [Some logical leaps between sentences; assumes prior knowledge]
====================
	 Bridging the Gap: Fine-to-Coarse Sketch Interpolation Network for High-Quality Animation Sketch Inbetweening
==========
Introduction:  The hand-drawn 2D animation workflow typically involves three key stages: sketching keyframes, inbetweening keyframes to produce intermediate sketch frames (i.e., inbetweens), and colorization to produce the final, full-color animations. The meticulous creation of inbetweens is crucial for achieving a smooth animation with lifelike motion transitions, effectively conveying the intended story or message. For a feature-length animation created through this process, the sheer volume of required inbetweens can be staggeringThomaset al.(1995), making it a highly specialized and labor-intensive task and serving as a limiting factor in overall animation productivity. To streamline the process of 2D sketch animation production, various studies have focused on the automatic synthesis of inbetweening sketch frames, which take two consecutive sketch keyframes as input and produce interpolated intermediate sketch frames (i.e., inbetweens) as output. These methods can be categorised into stroke-based and image-based. The stroke-based methods often rely on a cumbersome pre-processing step for sketch vectorisationWhitedet al.(2010); Yang (2017); Yanget al.(2018), which require additional specialized software or techniques. Image-based methods treat sketch frames as bitmap images, applying conventional image or video interpolation algorithms. However, they commonly face two significant challenges: 1) the absence of texture and color details in sketch frames, hindering reliable image-based inbetweening correspondence, and 2) exaggerated changes due to substantial object movements between two consecutive sketch keyframesNaritaet al.(2019). As a result, when image-based methods, especially devised for videosZhouet al.(2023)and colour animationsSiyaoet al.(2021); Chen and Zwicker (2022), are applied to sketch interpolation, they invariably introduce various artifacts into the produced interpolated frames. These discrepancies can adversely affect the continuity and quality of the animation produced. As illustrated in the Figure1, LDFINaritaet al.(2019)proposed for sketch interpolation generates broken strokes due to the missing sketch keypoint correspondence, while EISAIChen and Zwicker (2022)proposed for interpolating color animation frames and DQBCZhouet al.(2023)for video interpolation introduce blurriness (ornaments) and artifacts (e.g., distortion in face regions) in their results. Therefore, in this study, we propose a novel deep learning method for sketch interpolation, the Fine-to-Coarse Sketch Interpolation Network (FC-SIN), to comprehend and model the intricate and sparse patterns found in animation sketches. Specifically, FC-SIN adopts a fine-to-coarse approach that integrates multi-level guidance through three distinct aspects: 1)pixel-level dynamicsat a fine level with a bi-directional optical flow estimation module, 2)sketch-level correspondencewith a sketch matching and tracking mechanism for obtaining sketch keypoint traces and 3)region-level correspondenceat a coarse level with a region matching and bi-directional optical flow aggregation module. Guided by these multi-level perspectives, a multi-stream U-Transformer architecture is further devised to produce the intermediate sketch frames. It consists of two attention-based building blocks:convolution and self-attention block(CSB) and theconvolution and cross-attention block(CCB) to leverage the diverse multi-level insights for producing precise inbetween sketch patterns. To facilitate the research on animation sketch interpolation, we constructed a large-scale sketch triplet dataset, namely STD-12K, from 30 sketch animation series (e.g., over 25 hours) with various artistic styles. Comprehensive experimental results demonstrate that the proposed method FC-SIN clearly outperforms the state-of-the-art frame interpolation methods for animation sketch interpolation. Overall, the key contributions of this study are as follows: A novel deep learning architecture, FC-SIN, for sketch interpolation by effectively formulating sparse sketch patterns with fine-to-coarse multi-level guidance. A novel self- and cross-attention based multi-stream U-Transformer formulated with the guidance from multi-level perspectives. A large-scale sketch triplet dataset with various artistic styles constructed for the research community.
Fluency score: 4
Fluency issues: "staggeringThomaset al.(1995)" needs a space; "LDFINaritaet al.(2019)" needs a space; "proposed for videosZhouet al.(2023)" needs a space; "colour animationsSiyaoet al.(2021);" needs a space; "Figure1" needs a space

Clarity score: 3
Clarity issues: Some sentences are overly long and complex, making them hard to follow, particularly the one starting with "As a result, when image-based methods...". The text has a high density of technical terms without explanations which might confuse readers unfamiliar with the topic.

Grammaticality score: 4
Grammaticality issues: Missing spaces after citations; missing article "the" before "Figure1".

Readability score: 3
Readability issues: Lengthy and complex sentences with technical jargon; insertion of citations disrupts the flow.

Coherence score: 3
Coherence issues: The text frequently switches between discussing methodological problems and introducing solutions which can be disorienting, the structure could be improved to group similar themes together for better coherence.
====================
	 Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters
==========
Introduction:  Self-supervised learning has been a dominant paradigm in natural language processing (NLP)[1]and in recent years, it has also been adopted by the speech community to learn high-fidelity representations[2,3,4,5]that capture various non-lexical aspects of speech and audio such as lip-smacking, laughter, hesitation, etc. In this paradigm, the targets to learn are derived from the input signal itself, making the learned representations more powerful in principle compared to those learned using textual labels and annotations of any kind. These powerful base representations have been successfully adopted for several downstream tasks[6], some of which include: ASR, speaker identification and speech translation. Pre-training models with a very large number of parameters on proportionally large datasets has been a central theme in self-supervised learning. However, these datasets may understandably fall short in terms of sufficiently capturing non-canonical and diverse speech and audio characteristics such as rare non-native accents, stammering, etc. This leads to great disparity in downstream task performance across well-represented and underrepresented speaker populations. This data problem has also existed with supervised models for specific tasks such as ASR and in such scenarios, the typical path has been to patch task performance by collecting task-specific labeled datasets with non-canonical characteristics and fine-tuning for the task[7]. This unfortunately entangles speech and audio characteristics with the task itself, which can limit effective learning of such characteristics in task-specific representations as well as limiting their re-usability across tasks. In this paper, we consequently posit that continued self-supervised learning of speech and audio representations on task-agnostic unlabeled datasets is an effective strategy to adapt to non-canonical speech characteristics. The specific characteristic we choose to study is accents but the methodology holds for any characteristic. We propose learning different high-dimensional spaces for different accents via independently adding residual adapters for each target accent to the model and continuing pre-training on accent-specific datasets. Since residual adapters are parameter-wise much smaller than the base model, this serves as a parameter-efficient way for personalized adaptation without over-fitting and saves on storage costs for inference since only a single copy of the base model needs to be stored. We conduct our experiments with HuBERT-large[2]as the base model and ASR as the downstream task but posit that our proposed approach is both model and task agnostic. Our chosen base model is a state-of-the-art model with low word error rates on canonical datasets such as LibriSpeech. By design, we pick 4 non-native English accents where the HuBERT-large model has high word error rates (WER), in the range 24-50% and show strong results on all 4 accents with over 22% WERR over the baseline. Previous work has shown improvements in WER on such accents by supervised training using labeled datasets[7]. In contrast, we achieve our WER improvements by continuing to self-supervise models using unlabeled data alone. We show that the gains from adapting to an accent using a particular dataset translate to other evaluation sets with the same accent as well, indicating that the effectiveness of our approach is due to adaptation to the accents’ acoustic characteristics and not other confounding factors. Finally, we also explore the degree of parameter-efficiency possible when adapting to target accents, finding that we can achieve strong WERR over the baseline while updating only 16% of the total model parameters.
Fluency score: 4
Fluency issues: Absence of space after "NLP)[1]and", "speech. Pre-training", "HuBERT-large[2]as".

Clarity score: 4
Clarity issues: Dense sentence structures, especially in the latter half, make it somewhat difficult to follow.

Grammaticality score: 3
Grammatical issues: Formatting errors such as missing spaces after references "[1]and", "[2,3,4,5]that", and "[2]as" affect grammaticality.

Readability score: 3
Readability issues: The long sentences with embedded clauses and the technical jargon could hinder readability for non-specialists.

Coherence score: 4
Coherence issues: The overall structure is coherent, but lengthy sentences can make it challenging to maintain the thread of the argument.
====================
	 Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs
==========
Introduction:  Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient’s question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V+L) tasks and many related benchmarks like Chart Question Answering (CQA)2022-chartqa;kafle2018dvqa;Methani_2020_WACVand Chart Summarization (CS)2022-chartare introduced. While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V+L research, applying these existing methods to handle chart-related tasks is hard. Recent research ChartQA2022-chartqaand Chart-to-Text2022-chartattempt to first convert chart images to their underlined tables and use the extracted tables to perform chart-related V+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS. Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1) Automatic table extraction is unreliable. Existing methodsluo2021chartocr;parsinglineare often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2) In most cases, the whole table is optional for resolving the chart-related V+L task. As illustrated in Fig1, to answer the question"What is the value of India Bar", the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V+L model with the capability to dynamically interpret the table value from the chart information. Therefore, in this paper, we proposeChartT5, an OCR-based image-to-text generation model pre-trained on a self-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model’s capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value. We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart-to-Text. In ChartQA, ChartT5 outperforms all the non-pretraining methods that use extracted tables by at least 8\%performance gains. ChartT5 also beats the pre-training table-based methods, which demonstrates the effectiveness of the proposed pre-training strategies. On Chart-to-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metricsbarzilay-2005-collectivewhich values the model’s capability to extract the critical information from the chart. In summary, our contributions are summarized below: We propose chart-to-table pre-training for V+L model to learn the capability of interpreting table data from the chart. We demonstrate that the pre-trained model consistently outperforms table-based methods on two chart understanding tasks. We conduct comprehensive ablation studies to validate the effectiveness of chart-to-table pre-training and the proposed pre-training objectives.
Fluency score: 3
Fluency issues: Erratic use of citations and acronyms ("CQA2022-chartqa;kafle2018dvqa;Methani_2020_WACV", "ChartQA2022-chartqaand", "loro2021chartocr;parsinglineare"), inconsistent tense ("ChartT5 learns", "ChartT5 outperforms"), and unclear referents ("As illustrated in Fig1").

Clarity score: 3
Clarity issues: Overuse of jargon, acronyms without proper introduction, and a high density of information without sufficient explanation.

Grammaticality score: 4
Grammaticality issues: Awkward construction ("the model just needs access to the second row to give the correct answer") and minor issues with punctuation and spacing around citations.

Readability score: 3
Readability issues: Technical language, acronyms, run-on sentences, and citation formatting make the text challenging for a general audience.

Coherence score: 3
Coherence issues: The progression from the problem to the solution is logical but is obscured by the complexity of the text and lack of clear transitions.
====================
	 Co-Learning Empirical Games and World Models
==========
Introduction:  Even seemingly simple games can actually embody a level of complexity rendering them intractable to direct reasoning. This complexity stems from the interplay of two sources: dynamics of the game environment, and strategic interactions among the game’s players. As an alternative to direct reasoning, models have been developed to facilitate reasoning over these distinct aspects of the game.Empirical gamescapture strategic interactions in the form of payoff estimates for joint policiesWellman (2006).World modelsrepresent a game’s transition dynamics and reward signal directlySutton and Barto (2018); Ha and Schmidhuber (2018b). Whereas each of these forms of model have been found useful for game reasoning, typical use in prior work has focused on one or the other, learned and employed in isolation from its natural counterpart. Co-learning both models presents an opportunity to leverage their complementary strengths as a means to improve each other. World models predict successor states and rewards given a game’s current state and action(s). However, their performance depends on coverage of their training data, which is limited by the range of strategies considered during learning. Empirical games can inform training of world models by suggesting a diverse set of salient strategies, based on game-theoretic reasoningWellman (2006). These strategies can expose the world model to a broader range of relevant dynamics. Moreover, as empirical games are estimated through simulation of strategy profiles, this same simulation data can be reused as training data for the world model. Strategic diversity through empirical games, however, comes at a cost. In the popular framework of Policy-Space Response Oracles (PSRO)Lanctotet al.(2017), empirical normal-form game models are built iteratively, at each step expanding a restricted strategy set by computing best-response policies to the current game’s solution. As computing an exact best-response is generally intractable, PSRO uses Deep Reinforcement Learning (DRL) to compute approximate response policies. However, each application of DRL can be considerably resource-intensive, necessitating the generation of a vast amount of gameplays for learning. Whether gameplays, or experiences, are generated via simulationObando-Ceron and Castro (2021)or from real-world interactionsHester and Stone (2012), their collection poses a major limiting factor in DRL and by extension PSRO. World models present one avenue to reduce this cost by transferring previously learned game dynamics across response computations. We investigate the mutual benefits of co-learning a world model and an empirical game by first verifying the potential contributions of each component independently. We then show how to realize the combined effects in a new algorithm,Dyna-PSRO, that co-learns a world model and an empirical game (illustrated in Figure1). Dyna-PSRO extends PSRO to learn a world model concurrently with empirical game expansion, and applies this world model to reduce the computational cost of computing new policies. This is implemented by a Dyna-based reinforcement learnerSutton (1990,1991)that integrates planning, acting, and learning in parallel. Dyna-PSRO is evaluated against PSRO on a collection of partially observable general-sum games. In our experiments, Dyna-PSRO found lower-regret solutions while requiring substantially fewer cumulative experiences. The main points of novelty of this paper are as follows: (1) empirically demonstrate that world models benefit from the strategic diversity induced by an empirical game; (2) empirically demonstrate that a world model can be effectively transferred and used in planning with new other-players. The major contribution of this work is a new algorithm, Dyna-PSRO, that co-learns an empirical game and world model finding a stronger solution at less cost than the baseline, PSRO.
Fluency score: 3
Fluency issues: Citations interrupting the flow ("Wellman (2006)", "Sutton and Barto (2018); Ha and Schmidhuber (2018b)") and unwieldy sentences.

Clarity score: 3
Clarity issues: Overly technical language, lack of explanation for abbreviated terms on first use (DRL, PSRO), and jargon (e.g., "normal-form game models," "best-response policies").

Grammaticality score: 4
Grammaticality issues: Citations not formatted correctly ("Lanctotet al.(2017)") and a missing space ("gameplays").

Readability score: 3
Readability issues: Jargon-heavy text, complex sentence structure, and inadequate formatting of citations negatively impact readability.

Coherence score: 3
Coherence issues: Jumps between concepts without smooth transitions, disjointed sentences, and the integration of citations disrupt the narrative flow.
====================
	 Cross-domain Iterative Network for Simultaneous Denoising, Limited-angle Reconstruction, and Attenuation Correction of Low-dose Cardiac SPECT
==========
Introduction:  Cardiac Single-Photon Emission Computed Tomography (SPECT) is the most widely performed non-invasive exam for clinical diagnosis of ischemic heart diseases[7,10]. Reducing the tracer dose can lower patient radiation exposure, but it will result in increased image noise[22,8]. Acquiring projections in fewer angles using fewer detectors allows for faster scanning and lower hardware costs, but it also leads to decreased reconstruction accuracy[1,13]. Additionally, in clinical practice, computed tomography (CT)-derived attenuation maps (\mu-maps) are commonly used for SPECT attenuation correction (AC)[3,11]. However, most SPECT scanners are stand-alone without the assistance of CT[16]. The CT scan also causes additional radiation exposure and SPECT-CT misalignments[18,11]. Deep learning-based methods have been extensively explored to address the aforementioned limitations individually. To reduce image noise in low-dose (LD) SPECT, convolutional neural networks (CNNs) were employed to process the LD projection, producing the full-dose (FD) projection for SPECT reconstruction[21,1]. Similarly, to perform limited-angle (LA) reconstruction, the LA projection was input to CNNs to predict the full-angle (FA) projection[2,23,20]. In addition, a dual-domain approach, known as Dual-domain Sinogram Synthesis (DuDoSS), utilized the image-domain output as a prior estimation for the projection domain to predict the FA projection[5]. For the CT-free AC, CNNs were used to generate pseudo attenuation maps (\mu-maps) from SPECT emission images[19,6]. Although various methods have been developed to address these limitations individually, it is of great interest to address all these limitations simultaneously to enable CT-free, low-dose, low-cost, and accelerated SPECT, which could potentially lead to better performance on those separated but correlated tasks. Thus, we propose a Cross-Domain Iterative Network (CDI-Net) for simultaneous denoising, LA reconstruction, and CT-free AC in cardiac SPECT. In CDI-Net, projection and image-domain networks are end-to-end connected to fuse the predicted emission and anatomical features across domains and iterations. Adaptive Weight Recalibrators (AWR) calibrate the fused features to improve the prediction accuracy. We tested CDI-Net using clinical data and compared it to existing methods. Ablation studies were conducted to verify the impact of cross-domain, cross-iteration fusions and AWR on enhancing network performance.
Fluency score: 4
Fluency issues: [None]

Clarity score: 4
Clarity issues: [Use of specialized terminology without explanation may limit understanding for general audience]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [Sentences are long and complex, which may challenge some readers]

Coherence score: 4
Coherence issues: [Highly technical language may affect coherence for readers without a background in the field]
====================
	 Plug-in Performative Optimization
==========
Introduction:  Predictions have the power to influence the patterns they aim to predict. For example, stock price predictions inform trading decisions and hence prices; traffic predictions influence routing decisions and thus traffic outcomes; recommendations shape users’ consumption and thus preferences. This pervasive phenomenon has been formalized in a technical framework calledperformative predictionPerdomoet al.(2020). A central feature that distinguishes the framework from traditional supervised learning is the concept of adistribution map\mathcal{D}(\cdot). This object, aimed to capture the feedback from predictions to future observations, is a mapping from predictorsf_{\theta}to their induced data distributions\mathcal{D}(\theta). The overarching goal in performative prediction is thus to deploy a predictorf_{\theta}that will have good performance after its deployment, that is, on its induced data distribution\mathcal{D}(\theta). Formally, the goal is to choose predictor parameters\theta\in\Theta\subseteq\mathbb{R}^{d_{\theta}}so as to minimize theperformative risk: where\ell(z;\theta)measures the loss incurred by predicting on instancezwith model\theta. Typically,zis a feature–outcome pair(x,y). We refer to\theta_{\mathrm{PO}}=\operatorname*{arg\,min}_{\theta\in\Theta}\mathrm{PR}(\theta)as theperformative optimum. The main challenge in optimizing the performative risk lies in the fact that the map\mathcal{D}(\cdot)is not known. We only observe samples from\mathcal{D}(\theta)for models\thetathat have been deployed; we do not observe any feedback for other models, of which there are typically infinitely many. A key discriminating factor between existing solutions for optimizing under performativity is how they cope with this uncertainty. One group of methods accounts for the feedback without assuming a problem-specific structure for it. This group includes bandit strategiesKleinberget al.(2008); Jagadeesanet al.(2022)and derivative-free optimizationFlaxmanet al.(2004); Milleret al.(2021). These methods converge to optima at typically slow—without convexity, even exponentially slow—convergence rates. Moreover, their rates rely on unverifiable regularity conditions that are out of the learner’s control, such as convexity of the performative riskMilleret al.(2021); Izzoet al.(2021); Donget al.(2018)or bounded performative effectsJagadeesanet al.(2022). A complementary group of methods—an important starting point for this work—takes feedback into account by positing explicitmodelsfor it. Such models include best-response models for strategic classificationHardtet al.(2016); Jagadeesanet al.(2021); Levanon and Rosenfeld (2021); Ghalmeet al.(2021), rational-agent models in economicsSpence (1978); Wooldridge (2003), and parametric distribution shiftsIzzoet al.(2021); Milleret al.(2021); Izzoet al.(2022), among others. To argue that the methods building on such models find optimal solutions, existing analyses assume that the model iswell-specified. However, models of social behavior are widely acknowledged to be simplistic representations of real-world dynamics. Yet, despite the unavoidable misspecification of models, they are ubiquitous in practice. Though their simplicity leads to misspecification, it also allows for efficient, interpretable, and practical solutions. Motivated by this observation, in this work we ask:can models for performative feedback be useful, even if misspecified? We initiate a study of the benefits of modeling feedback in performative prediction. We show that models—even if misspecified—can indeed help with learning under performativity. We begin by defining a general protocol for performative optimization with feedback models, which we callplug-in performative optimization. The protocol consists of three steps. First, the learner deploys models\theta_{i}\sim\tilde{\mathcal{D}}and collects dataz_{i}\sim\mathcal{D}(\theta_{i}),i\in[n]. Here,\tilde{\mathcal{D}}is an exploration distribution of the learner’s choosing (for example, it can be uniform on\Thetawhen\Thetais bounded). The second step is to use the observations\{(\theta_{i},z_{i})\}_{i=1}^{n}to fit an estimate of the distribution map. The map is chosen from a parametric family of possible maps\bm{\mathcal{D}}_{\mathcal{B}}=\{\mathcal{D}_{\beta}\}_{\beta\in\mathcal{B}}, obtained through modeling. The estimation of the map thus reduces to computing an estimate\hat{\beta}. For example, in strategic classification,\betacould be a parameter quantifying the strategic agents’ tradeoff between utility and cost. Finally, the third step is to compute theplug-in performative optimum: We prove a general excess-risk bound on\mathrm{PR}(\hat{\theta}_{\mathrm{PO}})-\mathrm{PR}(\theta_{\mathrm{PO}}), showing that the error decomposes into two terms. The first is amisspecification error term,MisspecErr, which captures the gap between the true performative risk and the plug-in performative risk\mathrm{PR}^{\hat{\beta}}(\theta)in the large-sample regime. This term is irreducible and does not vanish as the sample sizengrows. The second is astatistical error termthat captures the imperfection in fitting\hat{\beta}due to finite samples. For a broad class of problems, our main result can be summarized as follows. The excess risk of the plug-in performative optimum is bounded by: for some universal constantc>0. Therefore, although the misspecification error is irreducible, the statistical error vanishes at afast rate. In contrast, model-agnostic strategies such as bandit algorithmsKleinberget al.(2008); Jagadeesanet al.(2022)do not suffer from misspecification but have an exceedingly slow, often exponentially slow, statistical rate. For example, the bandit algorithm of Jagadeesan et al.Jagadeesanet al.(2022)has an excess risk of\tilde{O}\left(n^{-\frac{1}{d_{\theta}+1}}\right). This is why feedback models are useful—for a finiten, their excess risk can be far smaller than the risk of a model-agnostic strategy due to the rapidly vanishing statistical rate. The statistical rate is fast because it only depends on the parametric estimation rate of\hat{\beta}; it does not depend on the complexity of\mathrm{PR}. One important special case of performative prediction isstrategic classification. We apply our general theory to common best-response models in strategic classification. We also conduct numerical evaluations that empirically confirm our theoretical findings. Overall our results support the use of models in optimization under performative feedback. We give an overview of existing threads most closely related to our work. We build on the growing body of work studying performative predictionPerdomoet al.(2020). Existing work studies different variants of retrainingPerdomoet al.(2020); Mendler-Dünneret al.(2020); Drusvyatskiy and Xiao (2022), which converge to so-called performatively stable solutions, as well as methods for finding performative optimaMilleret al.(2021); Izzoet al.(2021); Jagadeesanet al.(2022). The methods in the latter category are largely model-agnostic and as such converge at slow rates. Exceptions include the study of parametric distribution shiftsIzzoet al.(2021,2022)and location familiesMilleret al.(2021); Jagadeesanet al.(2022), but those analyses crucially rely on the model being well-specified. We are primarily interested in misspecified settings. Other work in performative prediction includes the study of time-varying distribution shiftsBrownet al.(2022); Izzoet al.(2022); Li and Wai (2022); Rayet al.(2022), multi-agent settingsDeanet al.(2022); Qianget al.(2022); Naranget al.(2022); Piliouras and Yu (2022), and causality and robustnessMaheshwariet al.(2022); Mendler-Dünneret al.(2022); Kim and Perdomo (2022), among others; it would be valuable to extend our theory on the use of models to those settings. Strategic classificationHardtet al.(2016); Donget al.(2018); Levanon and Rosenfeld (2021); Zrnicet al.(2021), as well as other problems studying strategic agent behavior, frequently use models of agent behavior in order to compute Stackelberg equilibria, which are direct analogues of performative optima. However, convergence to Stackelberg equilibria assumes correctness of the models, a challenge we circumvent in this work. We use strategic classification as a primary domain of application of our general theory. Our work is partially inspired by works in statistics studying the benefits and impact of modeling, including under misspecificationWhite (1980,1982); Bujaet al.(2019a,b). At a technical level, our results are related to M-estimationVan der Vaart (2000); Geer (2000); Mouet al.(2019), as well as semi-parametric statisticsTsiatis (2007); Kennedy (2022), where the goal is to find models that lead to minimal estimation error. Plug-in performative optimization serves as an alternative to black-box baselines for zeroth-order optimization, which have previously been studied in the performative prediction literature. These include bandit algorithmsKleinberget al.(2008); Jagadeesanet al.(2022)and zeroth-order convex optimization algorithmsFlaxmanet al.(2004); Milleret al.(2021). As mentioned earlier, we show that the use of models can give far smaller excess risk, given the fast convergence rates of parametric learning problems.
Fluency score: 4
Fluency issues: [Latex and mathematical notation interspersed within the text]

Clarity score: 2
Clarity issues: [The text is dense with technical jargon and complex concepts that require prior knowledge to understand fully]

Grammaticality score: 4
Grammaticality issues: [Proper mathematical symbols and formatting are needed for correct grammar within this context]

Readability score: 2
Readability issues: [Dense paragraphs, heavy use of domain-specific terminology and complex sentence structures]

Coherence score: 3
Coherence issues: [The text switches between discussing theoretical framework and applied methods, which might confuse readers without detailed background knowledge]
====================
	 Approaching Test Time Augmentation in the Context of Uncertainty Calibration for Deep Neural Networks
==========
Introduction:  Deep Neural Networks (DNNs) changed the paradigm with regards to the applicability of machine learning (ML) systems to real-world scenarios. Consequently, deep learning (DL) models are now present in critical application domains (e.g., autonomous driving, medicine, remote sensing, robotics), where bad decision-making can bear potentially drastic consequences. This requires that DNNs are not only highly accurate, but also highlyreliable- decision-makers should be able to “trust” the predictions of these models. This lead us to the problem ofuncertainty calibration(also referred as confidence calibration or simplycalibration): it is required that the confidence output generated by the DL model - that translates as the confidence the model has in the prediction that is making - realistically represents the true likelihood of correctness. For the sake of intuition, a calibrated model would for example, in the long run, correctly classify70\%of those predictions that have a confidence value of0.7associated. This accurate quantification of predictive uncertainty results in reliable confidence values associated with each prediction, and therefore, in a more reliable model. As such, it is important to understand how well calibrated are modern DNNs. Further details, including the formalization of the uncertainty calibration problem, will be described in Section3.Although increasingly accurate, modern DL architectures have been found to be tendentiouslyuncalibrated[7,20]. Furthermore, “modern neural networks exhibit a strange phenomenon: probabilistic error and miscalibration worsen even as classification error is reduced”[7]. For this reason, the goal of this work is to improve the uncertainty calibration of DNNs in the task of image classification, by proposing a novel accuracy-consistent weighted test time augmentation method.Test time augmentation is a general methodology that leverages data augmentation techniques to create multiple samples from the original input at inference (i.e., at test time). Therefore, test time augmentation methods can be applied to pre-trained models, since, in this case, the augmentation process is not applied during the training phase. The technique introduced in this work combines the use of test time augmentation with a custom weighting system, guaranteeing that the accuracy of the original DL model is not corrupted, while still being optimized to improve uncertainty calibration. This builds, partially, on the work done in[3], proposing both a reformulated version -V-ATTA(Vector Adaptive Test Time Augmentation) - of the preliminary method presented in[3]and also a generalized version -M-ATTA(Matrix Adaptive Test Time Augmentation) - with a broader and extended empirical evaluation.M/V-ATTAis evaluated on the benchmark CIFAR-10/CIFAR-100[11]datasets, as well as on a benchmark satellite imagery dataset, the Aerial Image Dataset (AID)[28]. The results are compared with state-of-the-artpost-hoccalibration methods, with respect to the Brier score[2]and theExpected Calibration Error(ECE), for bothcommonandstronguncertainty calibration (see Section3for further details).Contribution: We propose a novel calibration technique - with two different variations (M/V-ATTA) - based on test time augmentation, that guarantees consistency in the accuracy of deep models in which is applied, while being shown to improve uncertainty calibration-related evaluation metrics, outperforming state-of-the-artpost-hoccalibration methods in most cases. To the best of our knowledge,M/V-ATTAis the first method based on test time augmentation that has been proposed to improve the uncertainty calibration of deep models (besides its predecessor in[3]). Furthermore, the method presented here can be used with pre-trained DNNs, which is advantageous in terms of applicability.
Fluency score: 4
Fluency issues: Occasional lack of space after punctuation ("highlyreliable-", "problem ofuncertainty calibration", "simplycalibration"), which might disrupt the flow.

Clarity score: 3
Clarity issues: Overuse of parentheses and technical jargon can make the text dense and less clear to the lay reader; "0.7associated" should have a space.

Grammaticality score: 3
Grammaticality issues: Missing spaces ("highlyreliable-", "ofuncertainty", "simplycalibration", "70\%", "0.7associated"), missing "to" in "This lead us to the problem".

Readability score: 3
Readability issues: Dense technical language, run-on sentences, and formatting issues reduce readability.

Coherence score: 4
Coherence issues: The text is generally coherent but would benefit from clearer transitions and explanations of technical terms for non-experts.
====================
	 GATGPT: A Pre-trained Large Language Model with Graph Attention Network for Spatiotemporal Imputation
==========
Introduction:  The presence of multivariate time series data is extensively documented across a variety of sectors including economics, transportation, healthcare, and meteorology, as evidenced in several studies[4,18,32,26]. A range of statistical and machine learning techniques have been shown to perform effectively on complete datasets in several time series tasks, including forecasting[14], classification[16], and anomaly detection[6]. However, it is often observed that multivariate time series data collected from real-world scenarios are prone to missing values due to various factors, such as sensor malfunctions and data transmission errors. These missing values can considerably affect the quality of the data, subsequently impacting the effectiveness of the aforementioned methods in their respective tasks. Extensive research efforts have been dedicated to addressing the challenges in spatiotemporal imputation. A typical approach involves the development of a distinct framework for initially estimating missing values, followed by the application of the completed dataset in another sophisticated framework for subsequent operations like forecasting, classification, and anomaly detection. To fill in missing values, various statistical and machine learning techniques are applied. Popularly employed methods include autoregressive moving average (ARMA)[1], expectation-maximization algorithm (EM)[23], and k-nearest neighbors (kNN)[20,15]. These models often depend on rigorous presumptions such as temporal uniformity and similarity between series, which may not be suitable for the complexity of real-world multivariate time series data, potentially leading to suboptimal performance. Deep learning techniques have gained significant traction among researchers in the realm of spatiotemporal imputation. The utility of Recurrent Neural Networks (RNN) was initially recognized in this context[7,9]. Subsequently, Generative Adversarial Networks (GAN) demonstrated impressive results, thanks to their robust generative properties[19]. More recently, advancements have been made by integrating RNN-based methods with Graph Neural Networks (GNN) to enhance the capability of extracting spatial dependencies within these frameworks. Nevertheless, the current focus in this field is predominantly on developing intricate and specialized frameworks aimed at improving performance on specific public industrial datasets, often overlooking the aspects of generalizability and adaptability for varied datasets and downstream applications. Additionally, the rise of pre-trained models in Natural Language Processing (NLP) has been noteworthy, with their applications extending into diverse areas such as Computer Vision (CV)[5], Multi-modality[28], and Recommendation Systems[33]. These Large Language Models (LLMs) have demonstrated remarkable success due to their advanced representation learning abilities. Yet, there is a scarcity of research exploring using pre-trained LLMs in the domain of time series data analysis. Our research is focused on exploring the capacity of LLMs to act as potent representation learners, particularly in capturing temporal dependencies. Moreover, we recognize LLMs’ exceptional few-shot learning capabilities, which positions them as highly suitable for time series contexts, where gathering extensive training data is often a formidable challenge. In this paper, we aim to introduce the pre-trained Large Language Models (LLMs) to solve the spatiotemporal imputation problem. The main contributions of this paper are summarized as follows: To the best of our knowledge, we are the first to introduce pre-trained LLMs to the spatiotemporal imputation. The integration of these models, with their pre-existing knowledge and architectural design, is instrumental in deciphering the intricacies of spatiotemporal dependencies. We have incorporated a graph attention module specifically designed to discern spatial dependencies. This module is devised to augment the LLMs in comprehending and assimilating the intrinsic characteristics of spatiotemporal data. Through our experimentation with various real-world datasets, we have demonstrated the effectiveness of our proposed framework. The results highlight its proficiency in spatiotemporal imputation, standing in comparison with multiple established baselines. The rest of this paper is organized as follows. Section2reviews the related work on spatiotemporal imputation. Section3presents the details of our proposed GATGPT framework, including the graph attention module and the pre-trained LLM block. Section4demonstrates our evaluation of the proposed framework performance on real-world datasets. Finally, Section5draws our conclusions.
Fluency score: 4
Fluency issues: [No significant fluency issues]

Clarity score: 4
Clarity issues: [Complex sentences with technical jargon]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues]

Readability score: 3
Readability issues: [Long and complex sentences; excessive use of technical terms and acronyms]

Coherence score: 4
Coherence issues: [Some transitions between ideas could be smoother; high density of information]
====================
	 CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model
==========
Introduction:  Pansharpening refers to fusing a low-spatial-resolution multispectral (LRMS) image with a single-band high-spatial-resolution panchromatic (PAN) image to obtain a high spatial and spectral resolution MS (HRMS) image. Owing to the technological and physical limitations of imaging devices, remote sensing images obtained by satellite sensors always have a trade-off between spatial and spectral resolution[1]. Therefore, most of available satellites carry two types of sensors,i.e. , an MS sensor and a PAN sensor, to acquire LRMS and PAN images simultaneously. Researchers can then combine them through pansharpening technology. Benefiting from the availability of large-scale remote sensing imagery, numerous deep-learning (DL)-based methods achieve great success in pansharpening. According to the base models they used, these methods can be roughly divided into convolutional neural networks (CNNs) based[2,3,4], generative adversarial networks (GANs) based[5,6,7], and Transformer based[8,9,10]. CNNs-based methods mainly extract spatial and spectral features layer-by-layer. They take pansharpening as a regression task, and are usually supervised by a reconstruction loss. GANs-based models utilize generators to fuse MS and PAN images and the discriminators to adversarially train the model for the generation of high-fidelity fused products. To model the long-range dependency of images, researchers introduce the Transformer architecture to pansharpening[8]. These models can learn either the self-similarity in a single image[9]or the interactive information between two kinds of modality images[10]. Most of available DL-based pansharpening methods are within the supervised-learning framework. However, there are no high-resolution MS images that can be taken as groundtruths. Therefore, researchers train the models at reduced resolution, where the training samples are prepared according to Wald’s protocol[11],i.e. , downsample PAN and MS to take them as inputs, and the original MS images are treated as references, which are actually pseudo-groundtruths[12]. Nevertheless, due to the scale variation[13,7], models trained at reduced resolution are unsuitable for pansharpening at the original images, resulting in inferior performance at full resolution[6]. Recently, researchers devote to developing unsupervised pansharpening methods. Most of them[14,15]concentrate only on the design of unsupervised loss function, however their effectiveness also depends on the features extracted[16]. We argue that the scale dependency issue can be alleviated from the perspective of not only unsupervised loss function, but also a good representation learner. Recent works utilize generative models to act as representation learners[16,17,18]. Among them, Denoising Diffusion Probabilistic Model (DDPM)[19]outperforms other alternative generative models and provides more semantically-valuable pixel-wise representations[20], which highlights the potential of using the state-of-the-art DDPM as strong unsupervised representation learners. Based on this insight, we build model upon DDPM to explore its representation ability to pansharpening. Furthermore, available DL-based methods have limited generalization ability, where the model trained on a specific dataset always performs unsatisfactorily on datasets collected from other satellite sensors. This further arises a question: how can we develop a unified representation learner based on DDPM to make it accountable for the extraction of general spatial and spectral features, regardless of the distinctiveness of sensors’ attributes. For pansharpening task, PAN and MS images have distinctive spatial and spectral characteristics respectively, which naturally constitutes a self-supervised pretext task that one can predict high-resolution PAN image from low-resolution MS image, and inversely predict spectral-appealing MS image from spectral-scarce PAN image. In this paper, we embed pansharpening into diffusion model to construct a new self-supervised paradigm, dubbed CrossDiff. As Figure1shows, the model consists of two stage training. In the first stage, the self-supervised pre-training is conducted based on DDPM to obtain a noise predictor with UNet structure[21]. Afterwards, the frozen encoders act as spatial-spectral feature representation learners to fuse MS and PAN images with a tunable fusion head. By introducing a cross-predictive diffusion process, CrossDiff outperforms state-of-the-art unsupervised pansharpening methods and has strong generalization ability. The main contributions are summarized as follows: We design a new two-stage pansharpening paradigm to explore the potential of DDPM to the self-supervised spatial and spectral features extraction. A cross-predictive diffusion process is introduced to pre-train the spatial and spectral representation learners. Following the process of DDPM, the effective training objectives encourage the pretext task to explicitly learn spatial and spectral diffusion latent. By freezing the pre-trained models and only tuning the fusion head, our proposed CrossDiff performs well at both full and reduced resolution, and has strong cross-sensor generalization ability.
Fluency score: 4
Fluency issues: "[1]" after "resolution" disrupts flow; "i.e. ," should be "i.e.,"; "[8]" after "pansharpening" disrupts flow; "single image[9]or" missing space; "methods[14,15]concentrate" missing space; "Model (DDPM)[19]outperforms" missing space

Clarity score: 3
Clarity issues: Dense terminology and complex sentences; could benefit from simpler language and explanations for a broader audience.

Grammaticality score: 3
Grammaticality issues: Misuse of commas, e.g., "i.e. ,"; missing spaces after references, e.g., "image[9]or", "methods[14,15]concentrate", "Model (DDPM)[19]outperforms".

Readability score: 3
Readability issues: Dense use of specialized terms and acronyms; long, complex sentences.

Coherence score: 4
Coherence issues: The text is coherent but could improve by linking concepts more explicitly and reducing heavy terminology.
====================
	 OctFormer: Octree-based Transformers for 3D Point Clouds
==========
Introduction:  3D point cloud understanding is a fundamental task in computer graphics and vision and has a broad range of applications, including robotics, autonomous driving, and augmented reality. A variety of deep learning methods have been proposed for it, such as voxel-based CNNs(Wu et al.,2015; Wang et al.,2017; Graham et al.,2018), view-based CNNs(Su et al.,2015), and point-based networks(Qi et al.,2016,2017b; Li et al.,2018), and remarkable progress has been made. Recently, point cloud transformers have emerged(Guo et al.,2021; Zhao et al.,2021; Misra et al.,2021)as an effective alternative with the potential for cross-multimodality training and general intelligent models(Radford et al.,2021; Ramesh et al.,2022). However, the efficiency of point cloud transformers is still much worse than their CNN counterparts(Wang et al.,2017; Choy et al.,2019; Nekrasov et al.,2021), especially on scene-scale datasets like ScanNet(Dai et al.,2017), and the performance of point cloud transformers is also just comparable. Since it has been proven that transformers are at least as expressive as CNNs(Cordonnier et al.,2020), one of the key challenges of applying transformers to point clouds is to overcome the huge computational complexity of transformers, which is quadratic with the number of elements involved. Several methods(Guo et al.,2021; Yu et al.,2022; Pang et al.,2022)directly apply transformers to all points globally, thus limiting their applicability to large-scale point clouds. Following the progress in scaling up vision transformers(Liu et al.,2021a; Dong et al.,2022), one effective strategy is to constrain point cloud transformers within non-overlapping local windows(Lai et al.,2022; Sun et al.,2022; Fan et al.,2022; Mao et al.,2021). However, unlike images, the number of points across different local windows varies significantly due to the sparsity of point clouds. To deal with this issue, sophisticated implementations like region batching(Fan et al.,2022; Sun et al.,2022)or customized GPU kernels(Lai et al.,2022)have to be adopted, which severely impedes massive parallelism on GPUs. Another strategy to speed up point cloud transformers is to apply transformers in downsampled feature maps(Park et al.,2022; Cheng et al.,2021b), which also weakens the network capability and incurs a decrease in performance. In this paper, we present a general and scalable octree-based transformer, abbreviated as OctFormer, for learning on 3D point clouds. The key building block of OctFormer is a novel octree attention mechanism for point clouds. To retain linear complexity, we divide each point cloud into small groups when applying attentions. Our key observation is that attentions are insensitive to the actual shape of underlying local windows. Instead of using cubic windows as in previous works, which incur variant point numbers in each window, we divide point clouds into groups with irregular windows while keeping the point number in each window the same. Consequently, we can easily implement our attention using standard operators provided by deep learning frameworks like PyTorch(Paszke et al.,2019). To generate the required window partition, our second observation is that after constructing an octree with the parallel algorithm in(Zhou et al.,2011), the octree nodes are sorted in z-order by shuffled keys(Wilhelms and Van Gelder,1992), which ensures that spatially-close octree nodes are contiguously stored in memory. We store features in tensors according to the order of octree nodes. After padding a few zeros to make the spatial numbers of tensors divisible by the specified point number in each window, we can efficiently generate the window partition by simply reshaping the tensors at almost zero cost. An example is shown in Figure1-(b), where the point number in each window is the same. To further increase the receptive fields of OctFormer, we introduce a dilated octree attention with dilated partitions along the spatial dimension of tensors, which can also be efficiently implemented with tensor reshaping and transposing. Our OctFormer challenges conventional wisdom in designing point cloud transformers from two aspects. First, instead of using fixed-sized local windows, we fix the point number in each window when doing point cloud partition, enabling simple implementation and easy parallelization; second, instead of regarding point clouds as unordered and unstructured point sets, we actually sort the quantized points with shuffled keys by building octrees, resulting in a convenient window partition. Our octree attention completely eliminates the expensive neighborhood searching used in previous designs(Wu et al.,2022; Lai et al.,2022), bypasses the sparsity of point clouds, and can be reduced to a standard multi-head self-attention(Vaswani et al.,2017)on small groups of equal size. Consequently, our octree attention can be implemented in10 lines of codewith open-sourced libraries freely available on the web. Onesingletransformer block on top of octree attention runs at least17 times fasterthan previous state-of-the-art point transformer blocks(Lai et al.,2022; Wu et al.,2022)when the number of elements involved is200k. We also introduce feature hierarchies following the multiscale structure of octrees, endowing OctFormer with the capability as a general backbone for 3D segmentation and detection. We verify the effectiveness of OctFormer on a series of 3D benchmarks. Specifically, our OctFormer achieves the best performance on the validation set of ScanNet segmentation(Dai et al.,2017), SUN RGB-D detection(Song et al.,2015), and ScanNet200 segmentation(Rozenberszki et al.,2022), surpassing all previous state-of-the-art sparse-voxel-based CNNs(Choy et al.,2019; Wang et al.,2017; Graham et al.,2018)and point cloud transformers(Lai et al.,2022; Wu et al.,2022)by a large margin. Notably, on ScanNet200 segmentation, which contains 200 semantic categories (ten times more than ScanNet), the mIoU of our OctFormer is higher than MinkowskiNet(Choy et al.,2019)by7.3and even higher than the recently-proposed LGround(Rozenberszki et al.,2022)by5.4, which pretrains a sparse-voxel-based CNN with a powerful CLIP model(Radford et al.,2021). In summary, our main contributions are as follows: [leftmargin=10pt,itemsep=2pt] We propose a novel octree attention and its dilated variant, which are easy to implement and significantly more efficient than previous point cloud attentions; We propose OctFormer, which can serve as a general backbone for 3D point cloud segmentation, detection, and classification; OctFormer achieves state-of-the-art performances on a series of 3D segmentation and detection benchmarks, and the computational efficiency of OctFormer is much higher than previous point cloud transformers and even surpasses highly optimized sparse-voxel-based CNNs.
Fluency score: 4
Fluency issues: [presence of technical jargon may hinder fluency for general audiences, but it’s fluent within the academic context]

Clarity score: 3
Clarity issues: [dense technical details and inline citations can obscure the main points for readers not familiar with the field]

Grammaticality score: 4
Grammaticality issues: [minor punctuation errors related to parentheses and inline citations]

Readability score: 3
Readability issues: [the text is dense with technical language and could benefit from better structuring and paragraphing]

Coherence score: 4
Coherence issues: [while the text follows a logical structure in presenting the problem and solution, the abundance of inline citations may disrupt the flow]
====================
	 Bayesian Self-Supervised Contrastive Learning
==========
Introduction:  Learning good representations without supervision has been a long-standing problem in machine learning(Aroraet al.,2019; Heet al.,2020; Chuet al.,2023). Many state-of-the-art models utilize self-supervised learning (SSL) techniques(Chenet al.,2020b; Grillet al.,2020; Liuet al.,2021; Tonget al.,2023)to design pretext tasks for (pre)training the models. For instance, generative methods train models to reconstruct input data(Kingma and Dhariwal,2018; Mikolovet al.,2013), while contrastive methods train models to encode differential features between positive and negative samples(Chopraet al.,2005; Hadsellet al.,2006; Gidariset al.,2018; Hjelmet al.,2018; Chenet al.,2020a; Grillet al.,2020; Heet al.,2020; Wang and Isola,2020; Radfordet al.,2021a). Self-supervised learning has been extensively researched for its advantages in learning representations without the need for human labelers to manually label the data(Heet al.,2020; Tianet al.,2020; Chenet al.,2020a; Radfordet al.,2021b; Wuet al.,2023; Luoet al.,2023). Contrastive learning is a primary implementation form of self-supervised learning, and remarkable successes have been observed for many applications in different domain(Alecet al.,2019; Misra and Maaten,2020; Heet al.,2020; Tianet al.,2020; Chenet al.,2020b; Liuet al.,2021). However, self-supervised learning derives “pseudo-labels” from co-occurring inputs to relate information(Liuet al.,2021). For negative samples, there exists a distinction in the semantic interpretation of “classes” in pre-training phase and the generalized semantic of “classes” used in downstream tasks. As illustrated in Fig.1, for the anchor point “dog”, the semantic of negative samples is all the unlabeled samples excluding the anchor data point itself in the pre-training phase. That is, each sample is treated as an individual class. However, in the downstream task, the semantic of negative sample are samples labeled as “not dogs”. Aligning the semantic representation of negative examples between the pretraining tasks and downstream tasks is crucial for improving performance in the latter. In the pretraining phase, there are two types of negative samples that disrupt the semantic structure of generalized “classes”. The first type consists of false negative (FN) samples, represented byx_{3}^{\prime}in Fig.1. Despite being labeled as “dog”, it were erroneously treated as negative belonging to a different class than the anchor data point. This necessitates the task of false negative debiasing, aiming to prevent the false negative samples from being erroneously pushed apart. The second type comprises hard negative (HN) samples, exemplified byx_{1}^{\prime}in Fig.1. These samples possess a “wolf” label and exhibit similarities to the anchor data point, despite belonging to a distinct class. It is essential to push these samples further apart, corresponding to the task of hard negative mining, otherwise the underlying semantic structure can be disrupted(Wang and Liu,2021; Chuanget al.,2020). In this work, we focus on the end-to-end self-supervised contrastive learning method with SimCLR as a representative approach. We address two critical tasks in self-supervised contrastive learning, namely debiasing false negatives and mining hard negatives, within a Bayesian framework. These tasks are effectively achieved through re-weighting the negative samples, thereby correcting the contrastive loss within the self-supervised setting and obtaining the modified Bayesian self-supervised contrastive loss (BCL), which offers a flexible and principled framework for self-supervised contrastive learning and presents a generalized perspective of contrastive loss. We analyzed the small-sample properties of BCL, and established the relationship between the importance weight applied to each unlabeled sample and the posterior probability of them being true negatives. Furthermore, we investigated the large-sample properties of BCL and demonstrated its consistency as an estimation of the contrastive loss under the supervised setting.
Fluency score: 4
Fluency issues: Use of domain-specific terms without explanation may hinder fluency for general audiences.

Clarity score: 3
Clarity issues: Heavy use of jargon and parenthetical citations can be overwhelming and obscure the main message.

Grammaticality score: 4
Grammaticality issues: Inconsistent use of articles ("the" missing before "pretraining tasks" and "downstream tasks") and pluralization ("different domain" should be "different domains").

Readability score: 3
Readability issues: Dense with technical language and complex sentence structures, reducing ease of reading.

Coherence score: 3
Coherence issues: Overuse of citations interrupts the flow, and the piece assumes a level of prior knowledge, which may confuse readers unfamiliar with the subject.
====================
	 Neural Symbolic Regression using Control Variables
==========
Introduction:  Symbolic regression (SR) aims to uncover the underlying mathematical expressions from observed data[16,9]. It has been widely used for scientific discovery across various disciplines[1,29]owing to its ability to learn analytical expressions between the input and output. The implementation of SR involves two steps[15]. The first step is to predict the skeleton of mathematical expressions based on a pre-defined list of basic operations (+,-,\times,\div) and functions (\sin,\cos,\exp,\log). For instance, we can identify the skeleton of a symbolic equation asf(x)=\log{ax}+\sin(bx)+c. Next, we adopt optimization methods, such as Broyden–Fletcher–Goldfarb–Shanno (BFGS), to estimate the parametersa,b,cin the skeleton. The key challenges of SR lie in: 1) how to improve the accuracy and scalability for multiple input variables, and 2) how to speed up the discovery process. In the past few decades, a plethora of SR methods[22]have been developed to discover underlying mathematical equations from data in science and engineering domains. One popular approach among them is genetic programming (GP)[4,7,27,10,2], which uses evolutionary operations, such as mutation, crossover, and selection, to estimate the symbolic expressions in a tree structure. However, GP would suffer from instability and its inference time is expensive in the context of multiple input variables[15]. Another method, SINDy[5], adopts sparse linear regression to discover the governing equations of dynamical systems. However, SINDy’s performance relies heavily on prior knowledge of a known set of candidate functions, and it is difficult to uncover complex equations from data solely through linear regression. To overcome these limitations, some studies explore deep neural networks-based techniques, such as Deep Symbolic Regression (DSR)[25]and Transformer-based pre-training, for symbolic learning. Although these approaches obtain good prediction accuracy, they do not scale well to mathematical equations with multiple variables. Recently, researchers develop Symbolic Physics Learner (SPL), a physics-informed Monte Carlo Tree Search (MCTS) algorithm for symbolic regression. While SPL outperforms most GP-based methods, it still struggles with multiple variables in mathematical expressions. In summary, existing methods suffer from scalability issues when dealing with complex multi-variable equations as they require a much larger search space to identify the combination of different variables. Thus, the question is, how can we reduce the search space of symbolic regression for complex equations involving multiple variables? In this paper, we propose a novel neural symbolic regression with control variables (SRCV) that combines neural networks and symbolic regression to discover analytical expressions from data, as illustrated in Fig.1. Inspired by divide and conquer[26], SRCV addresses the multi-variable symbolic regression by decomposing it into a set of single-variable SR problems and then combines the estimated symbolic equation for each variable in a bottom-up manner. The proposed method is performed in four steps as follows. 1) We learn a data generator from observed data using DNNs, allowing for generating data for a specific variable. 2) Generate data via control variables. Specifically, we generate data samples for the current independent variable by manipulating the previously learned variables and other control variables. For example, for estimating the symbolic expression of variablex_{i}, we can generate data samples by varyingx_{i}while fixing the other variables. 3) Single-variable symbolic regression is employed to estimate the mathematical expression of the current variable based on the generated data in step 2. Here any symbolic regression models can be inserted into the framework. 4) We gradually add the remaining variables one by one to step 2 and proceed with step 3 until all the variables are covered. Extensive experimental results on multiple SR benchmarks demonstrate the superiority of our SRCV over the state-of-the-art methods in discovering complex multi-variable equations. Moreover, the proposed approach is able to discover complex expressions in a reduced search space. Our main contributions are three-fold: 1) we propose SRCV, a simple and effective neural symbolic regression method using control variables; 2) we illustrate that the proposed method exhibits a significant reduction in search space for complex symbolic equations; 3) the evaluation results demonstrate that our method can significantly outperform the baselines in terms of accuracy and inference time.
Fluency score: 4
Fluency issues: "[16,9]", "[1,29]", "asf(x)", "parametersa,b,c", "[22]", "[4,7,27,10,2]", "SINDy[5]", "DSR[25]"

Clarity score: 3
Clarity issues: Frequent inline citations disrupt flow, abbreviations without explanation (e.g., "GP", "SINDy", "DNNs").

Grammaticality score: 4
Grammaticality issues: "parametersa,b,c", missing spaces in citations, possibly improper uses of slashes in "+,-,\times,\div"

Readability score: 3
Readability issues: Frequent technical jargon, dense information without adequate lay explanation, inline citations.

Coherence score: 4
Coherence issues: Inline citations and technical terms are distracting and assuming prior knowledge, disrupting coherence.
====================
	 Self-Interpretable Time Series Prediction with Counterfactual Explanations
==========
Introduction:  Deep learning (DL) has become increasingly prevalent, and there is naturally a growing need for understanding DL predictions in many decision-making area, such as healthcare diagnosis and public policy-making. The high-stake nature of these areas means that these DL predictions are considered trustworthy only when they can be well explained. Meanwhile, time-series data has been frequently used in these areas(Zhaoet al.,2021; Jinet al.,2022; Yanget al.,2022), but it is always challenging to explain a time-series prediction due to the nature of temporal dependency and varying patterns over time. Moreover, time-series data often comes with confounding variables that affect both the input and output, making it even harder to explain predictions from DL models. On the other hand, many existing explanation methods are based on assigning importance scores for different parts of the input to explain model predictions(Ribeiroet al.,2016; Lundberg and Lee,2017b; Chenet al.,2018; Wanget al.,2019b; Weinbergeret al.,2020; Plumbet al.,2020). However, understanding the contribution of different input parts are usually not sufficiently informative for decision making: people often want to know what changes made to the input could have lead to a specific (desirable) prediction(Wachteret al.,2017a; Goyalet al.,2019; Nemirovskyet al.,2022). We call such changed input that could have shifted the prediction to a specific targetactionable counterfactual explanations. Below we provide an example in the context of time series. Suppose there is a model that takes as input a time series of breathing signal{\bf x}\in{\mathbb{R}}^{T}from a subject of ageu=60to predict the corresponding sleep stage asy^{pred}=\mbox{`Awake'}\in\{\mbox{`Awake'},\mbox{`Light Sleep'},\mbox{`Deep % Sleep'}\}. Typical methods assign importance scores to each entry of{\bf x}to explain the prediction. However, they do not provideactionablecounterfactual explanations on how to modify{\bf x}to{\bf x}^{cf}such that the prediction can change toy^{cf}=\mbox{`Deep Sleep'}. An ideal method with such capability could provide more information on why the model make specific predictions. Actionable counterfactual explanations help people understand how to achieve a counterfactual (target) output by modifying the current model input. However, such explanations may not be sufficiently informative in practice, especially under the causal effect of confounding variables which are often immutable. Specifically, some variables can hardly be changed once its value has been determined, and suggesting changing such variables are both meaningless and infeasible (e.g., a patient age and gender when modeling medical time series). This leads to a stronger requirement: a good explanation should make as few changes as possible on immutable variables; we call such explanationsfeasible counterfactual explanations. Below we provide an example in the context of time series. In Example1, ageuis a confounder that affects both{\bf x}andysince elderly people (i.e., largeru) are more likely to have irregular breathing{\bf x}and more ‘Awake’ time (i.e.,y=\mbox{`Awake'}) at night. To generate a counterfactual explanation to changey^{pred}to‘Deep Sleep’, typical methods tend to suggest decreasing the ageufrom60to50, which isinfeasible(since age cannot be changed in practice). An ideal method would first infer the ageuand search for afeasiblecounterfactual explanation{\bf x}^{cf}that could changey^{pred}to‘Deep Sleep’while keepinguunchanged. In this paper, we propose a self-interpretable time series prediction model, dubbed Counterfactual Time Series (CounTS), which can both (1) perform time series predictions and (2) provide actionable and feasible counterfactual explanations for its predictions. Under common causal structure assumptions, our method is guaranteed to identify the causal effect between the input and output in the presence of exogenous (confounding) variables, thereby improving the generated counterfactual explanations’ feasibility. Our contribution is summarized as follows: We identify the actionability and feasibility requirements for generating counterfactual explanations for time series models and develop the first general self-interpretable method, dubbed CounTS, that satisfies such requirements. We provide theoretical guarantees that CounTS can identify the causal effect between the time series input and output in the presence of exogenous (confounding) variables, thereby improving feasibility in the generated explanations. Experiments on both synthetic and real-world datasets show that compared to state-of-the-art methods, CounTS significantly improves performance for generating counterfactual explanations while still maintaining comparable prediction accuracy.
Fluency score: 4
Fluency issues: complex and long sentences; parentheses with citations disrupt flow.

Clarity score: 3
Clarity issues: excessive use of jargon and complex sentence structures; multiple examples and concepts introduced without clear transitions.

Grammaticality score: 3
Grammaticality issues: "area" should be plural "areas"; missing space in citations (e.g., "Zhaoet al."), improper formatting for mathematical notation.

Readability score: 3
Readability issues: Text is dense with technical terms and lacks clear structure, which may confuse non-specialist readers.

Coherence score: 3
Coherence issues: Ideas are introduced rapidly without sufficient explanation, and the text jumps between examples and explanations without clear linkages.
====================
	 Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs
==========
Introduction:  Recently, large language models (LLMs) like ChatGPT have drawn numerous attention from researchers and practitioners due to theirgeneralistcapabilities(Qinet al.,2023). For instance, sufficiently large language models could perform well for different tasks in zero-shot manner, such as text summarization(Yanget al.,2023; Zhanget al.,2023), machine translation(Moslemet al.,2023), and question answering(Singhalet al.,2023). However, in some scenarios, LLMs lack domain-specific knowledge or are not able to recall facts and knowledge correctly, which causes hallucination(Banget al.,2023). Hallucination refers to models generating text that is nonsensical, or unfaithful to the provided source input(Jiet al.,2023; Koehn and Knowles,2017; Raunaket al.,2021; Rohrbachet al.,2018; Vinyals and Le,2015; Maynezet al.,2020). Retrieving relevant texts from knowledge bases is a classic way to augment language models’ performance like generation quality(Borgeaudet al.,2022; Lewiset al.,2020a; Levineet al.,2022; Guuet al.,2020). Besides, it can also help improve the factuality of generated texts. Typically, retrieval modules are employed to find the most relevant documents with the highest similarity scores to the query. Then input texts and retrieved documents would be combined in a specific way fed into models. Motivated by this, some methods(Ramet al.,2023; Penget al.,2023a)utilize retrieved texts to augment LLMs.Ramet al.(2023)directly prepends retrieved documents to the input to obtain a performance gain for LLMs.(Penget al.,2023a)designs an LLM-Augmenter to retrieve and merge evidence from external knowledge for alleviating hallucination. However, relying on similarity between embeddings would only make model learn shallow features instead of understanding semantics, which in turn hinder the model from searching truly useful knowledge. On the contrary, Knowledge Graphs (KGs) are clear, logical, and superior mediums of knowledge. Thus, effectively leveraging KGs for LLMs should benefit LLMs’ performance on knowledge-required tasks. For this reason, there is a line of work(Yasunagaet al.,2021; Linet al.,2019; Fenget al.,2020)using KGs to help LLMs make predictions. KagNet(Linet al.,2019)proposes a graph neural network module to model relational graphs for relational reasoning under the context of both knowledge symbolic space and language semantic space. MHGRN(Fenget al.,2020)equips pretrained language models with a multi-hop relational reasoning module, which unifies path-based reasoning methods and graph neural networks. QA-GNN(Yasunagaet al.,2021)learn representations over joint graphs formed by connecting QA context and KG. However, they(Yasunagaet al.,2021; Linet al.,2019; Fenget al.,2020)all require training additional knowledge-aware modules like graph neural networks (GNNs) on retrieved knowledge. There are two shortcomings of training additional modules: 1) would suffer from pains of retraining when encountering novel domains; 2) would become a bottleneck since LLMs’ strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to solve these shortcomings, which teaches LLMs themselves to search for knowledge from external knowledge bases. To be specific, we simplify the process of searching for necessary knowledge from KGs into a multi-hop decision sequence. At each step, we transform local information within KGs into text prompts (including the historical path selected by LLMs), based on which LLMs select relevant knowledge in the context to perform tasks, as shown in Figure1. The whole process is similar to humans searching over the Internet for achieving some goals. Furthermore, based on the complete paths chosen by LLMs, we can explain the whole decision-making process of LLMs. It allows for analysis when bad cases arise, a capability not present in previous black-box retrieval methods. We evaluate our approach, Knowledge Solver (KSL), with three LLMs (GPT-3.5, LLaMA(Touvronet al.,2023a), and LLaMA 2(Touvronet al.,2023b)) on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, where reasoning with knowledge is required. KSL improves two LLM baselines’ performance across these three datasets in zero-shot and finetuning settings. Our main contributions are summarized as follows: We propose Knowledge Solver (KSL), which is the first paradigm employing LLMs to search for relevant knowledge on KGs by themselves. Our proposed paradigm Knowledge Solver can boost LLMs’ performance on knowledge-required tasks by a relatively large margin in zero-shot manner without additional modules and training. Knowledge Solver can provide explainability for LLMs’ whole reasoning processes. When the computational burden is affordable, finetuning LLMs on our specially constructed dataset, with the help of KGs, can benefit LLMs further.
Fluency score: 4
Fluency issues: [The flow is sometimes disrupted by the technical content and parentheses.]

Clarity score: 3
Clarity issues: [The text is dense with jargon and assumes reader familiarity with the topic.]

Grammaticality score: 4
Grammaticality issues: [Minor grammatical issues, such as punctuation errors related to the citation style.]

Readability score: 3
Readability issues: [The text is complex, with some long sentences and several technical terms.]

Coherence score: 4
Coherence issues: [The many references and technical terms can make it difficult to follow the narrative thread without prior knowledge.]
====================
	 Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing
==========
Introduction:  Sequence-to-Sequence (S2S) models pretrained for language modeling (PLM) and denoising objectives have been successful on a wide range of NLP tasks where both inputs and outputs are sequencesRadfordet al.(2019); Raffelet al.(2020); Lewiset al.(2020); Brownet al.(2020). However, for non-sequential outputs like trees and graphs, a procedure called linearization is often required to flatten them into ordinary sequencesLiet al.(2018); F-G and G-R (2020); Yanet al.(2021); Bevilacquaet al.(2021); He and Choi (2021a), where labels in non-sequential structures are mapped heuristically as individual tokens in sequences, and numerical properties like indices are either predicted using an external decoder such as Pointer NetworksVinyalset al.(2015a)or cast to additional tokens in the vocabulary. While these methods are found to be effective, we hypothesize that S2S models can learn complex structures without adapting such patches. To challenge the limit of S2S modeling, BARTLewiset al.(2020)is finetuned on four tasks without extra decoders: part-of-speech tagging (POS), named entity recognition (NER), constituency parsing (CON), and dependency parsing (DEP). Three novel linearization schemas are introduced for each task: label sequence (LS), label with text (LT), and prompt (PT).LStoPTfeature an increasing number of lexicons and a decreasing number of labels,which are not in the vocabulary (Section3). Every schema is equipped with a constrained decoding algorithm searching over valid sequences (Section4). Our experiments on three popular datasets depict that S2S models can learn these linguistic structures without external resources such as index tokens or Pointer Networks. Our best models perform on par with or better than the other state-of-the-art models for all four tasks (Section5). Finally, a detailed analysis is provided to compare the distinctive natures of our proposed schemas (Section6).111All our resources including source codes are publicly available:https://github.com/emorynlp/seq2seq-corenlp
Fluency score: 4
Fluency issues: Missing spaces after "sequencesRadfordet", "sequencesLiet", "vocabulary. While", "BARTLewiset", "PT).LStoPTfeature", and "analysis (Section6).111All"

Clarity score: 3
Clarity issues: Dense use of jargon and acronyms without explanations, making the text not readily accessible to those not familiar with the field.

Grammaticality score: 4
Grammaticality issues: Missing spaces as mentioned under fluency.

Readability score: 3
Readability issues: The text is dense with technical terms and lacks introductory explanations which impair its accessibility. The missing spaces mentioned earlier also disrupt reading flow.

Coherence score: 4
Coherence issues: The text reads like a summary of work without clear transitions; references to sections feel abrupt.
====================
	 Adaptive whitening with fast gain modulation and slow synaptic plasticity
==========
Introduction:  Individual neurons in early sensory areas rapidly adapt to changing sensory statistics by normalizing the variance of their responses[11;21;43]. At the population level, neurons also adapt by reducing correlations between their responses[42;10]. These adjustments enable the neurons to maximize the information that they transmit by utilizing their entire dynamic range and reducing redundancies in their representations[4;8;33;7]. A natural normative interpretation of these transformations isadaptive whitening, a context-dependent linear transformation of the sensory inputs yielding responses that have unit variance and are uncorrelated. Decorrelation of the neural responses requires coordination between neurons and the neural mechanisms underlying such coordination are not known. Since neurons communicate via synaptic connections, it is perhaps unsurprising that most existing mechanistic models of adaptive whitening decorrelate neural responses by modifying the strength of these connections[60;30;45;46;14;37;59]. However, long-term synaptic plasticity is generally associated with long-term learning and memory[38], and thus may not be a suitable biological substrate for adaptive whitening (though short-term synaptic plasticity has been reported[63]). On the other hand, there is extensive neuroscience literature on rapid and reversible gain modulation[1;48;50;13;51;47;22;61]. Motivated by this,Duonget al.[18]proposed a mechanistic model of adaptive whitening in a neural circuit withfixedsynaptic connections that adapts exclusively by modifying the gains of interneurons that mediate communication between the primary neurons. They demonstrate that an appropriate choice of the fixed synaptic weights can both accelerate adaptation and significantly reduce the number of interneurons that the circuit requires. However, it remains unclear how the circuitlearnssuch an optimal synaptic configuration, which would seem to require synaptic plasticity. In this study, we combine the learning and adaptation of synapses and gains, respectively, in a unified mechanistic neural circuit model that adaptively whitens its inputs over multiple timescales (Fig.1). Our main contributions are as follows: We introduce a novel adaptive whitening objective in which the (inverse) whitening matrix is factorized into a synaptic weight matrix that is optimized across contexts and a diagonal (gain) matrix that is optimized within each statistical context. With this objective, we derive a multi-timescale online algorithm for adaptive whitening that can be implemented in a neural circuit comprised of primary neurons and an auxiliary population of interneurons with slow synaptic plasticity and fast gain modulation (Fig.1). We test our algorithm on synthetic and natural datasets, and demonstrate that the synapses learn optimal configurations over long timescales that enable the circuit to adaptively whiten its responses on short timescales exclusively using gain modulation. Beyond the biological setting, multi-timescale learning and adaptation may also prove important in machine learning systems. For example,Mohanet al.[41]introduced “gain-tuning”, in which the gains of channels in a deep denoising neural network (with pre-trained synaptic weights) are adjusted to improve performance on samples with out-of-distribution noise corruption or signal properties. The normative multi-timescale framework developed here offers a new approach to continual learning and test-time adaptation problems such as this.
Fluency score: 4
Fluency issues: Frequent breaks with citations, somewhat complex sentence structures.

Clarity score: 3
Clarity issues: Dense with jargon, references may interrupt flow, some sentences are lengthy and complex.

Grammaticality score: 5
Grammaticality issues: -

Readability score: 3
Readability issues: High density of technical terminology and parenthetical citations may challenge lay readers.

Coherence score: 4
Coherence issues: Jumps between topics (e.g. from synaptic plasticity to gain modulation) can be jarring without more transitional phrases.
====================
	 NMS Threshold matters for Ego4D Moment Queries -- 2nd place solution to the Ego4D Moment Queries Challenge 2023
==========
Introduction:  The Ego4D Moment Queries (MQ) task aims to localize all moments of actions in time and recognize their categories within an untrimmed egocentric video. We adopt a two-stage approach for this task, where clip-level features are first extracted from raw video frames using a pre-trained feature network, followed by a temporal localization model that predicts the onset and offset of action instances as well as their categories. Our submission last year explored the combination of a latest localization model (ActionFormer[12]) and a strong set of video features[9]. This work seeks to improve the localization model . A limitation of ActionFormer[12]lies in its label assignment at training time; annotated action instances are assigned to candidate moments based oncenter sampling, a heuristic that designates positive labels to moments proximal to the center of an action instance. Recent literature in object detection, however, shows that such static assignment strategy is insufficient for complex spatial configuration of objects. Inspired by this insight, we propose to adapt SimOTA[5], a dynamic label assignment strategy, for temporal action localization. SimOTA assigns ground-truth action instances to the candidate moments on the fly by solving an optimal transport problem. Further, we refine SoftNMS to account for densely overlapping actions in Ego4D. Equipped with these modifications, our solution extends our prior work and is ranked 2ndon the public leaderboard. Specifically, our solution attains 26.62% average mAP and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge. We hope our work will shed light on future development in temporal action localization and egocentric vision.
Fluency score: 4
Fluency issues: [Slight awkwardness in "A limitation of ActionFormer[12]lies" - space needed between "ActionFormer[12]" and "lies"]

Clarity score: 4
Clarity issues: [Use of domain-specific jargon and acronyms may obscure meaning for non-experts]

Grammaticality score: 4
Grammaticality issues: [Missing space in "ActionFormer[12]lies"]

Readability score: 3
Readability issues: [Dense technical language makes it less accessible to a general audience]

Coherence score: 4
Coherence issues: [Some jumps in logic and use of technical terms could be bridged better for smoother flow]
====================
	 Textbooks Are All You Need II: phi-1.5 technical report
==========
Introduction:  Over the past few years, Large Language Models (LLMs) have transformed the field of Natural Language Processing. More broadly, they hold the promise of a paradigm shift for human-computer interaction. These advancements have far-reaching economic implications, as well as the potential to redefine our conceptual frameworks of artificial intelligence and perhaps even cognition itself. Moreover, the latest generation of models such as GPT-4[Ope23]have demonstrated remarkable improvements over their predecessors, offering capabilities previously thought to be unattainable in the short term; see for example[BCE{}^{+}23]for an in-depth comparison between GPT-4 and its predecessor GPT-3.5. The improvement from one generation of LLMs to the next seems at the moment to primarily stem fromscale, with the most powerful models nearing trillions of parameters and trillion of tokens for training data (for example, PaLM[CND{}^{+}22]has 540 billion parameters and was trained on 780 billion tokens). A natural question arises: Is this large scale indispensable for achieving high levels of capability? Far from being merely an academic question, answering this holds implications across several dimensions. Economically, the cost of training, deploying, and maintaining such large models can be substantial. Scientifically, understanding whether similar capabilities can be achieved at a smaller scale could provide insights into the architectures and development of intelligent systems. From a responsible AI standpoint, the energy consumption of large-scale models is becoming an increasing concern, as is the question of how controllable or governable these large models can be. Finally, the ability to train compact models with cutting-edge capabilities would democratize advanced AI, enabling a broader range of individuals and organizations to study and deploy them, instead of being an exclusive domain of a few with vast computational resources. In this work we continue the investigation into the fundamental question of “how small can a LLM be to achieve certain capabilities”. The prior work[EL23]considered this question for the task of “speaking fluent English”, while the subsequent work[GZA{}^{+}23]considered the more challenging task of coding simple functions in Python. Here we focus on the more elusive concept ofcommon sense reasoning, a notoriously challenging task for AI[SBBC21]. Our results are summarized in Figure1. In a nutshell we buildphi-1.5, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves common sense reasoning benchmark results comparable to models ten times its size that were trained on datasets more than ten times larger. Moreover, our dataset consists almost exclusively of synthetically generated data (closely following the approach from[GZA{}^{+}23], see next section for more details), which has important implications for the potential to control for the notoriously challenging issue of toxic and biased content generation with LLMs[BGMMS21]. Additionally, we discuss the performance of a relatedfiltered web dataenhanced version ofphi-1.5, which we callphi-1.5-web​. We open-source our rawphi-1.5model (without instruction fine-tuning or any other stage of alignment) to empower the research community in its work on some of the most urgent questions around LLMs: in-context learning, mechanistic interpretability, and mitigation strategies for hallucinations, toxic content generation, and biased outputs. Indeed,phi-1.5is the first LLM at the one billion parameters scale to exhibit most of the relevant traits of larger LLMs for research on these topics. We hope thatphi-1.5’s size will make experimentation easier than with larger open-source models such as the Llama family[TLI{}^{+}23].
Fluency score: 3
Fluency issues: [Bibliographic references formatted inappropriately for normal prose, use of phrases like "nearing trillions of parameters and trillion of tokens" could be more concise]

Clarity score: 3
Clarity issues: [Unclear bibliographic references, jargon usage without explanation, inconsistent use of the symbol "phi" as "phi-1.5" and "phi-1.5-web​"]

Grammaticality score: 4
Grammaticality issues: [Minor punctuation issues in the citation style]

Readability score: 3
Readability issues: [Dense with jargon, citations disrupt flow, phrases like "nearing trillions of parameters and trillion of tokens" could be clearer]

Coherence score: 3
Coherence issues: [Jumps between topics without transitions, inconsistent terminology, unclear references]
====================
	 A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition
==========
Introduction:  In the last decade, we have witnessed the spread and adoption of sensors, wearables, the Internet of Things (IoT), the Internet of Medical Things (IoMT), and edge computing technologies(Li et al.,2015). Sensors can detect and measure physical properties such as temperature, pressure, light, and motion. They are becoming ubiquitous in various industries, including automotive, aerospace, and consumer electronics. Moreover, their miniaturization has led to their integration into wearables, such as fitness trackers, smartwatches, clothes, and dedicated devices. Wearables are frequently used to track various aspects of a person’s health and activity. Recent developments even involve integrating medical sensors for remote patient monitoring, digital therapeutics, and real-time intervention into wearables(Seneviratne et al.,2017; Dunn et al.,2018; Cheng et al.,2021). On the other side, the IoT is formed by networks of interconnected devices, vehicles, and buildings that communicate with each other and exchange data. It has been adopted across various industries, including home automation, agriculture, and manufacturing. In addition, IoT devices can be remotely monitored and controlled, improving efficiency and productivity(Li et al.,2015). Instead, IoMT refers to using IoT devices in medical applications, enabling the healthcare providers’ capacity to monitor patients remotely, collect data for analysis, improve patient outcomes, and reduce healthcare costs(Vishnu et al.,2020). Finally, edge computing refers to processing data at or near the source rather than sending it to a central/remote server for processing. This technology has become increasingly important as the amount of data generated by IoT and IoMT devices grows. Edge computing enables faster processing times and reduces the latency and amount of data that needs to be transmitted over the network(Baker and Xiang,2023). The adoption and spread of these technologies have revolutionized various industries and enabled new applications and capabilities. With such systems now being ubiquitous, they serve as a common infrastructure for recognizing human activity, as described next. Human Activity Recognition (HAR):In such a context, HAR is a central research field that finds applications in various areas, including healthcare, sports, industry, and smart homes. HAR refers to the ability to identify and classify human activities using sensors, wearables, or other devices that capture data about the person’s movements and actions. With regard to healthcare, HAR can be used to monitor a patients’ status and detect abnormalities or changes in their behavior that may indicate a deterioration of health or the onset of a medical condition. For example, HAR can be used to detect falls of elderly patients or to monitor the movements of patients with Parkinson’s disease or other motor disorders(Demrozi et al.,2020). Moreover, HAR also has applications in sports and fitness to monitor the athletes’ performance and technique, helping them to improve their training and prevent injuries. HAR can also be used in activity tracking devices, such as fitness trackers, to provide users with insights into their daily activity levels and help them to achieve their fitness goals. In addition, HAR automates various tasks in smart homes based on the occupant’s activities. For example, lights can be turned on or off automatically based on the person’s movements, or the thermostat can be adjusted based on the person’s activity level(Demrozi et al.,2020,2021). HAR is related to various technologies, including sensors, wearables, IoT, IoMT, edge computing, machine learning (ML), Deep Learning (DL), and Artificial Intelligence (AI). Sensors and wearables are used to capture data about the person’s movements and actions, which is then used to identify and classify human activities in HAR applications. IoT and IoMT systems are used to collect data from sensors and wearables, which can be transmitted over the network for processing and analysis. Edge computing can process this data at or near the source, reducing latency and enabling real-time processing of HAR data(Baker and Xiang,2023; Vishnu et al.,2020). In HAR systems, the data collected from such devices is analyzed to classify a user’s activity. While, in principle, this analysis can be done based on heuristics (e.g., a feature exceeds certain thresholds, etc.), ML- and DL-based HAR techniques have become the most popular solution. Using them, also more complex analyses can be carried out, allowing for reliable recognition of activities even in data in which the properties or patterns that represent a certain activity or behavior are not obvious. ML- and Dl-based HAR methods can also integrate other data sources, such as environmental data, to provide more comprehensive insights into human behavior and activity(Demrozi et al.,2020). As the technology continues to improve and becomes widely available, we expect to see further advancements and new applications for ML-based HAR(Baker and Xiang,2023). When generating HAR model, a set of sensor data is recorded first. This data is then labeled with the activities under consideration. This step is calledannotation. Next, a machine-learning model is trained, which can then be used to classify unlabeled data. In the following, we describe the individual steps(Demrozi et al.,2020; Gupta et al.,2022)that are involved in creating a HAR system in more detail. An overview is shown in Figure1. Definition of Target Activities: Definition and analyzation of the real-world characteristics of the target activities to be recognized. For example, this can be their duration, distribution, similarity with other activities, etc. Device Setup: Identification and study of requirements and determination of the devices to be used in the data collection phase, based on the target human activities. Data Collection: In this phase, data is collected from sensors, wearables, or other devices that capture information about the person’s movements and actions. Data Annotation: The process of assigning labels to the human activities being performed. Labels are crucial in supervised learning as they provide the ground truth or correct answers that guide the learning process. By associating input data with corresponding labels, the model can learn to make accurate predictions and generalize its knowledge to unseen examples. Data Preprocessing: The collected data is then preprocessed to remove noise, irrelevant information are filtered out, and the data is prepared for analysis. As a part of this, the following analysis is carried out: Feature extraction: The preprocessed data is analyzed to extract relevant features that can be used to classify human activities. These features may include movement patterns, body position, or other characteristics. Feature selection: Once the features have been extracted, a subset of features may be selected for use in the classification model. This helps to reduce the dimensionality (e.g., the number of features) of the data and improve the accuracy of the model. Model generation and testing: A HAR (i.e., ML or DL) model is developed to classify human activities based on the selected features in this phase. The model may be trained using a labeled dataset or unsupervised learning techniques. After the model has been generated, the following steps are carried out before the model is ready to be used: Model evaluation: The developed model is then evaluated using a test dataset to assess its accuracy and performance. This phase helps to identify any issues or areas for improvement in the model. Deployment: Finally, the developed model is deployed to a real-world environment, where it is used to classify human activities. Data Annotation in HAR:The most labor-intensive step in creating a HAR system is data annotation, which involves creating a labeled dataset for training the ML/DL models. Manual labeling, in which human annotators manually label each recorded sample with the corresponding activity, is a common approach in data annotation. Although time-consuming and resource-intensive, it can produce high-quality labels that are accurate and consistent. Nevertheless, several factors can pose challenges in the manual data annotation process for HAR systems.Firstly, subjectivity can lead to inconsistencies and errors in labeling as the interpretation of the activity being performed can vary among annotators. This can ultimately affect the accuracy of the ML/DL model.Secondly, the data annotation process can be time-consuming, particularly when labeling large amounts of data, which can cause delays in the development of the HAR system and increase project costs.Thirdly, the economic cost can be a limiting factor since hiring human annotators or utilizing crowdsourcing platforms for data labeling can become expensive, mainly when the studied activities are complex.Fourthly, the variability of human activities can also pose a challenge in the annotation process. Since different individuals can perform activities differently, creating accurate and consistent labels for the data can be challenging.Lastly, label noise may exist in annotated data, resulting in errors in the labeling process. Label noise can occur due to human error, subjectivity, or inconsistencies in the annotation process, which ultimately reduces the performance of the HAR system’s ML/DL model. Careful consideration of these limitations and appropriate methods can help mitigate these challenges and improve the accuracy and performance of the final HAR system. Alternatively, automated methods, such as rule-based systems or unsupervised learning algorithms, can be employed for data annotation. These approaches are more efficient and scalable but may be less precise or necessitate additional manual validation. The quality of the annotated data is pivotal to the efficacy of the HAR system. Inaccurate or inconsistent labeling can cause poor ML/DL model performance, leading to the misclassification of human activities(Diete et al.,2017; Adaimi and Thomaz,2019). There are several (partial) possible solutions to the limitations of the annotation process in HAR(Diete et al.,2017; Adaimi and Thomaz,2019). Some of these solutions include: Standardization: Standardizing the annotation process can help to reduce subjectivity and increase consistency in the labeling process. This can be achieved by defining clear guidelines and procedures for annotators to follow and providing training and feedback to ensure the quality of the annotations. Automation: Automated methods, such as unsupervised learning algorithms or rule-based systems, can be used to annotate data. These methods can be faster and more scalable than manual labeling and reduce the annotation process’s cost. Active learning: Active learning techniques can reduce the labeled data needed for training an ML or DL model. This involves selecting the most informative data samples for annotation, which can help reduce the labeling process’s time and cost. Crowdsourcing: Crowdsourcing platforms can be used to engage many annotators to label the data. This can be a cost-effective solution, as well as provide a diverse range of perspectives on the activity being performed. Quality control: Quality control measures can be implemented to ensure the accuracy and consistency of the labeled data. This can include using multiple annotators to label the same data samples and comparing their annotations, as well as conducting regular checks on the quality of the annotations. While these solutions can enhance the accuracy and performance of the final HAR system, they do not completely eliminate the cost and time needed for the annotation process. Systematic Review Objectives:This paper aims to systematically review existing methodologies for automating data annotation in HAR. The objective is to identify the strengths and limitations of different techniques and provide insights into the current research and ongoing trends in this area. Specifically, the paper explores different approaches and algorithms used in automatic data annotation techniques. This does not only help in developing novel techniques in the future, but also supports the choice of an appropriate labeling technique for a given application. This review considers 2401 publications on automating data annotation in HAR. To the best of our knowledge, no systematic review has been published prior to this paper. The absence of such a review aggravates overseeing the different technologies used in this area, makes it difficult to follow recent trends, and leaves unclear which technical solution is most beneficial for realizing a given scenario. We in this paper close this gap by providing the first systematic review on this field of research. Paper organization:The rest of the paper is organized as follows. Section2delves into the background of HAR, presenting a comprehensive overview of the field, including its applications and challenges. Following that, Section3discusses the selection criteria for annotation methods in HAR, examining the key factors that we consider when choosing appropriate techniques. Section4presents an in-depth analysis and discussion of various annotation methods employed in HAR, exploring their strengths, limitations, and effectiveness in accurately identifying and classifying human activities Finally, Sections5and6conclude the paper by summarizing the key findings and contributions of the study, emphasizing the significance of automatic annotation methods in advancing HAR research and suggesting potential avenues for future exploration in this area.
Fluency score: 4
Fluency issues: [Occasional awkward phrasing such as "Instead, IoMT refers to using IoT devices in medical applications," which could be more fluently expressed; the use of "While, in principle," could flow better]

Clarity score: 4
Clarity issues: [Some lengthy sentences with complex structures that could be broken down for clarity; multiple parenthetical citations close together can distract]

Grammaticality score: 4
Grammaticality issues: [Minor punctuation inconsistencies like the missing space after "(IoT)," and in "(HAR):In"; inconsistent punctuation in citations like "(Demrozi et al.,2020,2021)" should have a space after the comma]

Readability score: 3
Readability issues: [Long, complex sentences; dense technical information; frequent use of abbreviations and parenthetical citations reduces ease of reading]

Coherence score: 4
Coherence issues: [Generally coherent, but the flow is sometimes interrupted by technical details and could benefit from better transitioning between sections]
====================
	 Elementwise Language Representation
==========
Introduction:  We understand texts from various levels of semantics but current language representation strategies leverage tokenization which relies on a certain level of semantics exclusively, fully ignoring the hierarchical structures of natural languages. Text is encoded to a sequence of integers then projected into fixed-size latent embeddings. These types of expressions result in a recursive trade-off between different levels of language representations: (sub)word-level models indirectly recover characters(Itzhak and Levy,2021)but it is not always sufficient for spelling-sensitive tasks, character-level models need much longer sequences to reach comparable performance to word-level models thus amplifying the computational complexity of self-attention. Some recently proposed studies(Clarket al.,2022; Godeyet al.,2022; Tayet al.,2021)attempt to solve this by downsampling long character sequences into an acceptable length, however, they share the same limitation as pure character-level models because their valid downsampling rates are constrained to relatively small values mainly due to the smoothing and overhead issues. Instead, we propose elementwise embedding, a language representation technique for addressing this trade-off in which a set of lower-dimensional character embeddings calledelementsare horizontally concatenated into a single latent embedding calledmaterialthat mimics a semantic unit such as a word, phrase, sentence and etc. Using this method, models with higher-dimensional hidden representations create each semantic unit (i.e., a material) by concatenating a greater numbers of characters (i.e., elements), which implies that larger models can process longer sequences than smaller ones at the same computational complexity. This means that the acceptable sequence length scales with the size of a transformer model, but the complexity is fixed as that of its attention. Assuming that a character-level GPT-3 [processing 2048 12,288-dimensional token embeddings with 96 attention heads;Brownet al.(2020)] is trained with elementwise embedding, it aligns a sequence of2,048\times 96=296,608characters which is 96x longer at the sameO(N\sqrt{N})_{N=2048}complexity. The proposed methodology follows the two-step framework of"reshape, then focus". First, the given text is encoded as a sequence ofuvUTF-8 bytes and projected into a(uv,c)embedding matrix in which each row is ac-dimensional element; it’s"reshaped"into a(u,w)embedding matrix in which each row is aw-dimensional material (e.g., a word), wherec=w/v. As a result, one material consists ofvelements so that we can alignuvelements at theO(u^{2})complexity using multihead self-attention(Vaswaniet al.,2017)withvattention heads. Eachi^{th}column of this(u,v)material matrix is a sequence of thei^{th}elements of allumaterials, soi^{th}attention head alignsi^{th}elements. This operation is most straightforward when a material is avletters word:i^{th}spellings of alluwords are aligned ini^{th}attention head, then concatenated back creating unique embedding representations wherei\in[1,v]. Each attendedi^{th}spelling is referred asfocusbecause it is quite similar to that we often read text inferring the meanings of words by"focusing"on a few important letters. The contextual importance of each word is determined jointly via linear transformation. Theoretically, this can be understood as lowering the entropy of character sequences concentrating distributed probabilities into several important spellings. Technically, it is just to pass a(u,w)word embedding matrix in which each row is a horizontal concatenation ofvc-dimensional character embeddings as input to a transformer model withw-dimensional hidden layers. It’s identical to aligning words using character-level semantics and vice versa. In practical implementation, focus is performed by multihead attention of the parent (any transformer model) by setting the number of attention heads toh=v, so applying elementwise embedding is simply to replace the embedding table of parent model with a set of 256c-dimensional character embeddings (each mapping to one of UTF-8 bytes; elements) and a following tensor reshaping operation. Neither structural modification of neural networks nor additional operations such as up/downsampling that entail unnecessary engineering efforts and overheads are required. Fig1offers an intuitive visualization of elementwise embedding.
Fluency score: 3
Fluency issues: Run-on sentences; inconsistent punctuation spacing; complex jargon not fluently integrated.

Clarity score: 2
Clarity issues: Dense terminology and jargon; run-on sentences.

Grammaticality score: 3
Grammaticality issues: Punctuation spacing; parentheses usage; possessive form ("it's" should be "its").

Readability score: 2
Readability issues: Excessive jargon; long, complex sentences; formatting (missing spaces after commas and periods).

Coherence score: 3
Coherence issues: Disjointed phrases; lack of clear transitions between ideas; hard to follow due to terminology and sentence complexity.
====================
	 Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications
==========
Introduction:  Cooperative communications are acknowledged schemes to improve the transmission quality. One of the most important categories of cooperative communications is in the form of relaying. Although relaying requires extra energy and time slot, it is an effective solution when the path-loss of the direct link is very large. On the other hand, with the research trend towards the sixth-generation (6G) wireless networks[13], various transmission technologies have been invented, among which semantic communications[16]are considered to have a great potential in media transmissions. The semantic encoder extracts the semantic features for transmissions[17], while the semantic decoder works in a similar way to the generativeartificial intelligence (AI)[4]. Inspired by the principle of semantic communications, this letter proposes a novel framework,semantic-forward (SF)relaying, for cooperative communications. There have been already diverse relaying schemes in the literature[8]. One simple relaying scheme isamplify-and-forward (AF), in which the relay directly amplifies the signals received from the source and then forwards to the destination. In 1979, Cover and El Gamal[2]established the fundamental theorems of relaying systems, and proposed thedecode-and-forward (DF)andcompress-and-forward (CF)schemes. In the DF scheme, the relay decodes the received signals at the first step, and then the recovered information sequence is forwarded or discarded, respectively, depending on the recovery is error-free or not. In the CF scheme, the relay quantizes and compresses its received signals into the relay information to be transmitted to the destination. Beyond DF,lossy-forward (LF)[10]was proposed to overcome the drawback of DF, where the communication resources are completely wasted once errors occur in the relay information. In the LF scheme, the relay always forwards the relay information to the destination regardless of whether or not intra-link error is detected at the relay. At the destination, a joint decoder recovers the source information with the help of the relayed information, based on the principle of correlated sources transmission. Nevertheless, the previous relaying schemes are designed for general types of information, which do not exploit the features of information to improve the information efficiency. By adopting the semantic communications, the system adaptively exploits diverse types of information. Hence, we aims at designing a relaying systems where the relay forwards the semantic information to the destination, i.e., SF relaying, so that the destination can utilize the semantic information to help recovering the source information. The terminology of SF has been used for the first time in[11], up to the authors’ maximum knowledge, where the source transmits semantic information to the relay, and the relay translates and forwards the processed semantic information to the destination. However, there is no direct source-destination link in[11]. Different from the relay-assisted semantic communications in[11], this work proposes a semantic-assisted relaying system. In our proposed SF relaying system, the relay reconstructs the information received from the source at the first step. Then, in spite of whether or not the reconstruction error-free, the relay extracts the semantic information and sends it to the destination. The semantic coding achieves robustness of the relaying system against errors, and hence can reduce the payload in therelay-destination (R-D)link. At the destination, a joint decoder performs iterative decoding utilizing the Turbo principle[1]that exchanges the extrinsic information between the lossy information of thesource-destination (S-D)link and the semantic information of theR-Dlink. In this way, the SF relaying can help for the lossless recovery of the original information at the destinations, even in bad channel conditions. With the pre-trained semantic encoder/decoder, the SF relaying can reduce the payload of the R-D link in practical systems. The contributions of this letter are summarized as follows: We propose a novel relaying framework, i.e., SF relaying, which adopts semantic communications at the relay and the destination to reduce the payload of theR-Dlink. We design a joint source-channel coding algorithm for SF relaying systems, where the destination can losslessly recover the source information with the assistance of the semantic information received from the relay. We conduct a series of simulations with image transmissions to evaluate the performance of SF relaying. The simulation results demonstrate that SF relaying systems can exploit the semantic information to reduce theEuclidean distance (ED)and improve the image quality. Notation.Capital lettersX,Y,V,Udenote the random variables for constructing information sequences.Mrepresents the codeword satisfying the link rate constraintR.
Fluency score: 3
Fluency issues: [Complex sentences and technical jargon obscure general readability for lay readers; use of parentheses obstructs flow.]

Clarity score: 3
Clarity issues: [Overuse of technical terms without explanation; assumes a high level of prior knowledge.]

Grammaticality score: 4
Grammaticality issues: ["...we aims at designing a relaying systems..." should be "we aim at designing relaying systems..." or "we aim to design a relaying system...".]

Readability score: 2
Readability issues: [Dense paragraphs without clear segmentation; excessive technical language and referencing style not common in general prose.]

Coherence score: 3
Coherence issues: [Jumps between concepts without clear transitions; references to prior work and terminology may confuse readers unfamiliar with the topic.]
====================
	 Learning Spatially-Continuous Fiber Orientation Functions
==========
Introduction:  Mapping the human connectome relies upon a continuous and accurate representation of the underlying brain tissue. This is needed for tracing streamlines, resolving crossing fibers, and deciding when to terminate a tract. Often, tractography algorithms rely on simple trilinear interpolation to “fill out” a continuous field from discretely-sampled diffusion magnetic resonance images (dMRIs). If this interpolation could be improved, then tractography algorithms could produce more detailed and accurate human white matter (WM) fiber tracts. In this work, we propose FENRI (Fiber orientations fromExplicitNeuralRepresentatIons), a novel deep learning-based super-resolution model for estimating fODFs continuously in space. We demonstrate FENRI’s capabilities through the following experiments: 1) a quantitative evaluation of fODF reconstruction in Human Connectome Project (HCP) data, 2) a qualitative evaluation of tractography in HCP data, and 3) a quantitative measure of tractography performance on a new, expanded simulation dataset. As an image upsampler, FENRI outperforms more generic single-image super-resolution (SISR) methods on a variety of test metrics. We also show how, as a tractography enhancement, FENRI’s explicit representation sampling provides a powerful improvement over standard tractography methods. Background.Reconstructing streamlines from diffusion-weighted images (DWIs) requires a model of neuron fiber directionality. One popular model is the general fODF represented by coefficients in the spherical harmonic (SH) orthonormal basis, estimated by constrained spherical deconvolution (CSD)[7]. Several deep learning models have recently been proposed to super-sample diffusion representations. For example, Qin et. al., 2021 used convolutional neural networks (CNNs), an efficient sub-pixel CNN (ESPCN) layer, and high-resolution T1w volumes to predict high-resolution diffusion model parameters[10,13]. However, these previous works were limited to upsampling by aninteger upscaling factor, e.g.2\times, which is not ideal for estimating continuous fields. The recently proposed implicit neural representation (INR) method, which learns continuous-valued representations in some Euclidean space, is one solution to this challenge[14]. INRs are most commonly applied to 3D rendering, but INR-like models have been used in SISR. For example, the Local Implicit Image Function where a low-resolution input image is encoded into a feature space and sampled continuously for upsampling[3]. To our knowledge, the only proposed model that utilizes INRs for super-resolving dMRIs is given in[4], which focused on uncertainty in continuous predictions rather than tractography. We place FENRI alongside these INR models, but note that FENRI does not model animplicitfunction, but anexplicitfunction of SH coefficients.
Fluency score: 4
Fluency issues: Occasionally complex and technical terms might impede fluency for general readers.

Clarity score: 3
Clarity issues: Overuse of jargon and abbreviations can confuse readers not familiar with the subject.

Grammaticality score: 5
Grammaticality issues: None.

Readability score: 3
Readability issues: High density of technical terms and lack of explanation for abbreviations diminish readability for a non-specialist audience.

Coherence score: 4
Coherence issues: The text is mostly coherent but assumes a high level of prior knowledge in the reader, which may impact coherence for lay readers.
====================
	 Explainable Boosting Machines with Sparsity -- Maintaining Explainability in High-Dimensional Settings
==========
Introduction:  Explainable boosting machines (Nori et al. 2019), or EBMs for short, are a modern class ofgeneralized additive models(GAMs) that can offer both competitive accuracy and explicit transparency and explainability, which are often considered to be two opposing goals of a machine learning (ML) model. For example, full-complexity models, like random forests (Breiman 2001), tend to be highly competitive in terms of accuracy, but are readily less transparent and explainable (due in part to the high-order interaction effects often captured by such black-box models). Essentially, with EBMs, it’s possible to “have your cake and eat it too.” In short, EBM models have the general form where gis alink functionthat allows the model to handle various response types (e.g., thelogitlink for logistic regression or the identify function for ordinary regression with continuous outcomes); \theta_{0}is a constant intercept (or bias term); f_{i}is theterm contribution(orshape function) for predictorx_{i}(i.e., it captures the main effect ofx_{i}); f_{ij}is the term contribution for the pair of predictorsx_{i}andx_{j}(i.e., it captures the joint effect, or pairwise interaction effect ofx_{i}andx_{j}). Similar togeneralized additive models plus interactions(Lou et al. 2013), or GA2Ms for short, the pairwise interaction terms are determined automatically using the FAST algorithm described in Lou et al. (2013). In short, FAST is a novel, computationally efficient method for ranking all possible pairs of feature candidates for inclusion into the model (by default, the top 10 pairwise interactions are used). The primary difference between EBMs and GA2Ms is in how the shape functions are estimated. In particular, EBMs usecyclic gradient boosting(Nori et al. 2019; Wick, Kerzel, and Feindt 2020) to estimate the shape functions for each feature (and selected pairs of interactions) in a round-robin fashion using a low learning rate to help ensure that the order in which the feature effects are estimated does not matter. Estimating each feature one-at-a-time in a round-robin fashion also helps mitigate potential issues withcollinearitybetween predictors (Nori et al. 2019; Wick, Kerzel, and Feindt 2020). However, in contrast to the more common gradient boosting machine (J. H. Friedman 2001, 2002), or GBM for short, which can ignore irrelevant inputs, EBMs include at least one term in the model for each feature: one main effect (f_{i}) for each predictor, and a term for each selected pairwise interaction effect. This is due to the cyclic nature of the underlying boosting framework. For example, an EBM applied to a training set withp=300features will result in a model with at least 300 terms. While EBMs are consideredglass-boxmodels, an EBM with, say, hundreds or thousands of terms, starts to become much less transparent and explainable. Moreover, the larger the fitted model (i.e., the more terms there are), then the more time it will take the EBM to make predictions, making larger models less fit for deployment and production. To this end, we propose in Section2a way to introduce sparsity into a fitted EBM by rescaling the individual terms (i.e., thef_{i}andf_{ij}) via regression coefficients estimated from a method called the LASSO (Tibshirani 1996). Consequently, by nature of theL_{1}regularization enforced by the LASSO, many of these coefficients can be estimated to be zero, resulting in a reduced EBM with far fewer terms (and hopefully, comparable accuracy)!
Fluency score: 3
Fluency issues: Run-on sentences, excessive use of parentheses and technical jargon without proper spacing "class ofgeneralized", "functionthat", "theidentify", "thelogitlink".

Clarity score: 2
Clarity issues: Overloaded with technical terms and complex sentences that hinder understanding, lack of spacing in certain phrases.

Grammaticality score: 4
Grammaticality issues: Missing spaces after parentheses and typographical errors (e.g., "class ofgeneralized", "functionthat").

Readability score: 2
Readability issues: Dense technical content that's difficult to parse, long sentences, and lack of clear organization.

Coherence score: 3
Coherence issues: The text jumps between explanations without clear transitions, assumes a high level of prior knowledge.
====================
	 Approximate inference of marginals using the IBIA framework
==========
Introduction:  Discrete probabilistic graphical models (PGM) including Bayesian networks (BN) and Markov networks (MN) are used for probabilistic inference in a wide variety of applications. An important task in probabilistic reasoning is the computation of posterior marginals of all the variables in the network. Exact inference is known to be #P-complete[Roth,1996], thus necessitating approximations. Approximate techniques can be broadly classified as sampling based and variational methods. Sampling based methods include Markov chain Monte Carlo based techniques like Gibbs sampling[Gelfand,2000, Kellyet al.,2019]and importance sampling based methods[Gogate and Dechter,2011, Friedman and Van den Broeck,2018, Kasket al.,2020, Broka,2018, Louet al.,2019,2017b,2017a, Marinescuet al.,2019,2018]. An advantage of these methods is that accuracy can be improved with time without increasing the required memory. However, in many benchmarks the improvement becomes slow with time.Moreover, many of the recent sampling/search based techniquesKasket al.[2020], Broka [2018], Louet al.[2019,2017b,2017a], Marinescuet al.[2019,2018]have been evaluated either for approximate inference of partition function (PR) or for finding the marginal maximum a posteriori assignment (MMAP). Currently, there are no published results for posterior marginals (MAR) using these methods, and the publicly available implementations do not support the MAR task.Alternatively, variational techniques can be used. These include loopy belief propagation (LBP)[Frey and MacKay,1998]region-graph based techniques like generalized belief propagation (GBP)[Yedidiaet al.,2000]and its variants[Heskeset al.,2003, Mooij and Kappen,2007, Linet al.,2020], mini-bucket based schemes like iterative join graph propagation (IJGP)[Mateescuet al.,2010]and weighted mini-bucket elimination (WMB)[Liu and Ihler,2011]and methods that simplify the graph structure like edge deletion belief propagation (EDBP) and the related relax-compensate-recover (RCR) techniques[Choiet al.,2005, Choi and Darwiche,2006,2010]. While the accuracy-complexity trade-off can be achieved using a single user-defined clique size bound in mini-bucket based methods, it is non-trivial in many of the other region graph based methods. Most of these techniques use iterative message passing to solve an optimization problem, for which convergence is not guaranteed and even if possible, can be slow to achieve.Non-iterative methods like Deep Bucket Elimination (DBE)[Razeghiet al.,2021]and NeuroBE[Agarwalet al.,2022]are extensions of bucket elimination that approximate messages using neural networks. However, training these networks takes several hours. Moreover, the publicly available implementations of these methods do not support the MAR task. The recently proposedincremental build-infer-approximate(IBIA) framework[Bathla and Vasudevan,2023]uses a different approach. It converts the PGM into a sequence of calibrated clique tree forests (SCTF) with clique sizes bounded to a user-defined value.Bathla and Vasudevan [2023]show that the normalization constant (NC) of clique beliefs in the last CTF in the sequence isa good approximationof the partition function of the overall distribution. This framework has two main advantages. Firstly, since it is based on clique trees and not loopy graphs, the belief propagation step is non-iterative. Therefore, it is fast and has no issues related to convergence. Secondly, it provides an easy control of the accuracy complexity trade-off using two user-defined parametersand hence can be used in anytime manner.However, the framework inBathla and Vasudevan [2023]cannot be used to infer marginals. This is because only the clique beliefs in the last CTF account for all factors in the PGM. Beliefs in all other CTFs account for a subset of factors and thus, cannot be used for inference of marginals. Contributions of this work:In this paper, we propose a method for marginal inference that uses the IBIA framework. We show that the approximation algorithm used in this framework preserves the within-clique beliefs. Based on this property, we modify the data structure generated by IBIA to add links between adjacent CTFs. We refer to the modified data structure as asequence of linked clique tree forests(SLCTF). We propose a heuristic belief update algorithmthat back-propagates beliefs from the last CTF to the previous CTFs via the links and re-calibrates each CTF so that the updated beliefs account for all factors in the PGM. We also propose a greedy heuristic for the choice of links used for belief update.Results for several UAI benchmark sets show that our method givesan accuracy that is better than or comparableto the existing variational and sampling based methods, with competitive runtimes. For the special case of BNs, we show that if the incremental build step in IBIA is performed in the topological order of variables then (a) the estimated partition function is guaranteed to be one if no evidence variables are present (b) the prior marginals of all variables are consistent across all CTFs in the sequence and (c) once all the evidence variables have been added to the SLCTF, the posterior marginals of variables in subsequent CTFs are consistent. Our results show that using the topological ordering for BNs leads to better estimates of partition function, prior marginals and posterior marginals in most benchmarks.
Fluency score: 3
Fluency issues: Several instances of lacking space after citations, e.g., "like Gibbs sampling[Gelfand,2000, Kellyet al.,2019]", makes the text awkward to read. The word "incremental" before "build-infer-approximate" is missing a space. There are also instances like "sampling/search based techniquesKasket al." and "for the special case of BNs, we show that if the incremental build step in IBIA is performed in the topological order of variables then".

Clarity score: 3
Clarity issues: Heavy use of jargon, abbreviations, and domain-specific terminology can make the text challenging to decipher for non-experts. Overly long sentences and lack of explanations for abbreviations, such as MAR, PR, MMAP, SCTF, and others, can confuse readers.

Grammaticality score: 4
Grammaticality issues: The text is mostly grammatically correct but suffers from punctuation issues (e.g., missing spaces after periods and commas) which can be considered grammatical errors too.

Readability score: 2
Readability issues: The heavy reliance on specialized terms, acronyms, complex sentence structures, and the sheer density of information make the text hard to follow. In particular, the section listing various methods is a long run-on sentence that could be broken up for easier comprehension.

Coherence score: 3
Coherence issues: The dense use of technical language and lack of clear transitions between aspects of the discussion can hinder coherence. References are extensive but disrupt the flow. There's also a forward reference problem: the reader encounters the abbreviation "MAR" before it's explained that this stands for "posterior marginals".
====================
	 The Distortion of Binomial Voting Defies Expectation
==========
Introduction:  In an election, voters report their preferences by casting ballots. Under the ubiquitous plurality rule, each voter names a single alternative, whereas other rules — such as the badly namedranked-choice voting,111Also known as “instant-runoff voting” or “alternative vote.”whose adoption is rapidly expanding in the United States — require voters to rank the alternatives. However, even these ostensibly expressive ordinal ballots (whereby voters rank the alternatives) fail to capture voters’intensityof preference under truthful reporting. If voters could report utility functions that are comparable to each other, then we would want to select socially desirable alternatives with respect to these utilities, but it is typically impractical to expect voters to compute and report such utilities. This creates a tension between the limited information available to the voting rule (through the report of ordinal ballots only) and its goal (good outcomes with respect to latent cardinal utilities). A significant body of work in computational social choice aims to understand and alleviate this tension(Anshelevichet al.,2021). It revolves around the notion ofdistortion, defined as the worst-case ratio between the utilitariansocial welfare(sum of utilities) of the voting rule’s outcome and that of the welfare-maximizing alternative. The worst case is taken over rankings, which serve as input to the voting rule, and over utilities that are consistent with these rankings. As is often the case with worst-case analysis, however, the classic notion of distortion is arguably too conservative. In particular, nontrivial guarantees require restrictive assumptions (see Section1.2), and so this type of analysis may not help identify appealing voting rules. With this difficulty in mind, we focus onexpected distortion. Its definition includes the same ratio as before, and we are still interested in the worst case over rankings. However, we now take the conditional expectation over utilities consistent with the rankings, given an i.i.d. distribution over the utilities. Of course, it might not be realistic for such a distribution to be known a priori,222For this reason, we avoid calling this distribution a Bayesianprior.and thus we search for voting rules that aredistribution independent333This term is inspired by the literature on prior-independent mechanisms within mechanism design.: rules that choose the outcome in a way that does not depend on this distribution, even though their guarantees are stated in terms of this distribution. Overall, our goal is todesign distribution-independent voting rules that provide appealing expected distortion guarantees.We uncover novel, potentially practical, voting rules with these properties. We start by considering the important case of two alternatives (e.g., US presidential elections or yes/no decisions) in Section3. We show that, for any underlying distribution, themajorityrule optimizes both expected distortion and expected social welfare. This result suggests that maximizing expected social welfare may be a good approach for optimizing expected distortion. Indeed, our main result in Section4is that, under mild conditions on the underlying distribution and for a sufficiently large number of votersoralternatives, the expected-welfare-maximizing rule optimizes expected distortion almost perfectly. The expected-welfare-maximizing rule, however, is tailored to the underlying distribution and relies on intimate knowledge thereof. Our aim is therefore to approximately optimize expected welfare via a distribution-independent rule. In Section5, we design and analyze such a rule, which belongs to the family of (positional)scoring rules. Under this rule, which we callbinomial voting, each voter awards\sum_{\ell=k}^{m}\binom{m}{\ell}points to the alternative ranked in thekth position, wheremis the number of alternatives, and the alternative awarded the largest number of points overall is selected; note that this rule is distribution independent. Our main result of Section5is that for any underlying distribution supported on[0,1]with (largest) median\nu, binomial voting provides a multiplicative\frac{\nu}{2}-approximation to the optimal expected welfare. Combining this result with that of Section4, it follows that binomial voting gives almost the same\frac{\nu}{2}guarantee for expected distortion, when the number of voters or alternatives is sufficiently large. It is worth noting that binomial voting isnotan outlandish rule designed purely to achieve low expected distortion. On the contrary: as a positional voting rule, it inherits the desirable properties of this family. In fact, positional scoring rules are characterized by a number of natural axioms(Young,1975). Furthermore, we are aware of very few positional scoring rules that have received attention in their own right, as it is typically difficult to justify any specific choice of scores; in this sense, the expected distortion framework can be seen as a way of pinpointing particularly useful parameters. In summary, we identify binomial voting as an unusually attractive rule when viewed through the lens of expected distortion. The literature on (worst-case) distortion(Anshelevichet al.,2021)can generally be partitioned into two threads. In the first thread(Procaccia and Rosenschein,2006; Boutilieret al.,2015; Caragianniset al.,2017; Mandalet al.,2019; Ebadianet al.,2022), it is assumed that voters have normalized utilities, that is, for each voter, the sum of utilities is one. Even with this restrictive assumption, deterministic voting rules cannot give nontrivial distortion bounds(Caragianniset al.,2017), and the best possible distortion for randomized rules is\Theta(\nicefrac{{1}}{{\sqrt{m}}})(Ebadianet al.,2022). In the second thread(Anshelevich and Postl,2017; Grosset al.,2017; Anshelevichet al.,2018; Gkatzeliset al.,2020; Kizilkaya and Kempe,2022), known asmetric distortion, it is assumed that utilities (rather, costs) are induced by an underlying metric space. While some well-known voting rules have constant distortion in this setting(Anshelevichet al.,2018), the metric assumption is arguably difficult to justify in most domains of interest. By contrast, we make no assumptions on utilities. Our work is most closely related to that ofBoutilieret al.(2015). While their most substantial results pertain to worst-case distortion, one of their results deals with a distributional setting that can be seen as the starting point for our work. They show that the voting rule that maximizes expected social welfare is a scoring rule whose scores depend on the underlying distribution; we will revisit and build on this result. However, they do not study expected distortion, nor do they explore (in this context) voting rules that are agnostic to the distribution. Previous papers that analyze expected distortion include those ofChenget al.(2017,2018). However, their papers are fundamentally different. For one, they study metric distortion. More importantly, they focus on one intuitive but very specific distribution, where the positions of alternatives in the underlying metric space are drawn uniformly at random from the voter positions. By contrast, we study general (i.i.d.) distributions over utilities and design distribution-independent voting rules. The work ofGhodsiet al.(2019)is more distantly related: they also analyze expected distortion in the metric setting, assuming that voters abstain with some probability. By replacing worst-case analysis with expectation, our work introduces a Bayesian view canonical to economic theory into the literature on distortion. The Wilson doctrine(Wilson,1987)advocates for the use of mechanisms that require as little prior information as possible. Distribution-independent (sometimes called prior-independent) mechanisms are, through this lens, the most desirable ones as they require no prior information at all, and have been studied within economics(see, e.g., McAfee,1992; Segal,2003)as well as within computer science(see, e.g., Hartline and Roughgarden,2009; Devanuret al.,2011; Babaioffet al.,2018). Our work can also be seen as belonging to a recent push within computer science on “beyond worst-case” analysis of algorithms(Roughgarden,2021).
Fluency score: 3
Fluency issues: Frequent use of technical language and in-text references disrupt the flow. "badly namedranked-choice voting,111Also known as" appears to have formatting and spacing issues.

Clarity score: 2
Clarity issues: The technical nature of the text, dense terminology, and jargon make it difficult for a general audience to follow.

Grammaticality score: 4
Grammaticality issues: Minor issues include spacing errors, e.g., "voters’intensityof preference".

Readability score: 2
Readability issues: The complexity of the subject matter and text structure, including long sentences with numerous clauses and embedded references, detracts from readability.

Coherence score: 3
Coherence issues: The logical structure is present but difficult to follow due to the specialized content and the way technical details are presented.
====================
