	 GPT-4 Technical Report
==========
Abstract:  We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: "behaving predictably" is slightly vague.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Dense with technical jargon which may affect a layperson's reading experience.

Coherence score: 5
Coherence issues: None
====================
	 Llama 2: Open Foundation and Fine-Tuned Chat Models
==========
Abstract:  In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
Fluency score: 5
Fluency issues:

Clarity score: 4
Clarity issues: [The term "LLMs" could be unclear to those unfamiliar with the acronym for large language models.]

Grammaticality score: 5
Grammaticality issues:

Readability score: 4
Readability issues: [The dense technical language could be challenging for a general audience.]

Coherence score: 5
Coherence issues:
==========
Introduction:  Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM(scao2022bloom), LLaMa-1(Touvron2023LLaMAOA), and Falcon(penedo2023refinedweb)) that match the performance of closed pretrained competitors like GPT-3(gpt3)and Chinchilla(hoffmann2022training), but none of these models are suitable substitutes for closed “product” LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,Llama 2andLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,Llama 2-Chatmodels generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures2and3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development ofLlama 2andLlama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. We are releasing the following models to the general public for research and commercial use111https://ai.meta.com/resources/models-and-libraries/llama/: Llama 2, an updated version ofLlama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention(gqa2023). We are releasing variants ofLlama 2with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.222We are delaying the release of the 34B model due to a lack of time to sufficiently red team. Llama 2-Chat, a fine-tuned version ofLlama 2that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,Llama 2is a new technology that carries potential risks with use(bender2021dangers;weidinger2021ethical;solaiman2023evaluating). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications ofLlama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide333https://ai.meta.com/llamaand code examples444https://github.com/facebookresearch/llamato facilitate the safe deployment ofLlama 2andLlama 2-Chat. More details of our responsible release strategy can be found in Section5.3. The remainder of this paper describes our pretraining methodology (Section2), fine-tuning methodology (Section3), approach to model safety (Section4), key observations and insights (Section5), relevant related work (Section6), and conclusions (Section7).
Fluency score: 4
Fluency issues: [formatting and citation style interruptions (e.g., “BLOOM(scao2022bloom)”, “GPT-3(gpt3)”, “Llama 2andLlama 2-Chat”, are not fluently integrated)]

Clarity score: 4
Clarity issues: [Jargon and acronyms without explanation such as "Auto-regressive transformers", "RLHF", "gqa2023"; format and style errors such as missing spaces after citations and model names, which may confuse readers]

Grammaticality score: 4
Grammaticality issues: [Missing spaces and inconsistent citation format (e.g., "Llama 2andLlama 2-Chat")]

Readability score: 3
Readability issues: [Formatting issues, heavy jargon, acronym use, and citation style directly embedded into the text disrupt the readability]

Coherence score: 4
Coherence issues: [The logical structure is present but the consistent interruption by formatting errors and inline citation disrupts the flow]
====================
	 LLaMA: Open and Efficient Foundation Language Models
==========
Abstract:  We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.
Fluency score: 5
Fluency issues: [None]

Clarity score: 5
Clarity issues: [None]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [The text is slightly less readable due to the use of several technical terms without explanations, e.g., "foundation language models," "parameters," "tokens," "Chinchilla-70B," and "PaLM-540B."]

Coherence score: 4
Coherence issues: [The text mentions specific models, LLaMA-13B and LLaMA-65B, without explaining their relationship to the larger LLaMA collection introduced at the beginning.]
==========
Introduction:  Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examplesBrownet al.(2020). These few-shot properties first appeared when scaling models to a sufficient sizeKaplanet al.(2020), resulting in a line of work that focuses on further scaling these modelsChowdheryet al.(2022); Raeet al.(2021). These efforts are based on the assumption that more parameters will lead to better performance. However, recent work fromHoffmannet al.(2022)shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws fromHoffmannet al.(2022)is to determine how to best scale the dataset and model sizes for a particulartrainingcompute budget. However, this objective disregards theinferencebudget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, althoughHoffmannet al.(2022)recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of language models that achieve the best possible performance at various inference budgets, by training on more tokens than what is typically used. The resulting models, calledLLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10\timessmaller. We believe that this model will help democratize the access and study of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter model is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g. “Books – 2TB” or “Social media conversations”). There exist some exceptions, notably OPT(Zhanget al.,2022), GPT-NeoX(Blacket al.,2022), BLOOM(Scaoet al.,2022)and GLM(Zenget al.,2022), but none that are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview of the modifications we made to the transformer architectureVaswaniet al.(2017), as well as our training method. We then report the performance of our models and compare with others LLMs on a set of standard benchmarks. Finally, we expose some of the biases and toxicity encoded in our models, using some of the most recent benchmarks from the responsible AI community.
Fluency score: 3
Fluency issues: Frequent lack of spaces after punctuation, unexpected line breaks, and inconsistent formatting of citations (e.g., "Brown et al.(2020)" should be "Brown et al. (2020)"). 

Clarity score: 3
Clarity issues: Occasional jargon without explanation, convoluted sentences, and the mention of a "10\timessmaller" model is unclear.

Grammaticality score: 4
Grammaticality issues: Missing spaces after periods in citations (e.g., "Kaplan et al.(2020)").

Readability score: 3
Readability issues: Jargon-heavy, complex sentences, and formatting errors compromise readability.

Coherence score: 4
Coherence issues: Though generally coherent, the flow is occasionally hampered by abrupt transitions and dense jargon.
====================
	 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena
==========
Abstract:  Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.
Fluency score: 4
Fluency issues: [--]
\][poiuytre
Clarity score: 3
Clarity issues: [Use of acronyms without explanation (LLM, MT-bench); specialized terms like "LLM-as-a-judge" may be confusing without context]

Grammaticality score: 5
Grammaticality issues: [--]

Readability score: 3
Readability issues: [Complex sentence structures; heavy use of jargon; the passage is dense with information, which makes it less accessible to general readers]

Coherence score: 4
Coherence issues: [Introduction of "MT-bench" and "Chatbot Arena" without clear explanations can confuse readers; it's not immediately clear how the benchmarks are used in conjunction with the LLM judges]
==========
Introduction:  There has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised instruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new instruction following and conversational abilities[31,2,30,8,52,48,14]. Once aligned with humans, these chat models are strongly preferred by human users over the original, unaligned models on which they are built. However, the heightened user preference does not always correspond to improved scores on traditional LLM benchmarks – benchmarks like MMLU[19]and HELM[24]cannot effectively tell the difference between these aligned models and the base models. This phenomenon suggests that there is a fundamental discrepancy between user perceptions of the usefulness of chatbots and the criteria adopted by conventional benchmarks. We argue that this discrepancy primarily arises due to existing evaluation that only measures LLMs’ core capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions), without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to accurately adhere to instructions in multi-turn dialogues. As a demonstration, we show conversation histories with two models on an MMLU question inFigure1. The two models are LLaMA-13B[39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned model from LLaMA-13B on high-quality conversations (the training details are inAppendixE). Despite the base LLaMA models showing competitive performance on conventional benchmarks (Table9), its answers to open-ended questions are often not preferred by humans. This misalignment of conventional benchmarks underscores the core problem driving this paper:the need for a robust and scalable automated method to evaluate LLM alignment with human preferences. To study this, we introduce two benchmarks with human ratings as the primary evaluation metric: MT-bench and Chatbot Arena. MT-bench is a series of open-ended questions that evaluate a chatbot’s multi-turn conversational and instruction-following ability – two critical elements for human preference. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities, such as reasoning and math. In addition, we develop Chatbot Arena, a crowdsourced platform featuring anonymous battles between chatbots in real-world scenarios – Users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences. While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow and costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans. Because these models are often trained with RLHF, they already exhibit strong human alignment. We call this approach“LLM-as-a-judge”. This approach has been tried in our earlier blog post[8]and other concurrent or follow-up work[5,29,14,12,52,18,33,40,7,43]. However, there has not been a systematic study of this approach. In this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of human evaluation. We examine several potential limitations of the LLM-as-a-judge approach including position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We show that some of the biases are minor or can be mitigated. Once addressed, our results from 3K controlled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match human evaluations at an agreement rate exceeding 80%, achieving the same level of human-human agreement (§4.2, Table4). Consequently, this suggests LLM-as-a-judge is a scalable method to swiftly evaluate human preference, serving as a promising alternative to traditional human evaluations. This paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human preference datasets with high-quality questions and diverse user interactions from MT-bench and Chatbot Arena. In addition, we argue for the adoption of a hybrid evaluation framework for future LLM benchmarks: by combining the existing capability-based benchmarks and the new preference-based benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core capabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study. Category 1st Turn Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions. 2nd Turn Rewrite your previous response. Start every sentence with the letter A. 1st Turn Given thatf(x)=4x^{3}-9x-14, find the value off(2). 2nd Turn Findxsuch thatf(x)=0. 1st Turn Provide insights into the correlation between economic indicators such as GDP, inflation, and unemployment rates. Explain how fiscal and monetary policies … 2nd Turn Now, explain them again like I’m five.
Fluency score: 4
Fluency issues: [Sentence structure is complex and could be challenging for some readers; multiple grammatical structures and technical terminology are used.]

Clarity score: 3
Clarity issues: [Overuse of acronyms and jargon specific to the field (like LLM, MMLU, RLHF) which may not be universally clear; some sentences are convoluted, making the information difficult to parse.]

Grammaticality score: 5
Grammaticality issues: [No notable issues detected.]

Readability score: 3
Readability issues: [Text dense with technical language and lacks clear segmentation which might impede readability for a general audience; paragraphing can aid readability.]

Coherence score: 3
Coherence issues: [Logical flow is somewhat obscured by the density of information and rapid introduction of concepts; need for clearer transitions between ideas.]
====================
	 Sparks of Artificial General Intelligence: Early experiments with GPT-4
==========
Abstract:  Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.
Fluency score: 5
Fluency issues: N/A

Clarity score: 4
Clarity issues: Slightly dense with technical jargon which may reduce clarity for general audience (e.g., "unprecedented scale of compute and data," "next-word prediction").

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 4
Readability issues: High density of complex concepts and extended sentence structure, may affect pacing for some readers.

Coherence score: 5
Coherence issues: N/A
====================
	 Gemini: A Family of Highly Capable Multimodal Models
==========
Abstract:  This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: "memory-constrained use-cases" might be slightly technical for general audiences.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: The use of technical terms like "multimodal benchmarks" and "cross-modal reasoning" may affect readability for non-specialized readers.

Coherence score: 5
Coherence issues: None
====================
	 BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
==========
Abstract:  The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.
Fluency score: 4
Fluency issues: "vision-and-language" and "vision-language" might confuse readers not familiar with the technical jargon.

Clarity score: 4
Clarity issues: Terms like "BLIP-2," "Querying Transformer," and "zero-shot VQAv2" require specific domain knowledge.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Some parts of the text are dense with technical terms and model names, which could be challenging for non-specialists.

Coherence score: 4
Coherence issues: The text is logical but assumes a high level of prior knowledge about the subject matter.
==========
Introduction:  Vision-language pre-training (VLP) research has witnessed a rapid advancement in the past few years, where pre-trained models with increasingly larger scale have been developed to continuously push the state-of-the-art on various downstream tasks(Radfordet al.,2021; Liet al.,2021,2022; Wanget al.,2022a; Alayracet al.,2022; Wanget al.,2022b). However, most state-of-the-art vision-language models incur a high computation cost during pre-training, due to end-to-end training using large-scale models and datasets. Vision-language research sits at the intersection between vision and language, therefore it is naturally expected that vision-language models can harvest from the readily-available unimodal models from the vision and natural language communities. In this paper, we propose agenericandcompute-efficientVLP method by bootstrapping from off-the-shelf pre-trained vision models and language models. Pre-trained vision models offer high-quality visual representation. Pre-trained language models, in particularlarge language models(LLMs), offer strong language generation and zero-shot transfer abilities. To reduce computation cost and counteract the issue of catastrophic forgetting, the unimodal pre-trained models remain frozen during the pre-training. In order to leverage pre-trained unimodal models for VLP, it is key to facilitate cross-modal alignment. However, since LLMs have not seen images during their unimodal pre-training, freezing them makes vision-language alignment in particular challenging. In this regard, existing methods (e.g. Frozen(Tsimpoukelliet al.,2021), Flamingo(Alayracet al.,2022)) resort to an image-to-text generation loss, which we show is insufficient to bridge the modality gap. To achieve effective vision-language alignment with frozen unimodal models, we propose a Querying Transformer (Q-Former) pre-trained with a new two-stage pre-training strategy. As shown in Figure1, Q-Former is a lightweight transformer which employs a set of learnable query vectors to extract visual features from the frozen image encoder. It acts as an information bottleneck between the frozen image encoder and the frozen LLM, where it feeds the most useful visual feature for the LLM to output the desired text. In the first pre-training stage, we perform vision-language representation learning which enforces the Q-Former to learn visual representation most relevant to the text. In the second pre-training stage, we perform vision-to-language generative learning by connecting the output of the Q-Former to a frozen LLM, and trains the Q-Former such that its output visual representation can be interpreted by the LLM. We name our VLP framework as BLIP-2: Bootstrapping Language-Image Pre-training with frozen unimodal models. The key advantages of BLIP-2 include: [leftmargin=*] BLIP-2 effectively leverages both frozen pre-trained image models and language models. We bridge the modality gap using a Q-Former pre-trained in two-stages: representation learning stage and generative learning stage. BLIP-2 achieves state-of-the-art performance on various vision-language tasks including visual question answering, image captioning, and image-text retrieval. Powered by LLMs (e.g. OPT(Zhanget al.,2022), FlanT5(Chunget al.,2022)), BLIP-2 can be prompted to perform zero-shot image-to-text generation that follows natural language instructions, which enables emerging capabilities such as visual knowledge reasoning, visual conversation, etc. (see Figure4for examples). Due to the use of frozen unimodal models and a lightweight Q-Former, BLIP-2 is more compute-efficient than exisiting state-of-the-arts. For example, BLIP-2 outperforms Flamingo(Alayracet al.,2022)by 8.7% on zero-shot VQAv2, while using 54\timesfewer trainable parameters. Furthermore, our results show that BLIP-2 is a generic method that can harvest more advanced unimodal models for better VLP performance.
Fluency score: 3
Fluency issues: Lack of space after "tasks(Radfordet al.," and in "compute-efficientVLP," "largelanguage models(LLMs)," "existing methods (e.g. Frozen(Tsimpoukelliet al.," and other similar instances. Usage of "\times" instead of "times" in "54\timesfewer." 

Clarity score: 3
Clarity issues: Dense with jargon and complex sentence structures, reducing readability for a non-specialist audience.

Grammaticality score: 4
Grammaticality issues: Missing spaces as mentioned in fluency; complex and jargon-heavy sentences may confuse grammar checkers but are mostly grammatical.

Readability score: 3
Readability issues: The text is dense, uses considerable domain-specific jargon, and long sentences, which can make it challenging for the average reader.

Coherence score: 4
Coherence issues: While the text logically progresses from problem to solution, the heavy use of jargon and acronyms may reduce coherence for readers not familiar with the field.
====================
	 Segment Anything
==========
Abstract:  We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: The phrase "promptable, so it can transfer zero-shot to new image distributions and tasks" may be jargon-heavy for readers unfamiliar with machine learning terminology.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Technical terms like "zero-shot" and "foundation models" might reduce readability for a general audience.

Coherence score: 5
Coherence issues: None
====================
	 Mamba: Linear-Time Sequence Modeling with Selective State Spaces
==========
Abstract:  Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.
Fluency score: 4
Fluency issues: The text is mostly fluent but the use of highly technical terms and a dense presentation of concepts may hinder fluency for non-expert readers.

Clarity score: 3
Clarity issues: Dense use of technical jargon (e.g., "subquadratic-time architectures," "structured state space models," "efficient convolutions") which could be confusing without context or explanation.

Grammaticality score: 5
Grammaticality issues: None.

Readability score: 3
Readability issues: The high density of technical terms and complex sentence structures (e.g., "simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities") can make the text challenging to read for laypersons.

Coherence score: 4
Coherence issues: The transition from discussing the general weakness of certain models to introducing the specific "Mamba" architecture could be smoother.
====================
	 QLoRA: Efficient Finetuning of Quantized LLMs
==========
Abstract:  We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.
Fluency score: 4
Fluency issues: Technical jargon might impede fluency for general audiences.

Clarity score: 3
Clarity issues: Overuse of technical terms and acronyms without explanation; "lemon-picked analysis" is unclear.

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: High density of technical information; assumes a highly knowledgeable reader.

Coherence score: 4
Coherence issues: Jump from detailing QLoRA's technical contributions to discussing benchmarks and evaluations without clear transition.
==========
Introduction:  Finetuning large language models (LLMs) is a highly effective way to improve their performance,[40,62,43,61,59,37]and to add desirable or remove undesirable behaviors[43,2,4]. However, finetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B parameter model[57]requires more than 780 GB of GPU memory. While recent quantization methods can reduce the memory footprint of LLMs[13,15,18,66], such techniques only work for inference and break down during training[65]. We demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any performance degradation. Our method,QLoRA, uses a novel high-precision technique to quantize a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights[28] that are tuned by backpropagating gradients through the quantized weights. QLoRAreduces the average memory requirements of finetuning a 65B parameter model from>780GB of GPU memory to<48GB without degrading the runtime or predictive performance compared to a 16-bit fully finetuned baseline. This marks a significant shift in accessibility of LLM finetuning: now the largest publicly available models to date finetunable on a single GPU. UsingQLoRA, we train theGuanacofamily of models, with the second best model reaching 97.8% of the performance level of ChatGPT on the Vicuna[10]benchmark, while being trainable in less than 12 hours on a single consumer GPU; using a single professional GPU over 24 hours we achieve 99.3% with our largest model, essentially closing the gap to ChatGPT on the Vicuna benchmark. When deployed, our smallestGuanacomodel (7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than 20 percentage points on the Vicuna benchmark (Table6). QLoRAintroduces multiple innovations designed to reduce memory use without sacrificing performance: (1)4-bit NormalFloat, an information theoretically optimal quantization data type for normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats. (2)Double Quantization, a method that quantizes the quantization constants, saving an average of about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3)Paged Optimizers, using NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when processing a mini-batch with a long sequence length. We combine these contributions into a better tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of the accuracy tradeoffs seen in prior work. QLoRA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot performance on model scales that would be impossible using regular finetuning due to memory overhead. Therefore, we train more than 1,000 models across several instruction tuning datasets, model architectures, and sizes between 80M to 65B parameters. In addition to showing thatQLoRArecovers 16-bit performance (§4) and training a state-of-the-art chatbot,Guanaco, (§5), we also analyze trends in the trained models. First, we find that data quality is far more important than dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2, subsampled) on chatbot performance, even when both are meant to support instruction following generalization. Second, we show that strong Massive Multitask Language Understanding (MMLU) benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice versa—in other words, dataset suitability matters more than size for a given task. Furthermore, we also provide a extensive analysis of chatbot performance that uses both human raters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete against each other in matches to produce the best response for a given prompt. The winner of a match is judged by either GPT-4 or human annotators. The tournament results are aggregated into Elo scores[16,17]which determine the ranking of chatbot performance. We find that GPT-4 and human evaluations largely agree on the rank of model performance in the tournaments, but we also find there are instances of strong disagreement. As such, we highlight that model-based evaluation while providing a cheap alternative to human-annotation also has its uncertainties. We augment our chatbot benchmark results with a qualitative analysis ofGuanacomodels. Our analysis highlights success and failure cases that were not captured by the quantitative benchmarks. We release all model generations with human and GPT-4 annotations to facilitate further study. We open-source our codebase and CUDA kernels and integrate our methods into the Hugging Face transformers stack[64], making them easily accessible to all. We release a collection of adapters for 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32 different open sourced, finetuned models.
Fluency score: 4
Fluency issues: Minor inconsistent spacing at "QLoRAreduces", "UsingQLoRA,", "smallestGuanacomodel", "QLoRAintroduces"; these should be separated into "QLoRA reduces", "Using QLoRA,", "smallest Guanaco model", "QLoRA introduces".

Clarity score: 3
Clarity issues: The text contains many technical terms and abbreviations which might not be clear to the layman; it can be quite dense and hard to parse without sufficient background knowledge.

Grammaticality score: 4
Grammaticality issues: Capitalization issues in "using a single professional GPU over 24 hours we achieve" should be "Using a single professional GPU over 24 hours, we achieve" including the missing comma after GPU.

Readability score: 3
Readability issues: The long sentences, technical jargon, and dense information make the text challenging for a general audience.

Coherence score: 3
Coherence issues: Frequent use of abbreviations and jargon without first defining or contextualizing for the reader decreases coherence. References to sections like "§4" and "§5" without context are confusing.
====================
	 Code Llama: Open Foundation Models for Code
==========
Abstract:  We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.
Fluency score: 4
Fluency issues: The term "Llama 2" might be confusing without context.

Clarity score: 4
Clarity issues: The text could be clearer in explaining what "infilling" means or how "zero-shot instruction following" works in layman's terms.

Grammaticality score: 5
Grammaticality issues: None.

Readability score: 4
Readability issues: Use of jargon and multiple specifications may overwhelm non-specialist readers.

Coherence score: 4
Coherence issues: The text jumps between model descriptions and performance benchmarks without clear transitions.
==========
Introduction:  Large language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency in natural language that allows them to be commanded and prompted to perform a variety of tasks(OpenAI,2023; Touvronet al.,2023b). By utilizing large, in-domain datasets, their efficacy can be greatly improved for applications that require a combination of both natural and domain-specific language and understanding of specialized terminology. By training on domain-specific datasets, they have proved effective more broadly on applications that require advanced natural language understanding. A prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation(for a survey, see Xu and Zhu,2022, also seeSection5). In this work, we presentCode Llama, a family of LLMs for code generation and infilling derived fromLlama 2(Touvronet al.,2023b)and released under the same custom permissive license. We provide inference code for both completion and infilling models in the accompanying repository.11footnotemark:1Our approach is based on gradually specializing and increasing the capabilities ofLlama 2models by applying a cascade of training and fine-tuning steps (Figure2):00footnotetext:\dagger: Core contributors.\ast: Meta AI, CERMICS École des Ponts ParisTech.\diamond: Meta AI & Hebrew University of Jerusalem [leftmargin=*,noitemsep,topsep=0pt] Code-training from foundation models.While most LLMs for code generation such as AlphaCode(Liet al.,2022), InCoder(Friedet al.,2023)or StarCoder(Liet al.,2023)are trained on code only, Codex(Chenet al.,2021)was fine-tuned from a general language model. We also start from a foundation model(Llama 2, Touvronet al.,2023b)pretrained on general-purpose text and code data. Our comparison (Section3.4.1) shows that initializing our model withLlama 2outperforms the same architecture trained on code only for a given budget. Infilling.Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does not provide the capability to fill a missing portion of text while taking the full surrounding context into account. Our code-training for 7B, 13B and 70BCode Llamamodels features a multitask objective(Friedet al.,2023)consisting of both autoregressive and causal infilling prediction, enabling applications such as real-time completion in source code editors or docstring generation. Long input contexts.Unlocking repository-level reasoning for completion or synthesis – as opposed to function-level or file-level – requires prompting the model with much longer context than the 4,096 tokens supported byLlama 2. We propose an additional fine-tuning stage that extends the maximum context length from 4,096 tokens to 100,000 tokens by modifying the parameters of the RoPE positional embeddings(Suet al.,2021)used inLlama 2. Our experiments showCode Llamaoperating on very large contexts with a moderate impact on performances on standard coding benchmarks (Section3.3). Instruction fine-tuning.For end-users, the utility of LLMs is significantly improved by instruction fine-tuning(Ouyanget al.,2022; Weiet al.,2022; OpenAI,2023; Touvronet al.,2023b), which also helps preventing unsafe, toxic or biased generations.Code Llama - Instructvariants are further fine-tuned on a mix of proprietary instruction data for improved safety and helpfulness, and a new machine-generatedself-instructdataset created by promptingLlama 2for coding problems andCode Llamato generate associated unit tests and solutions. Our results show thatCode Llama - Instructsignificantly improves performance on various truthfulness, toxicity and bias benchmarks at moderate cost in terms of code generation performance (Section4). Different combinations of these approaches lead to a family of code-specializedLlama 2models with three main variants that we release in four sizes (7B, 13B, 34B and 70B parameters): [leftmargin=*,noitemsep,topsep=0pt] Code Llama: a foundational model for code generation tasks, Code Llama - Python: specialized for Python, Code Llama - Instruct: fine-tuned with human instructions and self-instruct code synthesis data. Prompt:In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last monthResponse:You can use the ‘find‘ command in Bash to list all text files in the current directory (excluding subdirectories) that have been modified in the last month. The command would be:{minted}bash find . -maxdepth 1 -type f -mtime -30 -name "*.txt" Here’s a breakdown of the command: * ‘.‘ specifies the current directory.* ‘-maxdepth 1‘ tells ‘find‘ to only search the current directory and not any subdirectories.* ‘-type f‘ tells ‘find‘ to only search for files (not directories or other types of files).* ‘-mtime -30‘ tells ‘find‘ to only search for files that have been modified in the last 30 days.* ‘-name "*.txt"‘ tells ‘find‘ to only search for files with the ‘.txt‘ extension. Note that the ‘-mtime‘ option uses the number of days since the file was last modified, so ‘-mtime -30‘ means "modified in the last 30 days". If you want to search for files modified in the last month, you can use the ‘-mtime -30‘ option. An example of usingCode Llama - Instructis given inFigure1. It show-cases that the model interprets natural language to determine suitable options for a command-line program and provides an explanation of the solution. We provide further qualitative examples inAppendixL. We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval(Chenet al.,2021), MBPP(Austinet al.,2021), and APPS(Hendryckset al.,2021), as well as a multilingual version of HumanEval(MultiPL-E, Cassanoet al.,2023), where our best models establish a new state of the art amongst open-source LLMs. The technical details of our training and fine-tuning procedures are provided inSection2, followed by in-depth experiments and ablation studies, details of the safety/helpfulness evaluations and a discussion of related work.
Fluency score: 3
Fluency issues: [Unclear references like "seeSection5", "00footnotetext:", "11footnotemark:1"; inconsistent and unclear formatting throughout; fragmented list presentation ("Code Llama: a foundational model..."); complex sentence structures.]

Clarity score: 3
Clarity issues: [Fragmented list presentation; use of multiple acronyms and terms without adequate explanation; "00footnotetext:", "11footnotemark:1" which do not contribute to clarity.]

Grammaticality score: 4
Grammaticality issues: [Minor errors such as "Touvronet al.,2023b" should have a space "Touvron et al., 2023b".]

Readability score: 3
Readability issues: [Sections are mentioned without clear directions; dense and jargon-laden text; lack of clear formatting.]

Coherence score: 3
Coherence issues: [The text jumps abruptly between topics; unclear reference to figures and sections; use of inconsistent and unclear formatting.]
====================
	 Adding Conditional Control to Text-to-Image Diffusion Models
==========
Abstract:  We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.
Fluency score: 4
Fluency issues: Use of "eg," instead of "e.g.," or "for example,".

Clarity score: 4
Clarity issues: Might be technical for lay readers; "Extensive results show" could be more specific.

Grammaticality score: 5
Grammaticality issues: None.

Readability score: 4
Readability issues: The text is somewhat dense with technical jargon.

Coherence score: 4
Coherence issues: The jump from talking about ControlNet's robust training to its potential applications is somewhat abrupt.
====================
	 PaLM 2 Technical Report
==========
Abstract:  We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.   When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.
Fluency score: 4
Fluency issues: [No specific issues, but the text uses technical terms that might obscure fluency for non-expert readers]

Clarity score: 4
Clarity issues: ["user-facing products" and "these models" could be more clearly specified]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Complex sentence structure in the last sentence]

Coherence score: 4
Coherence issues: [Jump from discussing PaLM 2's capabilities to user-facing products could be smoother]
==========
Introduction:  Language modeling has long been an important research area sinceshannon_lmestimated the information in language with next word prediction. Modeling began withn-gram based approaches(kneser_ney)but rapidly advanced with LSTMs(Hochreiter;graves2014generating). Later work showed that language modelling also led to language understanding(NIPS2015_dai_le). With increased scale and the Transformer architecture(transformer_paper), large language models (LLMs) have shown strong performance in language understanding and generation capabilities over the last few years, leading to breakthrough performance in reasoning, math, science, and language tasks(howard-ruder-2018-universal;gpt3;du_glam_2021;chowdhery2022palm;rae2021scaling;lewkowycz2022solving;tay2023ul;openai2023gpt4). Key factors in these advances have been scaling up model size(gpt3;rae2021scaling)and the amount of data(hoffmann2022training). To date, most LLMs follow a standard recipe of mostly monolingual corpora with a language modeling objective. We introduce PaLM 2, the successor to PaLM(chowdhery2022palm), a language model unifying modeling advances, data improvements, and scaling insights. PaLM 2 incorporates the following diverse set of research advances: [noitemsep,topsep=0pt] Compute-optimal scaling: Recently, compute-optimal scaling(hoffmann2022training)showed that data size is at least as important as model size. We validate this study for larger amounts of compute and similarly find that data and model size should be scaled roughly 1:1 to achieve the best performance for a given amount of training compute (as opposed to past trends, which scaled the model 3\timesfaster than the dataset). Improved dataset mixtures: Previous large pre-trained language models typically used a dataset dominated by English text (e.g.,\sim78% of non-code inchowdhery2022palm). We designed a more multilingual and diverse pre-training mixture, which extends across hundreds of languages and domains (e.g., programming languages, mathematics, and parallel multilingual documents). We show that larger models can handle more disparate non-English datasets without causing a drop in English language understanding performance, and apply deduplication to reduce memorization(lee2021deduplicating) Architectural and objective improvements: Our model architecture is based on the Transformer. Past LLMs have almost exclusively used a single causal or masked language modeling objective. Given the strong results of UL2(tay2023ul), we use a tuned mixture of different pre-training objectives in this model to train the model to understand different aspects of language. The largest model in the PaLM 2 family, PaLM 2-L, is significantly smaller than the largest PaLM model but uses more training compute. Our evaluation results show that PaLM 2 models significantly outperform PaLM on a variety of tasks, including natural language generation, translation, and reasoning. These results suggest that model scaling is not the only way to improve performance. Instead, performance can be unlocked by meticulous data selection and efficient architecture/objectives. Moreover, a smaller but higher quality model significantly improves inference efficiency, reduces serving cost, and enables the model’s downstream application for more applications and users. PaLM 2 demonstrates significant multilingual language, code generation and reasoning abilities, which we illustrate in Figures2and3. More examples can be found in AppendixB.222Note that not all capabilities of PaLM 2 are currently exposed via PaLM 2 APIs.PaLM 2 performs significantly better than PaLM on real-world advanced language proficiency exams and passes exams in all evaluated languages (see Figure1). For some exams, this is a level of language proficiency sufficient to teach that language. In this report, generated samples and measured metrics are from the model itself without any external augmentations such as Google Search or Translate. PaLM 2 includes control tokens to enable inference-time control over toxicity, modifying only a fraction of pre-training as compared to prior work(korbak2023pretraining). Special ‘canary’ token sequences were injected into PaLM 2 pre-training data to enable improved measures of memorization across languages(carlini2019secret;carlini2021extracting). We find that PaLM 2 has lower average rates of verbatim memorization than PaLM, and for tail languages we observe that memorization rates increase above English only when data is repeated several times across documents. We show that PaLM 2 has improved multilingual toxicity classification capabilities, and evaluate potential harms and biases across a range of potential downstream uses. We also include an analysis of the representation of people in pre-training data. These sections help downstream developers assess potential harms in their specific application contexts(shelby2023identifying), so that they can prioritize additional procedural and technical safeguards earlier in development. The rest of this report focuses on describing the considerations that went into designing PaLM 2 and evaluating its capabilities.
Fluency score: 3
Fluency issues: [sinceshannon_lm, withn-gram, (kneser_ney), LSTMs(Hochreiter;graves2014generating), (NIPS2015_dai_le), (transformer_paper), (howard-ruder-2018-universal;gpt3;du_glam_2021;chowdhery2022palm;rae2021scaling;lewkowycz2022solving;tay2023ul;openai2023gpt4), (gpt3;rae2021scaling), (hoffmann2022training), 3\times, inchowdhery2022palm), (lee2021deduplicating), UL2(tay2023ul), Train the model, PaLM 2-L, Figures2and3, AppendixB.222, PaLM 2 APIs.PaLM, Figure1, (korbak2023pretraining), (carlini2019secret;carlini2021extracting), (shelby2023identifying)]

Clarity score: 4
Clarity issues: [3\timesfaster, AppendixB.222, PaLM 2 APIs.PaLM, (carlini2019secret;carlini2021extracting), Figures2and3]

Grammaticality score: 4
Grammaticality issues: [missing spaces after punctuation marks and references]

Readability score: 3
Readability issues: [jargon and parenthetical references, lack of spaces after citations and references]

Coherence score: 4
Coherence issues: [Complex sentence structures and nested parenthetical information disrupt flow]
====================
	 Direct Preference Optimization: Your Language Model is Secretly a Reward Model
==========
Abstract:  While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: Use of jargon like "RLHF," "PPO-based RLHF," and "DPO" without explanation.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 3
Readability issues: Dense technical terminology and complex sentence structure may hinder readability for non-expert readers.

Coherence score: 4
Coherence issues: Some readers may struggle to follow the progression of ideas without prior knowledge of the subject matter.
==========
Introduction:  Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities(Chowdhery et al.,2022; Brown et al.,2020b; Touvron et al.,2023; Bubeck et al.,2023). However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant tounderstandcommon programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to beawareof a common misconception believed by 50% of people, but we certainly do not want the model to claim this misconception to be true in 50% of queries about it! In other words, selecting the model’sdesired responses and behaviorfrom its very wideknowledge and abilitiesis crucial to building AI systems that are safe, performant, and controllable(Ouyang et al.,2022). While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline. At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF;(Christiano et al.,2017; Bai et al.,2022b)). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs. In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We proposeDirect Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model;Bradley and Terry (1952)) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data. Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.
Fluency score: 4
Fluency issues: [None]

Clarity score: 4
Clarity issues: ["tounderstandcommon" should be "to understand common", "beawareof" should be "be aware of", "responses and behaviorfrom" should be "responses and behavior from", "knowledge and abilitiesis" should be "knowledge and abilities is"]

Grammaticality score: 4
Grammaticality issues: [Missing spaces after semicolons in citations, missing spaces in "tounderstandcommon", "beawareof", "behaviorfrom", "abilitiesis"]

Readability score: 3
Readability issues: [Long sentences with multiple clauses and complex terminology can overwhelm the reader. Some phrases, such as "reward maximization with a KL-divergence constraint" or "relative log probability of preferred to dispreferred responses," are technical and may not be easily accessible to general audiences.]

Coherence score: 4
Coherence issues: [The overall structure is coherent, but the complexity of content and dense information presentation might pose challenges for coherence for some readers.]
====================
	 Visual Instruction Tuning
==========
Abstract:  Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.
Fluency score: [4]
Fluency issues: [No significant issues]

Clarity score: [4]
Clarity issues: [Uses jargon that might not be clear to all readers]

Grammaticality score: [5]
Grammaticality issues: [No issues]

Readability score: [4]
Readability issues: [Complex sentence structures and technical language make it less accessible]

Coherence score: [4]
Coherence issues: [Some concepts might require additional contextual information for full coherence]
==========
Introduction:  Humans interact with the world through many channels such as vision and language, as each individual channel has a unique advantage in representing and communicating certain concepts, and thus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language instructions, aligned with human intent to complete various real-world tasks in the wildaskell2021general;li2022elevater;li2023multimodal. To this end, the community has witnessed an emergent interest in developing language-augmented foundation vision modelsli2022elevater;gan2022vision, with strong capabilities in open-world visual understanding such as classification(radford2021learning;openclip;yuan2021florence;yang2022unicl;pham2021combined), detectionli2022grounded;zhong2022regionclip;liu2023grounding, segmentation(li2022language;zou2022generalized;zhang2023simple)and captioning(wang2022git;li2023blip), as well as visual generation and editing(DALLE2;LDM;PARTI;MAKEASCENE;Imagen;li2023gligen). We refer readers to theComputer Vision in the Wildreading list for a more up-to-date literature compilation(cvinw). In this line of work, each task is solved independently by one single large vision model, with the task instruction implicitly considered in the model design. Further, language is only utilized to describe the image content. While this allows language to play an important role in mapping visual signals to language semantics—a common channel for human communication, it leads to models that usually have a fixed interface with limited interactivity and adaptability to the user’s instructions. Large language models (LLM), on the other hand, have shown that language can play a wider role: a universal interface for a general-purpose assistant, where various task instructions can be explicitly represented in language and guide the end-to-end trained neural assistant to switch to the task of interest to solve it. For example, the recent success of ChatGPT(chatgpt)and GPT-4(gpt4)have demonstrated the power of aligned LLMs in following human instructions, and have stimulated tremendous interest in developing open-source LLMs. Among them, LLaMA(touvron2023llama)is an open-source LLM that matches the performance of GPT-3. Alpaca(alpaca), Vicuna(vicuna), GPT-4-LLM(peng2023instruction)utilize various machine-generated high-quality instruction-following samples to improve the LLM’s alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly, this line of work istext-only. In this paper, we presentvisual instruction-tuning, the first attempt to extend instruction-tuning to the language-image multimodal space, to pave the way towards building a general-purpose visual assistant. In particular, our paper makes the following contributions: [leftmargin=7.5mm] Multimodal instruction-following data. One key challenge is the lack of vision-language instruction-following data. We present a data reformation perspective and pipeline to convert image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4. Large multimodal models. We develop a large multimodal model (LMM), by connecting the open-set visual encoder of CLIP(radford2021learning)with the language decoder Vicunavicuna, and fine-tuning end-to-end on our generated instructional vision-language data. Our empirical study validates the effectiveness of using generated data for LMM instruction-tuning, and suggests practical tips for building a general-purpose instruction-following visual agent. When ensembled with GPT-4, our approach achieves SoTA on the Science QAlu2022learnmultimodal reasoning dataset. Multimodal instruction-following benchmark. We present LLaVA-Bench with two challenging benchmarks, with a diverse selection of paired images, instructions and detailed annotations. Open-source. We release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.
Fluency score: 3
Fluency issues: [presence of shorthand citations like "wildaskell2021general;", parentheses with missing space in front of them "(radford2021learning;openclip;" ]

Clarity score: 2
Clarity issues: [shorthand citations interrupting the flow; lack of clear introduction or summary of the methods; the structure of the last paragraph with "[leftmargin=7.5mm]" ]

Grammaticality score: 3
Grammaticality issues: [lack of space after semicolons in shorthand citations; inappropriate use of parentheses]

Readability score: 2
Readability issues: [dense text with heavy use of jargon and in-text citations making it difficult to identify key ideas; list presented as prose rather than bulleted or numbered]

Coherence score: 2
Coherence issues: [shorthand citations scattered throughout disrupt the logical flow; the sudden transition to discussing contributions without proper introduction]
====================
	 PaLM-E: An Embodied Multimodal Language Model
==========
Abstract:  Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.
Fluency score: 4
Fluency issues: [None mentioned specifically, but the complexity and technicality of language lower general fluency for a broader audience]

Clarity score: 3
Clarity issues: ["e.g., for robotics problems" could be more clearly integrated; "multi-modal sentences" is jargon; the use of "embodiments" is unclear]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [Complex sentence structures and technical jargon]

Coherence score: 4
Coherence issues: [Use of acronyms without definition (PaLM-E, OK-VQA); multiple complex ideas introduced rapidly]
====================
	 Baichuan 2: Open Large-scale Language Models
==========
Abstract:  Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: Use of abbreviations like "MMLU, CMMLU, GSM8K, and HumanEval" without explaining them can confuse readers unfamiliar with the context.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: The sentence structure is somewhat complex, which may hinder readability for some audiences.

Coherence score: 5
Coherence issues: None
==========
Introduction:  The field of large language models has witnessed promising and remarkable progress in recent years. The size of language models has grown from millions of parameters, such as ELMoPeterset al.(2018), GPT-1Radfordet al.(2018), to billions or even trillions of parameters such as GPT-3Brownet al.(2020), PaLMChowdheryet al.(2022); Anilet al.(2023)and Switch TransformersFeduset al.(2022). This increase in scale has led to significant improvements in the capabilities of language models, enabling more human-like fluency and the ability to perform a diverse range of natural language tasks. With the introduction of ChatGPTOpenAI (2022)from OpenAI, the power of these models to generate human-like text has captured widespread public attention. ChatGPT demonstrates strong language proficiency across a variety of domains, from conversing casually to explaining complex concepts. This breakthrough highlights the potential for large language models to automate tasks involving natural language generation and comprehension. While there have been exciting breakthroughs and applications of LLMs, most leading LLMs like GPT-4OpenAI (2023), PaLM-2Anilet al.(2023), and ClaudeClaude (2023)remain closed-sourced. Developers and researchers have limited access to the full model parameters, making it difficult for the community to deeply study or fine-tune these systems. More openness and transparency around LLMs could accelerate research and responsible development within this rapidly advancing field. LLaMATouvronet al.(2023b), a series of large language models developed by Meta containing up to 65 billion parameters, has significantly benefited the LLM research community by being fully open-sourced. The open nature of LLaMA, along with other open-source LLMs such as OPTZhanget al.(2022), BloomScaoet al.(2022), MPTMosaicML (2023)and FalconPenedoet al.(2023), enables researchers to freely access the models for examination, experimentation, and further development. This transparency and access distinguishes LLaMA from other proprietary LLMs. By providing full access, the open-source LLMs have accelerated research and advances in the field, leading to new models like AlpacaTaoriet al.(2023), VicunaChianget al.(2023), and othersWanget al.(2022); Zhuet al.(2023); Anandet al.(2023). However, most open-source large language models have focused primarily on English. For instance, the main data source for LLaMA is Common Crawl111https://commoncrawl.org/, which comprises 67% of LLaMA’s pre-training data but is filtered to English content only. Other open source LLMs such as MPTMosaicML (2023)and FalconPenedoet al.(2023)are also focused on English and have limited capabilities in other languages. This hinders the development and application of LLMs in specific languages, such as Chinese. In this technical report, we introduce Baichuan 2, a series of large-scale multilingual language models. Baichuan 2 has two separate models, Baichuan 2-7B with 7 billion parameters and Baichuan 2-13B with 13 billion parameters. Both models were trained on 2.6 trillion tokens, which to our knowledge is the largest to date, more than double that of Baichuan 1Baichuan (2023b,a). With such a massive amount of training data, Baichuan 2 achieves significant improvements over Baichuan 1. On general benchmarks like MMLUHendryckset al.(2021a), CMMLULiet al.(2023), and C-EvalHuanget al.(2023), Baichuan 2-7B achieves nearly 30% higher performance compared to Baichuan 1-7B. Specifically, Baichuan 2 is optimized to improve performance on math and code problems. On the GSM8KCobbeet al.(2021)and HumanEvalChenet al.(2021)evaluations, Baichuan 2 nearly doubles the results of the Baichuan 1. In addition, Baichuan 2 also demonstrates strong performance on medical and legal domain tasks. On benchmarks such as MedQAJinet al.(2021)and JEC-QAZhonget al.(2020), Baichuan 2 outperforms other open-source models, making it a suitable foundation model for domain-specific optimization. Additionally, we also released two chat models, Baichuan 2-7B-Chat and Baichuan 2-13B-Chat, optimized to follow human instructions. These models excel at dialogue and context understanding. We will elaborate on our approaches to improve the safety of Baichuan 2. By open-sourcing these models, we hope to enable the community to further improve the safety of large language models, facilitating more research on responsible LLMs development. Furthermore, in spirit of research collaboration and continuous improvement, we are also releasing the checkpoints of Baichuan 2 at various stages of training from 200 billion tokens up to the full 2.6 trillion tokens. We found that even for the 7 billion parameter model, performance continued to improve after training on more than 2.6 trillion tokens. By sharing these intermediary results, we hope to provide the community with greater insight into the training dynamics of Baichuan 2. Understanding these dynamics is key to unraveling the inner working mechanism of large language modelsBidermanet al.(2023b); Tirumalaet al.(2022). We believe the release of these checkpoints will pave the way for further advances in this rapidly developing field. In this technical report, we will also share some of the trials, errors, and lessons learned through training Baichuan 2. In the following sections, we will present detailed modifications made to the vanilla Transformer architecture and our training methodology. We will then describe our fine-tuning methods to align the foundation model with human preferences. Finally, we will benchmark the performance of our models against other LLMs on a set of standard tests. Throughout the report, we aim to provide transparency into our process, including unsuccessful experiments, to advance collective knowledge in developing LLMs. Baichuan 2’s foundation models and chat models are available for both research and commercial use athttps://github.com/baichuan-inc/Baichuan2
Fluency score: 4
Fluency issues: Overuse of abbreviations; needs occasional breaks to enhance readability.

Clarity score: 3
Clarity issues: Dense text with jargon, may be unclear to those unfamiliar with the subject matter; long sentences with complex structures.

Grammaticality score: 4
Grammaticality issues: Mostly grammatical, but compound sentences could be simplified.

Readability score: 3
Readability issues: Long paragraphs; heavy use of technical terms and citations impact smooth reading.

Coherence score: 4
Coherence issues: Generally coherent, but the text jumps between different topics and models, which could be better structured.
====================
	 The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only
==========
Abstract:  Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.
Fluency score: 4
Fluency issues: [No major issues, but clause ordering in sentences could be slightly improved for enhanced clarity.]

Clarity score: 4
Clarity issues: [Use of 'at variance with' can be confusing; technical terminology without explanation might confuse non-specialist readers.]

Grammaticality score: 5
Grammaticality issues: [No issues detected.]

Readability score: 4
Readability issues: [The complex sentence structure and use of technical jargon could be difficult for lay readers.]

Coherence score: 4
Coherence issues: [Minor coherence issue with transitioning from discussing curation to the introduction of the dataset.]
==========
Introduction:  Progress in natural language processing is increasingly driven by sheer compute scale alone(sevilla2022compute): as more compute is expended to train large language models (LLM), they gain and exhibit powerful emergent capabilities(brown2020language;weiemergent). To best benefit from scaling, recent scaling laws dictate that both model size and dataset size should jointly be increased(hoffmann2022training). This is at variance with earlier findings, which had argued that scaling should focus on model size first and foremost, with minimal data scaling(kaplan2020scaling). This joint scaling paradigm raises significant challenges: although plentiful, text data is not infinite, especially so when considerations on data quality and licensing are taken into account–leading some researchers to argue scaling may soon be bottlenecked by data availability(villalobos2022will). Concretely, optimally training a GPT-3 sized model (175B parameters) would require no less than 3,500 billion tokens of text according tohoffmann2022training. This is twice as much as the largest pretraining datasets ever demonstrated(hoffmann2022training;touvron2023llama), and ten times more than the largest publicly available English datasets such as OSCAR(OrtizSuarezSagotRomary2019), C4(2020t5), or The Pile(gao2020pile). Massively scaling-up pretraining data is made even more challenging by the fact LLMs are commonly trained using a mixture of web crawls and so-called “high-quality” data(brown2020language;gao2020pile). Typical high-quality corpora include curated sources of books, technical documents, human-selected web pages, or social media conversations. The increased diversity and quality brought forth by these curated corpora is believed to be a key component of performant models(scao2022language). Unfortunately, curation is labour intensive: typically, each source requires specialized processing, while yielding a limited amount of data. Furthermore, licensed sources raise legal challenges. Nevertheless, most pretraining data is still sourced from massive web crawls which can be scaled up to trillions of tokens with limited human intervention. However, the quality of this data has traditionally been seen as (much) inferior to that of the manually curated data sources. Even finely processed sources of web data, such as C4(2020t5)or OSCAR(OrtizSuarezSagotRomary2019), are regarded as inferior to curated corpora for LLMs(gopher;scao2022language), producing less performant models. To sustain the ever-increasing data needs of larger and larger LLMs, and to streamline data pipelines and reduce the need for human-intensive curation, we propose to explore how web data can be better processed to significantly improve its quality, resulting in models as capable, if not more capable, than models trained on curated corpora. We make the following contributions: We introduceRefinedWeb, a high-quality five trillion tokens web-only English pretraining dataset; We demonstrate thatweb data alone can result in models outperforming both public and private curated corpora, as captured by zero-shot benchmarks, challenging current views about data quality; We publicly release a 600B tokens extract of RefinedWeb, and 1/7B parameters LLMs trained on it, to serve as a new baseline high-quality web dataset for the natural language processing community.
Fluency score: 3
Fluency issues: [Misplaced parentheses and citation formatting, e.g., "sevilla2022compute", "according tohoffmann2022training"; inconsistent use of "web data" and "web-only data".]

Clarity score: 3
Clarity issues: [Complex sentence structures, overuse of jargon, and poor citation formatting obscure meaning.]

Grammaticality score: 4
Grammaticality issues: ["tohoffmann2022training" should be "to hoffmann2022training"; "1/7B parameters" is unclear.]

Readability score: 2
Readability issues: [Dense academic style with jargon, lack of clear citation format, and run-on sentences.]

Coherence score: 3
Coherence issues: [Frequent parenthetical interruptions, uneven flow between topics due to citation issues, and unclear progression of ideas.]
====================
	 InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning
==========
Abstract:  Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.
Fluency score: 5
Fluency issues: [None]

Clarity score: 4
Clarity issues: [the term "instruction tuning" could be unclear without further context; "held-in datasets" and "held-out datasets" might be confusing without clarification of terms]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [Complex sentences with technical jargon and acronyms could hinder readability for general audiences; terms like "Query Transformer" and specific performance metrics (e.g., "90.7% accuracy") may not be clear to lay readers]

Coherence score: 4
Coherence issues: [Assumes prior knowledge of the domain; uses jargon and acronyms without introduction (e.g., "BLIP-2," "InstructBLIP")]
==========
Introduction:  A longstanding aspiration of Artificial Intelligence (AI) research is to build a single model that can solve arbitrary tasks specified by the user. In natural language processing (NLP), instruction tuning[46,6]proves to be a promising approach toward that goal. By finetuning a large language model (LLM) on a wide range of tasks described by natural language instructions, instruction tuning enables the model to follow arbitrary instructions. Recently, instruction-tuned LLMs have also been leveraged for vision-language tasks. For example, BLIP-2[19]effectively adapts frozen instruction-tuned LLMs to understand visual inputs and exhibits preliminary capabilities to follow instructions in image-to-text generation. Compared to NLP tasks, vision-language tasks are more diverse in nature due to the additional visual inputs from various domains. This poses a greater challenge to a unified model that is supposed to generalize to diverse vision-language tasks, many unseen during training. Most previous work can be grouped into two approaches. The first approach, multitask learning[5,26], formulates various vision-language tasks into the same input-output format. However, we empirically find multitask learning without instructions (Table4) does not generalize well to unseen datasets and tasks. The second approach[19,2]extends a pre-trained LLM with additional visual components, and trains the visual components with image caption data. Nevertheless, such data are too limited to allow broad generalization to vision-language tasks that require more than visual descriptions. To address the aforementioned challenges, this paper presents InstructBLIP, a vision-language instruction tuning framework that enables general-purpose models to solve a wide range of visual-language tasks through a unified natural language interface. InstructBLIP uses a diverse set of instruction data to train a multimodal LLM. Specifically, we initialize training with a pre-trained BLIP-2 model consisting of an image encoder, an LLM, and a Query Transformer (Q-Former) to bridge the two. During instruction tuning, we finetune the Q-Former while keeping the image encoder and LLM frozen. Our paper makes the following key contributions: [leftmargin=2em] We perform a comprehensive and systematic study on vision-language instruction tuning. We transform 26 datasets into the instruction tuning format and group them into 11 task categories. We use 13 held-in datasets for instruction tuning and 13 held-out datasets for zero-shot evaluation. Moreover, we withhold four entire task categories for zero-shot evaluation at the task level. Exhaustive quantitative and qualitative results demonstrate the effectiveness of InstructBLIP on vision-language zero-shot generalization. We propose instruction-aware visual feature extraction, a novel mechanism that enables flexible and informative feature extraction according to the given instructions. Specifically, the textual instruction is given not only to the frozen LLM, but also to the Q-Former, so that it can extract instruction-aware visual features from the frozen image encoder. Also, we propose a balanced sampling strategy to synchronize learning progress across datasets. We evaluate and open-source a suite of InstructBLIP models using two families of LLMs: 1) FlanT5[6], an encoder-decoder LLM finetuned from T5[33]; 2) Vicuna[42], a decoder-only LLM finetuned from LLaMA[40]. The InstructBLIP models achieve state-of-the-art zero-shot performance on a wide range of vision-language tasks. Furthermore, InstructBLIP models lead to state-of-the-art finetuning performance when used as the model initialization on individual downstream tasks.
Fluency score: 3
Fluency issues: Awkward phrase "instruction tuning[46,6]proves" due to missing space; "BLIP-2[19]effectively adapts" has the same issue.

Clarity score: 3
Clarity issues: Acronyms like LLM, Q-Former need introduction; "follow instructions in image-to-text generation" is not entirely clear without context; Dense information throughout.

Grammaticality score: 4 
Grammaticality issues: Missing spaces after citations e.g., "instruction tuning[46,6]", "BLIP-2[19]", "multitask learning[5,26]", etc.

Readability score: 3 
Readability issues: Technical jargon and dense description of methods decrease readability for general audiences; list of contributions is not rendered correctly, likely due to missing bullet points.

Coherence score: 3
Coherence issues: The text jumps between different ideas and techniques too quickly, with limited transitions; The structure seems disorganized due to the abrupt listing of contributions which are not well integrated into the narrative.
====================
	 The Rise and Potential of Large Language Model Based Agents: A Survey
==========
Abstract:  For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.
Fluency score: 4
Fluency issues: [No specific parts to indicate; the text is generally fluent but may not flow perfectly in some areas due to complex structures]

Clarity score: 4
Clarity issues: ["actually" is informal and may disrupt flow; "a repository for the related papers" phrase is slightly awkward]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected]

Readability score: 4
Readability issues: [Complex sentence structures and specialized terminology may hinder ease of reading for a general audience]

Coherence score: 4
Coherence issues: [The switch from survey introduction to the repository mention at the end feels somewhat abrupt]
====================
	 Universal and Transferable Adversarial Attacks on Aligned Language Models
==========
Abstract:  Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.
Fluency score: 4
Fluency issues: [No specific issues but slightly complex sentence structures]

Clarity score: 3
Clarity issues: [Use of jargon "jailbreaks", "adversarial attack suffix", "aligning models" without explanation]

Grammaticality score: 5
Grammaticality issues: [No issues]

Readability score: 3
Readability issues: [Complex sentence structure and jargon]

Coherence score: 4
Coherence issues: [The paper's structure and techniques could be clearer]
==========
Introduction:  Large language models (LLMs) are typically trained on massive text corpora scraped from the internet, which are known to contain a substantial amount of objectionable content. Owing to this, recent LLM developers have taken to “aligning” such models via various finetuning mechanisms111“Alignment” can generically refer to many efforts to make AI systems better aligned with human values. Here we use it in the narrow sense adopted by the LLM community, that of ensuring that these models do not generate harmful content, although we believe our results will likely apply to other alignment objectives.; there are different methods employed for this task(Ouyang et al.,2022; Bai et al.,2022b; Korbak et al.,2023; Glaese et al.,2022), but the overall goal of these approaches is to attempt ensure that these LLMs do not generate harmful or objectionable responses to user queries. And at least on the surface, these attempts seem to succeed: public chatbots will not generate certain obviously-inappropriate content when asked directly. In a largely separate line of work, there has also been a great deal of effort invested into identifying (and ideally preventing)adversarial attackson machine learning models(Szegedy et al.,2014; Biggio et al.,2013; Papernot et al.,2016b; Carlini and Wagner,2017b). Most commonly raised in computer vision domains (though with some applications to other modalities, including text), it is well-established that adding small perturbations to the input of a machine learning model can drastically change its output. To a certain extent, similar approaches are already known to work against LLMs: there exist a number of published “jailbreaks”: carefully engineered prompts that result in aligned LLMs generating clearly objectionable content(Wei et al.,2023). Unlike traditional adversarial examples, however, these jailbreaks are typically crafted through human ingenuity—carefully setting up scenarios that intuitively lead the models astray—rather than automated methods, and thus they require substantial manual effort. Indeed, although there has been some work on automatic prompt-tuning for adversarial attacks on LLMs(Shin et al.,2020; Wen et al.,2023; Jones et al.,2023), this has traditionally proven to be a challenging task, with some papers explicitly mentioning that they had been unable to generate reliable attacks through automatic search methods(Carlini et al.,2023). This owes largely to the fact that, unlike image models, LLMs operate ondiscretetoken inputs, which both substantially limits the effective input dimensionality, and seems to induce a computationally difficult search. In this paper, however, we propose a new class of adversarial attacks that can in fact induce aligned language models to produce virtually any objectionable content. Specifically, given a (potentially harmful) user query, our attack appends an adversarialsuffixto the query that attempts to induce negative behavior. that is, the user’s original query is left intact, but we add additional tokens to attack the model. To choose these adversarial suffix tokens, our attack consists of three key elements; these elements have indeed existed in very similar forms in the literature, but we find that it is their careful combination that leads to reliably successful attacks in practice. Initial affirmative responses.As identified in past work(Wei et al.,2023; Carlini et al.,2023), one way to induce objectionable behavior in language models is to force the model to give (just a few tokens of) an affirmative response to a harmful query. As such, our attack targets the model to begin its response with “Sure, here is (content of query)” in response to a number of prompts eliciting undesirable behavior. Similar to past work, we find that just targeting thestartof the response in this manner switches the model into a kind of “mode” where it then produces the objectionable content immediately after in its response. Combined greedy and gradient-based discrete optimization.Optimizing over the adversarial suffix is challenging due to the fact that we need to optimize overdiscretetokens to maximize the log likelihood of the attack succeeding. To accomplish this, we leverage gradients at the token level to identify asetof promising single-token replacements, evaluate the loss of some number of candidates in this set, and select the best of the evaluated substitutions. The method is, in fact, similar to the AutoPrompt(Shin et al.,2020)approach, but with the (we find, practically quite important) difference that we search overallpossible tokens to replace at each step, rather than just a single one. Robust multi-prompt and multi-model attacks.Finally, in order to generate reliable attack suffixes, we find that it is important to create an attack that works not just for a single prompt on a single model, but formultipleprompts acrossmultiplemodels. In other words, we use our greedy gradient-based method to search for asinglesuffix string that was able to induce negative behavior across multiple different user prompts, and across three different models (in our case, Vicuna-7B and 13bZheng et al. (2023)and Guanoco-7BDettmers et al. (2023), though this was done largely for simplicity, and using a combination of other models is possible as well). Putting these three elements together, we find that we can reliably create adversarial suffixes that circumvent the alignment of a target language model. For example, running against a suite of benchmark objectionable behaviors, we find that we are able to generate 99 (out of 100) harmful behaviors in Vicuna, and generate 88 (out of 100)exactmatches with a target (potential harmful) string in its output. Furthermore, we find that the prompts achieve up to 84% success rates at attacking GPT-3.5 and GPT-4, and 66% for PaLM-2; success rates for Claude are substantially lower (2.1%), but notably the attacks stillcaninduce behavior that is otherwise never generated. Illustrative examples are shown in Figure1. Futhermore, our results highlight the importance of our specific optimizer: previous optimizers, specifically PEZ(Wen et al.,2023)(a gradient-based approach) and GBDA(Guo et al.,2021)(an approach using Gumbel-softmax reparameterization) are not able to achieve any exact output matches, whereas AutoPrompt(Shin et al.,2020)only achieves a 25% success rate, compared to our attack success rate of 88%. Overall, this work substantially pushes forward the state of the art in demonstrated adversarial attacks against such LLMs. It thus also raises an important question: if adversarial attacks against aligned language models follow a similar pattern to those against vision systems, what does this mean for the overall agenda of this approach to alignment? Specifically, in modern computer vision systems, adversarial attacks are still an omnipresent phenomenon. State-of-the-art methods to prevent adversarial examples(Madry et al.,2018; Cohen et al.,2019; Leino et al.,2021)are almost never used in practice, owing to 1) their computational inefficiency, 2) the fact that they lead to substantial performance drops (in terms of “clean” model performance) in the resulting systems, and 3) the fact that they can only prevent attacks usually against a narrowly-defined attack model. It remains to be seen how this “arms race” between adversarial attacks and defenses plays out in the LLM space, but historical precedent suggests that we should consider rigorous wholesale alternatives to current attempts, which aim at posthoc “repair” of underlying models that are already capable of generating harmful content. Prior to publishing this work, we shared preliminary results with OpenAI, Google, Meta, and Anthropic. We discuss ethical considerations and the broader impacts of this work further in Section6.
Fluency score: 3
Fluency issues: ["to attempt ensure," "identifying (and ideally preventing)adversarial," "operate ondiscretetoken," "that is, the user’s original query," "string that was able to induce"]

Clarity score: 3
Clarity issues: Lack of clarity due to run-on sentences, dense jargon, and parenthetical interruptions.

Grammaticality score: 3
Grammaticality issues: Missing spaces after quotations and parentheses, misplaced commas.

Readability score: 3
Readability issues: Long sentences with complex structures, technical jargon, and dense information.

Coherence score: 3
Coherence issues: Overuse of jargon and complex sentences may confuse the reader; the flow is periodically disrupted by parenthetical information.
====================
	 Tree of Thoughts: Deliberate Problem Solving with Large Language Models
==========
Abstract:  Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.
Fluency score: 4
Fluency issues: [No significant issues with fluency; the text is generally smooth.]

Clarity score: 4
Clarity issues: [Some complex phrases and specialized terminology may impact overall clarity for general audiences.]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected.]

Readability score: 4
Readability issues: [Sentences are quite long and complex, which may impact readability for some readers.]

Coherence score: 4
Coherence issues: [Specialized concepts are introduced rapidly, which might challenge coherence for readers unfamiliar with the subject matter.]
==========
Introduction:  Originally designed to generate text, scaled-up versions of language models (LMs) such as GPTRadford et al. (2018,2019); Brown et al. (2020); OpenAI (2023)and PaLMChowdhery et al. (2022)have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what problems would challenge the current paradigm, and what should be alternative mechanisms? The literature on human cognition provides some clues to answer these questions. Research on “dual process” models suggests that people have two modes in which they engage with decisions – a fast, automatic, unconscious mode (“System 1”) and a slow, deliberate, conscious mode (“System 2”)Sloman (1996); Stanovich (1999); Kahneman et al. (2002); Kahneman (2011). These two modes have previously been connected to a variety of mathematical models used in machine learning. For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative “model free” learning or more deliberative “model based” planningDaw et al. (2005). The simple associative token-level choices of LMs are also reminiscent of “System 1”, and thus might benefit from augmentation by a more deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions. To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950sNewell et al. (1959,1972). Newell and colleagues characterized problem solvingNewell et al. (1959)as search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. As Figure1illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where eachthoughtis a coherent language sequence that serves as an intermediate step toward problem solving (Table1). Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures2,4,6). This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking. Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4OpenAI (2023): Game of 24, Creative Writing, and Crosswords (Table1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search. We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems. We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs.
Fluency score: 4
Fluency issues: Unnatural placement and lack of spacing in citations (e.g. "GPTRadford et al. (2018,2019);")

Clarity score: 3
Clarity issues: Dense academic language; multiple references without explanation (e.g. "LMs", "System 1")

Grammaticality score: 3
Grammaticality issues: Incorrect punctuation and spacing; run-on sentences

Readability score: 3
Readability issues: Requires high level of expertise to understand; complex sentence structures

Coherence score: 3
Coherence issues: Jargon-heavy text; lack of clear introductory explanations for concepts
====================
	 Mixtral of Experts
==========
Abstract:  We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.
Fluency score: 4
Fluency issues: N/A

Clarity score: 4
Clarity issues: The use of "47B parameters" and "13B active parameters" may be unclear to readers unfamiliar with the terminology.

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 4
Readability issues: Use of technical terms and abbreviations such as "SMoE," "Mixtral," "Mistral 7B," and various model names may hinder comprehension for a general audience.

Coherence score: 4
Coherence issues: The transition from introducing the model to discussing its licenses is abrupt.
====================
	 Mistral 7B
==========
Abstract:  We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.
Fluency score: 4
Fluency issues: The text is mostly fluent but the use of jargon and abbreviations (GQA, SWA) without explanation can interrupt fluency for some readers.

Clarity score: 4
Clarity issues: Use of technical terms without explanation (grouped-query attention, sliding window attention, etc.), which may not be clear to all audiences.

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 3
Readability issues: Dense with technical jargon and complex phrases which might affect comprehension for non-specialist readers.

Coherence score: 4
Coherence issues: While the text is logically structured, the heavy use of domain-specific language may affect the coherence for lay readers.
====================
	 Efficient Memory Management for Large Language Model Serving with PagedAttention
==========
Abstract:  High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm
Fluency score: 4
Fluency issues: Overall, the text is slightly jargoned which might not be fluent for readers outside the field.

Clarity score: 3
Clarity issues: There's heavy use of domain-specific terminology (e.g., "KV cache," "fragmentation," "redundant duplication") without context which may not be clear to all readers.

Grammaticality score: 5
Grammaticality issues: There are no grammatical errors.

Readability score: 3
Readability issues: The text contains complex sentences and specialized terminology that requires prior knowledge, affecting readability for a general audience.

Coherence score: 4
Coherence issues: While the text is structured logically, the use of specialized language can inhibit coherence for readers not familiar with the topic.
==========
Introduction:  The emergence of large language models (LLMs) like GPT(OpenAI,2023b; Brownet al.,2020)and PaLM(Chowdheryet al.,2022)have enabled new applications such as programming assistants(Github,2022; Chenet al.,2021)and universal chatbots(OpenAI,2022; Google,2023)that are starting to profoundly impact our work and daily routines. Many cloud companies(OpenAI,2020; Services,2023)are racing to provide these applications as hosted services. However, running these applications is very expensive, requiring a large number of hardware accelerators such as GPUs. According to recent estimates, processing an LLM request can be 10\timesmore expensive than a traditional keyword query(Reuters,2023). Given these high costs, increasing the throughput—and hence reducing the cost per request—ofLLM servingsystems is becoming more important. At the core of LLMs lies an autoregressive Transformer model(Vaswaniet al.,2017). This model generates words (tokens),one at a time, based on the input (prompt) and the previous sequence of the output’s tokens it has generated so far. For each request, this expensive process is repeated until the model outputs a termination token. This sequential generation process makes the workloadmemory-bound, underutilizing the computation power of GPUs and limiting the serving throughput. Improving the throughput is possible by batching multiple requests together. However, to process many requests in a batch, the memory space for each request should be efficiently managed. For example, Fig.1(left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM. Approximately 65% of the memory is allocated for the model weights, which remain static during serving. Close to 30% of the memory is used to store the dynamic states of the requests. For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to asKV cache(Popeet al.,2022), which represent the context from earlier tokens to generate new output tokens in sequence. The remaining small percentage of memory is used for other data, including activations – the ephemeral tensors created when evaluating the LLM. Since the model weights are constant and the activations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size. When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig.1(right). In this paper, we observe that existing LLM serving systems(Yuet al.,2022; NVIDIA,2023a)fall short of managing the KV cache memory efficiently. This is mainly because they store the KV cache of a request in contiguous memory space, as most deep learning frameworks(Paszkeet al.,2019; Olstonet al.,2017)require tensors to be stored in contiguous memory. However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique characteristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori. These characteristics make the existing systems’ approach significantly inefficient in two ways: First, the existing systems(Yuet al.,2022; NVIDIA,2023a)suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, theypre-allocatea contiguous chunk of memory with the request’s maximum length (e.g., 2048 tokens). This can result in severe internal fragmentation, since the request’s actual length can be much shorter than its maximum length (e.g., Fig.11). Moreover, even if the actual length is known a priori, the pre-allocation is still inefficient: As the entire chunk is reserved during the request’s lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused. Besides, external memory fragmentation can also be significant, since the pre-allocated size can be different for each request. Indeed, our profiling results in Fig.2show that only 20.4% - 38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportunities for memory sharing. LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request. In these scenarios, the request consists of multiple sequences that can partially share their KV cache. However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. To address the above limitations, we proposePagedAttention, an attention algorithm inspired by the operating system’s (OS) solution to memory fragmentation and sharing:virtual memory with paging. PagedAttention divides the request’s KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens. In PagedAttention, the blocks for the KV cache are not necessarily stored in contiguous space. Therefore, we can manage the KV cache in a more flexible way as in OS’s virtual memory: one can think of blocks as pages, tokens as bytes, and requests as processes. This design alleviates internal fragmentation by using relatively small blocks and allocating them on demand. Moreover, it eliminates external fragmentation as all blocks have the same size. Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. In this work, we buildvLLM, a high-throughput distributed LLM serving engine on top of PagedAttention that achieves near-zero waste in KV cache memory. vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention. vLLM supports popular LLMs such as GPT(Brownet al.,2020), OPT(Zhanget al.,2022), and LLaMA(Touvronet al.,2023)with varying sizes, including the ones exceeding the memory capacity of a single GPU. Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by 2-4\timescompared to the state-of-the-art systems(Yuet al.,2022; NVIDIA,2023a), without affecting the model accuracy at all. The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms (§4.3). In summary, we make the following contributions: [noitemsep,topsep=0pt,leftmargin=*] We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance. We propose PagedAttention, an attention algorithm that operates on KV cache stored in non-contiguous paged memory, which is inspired by the virtual memory and paging in OS. We design and implement vLLM, a distributed LLM serving engine built on top of PagedAttention. We evaluate vLLM on various scenarios and demonstrate that it substantially outperforms the previous state-of-the-art solutions such as FasterTransformer(NVIDIA,2023a)and Orca(Yuet al.,2022).
Fluency score: 3
Fluency issues: Inconsistent formatting (e.g., space missing after "GPT"), difficult to follow.

Clarity score: 2
Clarity issues: Overly technical jargon and lack of introductory explanations for non-expert readers.

Grammaticality score: 4
Grammaticality issues: Minor issues such as missing spaces after citations and inconsistent use of hyphens.

Readability score: 2
Readability issues: Dense, academic-style writing with long sentences and technical details.

Coherence score: 3
Coherence issues: Dense information without clear transitions; use of undefined terms and acronyms may confuse readers.
====================
	 RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control
==========
Abstract:  We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: The extensive use of technical jargon ("co-fine-tune state-of-the-art vision-language models", "vision-language-action models", "chain of thought reasoning") may be confusing for readers not familiar with the field.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Sentences are lengthy and complex, which may challenge less expert readers.

Coherence score: 4
Coherence issues: While the text logically connects ideas, the density of information and specialized terminology might impede smooth reading for a broad audience.
==========
Introduction:  High-capacity models pretrained on broad web-scale datasets provide an effective and powerful platform for a wide range of downstream tasks: large language models can enable not only fluent text generation(Brohan et al.,2022; OpenAI,2023; Anil et al.,2023)but emergent problem-solving(Cobbe et al.,2021; Lewkowycz et al.,2022; Polu et al.,2022)and creative generation of prose(Brown et al.,2020; OpenAI,2023)and code(Chen et al.,2021), while vision-language models enable open-vocabulary visual recognition(Radford et al.,2021; Minderer et al.,2022; Kirillov et al.,2023)and can even make complex inferences about object-agent interactions in images(Alayrac et al.,2022; Hao et al.,2022; Wang et al.,2022; Chen et al.,2023b,a; Driess et al.,2023; Huang et al.,2023). Such semantic reasoning, problem solving, and visual interpretation capabilities would be tremendously useful for generalist robots that must perform a variety of tasks in real-world environments. However, it is unclear how robots should acquire such capabilities. While a brute force approach might entail collecting millions of robotic interaction trials, the most capable language and vision-language models are trained on billions of tokens and images from the web(Alayrac et al.,2022; Chen et al.,2023b,a; Huang et al.,2023)– an amount unlikely to be matched with robot data in the near future. On the other hand, directly applying such models to robotic tasks is also difficult: such models reason about semantics, labels, and textual prompts, whereas robots require grounded low-level actions, such as Cartesian end-effector commands. While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics(Ahn et al.,2022; Driess et al.,2023; Vemprala et al.,2023), such methods generally address only the “higher level” aspects of robotic planning, essentially taking the role of a state machine that interprets commands and parses them into individual primitives (such as picking and placing objects), which are then executed by separate low-level controllers that themselves do not benefit from the rich semantic knowledge of Internet-scale models during training. Therefore, in this paper we ask: can large pretrained vision-language models be integrated directly into low-level robotic control to boost generalization and enable emergent semantic reasoning? To this end, we explore an approach that is both simple and surprisingly effective: we directly train vision-language models designed for open-vocabulary visual question answering and visual dialogue to output low-level robot actions, along with solving other Internet-scale vision-language tasks. Although such models are typically trained to produce natural language tokens, we can train them on robotic trajectories bytokenizing the actions into text tokensand creating “multimodal sentences”(Driess et al.,2023)that “respond” to robotic instructions paired with camera observations by producing corresponding actions. In this way, vision-language models can be directly trained to act as instruction following robotic policies. This simple approach is in contrast with prior alternatives for incorporating VLMs into robot policies(Shridhar et al.,2022a)or designing new vision-language-action architectures from scratch(Reed et al.,2022): instead, pre-existing vision-language models, with already-amortized significant compute investment, are trained without any new parameters to output text-encoded actions. We refer to this category of models as vision-language-action (VLA) models. We instantiate VLA models by building on the protocol proposed for RT-1(Brohan et al.,2022), using a similar dataset, but expanding the model to use a large vision-language backbone. Hence we refer to our model as RT-2 (Robotics Transformer 2). We provide an overview in Figure1. We observe that robotic policies derived from such vision-language models exhibit a range of remarkable capabilities, combining the physical motions learned from the robot data with the ability to interpret images and text learned from web data into a single model. Besides the expected benefit of dramatically improving generalization to novel objects and semantically varied instructions, we observe a number of emergent capabilities. While the model’s physical skills are still limited to the distribution of skills seen in the robot data, the model acquires the ability to deploy those skills in new ways by interpreting images and language commands using knowledge gleaned from the web. Some example highlights are shown in Figure2. The model is able to re-purpose pick and place skills learned from robot data to place objects near semantically indicated locations, such as specific numbers or icons, despite those cues not being present in the robot data. The model can also interpret relations between objects to determine which object to pick and where to place it, despite no such relations being provided in the robot demonstrations. Furthermore, if we augment the command with chain of thought prompting, the model is able to make even more complex semantic inferences, such as figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink). Our main contribution is RT-2, a family of models derived from fine-tuning large vision-language models trained on web-scale data to directly act as generalizable and semantically aware robotic policies. Our experiments investigate models with up to 55B parameters trained on Internet data and instruction-annotated robotic trajectories from previous work(Brohan et al.,2022). Over the course of 6k robotic evaluations, we show that RT-2 enable significant improvements to generalization over objects, scenes, and instructions, and exhibit a breadth of emergent capabilities inherited from web-scale vision-language pretraining.
Fluency score: 3
Fluency issues: Punctuation issues around citations (e.g., "generation(Brohan et al.,2022;"), inconsistent use of hyphenation in "web-scale" and "Internet-scale", run-on sentences making the text less conversational.

Clarity score: 3
Clarity issues: Dense text with multiple concepts introduced without clear separation can confuse readers, heavy use of jargon and citations without explanation.

Grammaticality score: 4
Grammaticality issues: Missing spaces after citations, but overall grammar is acceptable.

Readability score: 3
Readability issues: Long sentences with complex structures and multiple parenthetical citations make the text hard to follow.

Coherence score: 3
Coherence issues: The text contains several ideas that are not always clearly connected, and the use of parenthetical citations disrupts the flow.
====================
	 MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models
==========
Abstract:  The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.
Fluency score: 4
Fluency issues: [None]

Clarity score: 4
Clarity issues: ["...using one projection layer."]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: ["...from the utilization of sophisticated large language models (LLM)." "Our work, for the first time, uncovers that properly aligning the visual features..."]

Coherence score: 4
Coherence issues: ["...using one projection layer." "Our work, for the first time, uncovers that properly aligning the visual features..."]
==========
Introduction:  In recent years, large language models (LLMs) have experienced rapid advancements(Ouyanget al.,2022; OpenAI,2022; Brownet al.,2020; Scaoet al.,2022a; Touvronet al.,2023; Chowdheryet al.,2022; Hoffmannet al.,2022). With exceptional language understanding capabilities, these models can perform a variety of intricate linguistic tasks in a zero-shot manner. Notably, GPT-4, a large-scale multimodal model, has been recently introduced and demonstrated several impressive capabilities of vision-language understanding and generation(OpenAI,2023). For example, GPT-4 can produce detailed and accurate image descriptions, explain unusual visual phenomena, and even construct websites based on handwritten text instructions. Although GPT-4 has exhibited remarkable vision language capabilities, the methods behind its exceptional abilities are still a mystery(OpenAI,2023). We believe that these impressive skills may stem from the utilization of a more advanced large language model (LLM). LLMs have demonstrated various emergent abilities, as evidenced in GPT-3’s few-shot prompting setup(Brownet al.,2020)and the findings of Weiet al. (2022)(Weiet al.,2022). Such emergent properties are hard to find in smaller-scale models. It is conjectured that these emergent abilities are also applicable to multi-modal models, which could be the foundation of GPT-4’s impressive visual description capabilities. To substantiate our hypothesis, we present a novel vision-language model named MiniGPT-4. It utilizes an advanced large language model (LLM), Vicuna(Chianget al.,2023), which is built upon LLaMA(Touvronet al.,2023)and reported to achieve 90% of ChatGPT’s quality as per GPT-4’s evaluation, as the language decoder. In terms of visual perception, we employ the same pretrained vision components of BLIP-2(Liet al.,2023)that consists of a ViT-G/14 from EVA-CLIP(Fanget al.,2022)and a Q-Former network. MiniGPT-4 adds a single projection layer to align the encoded visual features with the Vicuna language model and freezes all the other vision and language components. MiniGPT-4 is initially trained for 20k steps using a batch size of 256 on 4 A100 GPUs, leveraging a combined image captioning dataset that includes images from LAION(Schuhmannet al.,2021), Conceptual Captions(Changpinyoet al.,2021; Sharmaet al.,2018), and SBU(Ordonezet al.,2011)to align visual features with the Vicuna language model. Nevertheless, merely aligning visual features with the language model (LLM) is inadequate to ensure robust visual conversation capabilities, resembling that of a chatbot. The presence of underlying noise in raw image-text pairs can lead to subpar language outputs. Therefore, we collect another 3,500 detailed image description pairs to further fine-tune the model with a designed conversational template in order to improve the naturalness of the generated language and its usability. In our experiments, we discovered that MiniGPT-4 possesses numerous capabilities similar to those demonstrated by GPT-4. For instance, MiniGPT-4 can generate intricate image descriptions, create websites based on handwritten text instructions, and explain unusual visual phenomena. Furthermore, our findings revealed that MiniGPT-4 also has a variety of other intriguing abilities not showcased in the GPT-4 demonstrations. For example, MiniGPT-4 can directly generate detailed cooking recipes from food photos, write stories or poems inspired by images, write advertisements for products in images, identify problems shown in photos and provide corresponding solutions, and retrieve rich facts about people, movies, or art directly from images, among other capabilities. These abilities are absent in previous vision-language models like Kosmos-1(Huanget al.,2023)and BLIP-2(Liet al.,2023)that use less powerful language models. This further validates that integrating visual features with an advanced language model is one of the keys to enhancing vision-language models. We present a summary of our key findings: Our research reveals with compelling evidence that by aligning visual features with advanced large language models like Vicuna, MiniGPT-4 can achieve advanced vision-language capabilities comparable to those exhibited in the GPT-4 demonstrations. Our findings suggest that training merely one projection layer can effectively align a pretrained vision encoder with the large language model. Our MiniGPT-4 only requires training approximately 10 hours on 4 A100 GPUs. We discovered that simply aligning visual features with large language models using short image caption pairs is not sufficient for developing a well-performing model and leads to unnatural language generation. Further finetuning with a small but detailed image description pairs can address this limitation and significantly improves its usability.
Fluency score: 4
Fluency issues: [Misplaced brackets, slight redundancy "still a mystery" and "methods behind...are still"]

Clarity score: 3
Clarity issues: [Overuse of acronyms, dense technical information, long sentences]

Grammaticality score: 4
Grammaticality issues: [Inconsistent citations (spacing and punctuation)]

Readability score: 3
Readability issues: [Dense jargon, complex sentence structure, acronyms without first defining them]

Coherence score: 4
Coherence issues: [Jumps between different ideas may confuse readers]
====================
	 A Survey of Large Language Models
==========
Abstract:  Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.
Fluency score: 4
Fluency issues: [Repetitive use of "recently", "significant" twice in close proximity]

Clarity score: 4
Clarity issues: [The sentence structure in "Since researchers have found... small-scale language models." is complex and might be hard to follow; "Besides" at the beginning of the final sentence is informal]

Grammaticality score: 5
Grammaticality issues: []

Readability score: 4
Readability issues: [Complex sentence structures and academic vocabulary may challenge some readers]

Coherence score: 5
Coherence issues: []
====================
	 Lost in the Middle: How Language Models Use Long Contexts
==========
Abstract:  While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.
Fluency score: 5
Fluency issues: [None]

Clarity score: 4
Clarity issues: [use of "explicitly long-context models" may confuse some readers]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Complex sentence structures may hinder some readers]

Coherence score: 5
Coherence issues: [None]
==========
Introduction:  Language models have become an important and flexible building block in a variety of user-facing language technologies, including conversational interfaces, search and summarization, and collaborative writing(Shusteret al.,2022; Thoppilanet al.,2022; Leeet al.,2022,inter alia). These models perform downstream tasks primarily via prompting: all relevant task specification and data to process is formatted as a textual input context, and the model returns a generated text completion. These input contexts can contain thousands of tokens, especially when language models are used to process long documents (e.g., legal or scientific documents, conversation histories, etc.) or when language models are augmented with external information (e.g., relevant documents from a search engine, database query results, etc;Petroniet al.,2020; Ramet al.,2023; Shiet al.,2023; Mallenet al.,2023; Schicket al.,2023,inter alia). Handling these use-cases requires language models to successfully operate over long sequences. Existing language models are generally implemented with Transformers(Vaswaniet al.,2017), which require memory and compute that increases quadratically in sequence length. As a result, Transformer language models were often trained with relatively small context windows (between 512-2048 tokens). Recent improvements in hardware (e.g., faster GPUs with more memory) and algorithms(Daiet al.,2019; Daoet al.,2022; Poliet al.,2023; Rubin and Berant,2023,inter alia)have resulted in language models with larger context windows (e.g., 4096, 32K, and even 100K tokens), but it remains unclear how these extended-context language models make use of their input contexts when performing downstream tasks. We empirically investigate this question via controlled experiments with a variety of state-of-the-art open (MPT-30B-Instruct, LongChat-13B (16K)) and closed (OpenAI’s GPT-3.5-Turbo and Anthropic’s Claude-1.3) language models in settings that require accessing and using information within an input context. In particular, our experiments make controlled changes to the input context size and the position of the relevant information within the input context and study their effects on language model performance. If language models can robustly use information within long input contexts, then their performance should beminimally affectedby the position of the relevant information in the input context. We first experiment with multi-document question answering, which requires models to reason over provided documents to find relevant information and use it to answer a given question; this task mimics the retrieval-augmented generation setup underlying many commercial generative search and question answering applications (e.g., Bing Chat). In this setting, we control (i) the input context length by changing the number of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to place the relevant document at the beginning, middle or end of the context. We find that changing the position of relevant information in the input context can substantially affect model performance, indicating that current language models do not robustly access and use information in long input contexts. Furthermore, we observe a distinctive U-shaped performance curve (Figure1); language model performance is highest when relevant information occurs at the very beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and use information in the middle of their input context (§2.3). For example, when relevant information is placed in the middle of its input context, GPT-3.5-Turbo’s performance on the multi-document question task is lower than its performance when predictingwithout any documents(i.e., the closed-book setting; 56.1%). Furthermore, we find that models often have identical performance to their extended-context counterparts, indicating that extended-context models are not necessarily better at using their input context (§2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can language models evenretrievefrom their input contexts? We study this question with a synthetic key-value retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching tokens from the input context. In this task, models are given a collection of JSON-formatted key-value pairs and must return the value associated with a specific key. Similar to the multi-document QA task, the key-value retrieval task admits controlled changes to the input context length (adding more key-value pairs) and the position of relevant information. Although some models perform the synthetic key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. To better understand why language models struggle to robustly access and use information in their input contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), query-aware contextualization, and instruction fine-tuning (§4). We find that: Encoder-decoder models are relatively robust to changes in the position of relevant information within their input context, but only when evaluated on sequences within its training-time sequence length. When evaluated on sequences longer than those seen during training, we observe a U-shaped performance curve (§4.1). Query-aware contextualization (placing the query beforeandafter the documents or key-value pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA (§4.2). Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of relevant information in the input context. Our results indicate that prompting language models with longer input contexts is a trade-off—providing the language model with more information may help it perform the downstream task, but it also increases the amount of content that the model must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader models on open-domain question answering (§5). In contrast to our controlled multi-document QA task, where the context always contains exactlyonedocument that answers the question, none or many of the topkdocuments may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from NaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents—using 50 documents instead of 20 retrieved documents only marginally improves performance (\sim1.5% for GPT-3.5-Turbo and\sim1% for claude-1.3). Our analysis provides a better understanding of how language models use their input context and introduces new evaluation protocols for future long-context models; to claim that a language model can robustly use information within long input contexts, it is necessary to show that its performance is minimally affected by the position of the relevant information in the input context (e.g., minimal difference in best- and worst-case performance). To facilitate further work on understanding and improving how language models use their input context, we release our code and evaluation data.111nelsonliu.me/papers/lost-in-the-middle
Fluency score: 3
Fluency issues: [Run-on sentences; excessive use of parentheses and excessive technical jargon can disrupt natural flow]

Clarity score: 3
Clarity issues: [Overuse of domain-specific language and complex sentence structures diffuses the main point]

Grammaticality score: 4
Grammaticality issues: [Minor punctuation issues such as missing spaces after commas]

Readability score: 3
Readability issues: [Dense technical content; complex and lengthy sentences]

Coherence score: 3
Coherence issues: [Text is challenging to follow due to abrupt transitions and dense presentation of information]
====================
	 SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis
==========
Abstract:  We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models
Fluency score: 4
Fluency issues: [Slightly complex sentence structures that might hinder smooth reading for some readers]

Clarity score: 4
Clarity issues: [Use of jargon without explanation (e.g., "UNet", "cross-attention context", "latent diffusion model") might be confusing for readers not familiar with the field]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 4
Readability issues: [Complex sentence structures and use of technical language can make the text less accessible to a lay audience]

Coherence score: 5
Coherence issues: [No issues detected; the text logically presents the progression from presenting the model to its performance and sharing resources]
====================
	 Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models
==========
Abstract:  While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
Fluency score: 4
Fluency issues: [No significant issues; minor improvement could enhance the flow.]

Clarity score: 4
Clarity issues: ["downstream tasks" and "LLM hallucination phenomena" might be unclear to lay readers.] 

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Lengthy sentences with complex structure could impede ease of reading for some audiences.]

Coherence score: 4
Coherence issues: [Use of specialized terminology without explanations may affect coherence for general audience.]
==========
Introduction:  Large language models (LLMs), particularly characterized by their substantial number of parameters, have arisen as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence(zhao2023survey). With proper alignment techniques, such as supervised finetuning(SFT;zhang2023instruction)and reinforcement learning from human feedback(RLHF;ouyang2022training;fernandes2023bridging), recent LLMs(chatgpt;touvron2023llama2;openai2023gpt4,inter alia)have exhibited strong capabilities in solving various downstream tasks. Nonetheless, as exemplified in Figure1, LLMs, despite their remarkable success, occasionally produce outputs that, while seemingly plausible, deviate from user input(adlakha2023evaluating), previously generated context(liu-etal-2022-token), or factual knowledge(min2023factscore;muhlgay2023generating;li2023halueval)—this phenomenon is commonly referred to as hallucination, which significantly undermines the reliability of LLMs in real-world scenarios(kaddour2023challenges). For instance, LLMs can potentially fabricate erroneous medical diagnoses or treatment plans that lead to tangible real-life risks(umapathi2023med). While hallucination in conventional natural language generation (NLG) settings has been widely studied(nlg_hallucination_survey), understanding and addressing the hallucination problem within the realm of LLMs encounters unique challenges introduced by [leftmargin=*,topsep=2pt] Massive training data: in contrast to carefully curating data for a specific task, LLM pre-training uses trillions of tokens obtained from the web, making it difficult to eliminate fabricated, outdated or biased information; Versatility of LLMs: general-purpose LLMs are expected to excel in cross-task, cross-lingual, and cross-domain settings, posing challenges for comprehensive evaluation and mitigation of hallucination. Imperceptibility of errors: as a byproduct of their strong abilities, LLMs may generate false information that initially seems highly plausible, making it challenging for models or even humans to detect hallucination. In addition, the RLHF process(ouyang2022training), the vague knowledge boundary(ren2023boundary)and the black-box property of LLMs(sun2022black)also complicate the detection, explanation, and mitigation of hallucination in LLMs. There has been a notable upsurge in cutting-edge research dedicated to addressing the aforementioned challenges, which strongly motivates us to compile this survey. We organize this paper as follows, as also depicted in Figure2. We first introduce the background of LLMs and offer our definition of hallucination in LLMs (§2). Next, we introduce relevant benchmarks and metrics (§3). Subsequently, we discuss potential sources of LLM hallucinations (§4), and provide an in-depth review of recent work towards addressing the problem (§5). Finally, we present forward-looking perspectives (§6). We will consistently update the related open-source materials, which can be accessed athttps://github.com/HillZhang1999/llm-hallucination-survey.
Fluency score: 4
Fluency issues: [use of technical terms without explaining ("supervised finetuning", "reinforcement learning from human feedback"); inconsistent citation formatting; in-text citation missing a space before it]

Clarity score: 3
Clarity issues: [multiple parenthetical citations interrupt text flow; technical jargon without explanation; lack of clarity in the explanation of "hallucination"]

Grammaticality score: 4
Grammaticality issues: [inconsistent use of parentheses and spacing in in-text citations; missing space in "(SFT;zhang2023instruction)" and similar parts]

Readability score: 3
Readability issues: [dense with jargon and technical terms; excessive in-line citations disrupt reading; use of semicolons in in-text citations is unconventional and distracting]

Coherence score: 4
Coherence issues: [jumps abruptly between topics; use of bullets or numbering for points listed under "[leftmargin=*,topsep=2pt]"]
====================
	 mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality
==========
Abstract:  Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.
Fluency score: 4
Fluency issues: [The text is generally fluent but uses technical jargon that may be inaccessible to laypersons (e.g., "modularized learning," "visual knowledge module," "visual abstractor module," "low-rank adaption (LoRA) module"). This tends to interrupt the smooth reading flow for non-expert readers.]

Clarity score: 3
Clarity issues: [Phrases like "modality collaboration" and "multi-modal supervised datasets" are not well-explained for clarity. "Language-only and multi-modal" creates temporary ambiguity—does "language-only" apply to "supervised datasets" or "multi-modal"?]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected.]

Readability score: 3
Readability issues: [Dense technical language and frequent use of complex, novel terms decrease readability for a general audience.]

Coherence score: 4
Coherence issues: [While the text follows a logical structure, the introduction of many specialized terms without explicit definitions or clear connections can hinder coherence for readers unfamiliar with the subject matter.]
==========
Introduction:  Large language models (LLMs) such as GPT-3(Brownet al.,2020), BLOOM(Scaoet al.,2022), LLaMA(Touvronet al.,2023)have experienced rapid development to make general artificial intelligence possible, which demonstrates impressive zero-shot abilities on various linguistic applications. However, except GPT-4(OpenAI,2023), current general LLMs cannot support different modalities of input and develop impressive multimodal abilities. Although GPT-4(OpenAI,2023)has exhibited remarkable multimodal abilities, the methods behind its extraordinary abilities remain a mystery. Recently, researchers have been extending LLMs to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. However, systematic collaboration approaches, including Visual ChatGPT(Wuet al.,2023), MM-REACT(Yanget al.,2023), and HuggingGPT(Shenet al.,2023), are designed to facilitate the coordination of various vision models or tools to express visual information with text descriptions. However, these approaches may not be able to comprehend specific multimodal instructions due to their lack of alignment with different modalities. Additionally, these approaches may encounter challenges related to inference efficiency and cost. End-to-end models, such as BLIP-2(Liet al.,2023), LLaVA(Liuet al.,2023), and MiniGPT-4(Zhuet al.,2023a)aim to use unified models to support different modalities. However, these models have some limitations as they take frozen visual models, which may lead to inadequate alignment due to the limited number of parameters. Moreover, they cannot unlock various abilities due to missing unimodal and multimodal instruction. In this paper, we present mPLUG-Owl with an innovative modularized training paradigm for large multi-modal language models that can support multiple modalities concurrently, drawing inspiration from the concept of modularization(Xuet al.,2023b; Liet al.,2022; Xuet al.,2021; Yeet al.,2022). Our method harnesses the power of pre-trained LLM, visual knowledge module, and connected visual abstractor module to achieve effective alignment between images and text, and utilizes a two-stage training scheme to stimulate impressive unimodal and multimodal abilities. Our approach even enhances the strong generation abilities of LLM by modality collaboration between modalities. In the first step, we align the image and text to acquire comprehensive visual knowledge using text-image pairs, which is accomplished by training the visual knowledge module and abstractor module with the frozen LLM module. Subsequently, we fine-tune mPLUG-Owl with language-only and multi-modal instructions to unlock a range of unimodal and multimodal abilities. We freeze the visual knowledge module and train low-rank adaption (LoRA)(Huet al.,2022)on LLM and visual abstractor module jointly. This approach allows for the effective integration of textual and visual information, facilitating the development of versatile and robust cognitive abilities. Our experiments on a carefully-built visually related instruction evaluation set OwlEval shows that mPLUG-Owl outperforms existing models such as MiniGPT-4(Zhuet al.,2023a)and LLaVA(Liuet al.,2023). We separately verifies mPLUG-Owl’s remarkable abilities in instruction understanding, visual understanding, knowledge transfer, and multi-turn dialogue. Abundant ablation study is performed to show the effectiveness of our training paradigm. Furthermore, we find some unexpected emerging ability such as multi-image correlation, multilingual conversation and scene text understanding. Our main contributions can be highlighted as follows: We propose mPLUG-Owl, a novel training paradigm for large language models through modularization. We carefully construct an instruction evaluation set, dubbedOwlEval, to assess the capabilities of different models in the context of visual-related tasks. Experimental results demonstrate that mPLUG-Owl excels in multi-modal instruction understanding and multi-turn dialogue, surpassing the performance of existing models.
Fluency score: 4
Fluency issues: [Missing spaces after parentheses "GPT-3(Brown", "BLOOM(Scao", "LLaMA(Touvron", "GPT-4(OpenAI", "ChatGPT(Wu", "MM-REACT(Yang", "HuggingGPT(Shen", "BLIP-2(Li", "LLaVA(Liu", "MiniGPT-4(Zhu", "modularization(Xu", "adaption (Hu", "MiniGPT-4(Zhu" and "LLaVA(Liu". Unnatural sentence structure and phrasing like "we separately verifies" and "abundant ablation study is performed".]

Clarity score: 3
Clarity issues: [Overuse of technical jargon without explanation, complex sentences and the paragraph is very long, making it hard to follow. "We separately verifies" should be "separately verify" which affects clarity.]

Grammaticality score: 3
Grammaticality issues: [Incorrect verb agreement in "we separately verifies" and missing spaces after citations.]

Readability score: 3
Readability issues: [Long sentences with complex structure, excessive use of technical terms, and multiple errors like missing spaces affect readability.]

Coherence score: 4
Coherence issues: [Due to the mentioned fluency and clarity issues, the text becomes less coherent. However, the text remains generally organized around a central theme.]
====================
	 3D Gaussian Splatting for Real-Time Radiance Field Rendering
==========
Abstract:  Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.
Fluency score: 4
Fluency issues: [complex sentence structures may slightly hinder fluency for the general audience]

Clarity score: 3
Clarity issues: [use of technical jargon and dense information]

Grammaticality score: 5
Grammaticality issues: [none detected]

Readability score: 3
Readability issues: [technical language and complex topic-specific content]

Coherence score: 4
Coherence issues: [highly specialized content may affect coherence for readers not familiar with the subject]
==========
Introduction:  Meshes and points are the most common 3D scene representations because they are explicit and are a good fit for fast GPU/CUDA-based rasterization. In contrast, recent Neural Radiance Field (NeRF) methods build on continuous scene representations, typically optimizing a Multi-Layer Perceptron (MLP) using volumetric ray-marching for novel-view synthesis of captured scenes. Similarly, the most efficient radiance field solutions to date build on continuous representations by interpolating values stored in, e.g., voxel(Fridovich-Keil and Yu et al.,2022)or hash(Müller et al.,2022)grids or points(Xu et al.,2022). While the continuous nature of these methods helps optimization, the stochastic sampling required for rendering is costly and can result in noise. We introduce a new approach that combines the best of both worlds: our 3D Gaussian representation allows optimization with state-of-the-art (SOTA) visual quality and competitive training times, while our tile-based splatting solution ensures real-time rendering at SOTA quality for 1080p resolution on several previously published datasets(Knapitsch et al.,2017; Hedman et al.,2018; Barron et al.,2022)(see Fig.1). Our goal is to allow real-time rendering for scenes captured with multiple photos, and create the representations with optimization times as fast as the most efficient previous methods for typical real scenes. Recent methods achieve fast training(Fridovich-Keil and Yu et al.,2022; Müller et al.,2022), but struggle to achieve the visual quality obtained by the current SOTA NeRF methods, i.e., Mip-NeRF360(Barron et al.,2022), which requires up to 48 hours of training time. The fast – but lower-quality – radiance field methods can achieve interactive rendering times depending on the scene (10-15 frames per second), but fall short of real-time rendering at high resolution. Our solution builds on three main components. We first introduce3D Gaussiansas a flexible and expressive scene representation. We start with the same input as previous NeRF-like methods, i.e., cameras calibrated with Structure-from-Motion (SfM)(Snavely et al.,2006)and initialize the set of 3D Gaussians with the sparse point cloud produced for free as part of the SfM process. In contrast to most point-based solutions that require Multi-View Stereo (MVS) data(Aliev et al.,2020; Kopanas et al.,2021; Rückert et al.,2022), we achieve high-quality results with only SfM points as input. Note that for the NeRF-synthetic dataset, our method achieves high quality even with random initialization. We show that 3D Gaussians are an excellent choice, since they are a differentiable volumetric representation, but they can also be rasterized very efficiently by projecting them to 2D, and applying standard\alpha-blending, using an equivalent image formation model as NeRF. The second component of our method is optimization of the properties of the 3D Gaussians – 3D position, opacity\alpha, anisotropic covariance, and spherical harmonic (SH) coefficients – interleaved with adaptive density control steps, where we add and occasionally remove 3D Gaussians during optimization. The optimization procedure produces a reasonably compact, unstructured, and precise representation of the scene (1-5 million Gaussians for all scenes tested). The third and final element of our method is our real-time rendering solution that uses fast GPU sorting algorithms and is inspired by tile-based rasterization, following recent work(Lassner and Zollhofer,2021). However, thanks to our 3D Gaussian representation, we can perform anisotropic splatting that respects visibility ordering – thanks to sorting and\alpha-blending – and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required. To summarize, we provide the following contributions: The introduction of anisotropic 3D Gaussians as a high-quality, unstructured representation of radiance fields. An optimization method of 3D Gaussian properties, interleaved with adaptive density control that creates high-quality representations for captured scenes. A fast, differentiable rendering approach for the GPU, which is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis. Our results on previously published datasets show that we can optimize our 3D Gaussians from multi-view captures and achieve equal or better quality than the best quality previous implicit radiance field approaches. We also can achieve training speeds and quality similar to the fastest methods and importantly provide the firstreal-time renderingwith high quality for novel-view synthesis.
Fluency score: 4
Fluency issues: "voxel(Fridovich-Keil and Yu et al.,2022)or hash(Müller et al.,2022)grids", "previously published datasets(Knapitsch et al.,2017; Hedman et al.,2018; Barron et al.,2022)(see Fig.1)", "We first introduce3D Gaussians", "opacity\alpha," "... and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required."

Clarity score: 3
Clarity issues: Frequent use of jargon and acronyms without explanation; complex sentences with embedded clauses may confuse lay readers; the purpose of the approach described and end-user benefits aren't clearly stated for non-experts.

Grammaticality score: 4
Grammaticality issues: Missing spaces after commas and periods in the references, e.g., "voxel(Fridovich-Keil", "datasets(Knapitsch et", "introduce3D Gaussians".

Readability score: 3
Readability issues: Dense technical language and long, complex sentences; references within the text make it hard to read; the text is more readable for experts familiar with the field.

Coherence score: 4
Coherence issues: Some abrupt transitions between topics; relies on the reader's background knowledge to connect ideas; references inserted in the body disrupt flow.
====================
	 Zephyr: Direct Distillation of LM Alignment
==========
Abstract:  We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.
Fluency score: 4
Fluency issues: Flow is slightly technical, which might slightly impede fluency for general readers.

Clarity score: 3
Clarity issues: Use of acronyms and technical terms (dSFT, dDPO, RLHF, MT-Bench, Llama2-Chat-70B) without explanation.

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: Dense with jargon and acronyms; challenging for lay readers.

Coherence score: 4
Coherence issues: Jumps quickly between concepts; could benefit from linkage sentences.
==========
Introduction:  Smaller, open large language models (LLMs) have greatly increased in ability in recent years, from early GPT-2-like models(Wang2021-ti)to accurate and compact models(Touvron2023-af;penedo2023refinedweb;Jiang2023-qi)that are trained on significantly more tokens than the “compute-optimal” amount suggested by the Chincilla scaling laws(devries2023chinchilla_analysis). In addition, researchers have shown that these models can be further trained through distilled supervised fine-tuning (dSFT) based on proprietary models to increase their accuracy(Taori2023-ga). In this approach, the output of a more capable teacher model is used as supervised data for the student model. Distillation has proven to be an effective tool for improving open models on a range of different tasks(Chiang2023-rz); however, it does not reach the performance of the teacher models(Gudibande2023-vd). Users have noted that these models are not “intent aligned”, i.e. they do not behave in a manner that aligns with human users’ preferences. This property often leads to outputs that do not provide correct responses to queries. Intention alignment has been difficult to quantify, but recent work has led to the development of benchmarks like MT-Bench(Zheng2023-yf)and AlpacaEval(Li2023-zl)that specifically target this behavior. These benchmarks yield scores that correlate closely with human ratings of model outputs and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation. This motivates careful collection of human feedback for alignment, often at enormous cost at scale, such as inLlama2-Chat(Touvron2023-af). In this work, we consider the problem of aligning a small open LLM entirely through distillation. The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective(Rafailov2023-mv). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling compared to using other approaches like proximal preference optimization (PPO)(Schulman2017-mu). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB). To validate this approach, we constructZephyr-7B, an aligned version of Mistral-7B(Jiang2023-qi). We first use dSFT, based on the UltraChat(Ding2023-cc)dataset. Next we use the AI feedback data collected in the UltraFeedback dataset(Cui2023-cb). Finally, we apply dDPO based on this feedback data. Experiments show that this 7B parameter model can achieve performance comparable to 70B-parameter chat models aligned with human feedback. Results show improvements both in terms of standard academic benchmarks as well as benchmarks that take into account conversational capabilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available athttps://github.com/huggingface/alignment-handbook. We note an important caveat for these results. We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice(bai2022training). As distillation only works with the output of publicly available models this is technically more challenging to do because of added challenges in curating that type of synthetic data, and is an important subject for future work.
Fluency score: 3
Fluency issues: Numerous parenthetical citations (e.g., "Wang2021-ti", "Touvron2023-af", etc.) within the text disrupt the flow.

Clarity score: 3
Clarity issues: The heavy use of technical jargon, acronyms (e.g., "dSFT", "dDPO", "LLMs"), and lack of explanation for these terms make the text less clear to the general audience.

Grammaticality score: 4
Grammaticality issues: Overall grammatical construction is good, but minor issues such as missing spaces after citations (e.g., "models(Wang2021-ti)" should be "models (Wang2021-ti)") slightly affect grammaticality.

Readability score: 2
Readability issues: The text is dense with technical language, lacks transitions, and has long sentences, making it difficult for non-specialists to read.

Coherence score: 3
Coherence issues: The text jumps between topics (e.g., discussing various model types, training methods, alignment issues, and benchmarks) without clear transitions, affecting the coherence.
====================
	 Kosmos-2: Grounding Multimodal Large Language Models to the World
==========
Abstract:  We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.
Fluency score: 4
Fluency issues: [None]

Clarity score: 3
Clarity issues: ["represent refer expressions as links in Markdown" can be confusing without further context; "sequences of location tokens" may be technical jargon without explanation]

Grammaticality score: 4
Grammaticality issues: [Should be "referring expressions" instead of "refer expressions"]

Readability score: 3
Readability issues: [Complex sentence structures and technical jargon ("grounded image-text pairs", "perception-language tasks") may hinder readability for general audiences]

Coherence score: 4
Coherence issues: [The connection between the description of the model's function and the subsequent explanation of its evaluation could be smoother]
==========
Introduction:  Multimodal Large Language Models (MLLMs)[9,2,12,8,24]have successfully played a role as a general-purpose interface across a wide range of tasks, such as language, vision, and vision-language tasks. MLLMs can perceive general modalities, including texts, images, and audio, and generate responses using free-form texts under zero-shot and few-shot settings. In this work, we unlock the grounding capability for multimodal large language models. Grounding capability can provide a more convenient and efficient human-AI interaction for vision-language tasks. It enables the user to point to the object or region in the image directly rather than input detailed text descriptions to refer to it, the model can understand that image region with its spatial locations. Grounding capability also enables the model to respond with visual answers (i.e., bounding boxes), which can support more vision-language tasks such as referring expression comprehension. Visual answers are more accurate and resolve the coreference ambiguity compared with text-only responses. In addition, grounding capability can link noun phrases and referring expressions in the generated free-form text response to the image regions, providing more accurate, informational, and comprehensive answers. We introduceKosmos-2, a multimodal large language model with grounding capability built uponKosmos-1.Kosmos-2is a Transformer-based causal language model and is trained using the next-word prediction task. In order to unlock the grounding capability, we construct a web-scale dataset of grounded image-text pairs, and combine it with the multimodal corpora inKosmos-1to train the model. The grounded image-text pairs are built upon a subset of image-text pairs from LAION-2B[26]and COYO-700M[4]. We construct a pipeline to extract and link the text spans (i.e., noun phrases and referring expressions) in the caption to the spatial locations (e.g., bounding boxes) of its corresponding objects or regions in the image. We convert the spatial coordinates of the bounding boxes to a sequence of location tokens, which is then appended after its respective text spans. The data format serves as a “hyperlink” to connect the objects or regions of the image to the caption. Experimental results demonstrate thatKosmos-2not only achieves competitive performance on language and vision-language tasks evaluated inKosmos-1, but also achieves impressive performance on grounding tasks (phrase grounding and referring expression comprehension) and referring tasks (referring expression generation). As shown in Figure2, integrating the grounding capability enablesKosmos-2to be used for more downstream tasks, such as grounded image captioning, and grounded visual question answering.
Fluency score: 4
Fluency issues: [Missing spaces after MLLMs][9,2,12,8,24], [after introduceKosmos-2], [built uponKosmos-1.Kosmos-2], [inKosmos-1to]

Clarity score: 3
Clarity issues: [Complex sentences], [Jargon-heavy without explanation], [Unexplained references to figures (e.g., Figure2)]

Grammaticality score: 4
Grammaticality issues: [Missing spaces as noted in fluency issues]

Readability score: 3
Readability issues: [Dense and complex structure], [Lack of explanation for specialized terms]

Coherence score: 4
Coherence issues: [Sudden introduction of concepts without proper transition (e.g., Kosmos-2)]
====================
	 MVDream: Multi-view Diffusion for 3D Generation
==========
Abstract:  We introduce MVDream, a multi-view diffusion model that is able to generate consistent multi-view images from a given text prompt. Learning from both 2D and 3D data, a multi-view diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that such a multi-view prior can serve as a generalizable 3D prior that is agnostic to 3D representations. It can be applied to 3D generation via Score Distillation Sampling, significantly enhancing the consistency and stability of existing 2D-lifting methods. It can also learn new concepts from a few 2D examples, akin to DreamBooth, but for 3D generation.
Fluency score: 4
Fluency issues: [No significant issues, but uses some technical terms that could be confusing to a layperson]

Clarity score: 4
Clarity issues: ["...consistent multi-view images from a given text prompt" could be more specific about what "consistent multi-view images" are.]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [The sentence structure is complex, which could be challenging for some readers.]

Coherence score: 4
Coherence issues: [The text uses many specialized terms that could make it hard for those unfamiliar with the topic to follow the progression of ideas easily.]
==========
Introduction:  3D content creation is an important step in the pipeline of modern game and media industry, yet it is a labor-intensive task that requires well-trained designers to work for hours or days to create a single 3D asset. A system that can generate 3D content in an easy way for non-professional users is thus of great value. Existing 3D object generation methods can be categorized into three types: (1) template-based generation pipeline, (2) 3D generative models, and (3) 2D-lifting methods. Due to limited accessible 3D models and large data complexity, both template-based generators and 3D generative models struggle to generalize effectively to arbitrary object generation. Their generated content is often confined to common real-world objects with relatively simple topology and texture. Yet in industry, popular 3D assets usually come as a mixture of complicated, artistic, and sometimes non-realistic structures and styles(43). Recently, 2D-lifting methods have shown that pre-trained 2D generation models can be potentially applied to 3D generation. The typical representations are Dreamfusion(Pooleet al.,2023)and Magic3D(Linet al.,2023a)systems, which utilize 2D diffusion models as supervision for the optimization of a 3D representation via score distillation sampling (SDS). Trained on large-scale 2D image datasets, these 2D models are able to generate unseen and counterfactual scenes whose details can be specified through a text input, making them great tools for creating artistic assets. Nevertheless, in 2D-lifting techniques, challenges arise due to the lack of comprehensive multi-view knowledge or 3D-awareness during score distillation. These challenges encompass: (1) The multi-face Janus issue: The system frequently regenerates content described by the text prompt. (2) Content drift across different views. Examples can be seen in Fig. (1). The multi-face issue can stem from various factors. For instance, certain objects, like blades, may be nearly invisible from some angles. Meanwhile, vital parts of a character or animal might be hidden or self-occluded from specific viewpoints. While humans assess these objects from multiple angles, a 2D diffusion model cannot, leading it to produce redundant and inconsistent content. In spite of all the weaknesses of 2D-lifting methods, we believe that large-scale 2D data is crucial to generalizable 3D generation. Therefore, we propose multi-view diffusion models, which can be used as a multi-view 3D prior agnostic to 3D representations. The proposed model simultaneously generates a set of multi-view images that are consistent with each other. It can leverage pre-trained 2D diffusion models for transfer learning to inherit the generalizability. Then, by jointly training the model on multi-view images (from 3D assets) and 2D image-text pairs, we find that it can achieve both good consistency and generalizability. When applied to 3D generation through score distillation, our multi-view supervision proves significantly more stable than that of single-view 2D diffusion models. And we can still create unseen, counterfactual 3D contents as from pure 2D diffusion models. Inspired by DreamBooth(Ruizet al.,2023), we can also employ our multi-view diffusion model to assimilate identity information from a collection of 2D images and it demonstrates robust multi-view consistency after fine-tuning. Overall, our model, namelyMVDream, successfully generates 3D Nerf models without the multi-view consistency issue. It either surpasses or matches the diversity seen in other state-of-the-art methods.
Fluency score: 4
Fluency issues: No significant issues, but some sentences are complex and could be simplified for better fluency.

Clarity score: 3
Clarity issues: Complex sentences with technical jargon could be simplified for clarity. (e.g., "The system frequently regenerates content described by the text prompt" could be unclear to some readers.)

Grammaticality score: 5
Grammatical issues: No issues detected.

Readability score: 3
Readability issues: The density of technical terms and complex sentence structures may hinder readability for a general audience.

Coherence score: 4
Coherence issues: Overall good, but paragraph transitions could be smoother to enhance coherence.
====================
	 Voyager: An Open-Ended Embodied Agent with Large Language Models
==========
Abstract:  We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.
Fluency score: 5
Fluency issues: [None]

Clarity score: 4
Clarity issues: [The acronym "LLM" and technical terms such as "blackbox queries," "temporally extended," and "catastrophic forgetting" might confuse readers unfamiliar with AI terminology.]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Complex sentence structure and specialized jargon could hinder the readability for a general audience.]

Coherence score: 4
Coherence issues: [Use of acronyms and technical vocabulary without explanations might affect overall coherence for readers not versed in the field.]
==========
Introduction:  Building generally capable embodied agents that continuously explore, plan, and develop new skills in open-ended worlds is a grand challenge for the AI communitykolve2017ai2thor;savva2019habitat;zhu2020robosuite;xia2019igibson0.5; Shenet al.(2020). Classical approaches employ reinforcement learning (RL)Koberet al.(2013); Arulkumaranet al.(2017)and imitation learningBakeret al.(2022); Teamet al.(2021); Vinyalset al.(2019)that operate on primitive actions, which could be challenging for systematic explorationecoffet2019goexplore;huizinga2022evolving;wang2020enhanced;kanitscheider2021multitask;dennis2020paired, interpretabilityLianget al.(2022); Sunet al.(2020); Zhaoet al.(2021), and generalizationjiang2022vima; Shridharet al.(2021); Fanet al.(2021). Recent advances in large language model (LLM) based agents harness the world knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable policiesLianget al.(2022); Singhet al.(2022);jiang2022vima. They are applied to embodied tasks like games and roboticsFanet al.(2022); Zenget al.(2022); Ahnet al.(2022); Huanget al.(2022b,a), as well as NLP tasks without embodimentautogpt (2023); Yaoet al.(2022); Shinnet al.(2023). However, these agents are not lifelong learners that can progressively acquire, update, accumulate, and transfer knowledge over extended time spansParisiet al.(2019); Wanget al.(2023a). Let us consider Minecraft as an example. Unlike most other games studied in AImnih2013playing;openai2019dota; Vinyalset al.(2019), Minecraft does not impose a predefined end goal or a fixed storyline but rather provides a unique playground with endless possibilitiesFanet al.(2022). Minecraft requires players to explore vast, procedurally generated 3D terrains and unlock a tech tree using gathered resources. Human players typically start by learning the basics, such as mining wood and cooking food, before advancing to more complex tasks like combating monsters and crafting diamond tools. We argue that an effective lifelong learning agent should have similar capabilities as human players: (1)propose suitable tasksbased on its current skill level and world state, e.g., learn to harvest sand and cactus before iron if it finds itself in a desert rather than a forest; (2)refine skillsbased on environmental feedback andcommit mastered skills to memoryfor future reuse in similar situations (e.g. fighting zombies is similar to fighting spiders); (3)continually explore the worldand seek out new tasks in a self-driven manner. Towards these goals, we introduceVoyager, the firstLLM-powered embodied lifelong learning agentto drive exploration, master a wide range of skills, and make new discoveries continually without human intervention in Minecraft.Voyageris made possible through three key modules (Fig.2): 1) anautomatic curriculumthat maximizes exploration; 2) askill libraryfor storing and retrieving complex behaviors; and 3) a newiterative prompting mechanismthat generates executable code for embodied control. We opt to use code as the action space instead of low-level motor commands because programs can naturally represent temporally extended and compositional actionsLianget al.(2022); Singhet al.(2022), which are essential for many long-horizon tasks in Minecraft.Voyagerinteracts with a blackbox LLM (GPT-4OpenAI (2023)) through prompting and in-context learningwei2022emergent; Brownet al.(2020); Raffelet al.(2020). Our approach bypasses the need for model parameter access and explicit gradient-based training or finetuning. More specifically,Voyagerattempts to solve progressively harder tasks proposed by theautomatic curriculum, which takes into account the exploration progress and the agent’s state. The curriculum is generated by GPT-4 based on the overarching goal of “discovering as many diverse things as possible”. This approach can be perceived as an in-context form ofnovelty searchEysenbachet al.(2019); Contiet al.(2018).Voyagerincrementally builds askill libraryby storing the action programs that help solve a task successfully. Each program is indexed by the embedding of its description, which can be retrieved in similar situations in the future. Complex skills can be synthesized bycomposingsimpler programs, which compoundsVoyager’s capabilities rapidly over time and alleviates catastrophic forgetting in other continual learning methodsParisiet al.(2019); Wanget al.(2023a). However, LLMs struggle to produce the correct action code consistently in one shotChenet al.(2021a). To address this challenge, we propose aniterative prompting mechanismthat: (1) executes the generated program to obtain observations from the Minecraft simulation (such as inventory listing and nearby creatures) and error trace from the code interpreter (if any); (2) incorporates the feedback into GPT-4’s prompt for another round of code refinement; and (3) repeats the process until a self-verification module confirms the task completion, at which point we commit the program to the skill library (e.g.,craftStoneShovel()andcombatZombieWithSword()) and query the automatic curriculum for the next milestone (Fig.2). Empirically,Voyagerdemonstrates strongin-context lifelong learningcapabilities. It can construct an ever-growing skill library of action programs that are reusable, interpretable, and generalizable to novel tasks. We evaluateVoyagersystematically against other LLM-based agent techniques (e.g., ReActYaoet al.(2022), ReflexionShinnet al.(2023), AutoGPTautogpt (2023)) in MineDojoFanet al.(2022), an open-source Minecraft AI framework.Voyageroutperforms prior SOTA by obtaining3.3\timesmore unique items, unlocking key tech tree milestones up to15.3\timesfaster, and traversing2.3\timeslonger distances. We further demonstrate thatVoyageris able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other methods struggle to generalize.
Fluency score: 3
Fluency issues: [Inline citations disrupt flow; inconsistent formatting with missing spaces after periods and citations.]

Clarity score: 2
Clarity issues: [Excessive jargon; inline citations; some overlong sentences.]

Grammaticality score: 4
Grammaticality issues: [Some missing spaces after citations and sentences.]

Readability score: 2
Readability issues: [Inline citations hinder readability; sentences are too complex and jargon-heavy.]

Coherence score: 3
Coherence issues: [The integration of citations with the narrative is inconsistent; frequent domain-specific terms without explanation.]
====================
	 Wonder3D: Single Image to 3D using Cross-Domain Diffusion
==========
Abstract:  In this work, we introduce Wonder3D, a novel method for efficiently generating high-fidelity textured meshes from single-view images.Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of image-to-3D tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure consistency, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a geometry-aware normal fusion algorithm that extracts high-quality surfaces from the multi-view 2D representations. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and reasonably good efficiency compared to prior works.
Fluency score: 5
Fluency issues: 

Clarity score: 4
Clarity issues: [Use of niche technical terms without explanations may confuse non-expert readers: "Score Distillation Sampling (SDS)", "cross-domain diffusion model", "multi-view normal maps", "multi-view cross-domain attention mechanism"]

Grammaticality score: 5
Grammaticality issues: 

Readability score: 3
Readability issues: [Complex sentences and technical jargon lower accessibility for a general audience]

Coherence score: 4
Coherence issues: [The transition from introduction to proposed method and evaluation could be smoother for better flow]
====================
	 RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
==========
Abstract:  Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences. However, gathering high-quality human preference labels can be a time-consuming and expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al., offers a promising alternative that leverages a powerful off-the-shelf LLM to generate preferences in lieu of human annotators. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, RLAIF achieves comparable or superior performance to RLHF, as rated by human evaluators. Furthermore, RLAIF demonstrates the ability to outperform a supervised fine-tuned baseline even when the LLM preference labeler is the same size as the policy. In another experiment, directly prompting the LLM for reward scores achieves superior performance to the canonical RLAIF setup, where LLM preference labels are first distilled into a reward model. Finally, we conduct extensive studies on techniques for generating aligned AI preferences. Our results suggest that RLAIF can achieve human-level performance, offering a potential solution to the scalability limitations of RLHF.
Fluency score: 5
Fluency issues: N/A

Clarity score: 4
Clarity issues: Use of technical terms like "policy" and "distilled into a reward model" without explanation may confuse non-expert readers.

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 4
Readability issues: The dense use of acronyms (RLHF, LLMs, RLAIF) and jargon specific to the AI research community can make the passage less accessible for a general audience.

Coherence score: 5
Coherence issues: N/A
==========
Introduction:  Reinforcement Learning from Human Feedback (RLHF) is an effective technique for aligning language models to human preferences(Stiennonet al.,2020; Ouyanget al.,2022). It is cited as one of the key drivers of success in modern conversational language models, such as ChatGPT(Liuet al.,2023)and Bard(Manyika,2023). Training language models with reinforcement learning (RL) enables optimization on complex, sequence-level objectives that are not easily differentiable and therefore ill-suited for traditional supervised fine-tuning (SFT). One obstacle for employing RLHF at scale is its dependence on high-quality human preference labels. This raises the question of whether artificially generated labels can be a viable substitute. Generating labels with large language models (LLMs) is one promising approach, as LLMs have shown a high degree of alignment with human judgment(Gilardiet al.,2023; Dinget al.,2023).Baiet al.(2022b)was the first effort to explore Reinforcement Learning from AI Feedback (RLAIF)111This is distinct from “Constitutional AI”, which improves upon a supervised learning model through iteratively asking an LLM to generate better responses according to a a set of written value statements. Both were introduced inBaiet al.(2022b)and are sometimes conflated., where RL was conducted using a reward model trained on LLM preferences.Baiet al.(2022b)showed that utilizing a hybrid of human and AI preferences, in conjunction with their “Constitutional AI” self-revision technique, outperforms supervised fine-tuning for training a conversational assistant. However, it did not directly compare the efficacy of human vs. AI feedback, leaving the question of whether RLAIF can be a suitable alternative to RLHF unanswered. In this work, we study the impact of RLAIF and RLHF (see Figure2) on three text generation tasks: summarization, helpful dialogue generation, and harmless dialogue generation. Our experiments show that RLAIF and RLHF are preferred by humans over the SFT baseline 71% and 73% of the time for summarization and 63% and 64% of the time for helpful dialogue generation, respectively, where the differences between RLAIF and RLHF win rates are not statistically significant. We also conduct a head-to-head comparison of RLAIF against RLHF and find that both policies are equally preferred222The win rate for one policy vs. the other is not statistically significantly different from 50%. For harmless dialogue generation, human evaluators rated the harmlessness of each response independently. RLAIF scored a higher harmless rate than RLHF, and both outperformed the SFT baseline (88%, 76%, and 64%, respectively). These results suggest that RLAIF is a viable alternative to RLHF that does not depend on human annotation, while offering appealing scaling properties. Additionally, we investigate two related questions. First, we explore whether RLAIF can improve upon a SFT policy when the LLM labeler has the same number of parameters as policy. Even in this scenario, RLAIF significantly improves over the SFT baseline. Second, we conduct an experiment where the off-the-shelf LLM is directly prompted for reward scores during RL, bypassing the step of distilling LLM preference labels into a reward model. This method achieves an even higher win rate over SFT than the canonical distillation method. Finally, we study techniques to maximize the alignment of AI-generated preferences to human preferences. We find that soliciting chain-of-thought reasoning(Weiet al.,2022)consistently improves alignment, while using a detailed preamble and few-shot prompting(Brownet al.,2020)are only beneficial for certain tasks. We also conduct scaling experiments to examine the trade-off between the size of the LLM labeler and alignment with human preferences. The main contributions of this work are as follows: [noitemsep] We demonstrate that RLAIF achieves comparable or superior performance to RLHF on the tasks of summarization, helpful dialogue generation, and harmless dialogue generation. We show that RLAIF can improve upon a SFT policy even when the LLM labeler is the same size as the policy. We find that directly prompting the LLM for reward scores during RL can outperform the canonical setup where a reward model is trained on LLM preferences. We compare various techniques for generating AI labels and identify optimal settings for RLAIF practitioners.
Fluency score: 4
Fluency issues: "[RLHF)111This", "[RLAIF)222The", inconsistent use of spaces around parentheses and citations.

Clarity score: 4
Clarity issues: Complex sentences and multiple nested ideas can be challenging to follow.

Grammaticality score: 4
Grammaticality issues: Missing spaces and punctuation around citations, e.g., "Stiennonet al.,2020;", "Baiet al.(2022b)was".

Readability score: 3
Readability issues: Dense academic style, lack of line breaks or bullet points for the main contributions, and heavy use of acronyms reduce accessibility for broader readership.

Coherence score: 4
Coherence issues: The structure could be improved, especially the last part where bullet points or a different formatting would help in listing the main contributions.
====================
	 WizardLM: Empowering Large Language Models to Follow Complex Instructions
==========
Abstract:  Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM
Fluency score: 4
Fluency issues: No significant fluency issues detected.

Clarity score: 4
Clarity issues: "open-domain instruction following data" could be rephrased for better clarity.

Grammaticality score: 5
Grammaticality issues: No grammatical issues detected.

Readability score: 4
Readability issues: Use of multiple acronyms and technical terms without explanation ("LLM", "Evol-Instruct", "LLaMA", "Vicuna's testset") may reduce readability for a general audience.

Coherence score: 4
Coherence issues: The connection between "fine-tune LLaMA" and "WizardLM" is not immediately clear; the term "Evol-Instruct" is used without an introduction or definition.
==========
Introduction:  Large-scale language models (LLMs) have become the go-to approach for numerous natural language processing (NLP) tasks(Brownet al.,2020; Ouyanget al.,2022; OpenAI,2023; Touvronet al.,2023). LLMs are trained on large volumes of text data to predict the subsequent tokens, enabling them to generate coherent and fluent text in response to various inputs. However, these models often struggle to follow instructions or goals specified by users, which limits their usefulness and applicability in real-world scenarios. The NLP community has recently witnessed many endeavors to train LLMs to follow instructions better and be more helpful(Zhaoet al.,2023; Heet al.,2023; Guoet al.,2023; Liet al.,2023). Initial attempts(Aribandiet al.,2022; Weiet al.,2021; Xuet al.,2022; Sanhet al.,2022; Chunget al.,2022)to train instruction-following language models are based on a collection of various NLP tasks, with a small amount of hand-written instructions accompanying each task. These closed-domain instructions suffer from two main drawbacks: first, all the samples in an NLP dataset share only a few common instructions, severely limiting their diversity; second, the instructions usually only ask for one task, such as translation or summarization. But in real life, human instructions often have multiple and varied task demands. By using open-domain instruction data generated by real human users, OpenAI’s LLMs (e.g., InstructGPT(Ouyanget al.,2022)and ChatGPT111https://chat.openai.com/) have achieved great success. These open-domain instructions can fully unleash the unlimited potential of LLMs(Luoet al.,2023; Maet al.,2023; Huet al.,2023; Zhuet al.,2023)and enable them to perform more complex and diverse tasks. However, using humans to create open-domain instruction datasets like OpenAI did will encounter the following challenges. The whole annotating process is extremely expensive and time-consuming(Kopfet al.,2023; Chenet al.,2023; Sunet al.,2023; Yuanet al.,2023). On the other hand, the difficulty level distribution of human-created instructions is skewed towards being easy or moderate, with fewer difficult ones (according to the difficulty statistics of ShareGPT(Chianget al.,2023)from Figure6(a)). Possible reasons for this are that the proportion of experts among annotators is low and creating complex instructions demands a lot of mental effort. Human annotators are prone to fatigue and cannot sustain high-intensity work to produce a sufficient proportion of high-difficulty instructions(Zhanget al.,2023; Xiaoet al.,2023; Manakulet al.,2023; Zhonget al.,2023). Based on these issues, developing an automatic method that can mass-produce open-domain instructions (especially the more difficult ones) at a relatively low cost becomes the key to further advancing instruction-tuned language models(Baoet al.,2023; Liuet al.,2023; Bianet al.,2023; Cabanneset al.,2023). In this work, we introduceEvol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs. Figure1shows the running examples ofEvol-Instruct. Starting from a simple initial instruction “1+1=?”, our method randomly selects In-depth Evolving (blue direction line) or In-breadth Evolving (red direction line) to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes five types of operations: add constraints, deepening, concretizing, increase reasoning steps, and complicate input. The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction. These six operations are implemented by prompting an LLM with specific prompts. Since the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator to filter the failed instructions, which is called Elimination Evolving. We repeat this evolutionary process for several rounds to obtain enough instruction data containing various complexities. We validate ourEvol-Instructby fine-tuning open-source LLaMA(Touvronet al.,2023)with our evolved instructions and evaluating its performance similar to existing SOTA works (e.g., Alpaca(Taoriet al.,2023)and Vicuna(Chianget al.,2023)) on instruction finetuning. The instruction datasets we compare with are the data used by Alpaca (generated using self-instruct(Wanget al.,2022a)) and the 70k ShareGPT (shared by real users) used by Vicuna. To prove that the instruction dataset from our method is superior to human-created instruction datasets, we select Alpaca’s training data (generated from only 175 human-created seed instructions) as the initial dataset. We execute four epochs of evolution using OpenAI ChatGPT API222gpt-3.5-turbo fromhttps://oai.azure.com/portaland finally obtain 250k instructions. To ensure a fair comparison with Vicuna’s 70k real user data, we sampled an equal amount from the full 250k data and trained the LLaMA 7B model. We name our modelWizardLM. Due to the low proportion of difficult instructions in the previous instruction-following test dataset, we manually created a new difficulty-balanced test dataset, namedEvol-Instructtestset. We hire annotators and leverage GPT-4 to evaluate Alpaca, Vicuna, ChatGPT, andWizardLMonEvol-Instructtestset and Vicuna’s testset. Our main findings are as follows: Instructions fromEvol-Instructare superior to the ones from human-created ShareGPT.When we use the same amount ofEvol-Instructdata (i.e., 70k) as Vicuna to fine-tune LLaMA 7B, our modelWizardLMsignificantly outperforms Vicuna, with the win rate of12.4\%and3.8\%higher than Vicuna onEvol-Instructtestset and Vicuna’s testset respectively on human evaluation. In addition,WizardLMalso achieves better response quality than Alpaca and Vicuna on the automatic evaluation of GPT-4. Labelers preferWizardLMoutputs over outputs from ChatGPT under complex test instructions.OnEvol-Instructtestset,WizardLMperforms worse than ChatGPT, with a win rate12.8\%lower than ChatGPT (28.0\%vs.40.8\%). However, in the high-difficulty section ofEvol-Instructtest set (difficulty level\geq 8), ourWizardLMeven outperforms ChatGPT, with a win rate7.9\%larger than ChatGPT (42.9\%vs.35.0\%), that is human annotators even prefer the output of our model than ChatGPT on those hard questions. This indicates thatEvol-Instructcan significantly improve the ability of LLMs to handle complex instructions.
Fluency score: 4
Fluency issues: ["to train LLMs to follow instructions better and be more helpful(Zhaoet al.,2023; Heet al.,2023;..." spacing issues between references; "using LLMs instead of humans" can be rephrased for better flow; repeated "The In-depth Evolving...The In-breadth Evolving" disrupts flow]

Clarity score: 4
Clarity issues: ["using open-domain instruction data generated by real human users," slightly unclear antecedent; "instruction datasets like OpenAI did will encounter..." awkward construction; "our method randomly selects In-depth Evolving (blue direction line) or In-breadth Evolving (red direction line)," unnecessary detail may confuse]

Grammaticality score: 4
Grammaticality issues: Incorrect spacing in citations (e.g., "Brownet al.,2020"); inconsistent capitalization ("ShareGPT(Chianget al.,2023)from Figure6(a)");

Readability score: 3
Readability issues: Long sentences with complex structures; heavy use of jargon and acronyms without explanation; overuse of parentheses

Coherence score: 3
Coherence issues: Jump between topics without clear transitions (e.g., transition from discussing limitations to introducing Evol-Instruct); multiple references within parentheses distract from the main narrative; inconsistent explanation of methods and results
====================
	 Key-phrase boosted unsupervised summary generation for FinTech organization
==========
Abstract:  With the recent advances in social media, the use of NLP techniques in social media data analysis has become an emerging research direction. Business organizations can particularly benefit from such an analysis of social media discourse, providing an external perspective on consumer behavior. Some of the NLP applications such as intent detection, sentiment classification, text summarization can help FinTech organizations to utilize the social media language data to find useful external insights and can be further utilized for downstream NLP tasks. Particularly, a summary which highlights the intents and sentiments of the users can be very useful for these organizations to get an external perspective. This external perspective can help organizations to better manage their products, offers, promotional campaigns, etc. However, certain challenges, such as a lack of labeled domain-specific datasets impede further exploration of these tasks in the FinTech domain. To overcome these challenges, we design an unsupervised phrase-based summary generation from social media data, using 'Action-Object' pairs (intent phrases). We evaluated the proposed method with other key-phrase based summary generation methods in the direction of contextual information of various Reddit discussion threads, available in the different summaries. We introduce certain "Context Metrics" such as the number of Unique words, Action-Object pairs, and Noun chunks to evaluate the contextual information retrieved from the source text in these phrase-based summaries. We demonstrate that our methods significantly outperform the baseline on these metrics, thus providing a qualitative and quantitative measure of their efficacy. Proposed framework has been leveraged as a web utility portal hosted within Amex.
Fluency score: 4
Fluency issues: ["... can be further utilized for downstream NLP tasks." – Slightly awkward phrasing.]

Clarity score: 4
Clarity issues: ["... can be further utilized for downstream NLP tasks." – Vague reference to "downstream NLP tasks"; "Proposed framework has been leveraged..." – Sudden mention of Amex.]

Grammaticality score: 4
Grammaticality issues: ["Proposed framework has been leveraged..." – Missing article before "Proposed framework".]

Readability score: 4
Readability issues: Long sentences with complex structure could hinder easy comprehension, e.g., "We evaluated the proposed method with other key-phrase based summary generation methods..."

Coherence score: 4
Coherence issues: Jumps between topics without transitions, e.g., from describing the importance of the research to mentioning the specific web utility portal hosted within Amex.
==========
Introduction:  Post: Bank Rewards Checking Account. Comments: I hope they add the ability to make it ajoint account with a spouse.<sep>Nice to see they’re attempting tocompetewith neweronline banks.<sep>When they firstopenedup theirHYSAthey eventually had asignup bonus. I wonder if they’ll do something like that at some point<sep>The main difference between this and the HYSA is that you’ll be able to makes charges andget rewardsfor purchases<sep>If you link your new Checking account to your credit card there’s no way you wouldget MR pointson that transaction, right?<sep>Can youuse Zellethrough the account? signup bonus;compete online banks;opened HYSA;get MR points;use Zelle With the advent of social media, the analysis of data available on these platforms has the potential to provide a gold mine of actionable insights to business organizations. With the ubiquity of social media, the amount of available data has skyrocketed, and social media reflects popular discourse and discussion in a larger societal context. Thus, building a social media knowledge ecosystem in addition to existing data analysis platforms can help an organization to better understand its consumers’ preferences and reactions to the services they offer. These insights help businesses form an understanding of the “external perspective”, directly from the source - the consumers. This additional analysis can have long-term positive impacts and reveal insights that may not be surfaced through traditional channels like customer service call transcripts. Summarizing the discussions available on these social media platforms can give a precise external perspective to an organization about their products, offers, competitors etc, while helping them to make decision for their future products and offers. Hence, in this study, we focus our attention on the problem of phrase based unsupervised summary generation from the Reddit discussions data, with the goal of developing a broader understanding of social media discussions related to financial organization’s products and services. In the last decade, NLP techniques have been successfully applied to social media data in the FinTech context to improve business operations and understand customer opinions. They have been applied to problems like sentiment analysis, stock value prediction, document summarization, intent detection, and so on. A facet that has received particular attention is sentiment analysis from social media data and its utility in predicting trends in the stock market and share prices(Day and Lee,2016; Aji et al.,2019; Karalevicius et al.,2018; Esichaikul and Phumdontree,2018; Sangsavate et al.,2019). Lack of publicly available data and the cost of generating gold standard labels has been a challenge for multiple NLP tasks such as summarization, intent detection, which has lead to the requirement of more unsupervised or semi supervised approaches. Some of the work in this direction are,(Abdaljalil and Bouamor,2021; La Quatra and Cagliero,2020)for summarizing financial reports and FinTech data,(Weld et al.,2022)for task of identifying the user’s intention. Despite availability of some of the labelled datasets for summarization, intent detection, its difficult to solve domain specific problems with them. Most studies on intent detection focus on the spoken language understanding (SLU) datasets - ATIS(Hemphill et al.,1990), and SNIPS(Coucke et al.,2018), which were originally designed for mobile voice assistants, and hence do not generalize well. Secondly, the current formulation of intent detection frames it as a multi-class classification problem. Though pretrained models perform well(Hardalov et al.,2020), the intent class labels designed for ATIS and SNIPS do not apply to other domains and are too restrictive for real-world data. Finally, this supervised definition of intent detection means crowd-sourcing is the only viable option for creating domain-specific social media datasets. Crowd-sourcing comes with its own set of problems, as it is often expensive and presents problems with annotator bias(Eickhoff,2018). Unsupervised methods in intent detection have been explored in the past(Popov et al.,2019; Dopierre et al.,2021; Siddique et al.,2021), but they do not pertain to FinTech data. Traditionally employed in SLU, intent detection has great potential in this domain. Not only can it summarize and simplify lengthy social media discussions, but these salient features can also form representative features for various downstream NLP tasks. In this regard, we propose a completely unsupervised method for phrase-based summary generation, specifically for FinTech social media data. We build a database of posts and comments (threads) from the social news platform Reddit and extract key intent phrases to summarize lengthy threads. None of our data is annotated with summary text or keywords, and we propose a flexible unsupervised approach. In this work the summary of the Reddit discussions is represented by intent keywords in the data. For intent phrase extraction, we follow a two-stage pipeline by first identifying different categories of ’Action-Object’ pairs (tokens matching pre-defined rules), followed by a scoring function to rank them. Additionally for comparison purpose, we perform aspect-based sentiment analysis and identify the key phrases that are associated with the most positive or negative comments. Figure1, demonstrates an overview of our pipeline by identifying the most salient intent phrases from a Reddit thread. These intent phrases are further used as a as vector representations to cluster the post corpus and generate cluster-level summaries to gather insights from large corpus of customer feedback on Reddit. We compared our method by generating summary using an existing keyphrase extraction method, ‘Yake’(Campos et al.,2018)and aspect-based sentiment phrases. For evaluation, we additionally introduce ‘context metrics’ to evaluate the contextual information in the respective cluster summaries. The main contributions of this paper are as follows. (1) We propose a new unsupervised method for phrase-based summary generation using intent key phrases. (2) We study publicly available social media data (pertaining to FinTech), (without crowdsourcing annotation). Finally, (3) we suggest new metrics to evaluate the contextual information retrieved in the clusters and find that our method outperforms existing keyphrase based summary generation methods. Thus, our goal in this paper is to demonstrate that intent based summary can generate meaningful insights in domain-specific financial text without supervision or labeled datasets. In Section2, data collection and description of the end-to-end summary generation pipeline is described. Section3experimental setup, qualitative and qualitative results are demonstrated. Section4conclude the paper and briefly discuss the limitations of the study and future directions.
====================
	 Spatial Deep Learning for Site-Specific Movement Optimization of Aerial Base Stations
==========
Abstract:  Unmanned aerial vehicles (UAVs) can be utilized as aerial base stations (ABSs) to provide wireless connectivity for ground users (GUs) in various emergency scenarios. However, it is a NP-hard problem with exponential complexity in $M$ and $N$, in order to maximize the coverage rate of $M$ GUs by jointly placing $N$ ABSs with limited coverage range. The problem is further complicated when the coverage range becomes irregular due to site-specific blockages (e.g., buildings) on the air-ground channel, and/or when the GUs are moving. To address the above challenges, we study a multi-ABS movement optimization problem to maximize the average coverage rate of mobile GUs in a site-specific environment. The Spatial Deep Learning with Multi-dimensional Archive of Phenotypic Elites (SDL-ME) algorithm is proposed to tackle this challenging problem by 1) partitioning the complicated ABS movement problem into ABS placement sub-problems each spanning finite time horizon; 2) using an encoder-decoder deep neural network (DNN) as the emulator to capture the spatial correlation of ABSs/GUs and thereby reducing the cost of interaction with the actual environment; 3) employing the emulator to speed up a quality-diversity search for the optimal placement solution; and 4) proposing a planning-exploration-serving scheme for multi-ABS movement coordination. Numerical results demonstrate that the proposed approach significantly outperforms the benchmark Deep Reinforcement Learning (DRL)-based method and other two baselines in terms of average coverage rate, training time and/or sample efficiency. Moreover, with one-time training, our proposed method can be applied in scenarios where the number of ABSs/GUs dynamically changes on site and/or with different/varying GU speeds, which is thus more robust and flexible compared with conventional DRL-based methods.
Fluency score: 4
Fluency issues: Use of technical terms and symbols may impede fluency for general readers.

Clarity score: 3
Clarity issues: The text is dense with technical jargon and acronyms that may not be clear to a lay audience. The purposes of the SDL-ME algorithm are listed in a manner that may be confusing due to lack of elaboration.

Grammaticality score: 5
Grammaticality issues: No grammatical issues detected.

Readability score: 3
Readability issues: The complex sentence structures and heavy use of technical language reduce accessibility for non-specialist readers.

Coherence score: 4
Coherence issues: The text follows a logical structure but may lose coherence due to the complexity of the content and the specialized language used.
==========
Introduction:  With their high mobility and reducing cost, unmanned aerial vehicles (UAVs) have attracted increasing interests in military and civilian domains in recent years. In particular, integrating UAVs into wireless communication networks as aerial base stations (ABSs) to assist terrestrial communication infrastructure in various emergency scenarios such as battlefields, disaster scenes and hotspot events, has been regarded as an important and promising technology[36]. One of the key problems in UAV-aided communication systems is to find applicable placement ofNABSs with limited coverage range in order to achieve maximum coverage ofM(static) ground users (GUs)[21,13,24], which is known to be aNP-hard problemwith exponential complexity inMandN[21]. A tutorial in[31]discusses the ABS placement problem and the most commonly used schemes in scenarios with free space (FS) or non-FS propagation in recent literature. In particular, some algorithms including the spiral algorithm[21], K-means algorithm[13], circle packing theory[24], and user-majority based adaptive UAV deployment[34], are proposed to solve the type of problems with dominant line-of-sight (LoS) or probabilistic LoS/non-LoS (NLoS) channel model[7], under which each ABS has auniformcoverage range. However, due tosite-specific blockages(e.g., buildings), the above channel models might fail to capture the fine-grained structure of LoS or NLoS propagation at specific ABS and GU locations[8,28]. For example, with a slight change of its position, an ABS might transit from LoS to NLoS propagation to the GU due to building edges. This critically affects the ABS-GU channel and further complicates the problem. Some efforts have been made using deep learning (DL) to learnsite-specific channelinformation[15,11], and/or using radio map[8]to construct/utilize spatial channel distribution[20,38,37,29]. In[15], the authors propose to use an end-to-end neural network to learn a site-specific probing codebook in order to predict the optimal narrow beam for beam alignment. The authors in[11]propose a DL-based method for the optimal scheduling of interfering links in a dense wireless network with full frequency reuse. The proposed methods in[15,11]are able to reduce the time/computational overhead of channel estimation and/or schedule links efficiently based on geographic locations of the devices, which yet are not directly applicable to the ABS placement/movement problems. In the context of UAV communications, radio map has been utilized to represent site-specific spatial distribution of average received power radiated from given transmitting source(s), e.g., the fixed ground base stations for cellular-connected UAV[20,38,37], or the ABSs[29]to provide ground coverage. For ABS placement, the authors in[29]leverage on a given spatial loss field (SLF) function to construct the radio map with low complexity, whereas how to obtain/store site-specific SLF for any given ABS-GU location pair with high sample efficiency is yet to be addressed. Other authors in[9]partially circumvent this challenge by developing adaptive UAV positioning strategy with on-site LoS condition measurements for a given pair of UAV-relay and GU, whereas multi-ABS/multi-GU scenarios are yet to be considered. The ABS placement problem can be further complicated due toGU mobility, which brings additional complexity and the practical requirement of finding desired solutions within limited time. In this regard, machine learning methods including DL and reinforcement learning (RL)/deep RL (DRL) have been developed/applied to solve complicated problems of UAV joint optimization considering multiple factors such as UAV trajectory design, user association, resource allocation and power consumption[18,10,32](see the recent survey[30]for more references). In particular, RL/DRL methods have recently been applied to tackle the multi-UAV movement optimization problems[16,17,19,33,39,22]. In terms of ABS coverage and energy consumption trade-off, a DRL-based approach is proposed in[16]to achieve energy-efficient and fair communication coverage. A decentralized DRL-based framework is further proposed in[17]to provide energy-efficient long-term communication coverage. The authors in[19]propose a genetic algorithm-based K-means algorithm to partition GUs into clusters, and further apply the Q-learning algorithm in each cluster for ABS movement. The authors in[33]and[39]both formulate the ABS movement problem as a constrained markov decision process, and propose the dueling Deep Q-network (DQN) and/or constrained DQN algorithms to maximize the downlink capacity with full coverage constraint. However, the above works typically assume uniform coverage range in a generic environment, whereas thesite-specific LoS/NLoS propagationscenario is yet to be considered. Forsite-specific multi-ABS movementoptimization, the authors in[22]propose a single-agent Deep Deterministic Policy Gradient (DDPG) based approach to maximize the average sum-rate of all GUs via UAV dynamic movement and communication co-design, whereas the tested network is relatively small (e.g., withM=10andN=2). On the other hand, our early work in[28]proposes a Double DQN with Prioritized Experience Replay (PER-DDQN) to address the site-specific ABS placement problem with a moderate network size (e.g.,M=80andN=10). However, a straightforward extension to ABS movement optimization encounters further difficulties. In particular, the action space of RL/DRL methods growscombinatoriallywithNand the number of steps to explore. Moreover, a new neural network (NN) model in DRL would often need to be re-trained in order to cater fornetwork changes(e.g., ABSs/GUs turning on/off, varying GU speeds, etc.), which calls for more timely adaptation and more flexible design. To circumvent the above difficulties, we shift our mindset and attempt a different approach other than DRL. First, the ABS movement problem is partitioned into a time series of ABS placement sub-problems, each of which aims for maximum GU coverage under given GU locations, and thus amounts to apattern matching/search problem. Second, a state-of-the-art search algorithm called Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)[23]is adopted with tailored modifications to solve each ABS placement sub-problem. Third, an environment emulator is built to predict the site-specific coverage status of all GUs in the actual environment and assist fast evaluation of ABS placement solutions. Our main contributions are summarized as follows: Spatial Deep Learning (SDL)111We reuse the term SDL as in[11]to refer to the general method of learning spatial characteristics by DL, although different problem setup and DL architecture are considered here.for Coverage Prediction: An encoder-decoder type of deep-NN (DNN) with careful incorporation of domain knowledge is proposed. First, we use threegrid-mapsto quantize and represent the location patterns of ABSs, GUs, and covered GUs (CGUs), respectively, with the first two as input and the last one as output of the DNN. These grid-maps endow the DNN with 1) input/outputdimension invariancewith the number of ABSs/GUs; and 2) input/outputpermutation invariancewith ABS/GU indexes. These invariance properties significantly reduce the learning burden of DNN and also render more flexibility and robustness to scenarios where the number of ABSs/GUs dynamically changes on site. Second, tailored design techniques are proposed includingbinary mask processingandelement-wise binary classification, which effectively boost the training efficiency. MAP-Elites as Quality-Diversity Search Engine: Based on the trained DNN emulator, we further propose theSDL-based MAP-Elites (SDL-ME)algorithm. First, based on MAP-Elites, the search over the original variable space (ofO(N)dimensions) is effectively reduced to that over a low-dimensional (e.g., two in our proposed design) feature space, which flexibly tradeoffs betweencomplexity reductionandsolution diversity, thus encouraging more efficient search for better-quality solutions. Second, the SDL-based emulator captures the site-specific ABS-GU coverage states and enablesvirtual explorationof a much larger part of the search space compared with direct on-site trials and errors, thus leading to potentially better solutions. Top-kMechanism and Planning-Exploration-Serving (PES) Scheme: The SDL-ME based planning helps sift outktop performing candidate solutions (in terms of emulator-predicted coverage rate), which are then explored and validated on site to further elect the best performing solution (in terms of actual coverage rate) for ABS placement to serve GUs in the current time period. Such a top-kmechanism and PES scheme seamlessly amalgamate theemulator-based planningandon-site exploration/serving, thus significantly reducing the cost of extensive on-site search. Moreover, the measurements for the top-kcandidate solutions effectively compensate for the quantization/prediction errors due to model approximation. Numerical results demonstrate that the proposed approach significantly outperforms the benchmark DRL-based method and other two baselines in terms of average coverage rate, training time and/or sample efficiency. Moreover, with one-time training, our proposed method can be applied indynamic scenarioswhere the number of ABSs/GUs changes on site and/or with different/varying GU speeds, which is more robust and flexible compared with conventional DRL methods.
====================
	 Swing: Short-cutting Rings for Higher Bandwidth Allreduce
==========
Abstract:  The allreduce collective operation accounts for a significant fraction of the runtime of workloads running on distributed systems. One factor determining its performance is the distance between communicating nodes, especially on networks like torus, where a higher distance implies multiple messages being forwarded on the same link, thus reducing the allreduce bandwidth. Torus networks are widely used on systems optimized for machine learning workloads (e.g., Google TPUs and Amazon Trainium devices), as well as on some of the Top500 supercomputers. To improve allreduce performance on torus networks we introduce Swing, a new algorithm that keeps a low distance between communicating nodes by swinging between torus directions. Our analysis and experimental evaluation show that Swing outperforms by up to 3x existing allreduce algorithms for vectors ranging from 32B to 128MiB, on different types of torus and torus-like topologies, regardless of their shape and size.
Fluency score: 4
Fluency issues: [none]

Clarity score: 4
Clarity issues: [Use of jargon like "allreduce," "torus networks," "Swing," without explanation]

Grammaticality score: 5
Grammaticality issues: [none]

Readability score: 3
Readability issues: [Complex sentence structures; heavy use of technical terms]

Coherence score: 4
Coherence issues: [Lack of explanation about the context in which "Swing" operates; assumes prior knowledge]
==========
Introduction:  Allreduce is a collective operation used to aggregate vectors among a set of nodes and to distribute back to them the aggregated result. Among others, allreduce is widely used to perform distributed gradient aggregation during the training of deep learning models[10]. Several studies have shown that it can account for up to 40% of the total training time[44,28,39]and between 19% and 30% of the total core hours in MPI jobs running on production supercomputers[14]. Researchers proposed several allreduce algorithms[43,6,25], and the most performing one depends on a combination of vector size, number of nodes, and physical topology[42,23]. Those algorithms perform a predefined number of steps and, at each step, each node sends and receives data to and from some predetermined nodes. Different trade-offs exist between the number of steps to perform (more critical for allreduce on small vectors) and the total number of bytes it transmits (more relevant for larger allreduce). However, a third factor that must be considered when designing a new collective algorithm is the physical distance between communicating nodes[43,33,38,42]. This is particularly relevant on networks that do not provide full bisection bandwidth such as torus, since the higher the distance, the higher the number of flows sharing the same links. Torus networks are widely used, both on systems optimized for running machine learning (ML) workloads (e.g., Google deploys TPUs v4 on a 3D torus-like network[27], and Amazon deploys Trainium devices on a 2D torus[7]), and on some of the top supercomputers[2,11](e.g., Fugaku uses a 6D torus[4]). We show the importance of the distance between communicating nodes in the allreduce through an example. In Fig.1, we show a16node 1D torus (we only show a subset of the nodes since the communications are symmetric). We assume minimal (i.e., shortest path) routing and we show the communications performed by the bandwidth-optimal recursive doubling algorithm[43](also known asRabenseifneralgorithm[35], which we describe more in detail in Sec.2.3.3), and by theSwingalgorithm (that we propose in this work). Both algorithms perform the same number of steps (we show only the first three for simplicity). We denote withnthe number of bytes of the allreduce vector. In the first step, in both algorithms, each nodersendsn/2bytes to nodeq=r\text{ XOR }1(and receivesn/2bytes from it). In the second step, however, in the recursive doubling, each nodersendsn/4bytes to nodeq=r\text{ XOR }2(two hops distant), whereas in the Swing algorithm, each node still sendsn/4bytes of data, but with the other neighbor (one hop distant). Although both algorithms transmit the same number of bytes, two different messages cross the same link in the recursive doubling. For example, two messages cross the link between nodes1and2and that between nodes5and6. As a consequence, in the worst case all nodestransmit data at most at half the bandwidthof the link between1and2, thus slowing down the entire allreduce operation. Instead, in the Swing algorithm, each node can still transmit at full bandwidth because, in this example, in the second step each link is crossed at most by one message per direction. Something similar also happens in the third step. Indeed, when using Swing at most two messages cross each link instead of the four messages crossing the link between nodes3and4in recursive doubling. It is thus clear how even if two different algorithms transmit the same number of bytes and perform the same number of steps, they might be practically characterized by different performance, depending on the network characteristics and the distance between communicating nodes. In this example, we have shown an extreme case using a 1D torus. However, similar effects can happenon any topology that does not provide full bisection bandwidth. Although some algorithms (i.e., ring[33,43]and bucket[9,38,25]) avoid this problem by having each node communicate with its neighbors only, they perform more steps (linear in the number of nodes) and are thus not well-suited for small- and medium-sized vectors. Those are the sizes that, however, are practically used in most machine learning[30]and HPC[14]workloads. Indeed, larger allreduce are split into smaller ones to overlap better computation and communication, especially more when using 3D parallelism in machine learning training[10]. For these reasons, this work makes the following contributions: [leftmargin=*] We design a new allreduce algorithm calledSwing, which performs a logarithmic number of steps and transmits the minimal number of bytes while reducing the distance between communicating nodes compared to other known algorithms designed for small- and medium-sized vectors (Sec.3and Sec.4). We evaluate Swing on different torus and torus-like topologies (e.g., HammingMesh[22]and HyperX[3,18]), by comparing it with the best state-of-the-art algorithms (Sec.5). Our evaluation shows thatSwing outperforms the other existing algorithms for allreduce on vectors ranging from 32B to 128MiB on different torus-like topologies, and regardless of their shape and size. We show that Swing outperforms the best-known algorithm up to 2.2x on square torus with4096to16384nodes and up to 3x on rectangular tori and HyperX with4096nodes.
====================
	 Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search
==========
Abstract:  Achieving tight bounding boxes of a shape while guaranteeing complete boundness is an essential task for efficient geometric operations and unsupervised semantic part detection. But previous methods fail to achieve both full coverage and tightness. Neural-network-based methods are not suitable for these goals due to the non-differentiability of the objective, while classic iterative search methods suffer from their sensitivity to the initialization. We propose a novel framework for finding a set of tight bounding boxes of a 3D shape via over-segmentation and iterative merging and refinement. Our result shows that utilizing effective search methods with appropriate objectives is the key to producing bounding boxes with both properties. We employ an existing pre-segmentation to split the shape and obtain over-segmentation. Then, we apply hierarchical merging with our novel tightness-aware merging and stopping criteria. To overcome the sensitivity to the initialization, we also define actions to refine the bounding box parameters in an Markov Decision Process (MDP) setup with a soft reward function promoting a wider exploration. Lastly, we further improve the refinement step with Monte Carlo Tree Search (MCTS) based multi-action space exploration. By thoughtful evaluation on diverse 3D shapes, we demonstrate full coverage, tightness, and an adequate number of bounding boxes of our method without requiring any training data or supervision. It thus can be applied to various downstream tasks in computer vision and graphics.
Fluency score: 4
Fluency issues: [No specific issues, just a minor room for improvement in sentence structure to enhance flow]

Clarity score: 3
Clarity issues: [Use of technical terms without explanation (over-segmentation, tightness-aware merging), compound sentences with complex ideas]

Grammaticality score: 4
Grammaticality issues: [should be "in a Markov Decision Process" instead of "an Markov Decision Process"]

Readability score: 3
Readability issues: [Complex sentence structures, heavy use of jargon]

Coherence score: 4
Coherence issues: [Concepts are introduced in a logical order, but the technical nature of the content may impede coherence for a lay audience]
==========
Introduction:  Approximating complex 3D shapes using primitives offers several capabilities, including shape structure analysis, shape abstraction, and efficient geometric computations. To achieve these, many recent self-supervised learning approaches[tulsiani17abstract,paschalidou19sq,sun19abstract,yang21cubseg,paschalidou21np,chen20bspnet,niu22rimnet]have successfully addressed the problem while exploring different types of primitives. Despite the recent advances, in this work, we pay attention to some desired yet underinvestigated properties of the bounding primitives of a shape and propose a novel approach aiming to achieve them. The properties are 1)full coverage— guaranteeing the boundness of the entire shape by the primitives, 2)tightness, and 3)adequate number of primitives. Attaining these three properties is particularly crucial for the downstream applications requiringefficient geometric computations, such as intersection tests[collision_survey,orientedbbox], robust transmissions[robust_transmission], ray tracing[ellipsoids_intersection], or proximity computations[oriented]. The primitives that just approximate but do not fully cover the shape can result in imprecise computation in such tasks. Also, loosely bounding primitives and too many primitives increase the computation time. Moreover, the bounding primitives satisfying these properties typically provide a better abstraction of the shape aligned with thehuman perceptionof the shape decomposition. Although neural-network-based methods have demonstrated their powerful generalizability and expressivity, they typically fail to achieve these three properties mainly due to thenon-differentiablenature of the objectives. The full coverage and tightness can be computed with the volumetric intersection or difference operations, which are not differentiable. Finding the proper number of parts is also a discrete problem that cannot be easily solved via backpropagation. As tight bounding primitives have been essential in various applications, there also has been a line of work[robust_transmission,simari2005extraction,kalaiah2005statistical]addressing the problem before the deep learning era. One notable example is the work by Lu et al.[2007bvc]that proposes to find the tight bounding boxes by iteratively exchanging point-to-primitive assignments, starting from an initialization. The major drawback of such a method is its dependency and sensitivity to theinitialization, resulting in a suboptimal output when it starts from a poor initialization. To address the problem of achieving the three properties that introduce challenges ofnon-differentiabilityfor neural networks andinitialization sensitivityfor iterative search methods, we propose a framework that performsSplitting,Merging,AndRefinementTechniques, and is thus dubbedSMART. We first find that a simple post-processing applied to a pre-segmentation, can provide appropriateover-segmentationof a 3D shape. Hence, we performhierarchical mergingto find the adequate number of parts. We introducetightness-awaremerging and stopping criteria that enable selection of the optimal number of parts the best number of parts given the trade-off between tightness and parsimony in the decomposition. While merging over the over-segments already produces promising results, the results are yet dependent on the quality of the over-segmentation (third column of Fig.1-a). To overcome the dependency to the initialization, we present the nextrefinementstep that adjusts the bounding box parameters following a sequence of predefined actions. We design a Markov Decision Process (MDP) setup with asoftreward function that allows the bounding boxes to break the full coverage in themiddleof the process. This is the key to having more flexibility in traversing various cases and obtaining better results at the end (fourth column of Fig.1-a). Lastly, we extend the refinement step to see not only a single step of actions but multiple steps in a small sequence. We utilizeMCTS[mcts]and introduce acceleration techniques (in the supplementary) to speed up the additional refinement (last column of Fig.1-a). In our experiments with ShapeNet[shapenet2015], we demonstrate that our method guaranteeing full coverage provides better tightness and reconstruction compared with the baseline methods while approximating the shapes into similar numbers of cuboids. We additionally show that the decomposition of a shape based on our bounding boxes is better aligned with the semantic parts than other methods. Furthermore, we provide the result ofSMARTon Objaverse[objaverse]and OmniObject3D[wu2023omniobject3d]to show its applicability to real data and various categories of 3D shapes. To summarize, We present a novel framework for finding a set of tight bounding boxes of a 3D shape by optimizing volumetric objectives with iterative search methods. We first propose a hierarchical merging method exploiting over-segmentation from a pre-segmentation and tightness-aware merging and stopping criteria. We also introduce a bounding box refinement process with an effective soft reward function that allows wider exploration in the action space. Finally, we present an MCTS-based efficient exploration of multi-action sequences with acceleration techniques.
====================
	 Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration
==========
Abstract:  Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. In parallel, the problem of data scarcity has brought a growing interest in employing AIGC technology for high-quality data expansion. However, this paradigm requires well-designed prompt engineering that cost-less data expansion and labeling remain under-explored. Inspired by LLM's powerful capability in task guidance, we propose a new paradigm of annotated data expansion named as ChatGenImage. The core idea behind it is to leverage the complementary strengths of diverse models to establish a highly effective and user-friendly pipeline for interactive data augmentation. In this work, we extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and make the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks. Finally, we present fascinating results obtained from our ChatGenImage framework and demonstrate the powerful potential of our synthetic data for systematic vision adaptation. Our codes are available at https://github.com/Yuqifan1117/Labal-Anything-Pipeline.
Fluency score: 3
Fluency issues: Awkward phrasing such as "cost-less data expansion and labeling remain under-explored" and "communicate with AIGC model to achieve."

Clarity score: 3
Clarity issues: Unclear phrases like "cost-less data expansion and labeling remain under-explored" and the use of jargon such as "LLM," "AIGC," without clarity.

Grammaticality score: 4
Grammaticality issues: Minor issues such as missing articles like "the" before "AIGC model."

Readability score: 3
Readability issues: The text is dense with jargon and complex concepts that are not explained, making it less readable for a general audience.

Coherence score: 3
Coherence issues: The text uses many technical terms without explanation, and the flow of ideas could be better organized for coherence.
==========
Introduction:  In the past decade, deep learning techniques have demonstrated promising performance across diverse tasks, owing to the availability of large-scale annotated data[12,19,10]. However, it is time-consuming and expensive to manually collect a large-scale annotated dataset containing every possible domain for robust training. Besides, the problem of cross-domain and long-tail distributions within existing datasets have a detrimental effect on the performance and robustness of vision models, thereby impeding their generalization ability to novel categories or unseen domains. This promotes us to explore a less labor-intensive way to harvest labeled data containing multiple domains in one step for robust vision tasks. One effective strategy to improve generalization and robustness is to enlarge the scale of training data by intricate augmentations[14]. There are several GAN-based models[7,17]generating images for vision tasks, but their applicability remains constrained by their narrow focus on specific settings or small scales. Recently, AIGC models[27,28,29]have emerged as promising candidates for generating high-quality synthetic data, with the ability to address the limitations of the existing dataset. There are several early attempts at exploring synthetic data from generative models for data augmentation[13,23,3,37]. Albeit promising, early works usually produce simple scenarios or object-centric images only by global constraints (i.e., “airplane" or “a white airplane hovering over a beach and a city".), which limits downstream models’ perception of intricate scenes and fine-grained attributes. Additionally, these methods concentrate on generating images under typical scenarios (e.g., daylight, field), while neglecting less common but predictable circumstances (e.g., snow, forest, night). This limitation may impede the ability of deep learning models to generalize when deployed in real-world environments that exhibit unseen test distributions. In this paper, we present a novel approach named ChatGenImage that facilitates more controllabel data augmentation. ChatGenImage harnesses the collaborative power of the LLM and AIGC models, enabling iterative communication between them in a cost-effective and controllable manner. This automatically iterative process facilitates the generation of high-quality synthetic images depicting complex scenes and diverse domains, along with fine-grained annotations. Our fundamental intuition is that large language models have remarkable capabilities to perform new tasks in a zero-shot manner when presented with well-crafted instruction prompts[34,35,36,11]. We discover that these LLMs like ChatGPT possess the capability to autonomously navigate image editing processes. By strategically designing appropriate prompts, LLMs can leverage the inherent knowledge within the system and effectively guide the AIGC models to produce highly controllable and intricate images. While ChatGPT contains diverse world knowledge for simulating the human brain’s efficient processing, it is non-trival to elicit this knowledge from it for data augmentation with automatic labeling because ChatGPT is a pure language model that lacks the ability to visually perceive any information. We explore this issue in the context of generative data augmentation, showing that language can act as a bridge connecting LLMs and AIGC models, producing elaborate images for downstream tasks by globally controllable prompts and iteratively local editing instructions. To this end, we demonstrate three key findings. First, we find that the LLM such asChatGPTcontains a wealth of conceptual knowledge and can imagine vivid descriptions even with only one label word (e.g. Adogplaying in a lush green park, with a frisbee in its mouth. The dog should havea shiny coat of fur.)[33,6]. We further obverse that the existing AIGC models can only generate simple image with few objects and backgrounds, which are not diverse for domain generalization[20]. Thus, we establish the iterative pipeline to repair missing details and refine generated images with the help of label foundation toolkits and local editing prompts. Finally, we demonstrate our method flow to produce large amounts of high-quality synthetic data with fine-grained labels in a scalable manner for data augmentation in data scarcity scenarios.
====================
	 DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback
==========
Abstract:  Traffic Signal Control (TSC) aims to reduce the average travel time of vehicles in a road network, which in turn enhances fuel utilization efficiency, air quality, and road safety, benefiting society as a whole. Due to the complexity of long-horizon control and coordination, most prior TSC methods leverage deep reinforcement learning (RL) to search for a control policy and have witnessed great success. However, TSC still faces two significant challenges. 1) The travel time of a vehicle is delayed feedback on the effectiveness of TSC policy at each traffic intersection since it is obtained after the vehicle has left the road network. Although several heuristic reward functions have been proposed as substitutes for travel time, they are usually biased and not leading the policy to improve in the correct direction. 2) The traffic condition of each intersection is influenced by the non-local intersections since vehicles traverse multiple intersections over time. Therefore, the TSC agent is required to leverage both the local observation and the non-local traffic conditions to predict the long-horizontal traffic conditions of each intersection comprehensively. To address these challenges, we propose DenseLight, a novel RL-based TSC method that employs an unbiased reward function to provide dense feedback on policy effectiveness and a non-local enhanced TSC agent to better predict future traffic conditions for more precise traffic control. Extensive experiments and ablation studies demonstrate that DenseLight can consistently outperform advanced baselines on various road networks with diverse traffic flows. The code is available at https://github.com/junfanlin/DenseLight.
Fluency score: 4
Fluency issues: [The sentence beginning with "Although several heuristic..." and the sentence starting with "Therefore, the TSC agent..." are slightly complex and lengthy]

Clarity score: 4
Clarity issues: [The use of abbreviations such as "TSC" and "RL" without first defining them could confuse readers unfamiliar with the terms. Complex sentences could be broken into simpler ones for better clarity.]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Complex and lengthy sentences with compound ideas may hinder smooth reading for some audience members.]

Coherence score: 4
Coherence issues: [The challenges are labeled as "1)" and "2)" but the solutions are not correspondingly labeled, which could slightly affect the text's structural coherence.]
==========
Introduction:  Reducing traffic congestion is an essential task for efficient modern urban systems. As the number of vehicles in the cities increases year by year, backward traffic coordination not only does harm to the driving experience but also aggravates air contamination with more harmful fuel emissionsZhang and Batterman (2013). Alleviating traffic congestion by efficient traffic signal control (TSC)Mirchandani and Head (2001)is one of the most practical and economical approachesTaylor (2002). Specifically, TSC aims at coordinating the traffic lights of a road network to regulate the traffic flows to minimize the average travel time of vehicles. However, the traffic data collected from a road network is usually massive yet incomprehensibleLiuet al.(2020); Wanget al.(2023). Therefore, the widely-adopted TSC strategies either fix signal routineRoesset al.(2004)or adapt the traffic signal plans in real-time according to traffic flow patternsCoolset al.(2013). To automatically mine useful information from the massive traffic data, more and more studies leverage the powerful representation capability of deep neural networksLeCunet al.(2015)to learn TSC agentsVan der Pol and Oliehoek (2016); Zhanget al.(2021); Zhenget al.(2019); Oroojlooyet al.(2020); Weiet al.(2018,2019a); D’Almeidaet al.(2021)through the advanced reinforcement learning (RL) methodsMnihet al.(2015); Schulmanet al.(2017); Lianget al.(2021). A proper reward function is required for applying RL to the TSC problem and improving the policy. Since the travel time is only feasible after a vehicle leaves the road network and fails to provide instant/dense feedback to the TSC policy in time during its journey, many previous worksVaraiya (2013); Weiet al.(2019a,b); Xuet al.(2021)draw the traditional characteristics of traffic intersections for the reward design, such as traffic pressure and queue length at the intersection. However, most of these heuristic rewards may be biased from the ultimate goal of TSC, i.e., the average travel time minimization. The introduced biases could lead the RL methods to adjust the TSC policy in an incorrect direction. Apart from the reward design, it is also critical to endow the RL agents with the capability of precisely predicting the future dynamics of the environment to make well-founded decisionsVan Hasseltet al.(2016); Fujimotoet al.(2018); Yanget al.(2016); Louet al.(2020). However, it is non-trivial for an RL agent to capture the future traffic dynamics of the intersections in the context of TSCChenet al.(2022). The future arriving vehicles of one intersection may be running in adistantintersection at present. To this end, thelocalobservations of either the intersection itself (i.e., a snapshot of vehicles at the intersection)Varaiya (2013); Weiet al.(2019a); D’Almeidaet al.(2021)or neighboring intersectionsWeiet al.(2019b); Xuet al.(2021)might be insufficient to predict the long-horizontal traffic dynamics of the intersection. Additionally, the location and traffic flow trend of an intersection also play a role in predicting the traffic dynamics of the intersection. For example, downtown intersections tend to be more crowded than suburban intersections. For example, a large number of vehicles entering an empty intersection may indicate the beginning of the rush hour. Therefore, the non-local observations and the location of the intersections, and historical traffic conditions are all important for a TSC agent to estimate future traffic situations more accurately. To address the issues discussed above, we propose a novel RL-based TSC method namedDenseLight, which improves traffic light coordination by exploiting dense information from both an unbiased reward and non-local intersection information fusion. Specifically, to provide dense and unbiased feedback for the policy improvement, we propose an equivalent substitute for travel time, i.e., the gap between the ideal traveling distance (i.e., the ideal distance a vehicle could have traveled at the full speed during its journey) and the actual traveling distance during the whole journey, namely Ideal-Factual Distance Gap (IFDG). Since the length of the factual journey of a vehicle is fixed according to its traveling lanes of the road network, minimizing IFDG is equivalent to minimizing travel time. Most importantly, IFDG can be calculated at each intersection and at each timestep. Therefore, IFDG can also provide instant feedback on the effectiveness of the control policy at each intersection. Besides an unbiased and dense reward, DenseLight also features aNon-local enhanced Traffic Signal Control(NL-TSC) agent to benefit TSC from the spatial-temporal augmented observation and the non-local fusion network architecture. Specifically, the NL-TSC agent supplements the original observation of an intersection with its location information and the previous observation so that each intersection can customize its own signal plans w.r.t. historical and spatial information. As for facilitating the TSC agents with a better awareness of the future traffic dynamics affected by other intersections, a non-local branch is proposed to enhance the local features of each intersection with the features of the non-local intersections. By learning to communicate the non-local information across non-local intersections, the NL-TSC agent can better predict the long horizontal traffic condition of each intersection and can thus make better coordination at present. Overall, our contributions are three-fold:1)we propose a novel RL-based TSC method, i.e., DenseLight, which is optimized by an unbiased and dense reward termed IFDG;2)to better model the future accumulated IFDG of each intersection, we develop the NL-TSC agent, effectively gathering spatial-temporal features of each intersection and propagating the non-local intersection information to improve the multi-agent RL policy;3)comprehensive experiments conducted on different real-world road networks and various traffic flows show that DenseLight can consistently outperform traditional and advanced RL-based baselines.
====================
	 Predicting Privacy Preferences for Smart Devices as Norms
==========
Abstract:  Smart devices, such as smart speakers, are becoming ubiquitous, and users expect these devices to act in accordance with their preferences. In particular, since these devices gather and manage personal data, users expect them to adhere to their privacy preferences. However, the current approach of gathering these preferences consists in asking the users directly, which usually triggers automatic responses failing to capture their true preferences. In response, in this paper we present a collaborative filtering approach to predict user preferences as norms. These preference predictions can be readily adopted or can serve to assist users in determining their own preferences. Using a dataset of privacy preferences of smart assistant users, we test the accuracy of our predictions.
Fluency score: 4
Fluency issues: [No specific issues, but sentence structure could be more varied.]

Clarity score: 4
Clarity issues: ["consists in asking" could be clearer as "consists of asking"]

Grammaticality score: 4
Grammaticality issues: ["consists in asking" is grammatically unusual; "consists of asking" is more correct.]

Readability score: 4
Readability issues: [The sentence "Using a dataset of privacy preferences of smart assistant users, we test the accuracy of our predictions." could be more readable if structured differently, e.g., "We test the accuracy of our predictions using a dataset of smart assistant users' privacy preferences."]

Coherence score: 5
Coherence issues: [No issues detected]
==========
Introduction:  Artificial intelligence (AI) technologies are making their way into our daily lives and into our homes. We have grown accustomed to using our devices to call friends, set reminders, or check the weather.However, for these technologies to be adopted and trusted by users, they must act as users expect, and this problem is especially apparent in the area of privacy preferences. Studies show that users are deeply concerned about how their data is being collected onlinemadden2014perceptions. Interestingly, while they expect AI to act as they desire, they are unwilling to spend time setting their preferences. For example, despite users’ concerns about privacy, studies show that they ignore or blindly accept cookie bannerskretschmer2021cookieand privacy policies in social networksObar2018lie. Furthermore, in social networks, a large proportion of users do not change default privacy settingsKrishnamurthy2009leakage. This can be explained as a result of privacy fatigueChoi2018Fatigue, the sensation of loss of control and futility over protecting one’s privacy. This leads to privacy cynicism, when users do not adopt a privacy protecting behaviour even if they are concerned about their privacyHoffmann2016Cynicism. Thus, the current approach of directly asking the user when a preference is unknown but needed fails to capture the user’s true preferences. Additionally, continual questioning prevents users from achieving their objectives with the device. In response, this paper advocates for an approach that can understand user preferences with less user involvement, in turn bringing more importance to user interactions whenever such preferences are needed. A particular platform in which capturing privacy preferences is challenging and yet essential is that of smart speakers and other smart personal assistants. These devices have benefited from widespread early adoption, and it is estimated that 500 million units were installed in the last quarter of 2021Strategy2021smartspeakerinstalled. Nonetheless, the early adoption of these technologies means that they still have several vulnerabilities that pose a threat to the security and privacy of their usersEduSPASurvey. Indeed, there have already been cases reported in which smart assistants have not functioned as expected; for example, a smart speaker recorded and sent a private conversation without the user’s consentGuardianAlexa. These situations hinder user trust in the technology and can ultimately lead users to limit the functionalities of the devices used, or even to adopting coping mechanismsAbdi2019More. This paper describes an alternative approach that addresses the issues outlined above. The critical observation underpinning our approach is that smart devices are just one part of a larger ecosystem (e.g. seeEduSPASurveyfor a description of the ecosystem of smart speakers), and they interact and share data with agents like services, apps, and other devices. For example, a smart watch might send a voice recording to a smart speaker, or might share the wearer’s heart rate with a health app. In this respect, we can understand this ecosystem as a multi-agent system in which the use of norms can help to regulate these interactions, implementing privacy preferences. Norms can effectively summarise complex privacy preferences into simple sets of regulations, as shown by Abdi et al.NouraPrivacyNorms2021, who gathered over 800 privacy preferences on data transmissions, yet produced just 17 norms. Moreover, although here we assume no knowledge of the domain, if such knowledge is available there exist techniques to generalise norms (seeMoralesAAMAS2013;MoralesAAMAS2014for an example) or find and resolve inconsistencies among themVasconcelosKN09. Furthermore, norms are also used by people, and are naturally understood by them, representing a good base upon which to construct explanations. This can be used not only to generate explanations for a user if something unexpected happens, but also to tailor interactions with a user to validate predicted norms. Norms are regarded as expected patterns of behaviourWooldridgeIntrodMAS, causing agents (each component in the smart device ecosystem) to coordinate better and function more efficiently. As an example with smart devices, imagine a service knows in advance the privacy norms of a user with regard to each component of the ecosystem. If this service needs to interact with other components, it can use the user’s norms to adapt its behaviour to avoid violating norms or to avoid performing unregulated transmissions of information, which might require consent. As informally outlined inSerramiaPRIMA22Collaborative, we canexploit the large user bases of smart devices to use knowledge of previously specified privacy preferences to infer new preferences or to assist users in specifying their preferences. In particular, we aim to exploit similarities between users to make privacy preference predictions using collaborative filteringSu2009Collaborative.Effectively, we see the smart device ecosystem as a multi-layered multi-agent system. The lower level represents the multi-agent system associated with a single user (that is, the user’s device, and the other devices, skills, and services that can be accessed from it). The higher level is that of the multi-agent system composed of all the users. Our approach is centred on the norm creation stage in the lower level multi-agent systems related to each user. Therefore, each device user has its own associated set of norms, and all agents within its lower level multi-agent system, be they devices, skills, or other services, are informed and affected by the norms whenever they want access to the user’s personal data.While many researchers have studied different approaches to constructing norm systems, like norm synthesisMoralesAAMAS2013;morales2015compactor norm emergenceshoham1997emergence;SavarimuthuAAMAS07;sugawara2011emergence, we are not aware of any similar approach like the collaborative filtering presented here. In taking this approach, we make the following contributions. Formalisation of the problem of predicting norms to ensure that computational behaviour aligns with user preferences. This is divided into two subproblems, namely preference approximation (predicting unknown user preferences) and norm inference (inferring norms from predicted preferences). Formalisation of preference prediction functions. We provide a specific example of this type of function based on the preferences of similar users. Inference of norms from the predicted preferences, and specification of different methods to do so based on the confidence of the prediction or other variables. The paper is structured as follows: Section2formalises the core problems we aim to address in the paper. In Section3we detail the process of predicting preferences. We then use these predictions to infer norms in Section4. Section5is dedicated to validate our findings. In Section6we discuss related work. Finally, in Section7we discuss conclusions and future work.
====================
	 Space reduction techniques for the $3$-wise Kemeny problem
==========
Abstract:  Kemeny's rule is one of the most studied and well-known voting schemes with various important applications in computational social choice and biology. Recently, Kemeny's rule was generalized via a set-wise approach by Gilbert et. al. This paradigm presents interesting advantages in comparison with Kemeny's rule since not only pairwise comparisons but also the discordance between the winners of subsets of three alternatives are also taken into account in the definition of the $3$-wise Kendall-tau distance between two rankings. In spite of the NP-hardness of the 3-wise Kemeny problem which consists of computing the set of $3$-wise consensus rankings, namely rankings whose total $3$-wise Kendall-tau distance to a given voting profile is minimized, we establish in this paper several generalizations of the Major Order Theorems, as obtained by Milosz and Hamel for Kemeny's rule, for the $3$-wise Kemeny voting schemes to achieve a substantial search space reduction by efficiently determining in polynomial time the relative orders of pairs of alternatives. Essentially, our theorems quantify precisely the nontrivial property that if the preference for an alternative over another one in an election is strong enough, not only in the head-to-head competition but even when taking into account one or two more alternatives, then the relative order of these two alternatives in all $3$-wise consensus rankings must be as expected. As an application, we also obtain an improvement of the Major Order Theorems for Kememy's rule. Moreover, we show that the well-known $3/4$-majority rule of Betzler et al. for Kemeny's rule is only valid in general for elections with no more than $5$ alternatives with respect to the $3$-wise Kemeny scheme. Several simulations and tests of our algorithms on real-world and uniform data are provided.
Fluency score: 4
Fluency issues: Long sentences with multiple clauses can hinder fluency.

Clarity score: 3
Clarity issues: Usage of specialized terms without explanation, complex sentence structures, and multiple embedded clauses can reduce clarity.

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: Dense with jargon, long complex sentences, and the academic tone can challenge readability for general audiences.

Coherence score: 4
Coherence issues: Heavy use of domain-specific language may impede coherence for readers not familiar with the field.
==========
Introduction:  In this article, anelectionis a finite collectionC=\{c_{1},\dots,c_{n}\}of alternatives together with avoting profileconsisting of a finite number of (not necessarily distinct) votes. Arankingor avoteis simply a complete and strict total ordering\pi\colon c_{\pi(1)}>c_{\pi(2)}>\dots>c_{\pi(n)}which we identify with a permutation of\{1,2,\dots,n\}also denoted by\pi. Here,x>ymeans that the alternativexis ranked before the alternativey. The space of all rankings, for fixedn, can be equipped with several natural distances, for example, the Kendall-tau distance which counts the number of order disagreements between pairs of elements in two permutations, namely, the bubble-sort distance between two permutations, or more generally thek-wise Kendall-tau distance[14](see Equation (2.1)). The important Kemeny problem (cf.[15],[16],[26]) consists of finding the set ofk-wise medians of a given election, i.e., permutations whose total distance to the voting profile is minimized with respect to thek-wise Kendall-tau distance. In other words, a median is a ranking that agrees the most with the voting profile. By taking into consideration not only pairwise comparisons but also the discordance between the winners of subsets of three alternatives, the3-wise Kemeny voting scheme seems to be more resistant to coalitional manipulation than the classical2-wise Kemeny rule: it is much more difficult for an alternative to win an election or even to simply win another specific alternative in an election under the3-wise Kemeny voting scheme. In fact, most of the best-known space reduction results for Kemeny’s rule fail in the3-wise setting (see[21, Table 1]), including the powerful Major Order Theorems discovered in[19](see Example5.1) and the Condorcet criterion. It was shown in[21]that even the2/3majority in every duel is not enough to guarantee that an alternative will win an election according to the3-wise Kemeny voting scheme. This phenomenon is in stark contrast to the Condorcet criterion where a Condorcet winner, namely an alternative which is preferred by more voters than any others, must be the unique winner of the election. Nevertheless, we know that an alternative obtaining a3/4majority in every duel must be the unique winner in the3-wise Kemeny voting scheme[21]. In many situations, the3-wise Kemeny rule is also more suitable than Kemeny’s rule since it puts more weight on alternatives which are more frequently ranked in top positions in the votes. Indeed, Kemeny’s rule puts equal weight on the head-to-head competition of two alternativesx,ywhen in a particular vote,xis the winner followed byyand when in another vote,xandyoccupy the last two position in that order. However, it is reasonable to assume that typical voters only pay attention to a shortlist of their favorite alternatives and put a rather random order for the rest of the alternatives. Such undesirable behavior creates noises that can alter the Kemeny median ranking while the problem can be solved effectively using the3-wise Kemeny voting scheme in specific instances (see Example5.1and AppendixB). The above limitation of the Kemeny rule leads to the notion of weighted Kendall tau distances introduced by Kumar and Vassilvitskii[17]as well as the notion of set-wise Kemeny distance of Gilbert et. al.[14]. Motivated by the above potential and interesting features of the3-wise Kemeny rule as well as the NP-hardness of the various Kemeny problems (see[arrow],[12],[14]), our main goal is to formulate new quantitative results concerning the majority rules in3-wise Kemeny voting schemes associated with the3-wise Kendall-tau distance introduced recently in[14], which provide much more refined space reductions to the Kemeny problem in comparison to existing techniques in the literature. Our first result (cf. Theorem3.1) shows that the fundamental3/4-majority rule of Betzler et al.[6]for the classical Kemeny rule is also applicable for all elections with no more than5alternatives with respect to the3-wise Kemeny scheme. However, the3/4-majority rule fails in general as soon as there are at least6alternatives. Note that without restriction on the number of alternatives, the5/6-majority rule obtained in[21]serves as the3-wise counterpart of the3/4-majority rule. The second and central result of the paper is the Major Order Theorem for the3-wise voting scheme, denoted 3MOT (Theorem5.4), and its improved version, denoted Iterated 3MOT (Theorem6.2), which, to the limit of our knowledge, are the most efficient space reduction techniques for the3-wise Kemeny rule in the literature. In essence, our Major Order Theorems show that if the preference for an alternativexover another alternativeyin an election is strong enough, as measured quantitatively not only in the head-to-head competition but also when taking into account the interactions withoneortwomore other alternatives, thenxmust be ranked beforeyinall3-wise medians of the election. The corresponding algorithms not only function efficiently in polynomial timeO(n^{4}m), wherenis the number of alternatives andmis size of the voting profile, but also drastically reduce the search space of3-wise medians. To get an idea on the efficiency and interests of our results, let0\leq p<1be the proportion of pairs of alternatives solved by 3MOT or Iterated 3MOT out of the total ofn(n-1)/2pairs. Then the original search space consisting of alln!possible rankings would be reduced by a factor (reduction rate) at least equal to (cf.[linear-extension-2018, Lemma 2, Lemma 4], see also Table2) On real-world data, especially for political elections and competitions where there exists a high similarity between the votes, our algorithms prove to be particularly useful aspusually ranges from0.6to0.9after only a few iterations of Iterated 3MOT (see Table2and also AppendixC.1for some concrete examples). The performance is also very encouraging even on the hard case of uniformly generated data where, for example whenm=15, the3-wise Major Order Theorems can determine the relative rankings of approximately47\%pairs of alternatives on average whenn=10and approximately31\%whenn=15. Our results not only extend and improve the important2-wise Major Order Theorem of[19](see Section2.4and Theorem7.1) but also provide a unified approach and technique which should pave the way for the research of more refined algorithms and quantitative properties ofk-wise Kemeny voting schemes fork\geq 2. It is worth comparing our method to the space reduction method based on a3-wise majority digraph introduced in[14, Theorem 3]whose vertices are the alternatives. While we can obtain, under some assumptions on the3-wise digraph of an election, a set of rankings which containssome3-wise median using[14, Theorem 3], our3-wise Major Order Theorems provide, for all pairs of alternatives(x,y), easy-to-compute and mild sufficient conditions so thatx>yinall3-wise medians. In fact, by relaxing the conditions in our Major Order Theorems, we can determine more relative orderings of a pair of alternatives insome3-wise median (see Theorem8.1). Another major difference is that[14, Theorem 3]only considers the strength of the preference forxoveryin the presence ofoneother alternativez\neq x,ywhile the3-wise Major Order Theorems quantify the preference forxoverynot only in the head-to-head competition but even when taking into accountoneortwomore alternatives, which should provide a more refined space reduction method (see Example6.4). In any case, the constraints on all3-wise medians found by our 3-wise Major Order Theorems should greatly accelerate other complementary space reduction methods and vice versa, nontrivial constraints obtained by other methods can serve as the inputs to boost our algorithms, especially Iterated 3MOT. Table1below summarizes our results and some well-known space reduction criteria for the classical and3-wise Kemeny voting schemes. 3-wise Kemeny rule Extended Always theorem (Pareto efficiency, unanimity) Phung & Hamel[21, Theorem 3] Phung & Hamel[21](TheoremC.4) 3/4-majority rule (Section2.3) Betzler et al.[6] Valid only for elections of5alternatives or less, Theorem3.1 Extendeds-majority rule Phung & Hamel[21, Section 4] Valid ifs\geq 5/6[21, Section 8] Major Order Theorems Milosz & Hamel[19](Section2.4) Improved Iterated MOT (Theorem7.1) Not valid, see Example5.1 3-wise Major Order Theorems Theorems5.4,6.2,8.1. To illustrate the utility of our obtained algorithms, we performed several simulations and tests on real-world and uniform data. Table3and Table4in AppendixCrecord the approximate average proportion of pairs with relative order solved by the3-wise Extended Always Theorem obtained in[21](see TheoremC.4) and the 3-wise Major Order Theorems over 100 000 uniformly generated instances. Several concrete examples with real-world data (elections with a dozen and up to 250 alternatives) taken from PREFLIB[18]are described in AppendixC.1(see also Table2). A direct implementation shows that for voting profiles withn,m\leq 40, the list of all pairs of alternatives with relative order in all3-wise medians determined by 3MOT, resp. Iterated 3MOT, can be obtained in approximately less than 2.5 seconds, resp. 1 minute, with an M1 MacBook Air 16GB RAM. More optimized implementation can definitely improve the running time of the algorithms. Finally, we explain how to apply our results and the set-wise Kemeny rule to deal with incomplete votes in Section9and propose the usage of the proportion of pairs whose relative order are determined by the3-wise Major Order Theorems as a meaningful measure of the consensus level of the electorates (see AppendixA).
====================
	 A Systems-Theoretical Formalization of Closed Systems
==========
Abstract:  There is a lack of formalism for some key foundational concepts in systems engineering. One of the most recently acknowledged deficits is the inadequacy of systems engineering practices for engineering intelligent systems. In our previous works, we proposed that closed systems precepts could be used to accomplish a required paradigm shift for the systems engineering of intelligent systems. However, to enable such a shift, formal foundations for closed systems precepts that expand the theory of systems engineering are needed. The concept of closure is a critical concept in the formalism underlying closed systems precepts. In this paper, we provide formal, systems- and information-theoretic definitions of closure to identify and distinguish different types of closed systems. Then, we assert a mathematical framework to evaluate the subjective formation of the boundaries and constraints of such systems. Finally, we argue that engineering an intelligent system can benefit from appropriate closed and open systems paradigms on multiple levels of abstraction of the system. In the main, this framework will provide the necessary fundamentals to aid in systems engineering of intelligent systems.
Fluency score: 4
Fluency issues: [No specific parts to highlight]

Clarity score: 3
Clarity issues: ["closed systems precepts", "required paradigm shift", "subjective formation of the boundaries"]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: ["we provide formal, systems- and information-theoretic definitions of closure", "subjective formation of the boundaries and constraints"]

Coherence score: 4
Coherence issues: [Switch from "we proposed" to "we provide" to "we argue" creates slight coherence issues]
==========
Introduction:  There has long been a call for a theory of Systems Engineering (SE) within the SE community with the aim of establishing SE as a standalone engineering field capable of addressing modern engineering problems[hazelrigg2022toward][kasser2011unifying]. However, there is an existing gap in concrete formalism and distinction for some fundamental concepts within the field that has led to ambiguity in some SE practices[salado2021systems]. While such formalism might not have been necessary in the past, the emergence of new kinds of complex systems such as Artificial Intelligence (AI)-enabled systems has challenged traditional SE practices[llinas2021motivations,tolk2011towards,mcdermott2021artificial,llinas2021systems,shadab2021shifting,smith2017cognitive]. In our previous work, we identified potential gaps in the current SE foundations to address the unique nature of AI-enabled systems[shadab2022closed]. We argued that intelligence is a relational property that can be characterized and engineered as a relation between the system and its context with both learning and intelligence properties embodied in the context regardless of the nature of the relations between them[shadab2022closed]. In this situation, intelligence is no longer relegated to a component or the physical boundary of the system. Therefore, we posited that owing to this high coupling between AI-enabled systems and their environment, utilizing the concept of closure in SE is a potential path forward to build general engineered intelligence[shadab2022closed]. We proposed that closed SE practices could be employed to realize the closed notion of this relational property between intelligent systems and their context. We further examined the possibility of employing closed systems precepts in an engineering framework in our later paper[cody2022core], concluding a lack of concrete definitions and formalism in the theory and practice of SE presents a barrier to applying closed system precept in engineering applications. Currently, most of the theoretical foundations in both systems theory and the theory of SE are bounded to the open systems precepts (i.e., inputs-outputs relations)[dag2000introduction]. Although the concept of closed systems is being utilized in limited applications in SE, there is little to no theoretical basis for these practices, making SE activities based on closed system precepts prone to interpretation and over-abstraction. As we have identified at least one domain that can benefit from closed systems precepts (AI-enabled systems), the need for clear definitions and formalism becomes increasingly important in the field of SE. This paper revisits the concept of closure in SE, aiming to formalize, define, and evaluate this concept as the first step towards employing closed systems precepts for intelligent systems. As mentioned earlier, closure has been vaguely applied in SE with limited underlying formal foundations[hutchison2018framework,di2018closed]. Various types of closure have been introduced in systems theory literature, including functional closure, organizational closure, operational closure, and informational closure, among others[zeleny1981autopoiesis,varela1974autopoiesis,bednarz1988autopoiesis,bertschinger2006information,bertschinger2008autonomy,mora2012closure]. As a starting point, closure can be understood as a property of a system that makes the system closed, and a closed system is defined as one that does not exchange energy, information, or matter through its boundaries[bertalanffy1968bertalanffy]. (This concept will be revisited in detail later in the paper.) However, there is little to no formal framework to describe the relationships and differences between each type of closure, and many of the closure types lack formal systems-theoretic definitions that distinguish them from the other types of closure; in fact, on many occasions, these terms are used interchangeably, which can cause confusion in the application of each type of closure[goodenough2002concept][di2018closed]. In this paper, we develop formal systems-theoretic definitions for two types of closure, functional and informational closure, in systems and compare them in terms of system’s characteristics and the relations between systems and their environment. Among all types of closure, functional and informational closure were selected as we believe they were more relevant to the systems engineering practices. We utilize a mathematical definition of functional dependency, information theory, and the systems-theoretic foundations for open and closed systems to produce formalism for functional and informational closure. Then, we determine the conditions and constraints to meaningfully use each of the two types of closure in systems. Our aim is to elaborate on the relations between these types of closure to determine their applications at different levels of abstractions for systems. Throughout this paper, we will use the termsclosureandclosednessinterchangeably.
====================
	 Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems
==========
Abstract:  We study the multi-objective minimum weight base problem, an abstraction of classical NP-hard combinatorial problems such as the multi-objective minimum spanning tree problem. We prove some important properties of the convex hull of the non-dominated front, such as its approximation quality and an upper bound on the number of extreme points. Using these properties, we give the first run-time analysis of the MOEA/D algorithm for this problem, an evolutionary algorithm that effectively optimizes by decomposing the objectives into single-objective components. We show that the MOEA/D, given an appropriate decomposition setting, finds all extreme points within expected fixed-parameter polynomial time in the oracle model, the parameter being the number of objectives. Experiments are conducted on random bi-objective minimum spanning tree instances, and the results agree with our theoretical findings. Furthermore, compared with a previously studied evolutionary algorithm for the problem GSEMO, MOEA/D finds all extreme points much faster across all instances.
Fluency score: 4
Fluency issues: [None significant, but uses technical terminology that may not be fluent for all readers]

Clarity score: 3
Clarity issues: [Use of technical jargon like "MOEA/D," "oracle model," and "fixed-parameter polynomial time" without explanation; "convex hull of the non-dominated front" may also be unclear to readers not familiar with the subject]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 3
Readability issues: [Highly technical language and domain-specific terms reduce readability for a general audience]

Coherence score: 4
Coherence issues: [The text is coherent for those familiar with the field, but connections between the sophisticated terms and concepts might not be clear to all readers]
==========
Introduction:  Evolutionary algorithms have been widely used to tackle multi-objective optimization problems in many areas such as robotics, pattern recognition, data mining, bioinformatics, scheduling and planning, and neural network training[Zhou2011]. Their population-based search operators make them a natural choice for simultaneously handling several possibly conflicting objectives. Many generic evolutionary multi-objective frameworks have been developed to supply basic implementations for any problem, and to provide templates that can be fine-tuned for specific applications (we refer to[Wang2023]for an overview of common approaches). Such features, along with their strong empirical performances in challenging applications, have led them to becoming one of the most attractive topics to researchers and practitioners alike. Among evolutionary multi-objective algorithms (EMOs), arguably the most exemplary are dominance-based approaches such as GSEMO and NSGA variants, with the former often being considered a baseline. Another popular technique for multi-objective optimization is to decompose the multiple objectives into a single-objective subproblem. The MOEA/D algorithm is a state-of-the-art application of this technique in evolutionary computation[Trivedi2016,Xu2020]. Despite the prevalence of EMOs on practical applications, rigorous analyses of their runtime behavior on meaningful problems are scarce. Nevertheless, these kinds of analyses are critical for (1) providing performance guarantees and guidelines to practitioners who use and develop these techniques in the field, and (2) promoting the explainability of heuristic search and optimization techniques by clarifying their working principles through a careful mathematical analysis. Run-time analyses on the performance of evolutionary algorithms have been provided for simple algorithms such as GSEMO in both artificial benchmark problems[Bian2018,Doerr2021]and others such as bi-objective minimum spanning tree[Neumann20071,Roostapour2020]and constrained submodular optimization[NIPS2015_b4d168b4,NIPS2017_d7a84628,Do2020,Qian2020]. In recent years, theoretical analyses of state of the art approaches such as NSGA-II and MOEA/D have been conducted[Huang2019,Huang2021,Huang20211,Zheng2022,Doerr2023,cerf2023proven]. Most of these run-time results are on artificial benchmark problems, and the one for NSGA-II on bi-objective minimum spanning tree proves promising. In this paper, we present for the first time rigorous results on MOEA/D for a classical multi-objective optimization problem, namely the multi-objective minimum weight base problem. This problem, falling under the matroid optimization category, is a significant generalization of the previously studied bi-objective minimum spanning tree problem. In this work, we focus on approximating the non-dominated front, as determining whether the front is reached is EXPSPACE. In particular, we show that MOEA/D obtains a factor2-approximation for two objectives in expected polynomial time. Previous analyses for the special case of graphic matroid (i.e. spanning forests) were only able to show a pseudo-polynomial run-time for GSEMO to obtain this approximation[Neumann20071]. We further extend the analyses by deriving a fixed-parameter polynomial expected run-time in instances withk>2objectives to reach ak-approximation. Instrumental to our analyses is a deeper understanding of the problem, and as such, we formally examine certain properties of the multi-objective minimum weight base problem. We first prove a tight approximation guarantee from computing the convex hull of the non-dominated front, extending the known guarantee for two objectives[Neumann20071]. With this in mind, we explore insight regarding this convex hull, including its vertex complexity and the structural relation among solutions whose weights constitute said convex hull. In addition, we briefly formulate an efficient deterministic approach to enumerate extreme points. These findings may be of interest in areas beyond runtime analysis.
====================
	 A Strategy for Implementing description Temporal Dynamic Algorithms in Dynamic Knowledge Graphs by SPIN
==========
Abstract:  Planning and reasoning about actions and processes, in addition to reasoning about propositions, are important issues in recent logical and computer science studies. The widespread use of actions in everyday life such as IoT, semantic web services, etc., and the limitations and issues in the action formalisms are two factors that lead us to study how actions are represented.   Since 2007, there have been some ideas to integrate Description Logic (DL) and action formalisms for representing both static and dynamic knowledge. Meanwhile, time is an important factor in dynamic situations, and actions change states over time. In this study, on the one hand, we examined related logical structures such as extensions of description logics (DLs), temporal formalisms, and action formalisms. On the other hand, we analyzed possible tools for designing and developing the Knowledge and Action Base (KAB).   For representation and reasoning about actions, we embedded actions into DLs (such as Dynamic-ALC and its extensions). We propose a terminable algorithm for action projection, planning, checking the satisfiability, consistency, realizability, and executability, and also querying from KAB. Actions in this framework were modeled with SPIN and added to state space. This framework has also been implemented as a plugin for the Prot\'eg\'e ontology editor.   During the last two decades, various algorithms have been presented, but due to the high computational complexity, we face many problems in implementing dynamic ontologies. In addition, an algorithm to detect the inconsistency of actions' effects was not explicitly stated. In the proposed strategy, the interactions of actions with other parts of modeled knowledge, and a method to check consistency between the effects of actions are presented. With this framework, the ramification problem can be well handled in future works.
Fluency score: 4
Fluency issues: Slight awkwardness in sentence construction, e.g., "a terminable algorithm", "also querying from KAB".

Clarity score: 3
Clarity issues: Dense technical language and acronyms like "IoT", "DL", "KAB", "SPIN" may be unclear to the layperson, lack of explanations of these terms.

Grammaticality score: 4
Grammaticality issues: Minor issues like "analyzed possible tools" might be better as "analyzed potential tools".

Readability score: 3
Readability issues: The text is heavy with jargon and complex phrases making it difficult for a general audience, e.g., "terminable algorithm for action projection".

Coherence score: 4
Coherence issues: Text jumps between topics; transitioning could be smoother, for instance, when discussing the "terminable algorithm", "plugin for Protégé".
==========
Introduction:  Many of the knowledge modeling and representation in our everyday lives are in dynamic knowledge structures. For example, in a web service architecture that contains a requester, a broker, and a provider, we are faced with step-by-step actions, and we can illustrate and represent the web service compositions in different case studies based on semantic webs[83]. There are many ways that dynamic-temporal knowledge can be described, modeled, reasoned about, and implemented[79]. In a related work section, we will briefly review works about action formalism in formal logics, computer science, and artificial intelligence. Change and time formalism has a very long history from Aristotle, Diodorus, and early Islamic logicians such as Avicenna, up to modern logicians. Change formalism, mostly studies in the field of “temporal logics”. In temporal logic, using one of the quantified or modal approaches, we formalize time in either point-based or interval-based time. The hybridization of action and change formalism, especially in dynamic knowledge base projects, is done in different methods in many of the formal logic and AI fields. Some projects, introduce dynamic logics and explicitly formalize the notion of actions and states, whereas the notion of time and change are implicit in them[13]. Other works, introduce temporal logic and formalize the notion of time and change, while the notion of action and dynamic behavior of the knowledge base is implicit in them. In this paper, we introduce an individual-based algorithm for description dynamic logics that implicitly contains the time notion. Unlike ontology versioning, we don’t track the ontology changes manually, but the actions automatically cause a new version of the knowledge base to appear. Therefore, we do not only have the ontology versions. The ”changes”, semantically, with the help of actions within the ontology itself, cause communication between two different states. Ontology versioning ignores the parts of an ontology that are constant and haven’t changed. In this paper, the entire new knowledge base produced by changes in actions will be examined by projection, and all realizability, executability, and planning services will be provided using TBox and ABox reasoning.
====================
	 Consistent Text Categorization using Data Augmentation in e-Commerce
==========
Abstract:  The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.   To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. We use a semi-supervised approach for data augmentation and presents two different methods for utilizing unlabeled samples. One method relies directly on existing catalogs, while the other uses a generative model. We compare the pros and cons of each approach and present our experimental results.
Fluency score: 4
Fluency issues: ["presents two different methods" should be "present two different methods"]

Clarity score: 4
Clarity issues: ["for utilizing unlabeled samples" is a bit vague and could provide specifics on how the samples are utilized]

Grammaticality score: 4
Grammaticality issues: ["presents two different methods" should be "present two different methods"]

Readability score: 4
Readability issues: [Use of complex sentences with several clauses could be simplified for better readability]

Coherence score: 4
Coherence issues: [The logical flow from identifying the problem to proposing the solution could be enhanced with clearer transitions]
==========
Introduction:  In the last two decades, widespread use of e-commerce platforms such as Amazon and eBay has contributed to a substantial growth in online retail. Such platforms rely on both explicit and implicit product features in order to deliver a satisfying user experience. There, the inferred product category is typically a crucial signal for many application such as browsing, search and recommendation. We focus on improving an existing product categorization model, we refer to as ’the categorizer’, that is employed by our company for fast categorization of billions of items on a daily basis. It classifies e-commerce items, such as products or deals, based on a predefined hierarchy of categories, namely GPT (Google Product Taxonomy). Given a product title, the categorizer assigns the most appropriate label in the taxonomy. The model itself is highly scalable and effective, so it is well-suited for settings with large and rapidly growing item catalogs. In our company, the categorizer is used as a standalone component in various e-commerce related services, such as recommendation, search, and ad ranking. A recent examination of the categorizer’s output revealed inconsistencies in the labeling of similar items. It was evident that in some cases small variations in product titles, such as those relating to colors or measurements, significantly affect the categorizer’s output. This inconsistency negatively impacts search and recommendation algorithms that rely on the inferred category, leading to a poor user experience. The concept of consistency in NLP tasks has been studied in various research works, including robustness to paraphrasing(Elazaret al.,2021)and robustness to adversarial attacks(Jinet al.,2020; Wanget al.,2020). Other works relate consistency issues with the misuse of spurious features during the learning phase(Arjovskyet al.,2019; Veitchet al.,2021; Wanget al.,2021). When examining the performance of the categorizer in terms of accuracy alone, the inconsistency issue may be overlooked. But, since many recommendation pipelines depend on the output of the product categorizer, an inconsistent model can have severe implications on the user experience. In most cases, the differences include returning the parent category or a sibling category, rather than a completely different category path. To tackle this inconsistency problem, we use differentdata augmentationtechniques and enrich the training data with item versioning, leading to a more consistent model. Data augmentation for improving various NLP tasks has been widely studied and surveyed(Shortenet al.,2021), and particularly in the context of consistency(Xieet al.,2020). Generating such data, both manually(Kaushiket al.,2019)and automatically(Rizoset al.,2019; Bariet al.,2020; Kumaret al.,2020), has shown to contribute to the robustness of learnt models in different settings. We chose to use data augmentation, without changing the current architecture of the already-in-use product categorizer for two main reasons. First, for scalability reasons, any change in the architecture might degrade the model’s ability to infer the categories of billions of items per day. Second, maintaining the current model architecture expedites the productization process and requires only minimal engineering effort. This work defines a new framework,Consistent Semi-Supervised Learning (Consistent-SSL), for consistent text categorization in the context of e-commerce (Section2). We use an unlabeled clustered dataset as a source of legit item versioning. The dataset is derived from product catalogs, and includes clusters of different versions of items. We present two different methods to utilize this unlabeled clustered data: a self-training method and a generative approach (Section3). We describe the datasets and the experimental framework we use for the evaluation of the proposed methods (Section4). Finally, we detail results, showing an improvement in the consistency rate of 4-10% above the baseline model, and discuss the advantages and weaknesses of each method (Section5).
====================
	 FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis
==========
Abstract:  Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Extensive experimental results on BULL demonstrate that FinSQL achieves the state-of-the-art Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer.
Fluency score: 4
Fluency issues: [awkward phrasing in "may not well-skilled"; should be "may not be well-skilled"]

Clarity score: 4
Clarity issues: [use of "practical financial analysis business" may be vague]

Grammaticality score: 4
Grammatical issues: [missing "be" in "may not well-skilled"]

Readability score: 4
Readability issues: [sentence structure and complex terms could impair understanding for non-experts]

Coherence score: 4
Coherence issues: [the transition between discussing the lack of a benchmark and the introduction of BULL could be smoother]
==========
Introduction:  Text-to-SQL aims to transform natural language questions into executable SQL queries, which enables low-code operations for relational databases. It can facilitate the data access procedure for non-professional database users who are not familiar with SQL and has gained much attention in various areas, especially in financial analysis. While financial professionals (e.g., investment advisors) need to query relevant databases frequently, they are usually not well-skilled in SQL programming. Therefore, Text-to-SQL is significantly important for financial analysis and has gained much attention. However, there is no Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases used in financial analysis. To address these issues, we construct a practical Text-to-SQL dataset for financial analysis based on the intelligent investment assistant product of Hundsun Technologies Inc., which facilitates more than 50 financial institutions (including Alipay, China Merchants Bank, and so on) and serves millions of personal users. This dataset, dubbed BULL, contains three databases corresponding to fund, stock, and macro economy respectively. Besides, in this dataset, there are 4,966 natural language question-SQL pairs annotated by financial professionals, data scientists, and software engineers from Hundsun Technologies Inc. Furthermore, BULL has both English and Chinese versions. Compared with the widely used Text-to-SQL benchmark datasets (e.g., Spider(Yuet al.,2018b)and BIRD(Liet al.,2023c)), BULL has much more tables for each database and much more columns for each table, illustrating as in Table1. Furthermore, table and column names in BULL are often expressed with abbreviations or vague representations. These characters require financial Text-to-SQL models to support large input context length and have strong context understanding ability. Fortunately, Large Language Models (LLMs)-based Text-to-SQL can satisfy these requirements, and several LLMs-based Text-to-SQL methods have been proposed recently. However, existing state-of-the-art LLMs-based Text-to-SQL methods typically depend on OpenAI’s APIs, such as GPT-3.5-turbo or GPT-4, which are expensive and have risks of information leakage. Therefore, these methods cannot be used in the financial applications where the information privacy is critically important. To avoid information leakage, a feasible way is to adopt open-source LLMs (e.g., LLaMA(Touvronet al.,2023)and Baichuan(Yanget al.,2023)) and train them in private domains. However, it faces three challenges: (1) Schema linking dilemma and data scarcity. It is difficult to establish connections between question and schema items for financial databases usually have a large number of columns and tables. Furthermore, due to the labeling cost, the number and diversity of labeled question-SQL pairs are limited. These issues obstruct the construction of concise and diversified prompts, which hinders the model’s performance; (2) Resource-consuming fine-tuning and cross-database generalization difficulties. Fine-tuning LLMs on downstream tasks demands several days of computation across multiple GPUs. The substantial cost associated with model updates and iterations poses a considerable challenge. Additionally, transferring the model to a new database incurs significant costs, which impedes the cross-database transfer. (3) Inconsistent output. Due to the inherent randomness and the decoder strategy of sampling, LLMs often generate inconsistent outputs, leading to syntactically incorrect and invalid SQL queries. To tackle these challenges, this paper proposes a model-agnostic LLMs-based Finacial Text-to-SQL model training and inference framework, dubbed FinSQL. It can be used to develop Text-to-SQL models based on any open-source LLMs. Figure1demonstrates the overall overview of FinSQL, which consists of three key components: prompt construction, parameter-efficient fine-tuning, and output calibration, corresponding to the above challenges correspondingly. Specifically, prompt construction consists of a parallel schema linking method and a hybrid data augmentation method, which help to construct more concise and diverse prompts and enhance the model’s performance from the input side. The parameter-efficient fine-tuning component adopts Low-Rank Adaptation (LoRA) to fine-tune a very small percentage of parameters (¡1%) to obtain weight plugins for different business scenarios and manages these plugins through a plugin hub. Based on this plugin hub, the database-specific Text-to-SQL models can achieve efficiently few-shot cross-database transfer. In output calibration, SQL post-processing is performed to enhance the correctness of the generated SQL. These three components contribute to FinSQL’s superior performance. Our contributions can be summarized as follows: We propose BULL, a practical benchmark dataset for financial Text-to-SQL. We propose FinSQL, a model-agnostic LLMs-based Text-to-SQL framework for financial analysis. Extensive experimental results on BULL demonstrate that FinSQL is model-agnostic and able to achieve the state-of-the-art performance; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer.
====================
	 Semantic Tokenizer for Enhanced Natural Language Processing
==========
Abstract:  Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: The phrase "subword regularization" may not be clear to all readers.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Phrases like "subword regularization" and "Glue tasks" may pose difficulties for readers not familiar with NLP jargon.

Coherence score: 5
Coherence issues: None
==========
Introduction:  NLP models have two primary components—a deep neural network and a vocabulary of embeddings. Recent improvements in NLP model performance have focused on improving deep networks and increasing model sizes. Interestingly, little attention has been paid to optimizing vocabularies. An analysis of recent models[9], plotted in Figure1,111Credit for figure to Huggingface’s DistilBERT: https://research.aimultiple.com/gpt/shows that model sizes (i.e., number of parameters) have increased by 15,000% over the last few years (excluding 175B parameter GPT-3[1]or 1T parameters of GPT4). The increase in model size significantly increases training costs. Recent publications show that a single training run for GPT-3 could cost $12M.222https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/Even BERT-Large[3]training costs reach tens of thousands of dollars. Furthermore, increased model size place additional computational burden during model execution. These costs can have a detrimental effect on NLP innovation. At the same time, the size of vocabularies has increased only about 100% and the size of embedding vectors has increased about 200%. Hence, over the same period, the fraction of NLP parameters representing the vocabulary has shrunk from 21% in BERT-base[3]to 0.3% in GPT-3[1]. An average person uses 42,000 root words and hundreds of thousands of wordforms[2]. Technical terminology and jargon add tens of thousands of additional words to the vocabulary. Even some of the largest vocabularies currently in use, DeBERTa[4], have only 128,000 tokens. Hence, NLP vocabularies need to include subwords that can be combined to form multiple words[11]. Words that can not be represented using a single token are segmented into an initial subword followed by as many intermediate subwords as required. NLP models use a tokenizer to convert strings of characters into a sequence of lexical tokens. Tokenizers also construct the vocabulary of lexical tokens. We present a novel tokenizer that improves NLP performance by improving subword formation and embedding quality throughsemantic tokenization.
====================
	 Audio2Gestures: Generating Diverse Gestures from Audio
==========
Abstract:  People may perform diverse gestures affected by various mental and physical factors when speaking the same sentences. This inherent one-to-many relationship makes co-speech gesture generation from audio particularly challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, easily resulting in plain/boring motions during inference. So we propose to explicitly model the one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code is expected to be responsible for the motion component that is more correlated to the audio while the motion-specific code is expected to capture diverse motion information that is more independent of the audio. However, splitting the latent code into two parts poses extra training difficulties. Several crucial training losses/strategies, including relaxed motion loss, bicycle constraint, and diversity loss, are designed to better train the VAE.   Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than previous state-of-the-art methods, quantitatively and qualitatively. Besides, our formulation is compatible with discrete cosine transformation (DCT) modeling and other popular backbones (\textit{i.e.} RNN, Transformer). As for motion losses and quantitative motion evaluation, we find structured losses/metrics (\textit{e.g.} STFT) that consider temporal and/or spatial context complement the most commonly used point-wise losses (\textit{e.g.} PCK), resulting in better motion dynamics and more nuanced motion details. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline.
Fluency score: 4
Fluency issues: Use of technical jargon without explanations (e.g., CNNs/RNNs, VAE, DCT, STFT, PCK) may impede fluency for a general audience.

Clarity score: 3
Clarity issues: The high density of specialized terminology and lack of context for the layperson (e.g., "bicycle constraint," "relaxed motion loss") detracts from overall clarity.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 3
Readability issues: Complex sentence structures and technical language present challenges for readability.

Coherence score: 4
Coherence issues: While the text is logically structured, the coherence is impacted by the abrupt introduction of concepts without sufficient background information.
==========
Introduction:  Generating vivid human-like co-speech gestures is of great importance for producing attractive avatars that people are willing to interact with. There has been a surging demand for generating realistic human motions for given audio clips recently. However, this problem is very challenging because of the complicated one-to-many relationship between audio and motion. For example, a speaker may perform different gestures under different conditions (e.g.happy/peaceful mood, standing/sitting state, or different environments) when speaking the same words due to different mental and physical states. Existing algorithms developed for audio to body dynamics have some obvious limitations. For example,[speech2gesture]adapts a fully convolutional neural network to co-speech gesture synthesis tasks. Nevertheless, their model tends to predict averaged motion and thus generates motions lacking diversity. This is due to the underlying one-to-one mapping assumption of their model, which ignores that the relationship between speech and co-speech gesture is one-to-many in nature. Under such an overly simplified assumption, the model has no choice but to learn the averaged motion when several motions match almost the same audio clips in order to minimize the error. The above evidence inspires us to study whether or not explicitly modeling this multimodality improves the overall motion quality. To enhance the regression capability, we introduce an extra motion-specific latent code. With this varyingfulllatent code, which contains the same shared code and varying motion-specific code, the decoder can regress different motion targets well for the same audio, achieving one-to-many mapping results. Under this formulation, the shared code extracted from audio input serves as part of the control signal. The motion-specific code further modulates the audio-controlled motion, enabling multimodal motion generation. Although this formulation is straightforward, it is not trivial to make it work as expected. Firstly, there exists an easy degenerated solution since the motion decoder could utilize only the motion-specific code to reconstruct the motion. Secondly, we need to generate the motion-specific code since we do not have access to the target motion during inference. Our solution to the aforementioned problems is providingrandom noiseto the motion-specific code so that the decoder has to utilize the deterministic information contained in the shared code to reconstruct the target. But under this circumstance, it is unsuitable for forcing the motion decoder to reconstruct the exact original target motion anymore. So arelaxed motion lossis proposed to apply to the motions generated with random motion-specific code. Specifically, it only penalizes the joints deviating from their targets larger than a threshold. This loss encourages the motion-specific code to tune the final motion while respecting the shared code’s control. A preliminary version of this work was presented in[audio2gestures]. In this paper, we extend it from the following aspects: (1) We thoroughly investigate structured and perceptual metrics (i.e.STFT, SSIM, LPIPS, and FID) as training losses in our framework. We find them complementary to the previous point-wise losses since they additionally consider local temporal and/or spatial data structures. Especially with STFT, our network can consistently yield higher-quality motions. (2) We improve the user controllability by switching the latent space into discrete cosine transform (DCT) space, where different DCT components can control different dynamics of the generated motions. And we find the audio-motion shared code is more related to motion speed and rhythm while motion-specific code is more related to small range variations. (3) We conduct extensive ablation studies on different network backbones and hyper-parameters of motion losses and present more detailed discussions and analysis on the observations. The overall contributions are summarized as follows: We present a co-speech gesture generation model whose latent space is split into audio-motion shared code and motion-specific code, to better model the training data pairs and generate diverse motions. We propose a new relaxed motion loss, accompanied by other training losses/strategies, to better avoid degeneration of the proposed network, enabling multimodal motion generation given the same audio input. The effectiveness of the proposed method has been verified on 3D and 2D gesture generation tasks by comparing it with several state-of-the-art methods. And this split formulation is compatible with DCT space modeling and other backbones (i.e.GRU, Transformer), and robust to hyper-parameter choice in the relaxed motion loss. In complement to the most commonly used point-wise metrics/losses, we analyze the generated motion by taking local spatial and/or temporal structure into consideration and introduce them as training losses to further improve the motion quality. As a byproduct, the proposed method is suitable for motion synthesis from annotations since it can well respect the predefined actions on the timeline by simply using their corresponding motion-specific code.
====================
	 BiBench: Benchmarking and Analyzing Network Binarization
==========
Abstract:  Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support. The results and analysis also lead to a promising paradigm for accurate and efficient binarization. We believe that BiBench will contribute to the broader adoption of binarization and serve as a foundation for future research. The code for our BiBench is released https://github.com/htqin/BiBench .
Fluency score: 4
Fluency issues: [Overall fluent, no specific issues]

Clarity score: 4
Clarity issues: ["milestone binarization algorithms that function at the operator level and with extensive influence" slightly abstract]

Grammaticality score: 5
Grammaticality issues: [No issues observed]

Readability score: 4
Readability issues: [Use of technical terms without definitions could hinder readability for non-experts]

Coherence score: 4
Coherence issues: [Connection between benchmarking and the broader adoption could be clearer]
==========
Introduction:  The rising of deep learning leads to the persistent contradiction between larger models and the limitations of deployment resources. Compression technologies have been widely studied to address this issue, including quantization(Gonget al.,2014; Wuet al.,2016; Vanhouckeet al.,2011; Guptaet al.,2015), pruning(Hanet al.,2015,2016; Heet al.,2017), distillation(Hintonet al.,2015; Xuet al.,2018; Chenet al.,2018; Yimet al.,2017; Zagoruyko and Komodakis,2017), lightweight architecture design(Howardet al.,2017; Sandleret al.,2018; Zhanget al.,2018b; Maet al.,2018), and low-rank decomposition(Dentonet al.,2014; Lebedevet al.,2015; Jaderberget al.,2014; Lebedev and Lempitsky,2016). These technologies are essential for the practical application of deep learning. As a compression approach that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology(Rusciet al.,2020; Choukrounet al.,2019; Qinet al.,2022a; Shanget al.,2022b; Zhanget al.,2022b; Bethgeet al.,2020,2019; Martinezet al.,2019; Helwegenet al.,2019). The binarized models take little storage and memory and accelerate the inference by efficient bitwise operations. Compared to other compression technologies like pruning and architecture design, network binarization has potent topological generics, as it only applies to parameters. As a result, it is widely studied in academic research as a standalone compression technique rather than just a 1-bit specialization of quantization(Gonget al.,2019; Gholamiet al.,2021). Some state-of-the-art (SOTA) binarization algorithms have even achieved full-precision performance with binarized models on large-scale tasks(Denget al.,2009; Liuet al.,2020). However, existing network binarization is still far from practical, and two worrisome trends appear in current research: Trend-1: Accuracy comparison scope is limited.In recent research, several image classification tasks (such as CIFAR-10 and ImageNet) have become standard options for comparing accuracy among different binarization algorithms. While this helps to clearly and fairly compare accuracy performance, it causes most binarization algorithms to be only engineered for image inputs (2D visual modality), and their insights and conclusions are rarely verified in other modalities and tasks. The use of monotonic tasks also hinders a comprehensive evaluation from an architectural perspective. Furthermore, data noise, such as corruption(Hendrycks and Dietterich,2018), is a common problem on low-cost edge devices and is widely studied in compression, but this situation is hardly considered existing binarization algorithms. Trend-2: Efficiency analysis remains theoretical.Network binarization is widely recognized for its significant storage and computation savings, with theoretical savings of up to 32\timesand 64\timesfor convolutions, respectively(Rastegariet al.,2016; Baiet al.,2021). However, these efficiency claims lack experimental evidence due to the lack of hardware library support for deploying binarized models on real-world hardware. Additionally, the training efficiency of binarization algorithms is often ignored in current research, leading to negative phenomena during the training of binary networks, such as increased demand for computation resources and time consumption, sensitivity to hyperparameters, and the need for detailed optimization tuning. This paper presentsBiBench, a networkbinarizationbenchmark designed to evaluate binarization algorithms comprehensively in terms of accuracy and efficiency (Table1). Using BiBench, we select 8 representative binarization algorithms that are extensively influential and function at the operator level (details and selection criteria are in AppendixA.1) and benchmark algorithms on 9 deep learning datasets, 13 neural architectures, 2 deployment libraries, 14 hardware chips, and various hyperparameter settings. We invested approximately 4 GPU years of computation time in creating BiBench, intending to promote a comprehensive evaluation of network binarization from both accuracy and efficiency perspectives. We also provide in-depth analysis of the benchmark results, uncovering insights and offering suggestions for designing practical binarization algorithms. We emphasize that our BiBench includes the following significant contributions: (1)Thefirstsystematic benchmark enables a new view to quantitatively evaluate binarization algorithms at the operator level.BiBench is the first effort to facilitate systematic and comprehensive comparisons between binarized algorithms. It provides a brand new perspective to decouple the binarized operators from the neural architectures for quantitative evaluations at the generic operator level. (2)Revealing a practical binarized operator design paradigm.BiBench reveals a practical paradigm of binarized operator designing. Based on the systemic and quantitative evaluation, superior techniques for more satisfactory binarization operators can emerge, which is essential for pushing binarization algorithms to be accurate and efficient. A more detailed discussion is in AppendixA.5.
====================
	 SHIELD: Thwarting Code Authorship Attribution
==========
Abstract:  Authorship attribution has become increasingly accurate, posing a serious privacy risk for programmers who wish to remain anonymous. In this paper, we introduce SHIELD to examine the robustness of different code authorship attribution approaches against adversarial code examples. We define four attacks on attribution techniques, which include targeted and non-targeted attacks, and realize them using adversarial code perturbation. We experiment with a dataset of 200 programmers from the Google Code Jam competition to validate our methods targeting six state-of-the-art authorship attribution methods that adopt a variety of techniques for extracting authorship traits from source-code, including RNN, CNN, and code stylometry. Our experiments demonstrate the vulnerability of current authorship attribution methods against adversarial attacks. For the non-targeted attack, our experiments demonstrate the vulnerability of current authorship attribution methods against the attack with an attack success rate exceeds 98.5\% accompanied by a degradation of the identification confidence that exceeds 13\%. For the targeted attacks, we show the possibility of impersonating a programmer using targeted-adversarial perturbations with a success rate ranging from 66\% to 88\% for different authorship attribution techniques under several adversarial scenarios.
Fluency score: 4
Fluency issues: [No specific fluency issues]

Clarity score: 4
Clarity issues: [Complex sentences with lots of embedded clauses may reduce clarity for some readers]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected]

Readability score: 4
Readability issues: [The high density of technical terms and statistics could challenge the general reader]

Coherence score: 4
Coherence issues: [Usage of multiple statistics and technical jargon could slightly affect coherence for readers not familiar with the subject matter]
==========
Introduction:  Code authorship attribution is the process of recognizing programmers of a given software, and there has been several works on robust and scalable attribution[4,13,23,8,6]. These methods have shown that programmers can be accurately identified by their coding style, making this problem an easy task thanks to the rapid development of code analysis and machine learning techniques. Accurate attribution benefits software forensics and security, especially for identifying malicious code programmers, detecting plagiarism, and settling authorship disputes. However, the process also poses privacy risks for programmers who prefer to stay anonymous. Recent code authorship attribution techniques heavily use machine learning models. While effective, those techniques are prone to manipulations that force the identification models to generate specific desired outputs, e.g., misclassification. One line of work for general machine learning algorithms utilized small perturbations to the input domain, resulting in adversarial examples (AEs) that are similar to the original ones, making it hard to distinguish them and posing a significant threat to machine learning models[24]. Examining AEs in the context of authorship attributions is an under-explored topic. This work introduces SHIELD for generating AEs at the source code level to prevent attribution while preserving code functionality. Such AEs will fool a classifier into misidentifying programmers and lead to targeted attacks, e.g., imitation or mimicking. Investigating such capabilities by examining how prone authorship attribution is to practical AEs allows a finer understanding of the state-of-the-art methods and helps address their shortcomings, especially with the increasing reliance on them for identifying coding style of programmers for security applications[7,13,8]. As a byproduct, our attacks can serve as a building block for maintaining the privacy of programmers in the presence of attribution techniques. SHIELD examines both the non-targeted and targeted attacks. In the non-targeted attacks, known asconfidence reductionormisclassificationattacks, SHIELD manipulates the input source code so that the identification model outputs any other author, i.e.,authorship dodging[28,26], where SHIELD can use this strategy to conceal the code author identity. In the targeted attack, SHIELD manipulates the input so that the identification model outputs a specifictargetauthor, i.e.,authorship imitationorevasion, depending on the adversarial capability and objective. We apply those scenarios to various authorship attribution methods with SHIELD for an in-depth analysis of each method. Although possibly extendable to binaries[14], SHIELD targets source-code authorship attribution for its prevalence. We note that source code-level attacks are challenging, since the generated AEs should be syntactically correct, should preserve the code functionality, and should not be easily detected. Although those attacks can be conducted using code transformation[21,26,19], a process similar to author obfuscation whereby the authorship traits are hidden in the transformed code, code transformation techniques require analyzing and changing the code to the target features across various categories, including layout, lexical, syntactic, control-flow, and data-flow features. However, a code perturbation approach provides an effective alternative for targeted and non-targeted attacks, by generating AEs to meet a specific goal without targeting the features of different categories. Conventionally, adversarial perturbations are applied directly to the input source and not to the feature space, e.g., perturbations in an image, not in the features extracted from that image. However, the code authorship attribution techniques are typically based on the authorship traits extracted from code, assuming a closed system where the feature extraction is not manipulated and requires perturbations at the code level. Therefore, attribution attacks should be designed explicitly using perturbations applied directly to the code. SHIELD injects carefully chosen code samples into the target source code and then obfuscates it using an off-the-shelf obfuscator. Unlike the prior work, where obfuscation is used to conceal the identity of the code author, we use obfuscation to make it difficult for the adversary to recognize or remove the injected code parts statically. Our threat model is more restrictive than without adopting obfuscation because code attribution must also be capable of identifying obfuscated source codes. However, it is well known that the attribution models work even in the obfuscated domain[4,13]. We note that while our AEs are generated at the code level, the eventual effect of the injection will be perturbations in the feature space. For convenience, we use the term perturbation to refer to injection. Contributions.Our key contributions are as follows. (1) We introduce SHIELD, a simple yet effective approach for generating AEs on code authorship attribution. The proposed approach does not change the original code but adds carefully-crafted code blocks (i.e., perturbations) to alter the authorship attributes of the code, leading to authorship dodging and imitation. (2) We provide a comprehensive evaluation of our technique against six state-of-the-art authorship attribution methods: DL-CAIS[4], WE-C-CNN, WE-S-CNN, TF-IDF-C-CNN, and TF-IDF-S-CNN[7], and Code Stylometry[13]. Our evaluation features a large-scale analysis of code authorship robustness under various adversarial scenarios using a dataset of 200 programmers. Our approach achieved a misidentification rate exceeding 98.5% for non-targeted attacks while significantly reducing the confidence of the model output. We also demonstrate imitation attacks at a rate of more than 66% for all targeted systems when using sufficient perturbations and at 88% when the adversary has access to samples of the targeted author. Organization.In SectionII, we provide a brief overview of authorship attribution workflow alongside specific details of the six targeted systems. In SectionIII, we introduce SHIELD and the attack strategy adopted in this work. In SectionIV, we define the non-targeted attacks and present the associated experimental results, followed by the targeted attacks and associated results in SectionV. In SectionVI, we discuss our findings, including the limitations and shortcomings of our approach. In SectionVII, we review the related work. We conclude our work in SectionVIII.
====================
	 Large Language Models on Lexical Semantic Change Detection: An Evaluation
==========
Abstract:  Lexical Semantic Change Detection stands out as one of the few areas where Large Language Models (LLMs) have not been extensively involved. Traditional methods like PPMI, and SGNS remain prevalent in research, alongside newer BERT-based approaches. Despite the comprehensive coverage of various natural language processing domains by LLMs, there is a notable scarcity of literature concerning their application in this specific realm. In this work, we seek to bridge this gap by introducing LLMs into the domain of Lexical Semantic Change Detection. Our work presents novel prompting solutions and a comprehensive evaluation that spans all three generations of language models, contributing to the exploration of LLMs in this research area.
Fluency score: 4
Fluency issues: [No significant issues, but some sentences could be more concise]

Clarity score: 4
Clarity issues: ["comprehensive coverage of various natural language processing domains" is slightly verbose]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 4
Readability issues: [Sentence structures are slightly complex, which may slightly lower readability for some audiences]

Coherence score: 4
Coherence issues: [The transition from discussing the lack of literature to introducing the authors' work could be smoother]
==========
Introduction:  Languages are evolving. A perpetual flux of changes occurs, driven by an array of factors encompassing cultural, social, technological, and often, undiscovered influences. In this ever-shifting linguistic landscape, words shed unused senses while concurrently acquiring new meanings. Languages engage in a reciprocal exchange, borrowing senses from one another, and simultaneously exerting influence. This intricate web of linguistic evolution necessitates an automatic approach to comprehend and assess the fluidity of languages. Automation becomes the key to navigating and interpreting the dynamic currents of linguistic transformation. However, the development of advanced computational methods for Diachronic Lexical Semantic Change (LSC) has been a blank slate for researchers. Since the 2010s, traditional embedding methods like PPMI, SVD, and SGNS have shown significant statistical correlation with human annotators and produced promising results in detecting shifts in word meaning(kulkarni2014statistically;hamilton2018diachronic;schlechtweg_wind_2019). As a result, previous works often lean towards using existing tools to uncover new meaning shifts rather than exploring novel algorithms to enhance them. Additionally, frequency-based algorithms typically depend on large corpora(tahmasebi2019survey). Their performance on relatively low-resource datasets remains a challenge, and an efficient solution for this has yet to be discovered. Since its introduction byvaswani2017attention, models based on the Transformer architecture have become the latest trend. Contextualized word embeddings generated by BERT(devlin2019bert)have provided a solid foundation for various downstream language tasks. Moreover, recently, Large Language Models (LLMs) have showcased remarkable capabilities in logical thinking and solving language tasks based on instructions(bubeck2023sparks;zhao2023survey;openai_gpt-4_2023;roziere_code_2023). This has inspired researchers to embrace LLMs for a modern approach to a series of lexical semantic tasks and explore their ability to understand natural language meanings. In this study111Access the GitHub repository through thislink., we conducted a series of tasks to assess the suitability of LLMs for LSC detection forTempoWiC(loureiro_tempowic_2022), a low-resource annotated tweet dataset. Our key findings are outlined as follows: We reassess the performance oftraditional methods(i.e., PPMI, SGNS, SVD) in addressing diachronic LSC on a low-resource dataset. We introduce a simple yet innovative generative approach for diachronic LSC detection. This method achieves promising results without requiring fine-tuning on pre-trained models. We conduct comprehensive evaluations for LLMs, BERT-based methods, and traditional methods at both the corpus level and instance level, offering insights into their respective capabilities in diachronic LSC detection.
====================
	 Generalized Relation Modeling for Transformer Tracking
==========
Abstract:  Compared with previous two-stream trackers, the recent one-stream tracking pipeline, which allows earlier interaction between the template and search region, has achieved a remarkable performance gain. However, existing one-stream trackers always let the template interact with all parts inside the search region throughout all the encoder layers. This could potentially lead to target-background confusion when the extracted feature representations are not sufficiently discriminative. To alleviate this issue, we propose a generalized relation modeling method based on adaptive token division. The proposed method is a generalized formulation of attention-based relation modeling for Transformer tracking, which inherits the merits of both previous two-stream and one-stream pipelines whilst enabling more flexible relation modeling by selecting appropriate search tokens to interact with template tokens. An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the token division module. Extensive experiments show that our method is superior to the two-stream and one-stream pipelines and achieves state-of-the-art performance on six challenging benchmarks with a real-time running speed.
Fluency score: 4
Fluency issues: [none]

Clarity score: 3
Clarity issues: [Use of jargon such as "two-stream trackers," "one-stream tracking pipeline," and "adaptive token division" without explanation.]

Grammaticality score: 5
Grammaticality issues: [none]

Readability score: 3
Readability issues: [Complex sentences and domain-specific terminology make the text less accessible to a general audience.]

Coherence score: 4
Coherence issues: [Some references to specialized methods like "Gumbel-Softmax technique" and "attention masking strategy" without context may impact overall coherence for readers unfamiliar with the subject.]
==========
Introduction:  Given the target bounding box in the initial frame of a video, visual tracking[javed2022visual]aims to localize the target in successive frames. Over the past few years, two-stream trackers[bertinetto2016fully,li2018high,li2019siamrpn++,zhang2020ocean], which extract features of the template and search region separately and then model cross-relations of the template and search region in a sequential fashion, have emerged as a dominant tracking paradigm and made a significant progress. Following this two-stream pipeline, several Transformer-based trackers[wang2021transformer,chen2021transformer,gao2022aiatrack]utilize parallel self-attention blocks to enhance the extracted features by modeling global self-relations within each image as illustrated in Fig.1(a). Recently, leveraging the flexibility of the attention mechanism, the one-stream pipeline[cui2022mixformer,ye2022joint,chen2022backbone]is proposed to jointly extract features and model relations, achieving promising performance. By conducting self-attention among all concatenated tokens, both cross-relation modeling and self-relation modeling can be performed simultaneously as illustrated in Fig.1(b). It is demonstrated in[xie2022correlation,cui2022mixformer,ye2022joint,chen2022backbone]that letting the search region interact with the template as early as possible is beneficial to target-specific feature generation. However, there is no evidence suggesting that all parts inside the search region should always be forced to interact with the template. Actually, due to the cropping strategy[bertinetto2016fully], there is a large proportion of background inside the search region, where distractors with similar appearance to the target may exist. This would lead to undesired cross-relations between the template and search region as the highly discriminative representations have not been extracted in some early layers. Although the attention mechanism can inherently weaken improper cross-relations, applying global cross-relation modeling to all layers may still be more or less disruptive. On the one hand, for the search tokens outside the target region, if undesired cross-relations are modeled between the template and distractors, the aggregated features of the distractors may contain the target features from the template, which could cause confusion for precisely identifying the actual target in the search region. On the other hand, for the template tokens, their quality could also be degraded by undesired cross-relations during the iterative update since certain features from the background or even distractors could be aggregated into these tokens. These situations could weaken the target-background discrimination capability of the one-stream pipeline. Intuitively, only a portion of search tokens,e.g. tokens belonging to the target, are suitable for cross-relation modeling when the feature representations are not perfect for target-background discrimination. In some cases, the two-stream relation modeling pipeline could even be better if the feature representations of both the template and search region are imperfect to model cross-relations. The potential limitations of the one-stream pipeline motivates us to ponder: is it really optimal for the template to interact with all parts inside the search region through all encoder layers in the one-stream pipeline? In this paper, we answer this question by proposing GRM, a generalized relation modeling method that can adaptively select the appropriate search tokens to interact with the template. To be specific, we classify the template and search tokens as three categories. The template tokens form one category while the search tokens are divided into another two categories. Instead of modeling relations within all the tokens as the one-stream pipeline, we restrict the interaction among the three token categories. Only the search tokens that are suitable for cross-relation modeling will interact with the template tokens, whilst the interaction between the remaining search tokens and the template tokens is blocked. With proper divisions, the two-stream pipeline and one-stream pipeline become two degenerated forms of our relation modeling method as discussed in Sec.3.2. Consequently, our method is a generalized formulation of attention-based relation modeling for Transformer tracking, which embraces the advantages of both previous pipelines while being more flexible. The search token division is performed by a lightweight prediction module, which can adaptively determine which search tokens are suitable for cross-relation modeling based on the input tokens. To accomplish this objective, there are two obstacles to overcome. First, the separate relation modeling for different token categories makes it hard for parallel computation. Second, the discrete token categorization is non-differentiable, thus impeding the end-to-end learning of the token division module. To facilitate parallel computation, we adopt an attention masking strategy to unify the individual attention operations into a single one. Additionally, we introduce the Gumbel-softmax technique[jang2016categorical]to make the discrete token categorization differentiable. Consequently, the search token division module can be implicitly optimized in an end-to-end manner, which promotes its adaptability to deal with different situations. In summary, our main contributions are three-fold: We present a generalized formulation of relation modeling for Transformer trackers, which divides the input tokens into three categories and enables more flexible interaction between the template and search region. To realize the generalized relation modeling, we devise a token division module to adaptively classify the input tokens. An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the proposed module. We conduct extensive experiments and analyses to validate the efficacy of our method. The proposed GRM exhibits outstanding results on six challenging visual tracking benchmarks.
====================
	 Absorbing Phase Transitions in Artificial Deep Neural Networks
==========
Abstract:  Theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics.
Fluency score: 4
Fluency issues: [No specific issue, but the text could be slightly more fluid for a general audience]

Clarity score: 3
Clarity issues: ["mean-field theory," "universal critical phenomena," "absorbing phase transitions," "order-to-chaos transition," "universality class," "finite-size scaling," "intuitive phenomenological argument" could be confusing without further context]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [Complex sentence structures and technical terminology make the text less accessible]

Coherence score: 4
Coherence issues: [Detailed points (i) and (ii) are clear, but the overall coherence could be improved with better explanations of specialized terms]
==========
Introduction:  The 21st century has witnessed the tremendous success of deep learning applications. Properly trained deep neural networks have successfully demonstrated performance comparable with, or even superior to, that of human experts in various tasks, a few remarkable examples being the game of GoSilver2016, image synthesisRombach2022, and natural language processingStahlberg2020. Boosted by an exciting discovery of the so-called neural network scaling lawsHenighan2020;Kaplan2020, the frenetic pace in improving neural network performance is likely to persist, and hence it may be safe to say that deep learning technologies will constitute indispensable building blocks of the next-generation human society. Despite the fact that practically deep learning models can achieve such impressive performances, theoretically their behaviors are not yet fully understood. Deep neural networks are usually heavily over-parametrized, with the number of parameters in state-of-the-art neural networks growing exponentially over timeXu2018. From an energetic viewpoint, the state-of-the-art deep learning models consume a lot of energy, as the number of parameters is correlated to the amount of energy needed to perform an inference. In contrast, human brains seem to be good at learning and generalizing in an energetically efficient manner, even though strictly speaking they are generally not at doing arithmetic operations. This suggests that placing and comparing artificial neural networks in a broader context of biological neural networks on an equal footing, at least from a functional perspective, is promising for developing their understanding. The notion ofcriticalityis the key to linking biological and artificial neural networks. Systems at a particular condition (e.g. at the critical point of second-order phase transitions) exhibit anomalous behavior, referred to ascritical phenomena. They are universal in the sense that microscopically diverse systems can be described by a single mathematical model as long as the essential properties remain unchanged. The critical phenomena of particular interest in neuroscience are those ofabsorbing phase transitionsHenkel2008;Hinrichsen2000: transitions to a state from which a system cannot escape (hereafter referred to as “an absorbing state”). Besides the obvious analogy with brains without any neuronal activity (i.e. death), absorbing phase transitions are considered to be one of the essential ingredients for self-organized criticalityDickman1998, by which the systems can be automatically tuned to the critical point. Recent theoretical and experimental studies support the view that the brains may operate near the critical point (albeit in a slightly nuanced manner), and the universal scaling law in the critical phenomena of absorbing phase transition has been attracting considerable interest among the community; interested readers are referred to, for example, the recent review by Girardi-SchappoGirardi-Schappo2021. As a matter of fact, the deep learning research community is also familiar (albeit implicitly) with the notion of criticality. In theoretical studies on deep neural networks, the concept ofthe edge of chaoshas played a considerable role. While the discovery of chaos in random neural networks dates back to (at least) as early as the late 1980sSompolinsky1988, the concept has attracted recent interest among the community when Pooleet al. theoretically demonstrated that infinitely-wide deep neural networks also exhibit the order-to-chaos transitionPoole2016. Remarkably, at the onset of chaos,trainabledepth of the networks is suggested to divergeSchoenholz2017, which is reminiscent of the divergence of the correlation length at the critical point of second-order phase transitions at equilibrium. Furthermore, recent work has successfully applied the renormalization group method to classify the order-to-chaos transitions in the fully-connected feedforward neural networks for various activation functions into a small number of universality classesRoberts2022. Nevertheless, we argue that the notion of criticality has not been fully exploited in studies of artificial deep neural networks. As also discussed by Hesse and GrossHesse2014, bottom-up approaches (in which one derives macroscopic properties from microscopic theories) and top-down ones (in which one starts from phenomenological observations or some heuristics to deduce macroscopic properties) are complementary to each other for studying complicated systems. Numerous works, including those cited in the previous paragraph, have successfully adopted one of the bottom-up approaches for a specific architecture and/or an activation function. However, the situation with regard to the top-down approaches is less satisfactory. Since the universality of the critical phenomena enables the classification of the systems into a reduced number of universality classes based on their fundamental properties, taking full advantage of it would lead us to intuitive and yet powerful understanding of the behavior of deep neural networks across different architectures. Given all these observations, the purpose of the present work is to demonstrate that the notion of absorbing phase transition is a promising tool for theoretical understanding of the deep neural networks. First, we establish an analogy between the aforementioned order-to-chaos transition and an absorbing phase transition by studying the linear stability of the ordered state. In the framework of the mean-field theory of signal propagation in deep neural networksPoole2016, the critical point is characterized by loss of linear stability of the fixed point corresponding to the ordered phase. We extend the analysis to the networks of finite width, and we directly see that the transition to chaos in artificial deep neural networks is an emergent property of the networks which requires the participation of sufficiently many neurons (and thus more appropriately seen as a phase transition, rather than a mere bifurcation in dynamical systems). Next, we show that the order-to-chaos transitions in initialized artificial deep neural networks exhibit the universal scaling laws of absorbing phase transition. Actually it is fairly straightforward to find the scaling exponents associated with the transition in the framework of the mean-field theory (or equivalently in the infinitely-wide networks) for the fully-connected feedforward neural networksSchoenholz2017, but it is not clear how we can extend the analysis into the networks of finite width or a different architecture. Our empirical study reveals that the idea of the universal scaling can still be successfully applied to such cases. We also provide an intuitive way to understand the resulting universality class for each architecture, based on a phenomenological theory. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics in the finite networks. To summarize, we believe that the this work places the order-to-chaos transition in the initialized artificial deep neural networks in the broader context of absorbing phase transitions, and serves as the first step toward the systematic comparison between natural/biological and artificial neural networks.
====================
	 Cross-dataset domain adaptation for the classification COVID-19 using chest computed tomography images
==========
Abstract:  Detecting COVID-19 patients using Computed Tomography (CT) images of the lungs is an active area of research. Datasets of CT images from COVID-19 patients are becoming available. Deep learning (DL) solutions and in particular Convolutional Neural Networks (CNN) have achieved impressive results for the classification of COVID-19 CT images, but only when the training and testing take place within the same dataset. Work on the cross-dataset problem is still limited and the achieved results are low. Our work tackles the cross-dataset problem through a Domain Adaptation (DA) technique with deep learning. Our proposed solution, COVID19-DANet, is based on pre-trained CNN backbone for feature extraction. For this task, we select the pre-trained Efficientnet-B3 CNN because it has achieved impressive classification accuracy in previous work. The backbone CNN is followed by a prototypical layer which is a concept borrowed from prototypical networks in few-shot learning (FSL). It computes a cosine distance between given samples and the class prototypes and then converts them to class probabilities using the Softmax function. To train the COVID19-DANet model, we propose a combined loss function that is composed of the standard cross-entropy loss for class discrimination and another entropy loss computed over the unlabelled target set only. This so-called unlabelled target entropy loss is minimized and maximized in an alternative fashion, to reach the two objectives of class discrimination and domain invariance. COVID19-DANet is tested under four cross-dataset scenarios using the SARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging results compared to recent work in the literature.
Fluency score: 4
Fluency issues: [No specific issues with fluency; minor awkwardness in the phrase "minimized and maximized in an alternative fashion"]

Clarity score: 4
Clarity issues: ["minimized and maximized in an alternative fashion" might confuse readers unfamiliar with the concept, "two objectives of class discrimination and domain invariance" is a bit unclear without context]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 3
Readability issues: [Long sentences with complex terminology can hinder readability for general audience, use of acronyms like "DL", "CNN", "DA", "FSL" without specifying what they stand for might confuse readers]

Coherence score: 4
Coherence issues: [While generally coherent, the text jumps quickly between concepts, and the connection between "cross-dataset problem," "domain adaptation," and "few-shot learning" could be expounded upon for smoother coherence]
==========
Introduction:  On December 31, 2019, the World Health Organization (WHO) reported unknown cases of respiratory diseases that have spread in Wuhan, China[29]. The disease is identified as a new virus part of the Coronavirus family that can cause illnesses ranging from the common cold to more serious respiratory diseases. The new virus was later known as COVID-19. On January 30, 2020, due to the spread of this disease in China and many other parts of the world, it was classified as a public health emergency by the WHO[62]. COVID-19 is a respiratory illness with symptoms similar to typical influenza. The transcription-polymerase chain reaction (RT-PCR) laboratory test is used as a reference tool for diagnosing COVID-19. In addition, X-rays and chest Computed Tomography (CT) scans are considered to be new information technology (IT) tools for COVID-19 diagnostics. What distinguishes the IT approach is its interpretability, which can help in fast decisions taken by doctors regarding COVID-19. CT is a painless and non-surgical imaging method characterized by speed and high accuracy. CT uses advanced X-ray technology to help detect many diseases and obtain detailed images of bones, internal tissues, and organs, where CT images give more details than traditional X-rays. Body parts absorb X-rays in unequal ways which allow the doctor to distinguish body parts and any changes due to disease[44]. Figure1shows examples of CT images from COVID-19 patients. Ai et al.[2]report on 1014 patients who received both PCR and CT scans in Wuhan, China, during the epidemic. They found that 90% of confirmed diagnostics of PCR had clear signs of COVID19 in chest CT scans appearing in the form of bilateral opacity. In another study, chest CT revealed bilateral opacity in the lung in 40 out of 41 patients (98%) with COVID-19 in Wuhan[54]. In addition, they show a high rate of appearance of ground glass opacity and uniformity with the round shape or occasional peripheral pulmonary distribution[54]. The probability of error in PCR tests, their limitations, and the length of time of their results, especially in areas affected by epidemic[63], as well as the low accuracy and sensitivity of X-rays in COVID-19 diagnosis, all made CT scans of the chest a very promising diagnostic tool for COVID19[8]. Recently, advances in computer vision and Machine Learning (ML) have led to the emergence of a novel generation of techniques in computer-aided disease diagnosis (CAD)[41]. In particular, Deep Learning (DL) in medical imaging has achieved outstanding performance in disease diagnosis and follow-up. DL has proven its important role and efficiency in medical image processing including classification, detection and segmentation tasks[5]. In this Chapter, we propose a DL method for the diagnosis of COVID-19 disease using chest CT scans. In particular, we focus on the problem of removing the need for labeling large amounts of data to train DL models for COVID-19 detection. In machine learning, deep CNN models provide excellent results with large amounts of labeled data. However, this is neither reasonable nor practical. Ideally, our ultimate goal is to design a model that can provide good classification results for new datasets without the need for a large labeling effort. One solution is cross-dataset learning, where we transfer knowledge from one labeled dataset to another unlabeled dataset. This is also known as domain adaptation (DA) in which we develop learning models that can intelligently adapt from one source domain (dataset) to another target domain. To our knowledge, this is the first work that addresses the problem of DA in COVID-19 detection. First, recall that the basic assumption of many machine learning algorithms is that training (source) and test (target) data come from the same distribution. However, in DA, the training and testing data come from different datasets taken under different circumstances which disproves the validity of this assumption. The distribution of data between source (training) and target (test) domains may change, causing low classification accuracy on the target data. The difference in distribution between different domains is still a very relevant problem among medical image datasets due to different image acquisition machines and circumstances. As a result, there is increased interest in DA within the field of medical images to solve this problem and improve classification performance. But in the area of COVID-19 detection, the DA research is still very limited[52,22]. In this objective, our proposed method is based on a new family of powerful CNN models called EfficientNet and on DA techniques to transfer knowledge from one domain to another. Our main contributions can be summarized in the following points: We present a DA method, called COVID19-DANet that can adapt a DL model from a source COVID-19 dataset to a target dataset. It uses the unlabeled samples from the target dataset to reduce the data distribution shift between the source and target datasets. The proposed DA method uses EfficientNet-B3 CNN as a feature extractor and a classification layer inspired by prototypical networks from the few-shot learning area. Inspired by the semi-supervised learning methods proposed in the machine learning literature, COVID19-DANet uses the entropy of the output probabilities over the unlabelled target set as a loss function for reducing the distribution shift between domains. The remainder of the chapter is organized as follows. In Section2, we review some related work using CT images for COVID-19 detection. We also survey some approaches based on DA learning, especially its field of application and typical setup. In Section3, we present the proposed DL-based DA model for COVID19 CT classification. Next, we present our experimental results in section4in terms of the most relevant assessment metrics. Finally, we outline our concluding remarks and future work suggestions in Section5.
====================
	 Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound
==========
Abstract:  Self-supervised pretraining has been observed to improve performance in supervised learning tasks in medical imaging. This study investigates the utility of self-supervised pretraining prior to conducting supervised fine-tuning for the downstream task of lung sliding classification in M-mode lung ultrasound images. We propose a novel pairwise relationship that couples M-mode images constructed from the same B-mode image and investigate the utility of data augmentation procedure specific to M-mode lung ultrasound. The results indicate that self-supervised pretraining yields better performance than full supervision, most notably for feature extractors not initialized with ImageNet-pretrained weights. Moreover, we observe that including a vast volume of unlabelled data results in improved performance on external validation datasets, underscoring the value of self-supervision for improving generalizability in automatic ultrasound interpretation. To the authors' best knowledge, this study is the first to characterize the influence of self-supervised pretraining for M-mode ultrasound.
Fluency score: 5
Fluency issues: N/A

Clarity score: 4
Clarity issues: Dense terminology and concepts may be difficult for non-specialized audiences to understand.

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 3
Readability issues: The text is somewhat dense with technical jargon, which could make it less accessible to a general audience.

Coherence score: 4
Coherence issues: The text quickly transitions between different aspects of the study, which might be challenging for some readers to follow without re-reading.
==========
Introduction:  Pneumothorax (PTX) is a potentially life-threatening acute condition in which air occupies the space between the pleura of the lungs, resulting in collapse of the lung. Rapid identification of PTX is crucial in emergency, critical, and acute care settings to expedite medical intervention. Point-of-care lung ultrasound (LUS) is a quick, inexpensive, portable, imaging examination that does not expose patients to radiation. Despite its low prevalence compared to chest radiographs, LUS has been shown to exhibit superior diagnostic performance for the diagnosis of PTX[Nagarsheth2011,Alrajhi2012]. The lung sliding artefact, caused by the normal motion of the pleura, has been described as a means to rule out PTX[Lichtenstein1995]. Notably, the presence of lung sliding excludes a diagnosis of PTX within the purview of the ultrasound probe[Lichtenstein1995]. Conversely, PTX is likely present when lung sliding is absent. Previous studies have demonstrated that deep convolutional neural networks (CNN) can be trained to distinguish between the presence and absence of lung sliding on motion mode (M-mode) ultrasound images[Javsvcur2021,VanBerlo2022]. Prior studies were limited by the amount of labelled data available for training and evaluation. Furthermore, previous studies initialized their networks using weights pretrained on the ImageNet dataset[Deng2009]. Despite the fact that M-mode images are profoundly distinct from the natural images present in ImageNet, it is common for medical imaging studies to leverage ImageNet-pretrained weights. They are publicly available for several common architectures and are able to extract low-level features present in medical images. Unfortunately, there are no publicly available equivalents for M-mode images, let alone LUS. Self-supervised learning (SSL) is a representation learning strategy applicable in the absence of labelled data. CNNs pretrained using SSL have exhibited superior performance and label efficiency compared to fully supervised counterparts[Chen2020,Grill2020,Zbontar2021,Bardes2022]. Broadly, SSL pretraining consists of training a deep neural network to solve apretext task, whose solution can be computed from unlabelled examples. The pretrained weights may be fine-tuned to solve adownstream taskfor which labels are present. This study explores the impact of self-supervised pretraining for the downstream task of detecting absent lung sliding in M-mode LUS, varying the choice of SSL method, weight initialization, data augmentation strategy, and inclusion of unlabelled data. Crucially, we demonstrate that incorporating large volumes of unlabelled M-mode images during the pretraining phase improves the performance of a fine-tuned classifier on external datasets. More specifically, our major contributions are as follows: A pairwise relationship for contrastive and non-contrastive learning that is specific to M-mode images A data augmentation pipeline specific to M-mode images in the context of pretraining A comprehensive investigation of factors that influence the utility of SSL pretraining for the downstream task of absent lung sliding detection, such as label efficiency, ImageNet initialization, and data augmentation Evidence that the inclusion of unlabelled data results in improved generalization to external datasets for absent lung sliding detection Fig.1summarizes our methods. To the best of our knowledge, no study has investigated the efficacy of SSL for M-mode ultrasound tasks.
====================
	 PDL: Regularizing Multiple Instance Learning with Progressive Dropout Layers
==========
Abstract:  Multiple instance learning (MIL) was a weakly supervised learning approach that sought to assign binary class labels to collections of instances known as bags. However, due to their weak supervision nature, the MIL methods were susceptible to overfitting and required assistance in developing comprehensive representations of target instances. While regularization typically effectively combated overfitting, its integration with the MIL model has been frequently overlooked in prior studies. Meanwhile, current regularization methods for MIL have shown limitations in their capacity to uncover a diverse array of representations. In this study, we delve into the realm of regularization within the MIL model, presenting a novel approach in the form of a Progressive Dropout Layer (PDL). We aim to not only address overfitting but also empower the MIL model in uncovering intricate and impactful feature representations. The proposed method was orthogonal to existing MIL methods and could be easily integrated into them to boost performance. Our extensive evaluation across a range of MIL benchmark datasets demonstrated that the incorporation of the PDL into multiple MIL methods not only elevated their classification performance but also augmented their potential for weakly-supervised feature localizations.
Fluency score: 4
Fluency issues: ["was a weakly" could flow better as "is a weakly"]

Clarity score: 4
Clarity issues: Usage of terms like "progressive dropout layer," "weakly-supervised feature localizations" without definitions make the text less clear to non-experts.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 3
Readability issues: Dense jargon ("weakly supervised learning," "overfitting," "regularization," "Progressive Dropout Layer," "feature representations") and complex sentences structure can be challenging for the average reader.

Coherence score: 4
Coherence issues: The relation between "regularization methods for MIL" and the new "Progressive Dropout Layer" could be made clearer.
==========
Introduction:  [2]\lipsum[3]
====================
	 SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval
==========
Abstract:  Video moment retrieval aims to localize moments in video corresponding to a given language query. To avoid the expensive cost of annotating the temporal moments, weakly-supervised VMR (wsVMR) systems have been studied. For such systems, generating a number of proposals as moment candidates and then selecting the most appropriate proposal has been a popular approach. These proposals are assumed to contain many distinguishable scenes in a video as candidates. However, existing proposals of wsVMR systems do not respect the varying numbers of scenes in each video, where the proposals are heuristically determined irrespective of the video. We argue that the retrieval system should be able to counter the complexities caused by varying numbers of scenes in each video. To this end, we present a novel concept of a retrieval system referred to as Scene Complexity Aware Network (SCANet), which measures the `scene complexity' of multiple scenes in each video and generates adaptive proposals responding to variable complexities of scenes in each video. Experimental results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR) achieve state-of-the-art performances and demonstrate the effectiveness of incorporating the scene complexity.
Fluency score: 4
Fluency issues: [none]

Clarity score: 4
Clarity issues: ["However, existing proposals of wsVMR systems do not respect"... might confuse some readers]

Grammaticality score: 5
Grammaticality issues: [none]

Readability score: 3
Readability issues: [The sentence structure is complex, with multiple clauses. Use of technical terms ("weakly-supervised VMR (wsVMR) systems", "Scene Complexity Aware Network (SCANet)") without explanation might hinder readability for a general audience.]

Coherence score: 4
Coherence issues: [Transitions between some sentences could be smoother; "However, existing proposals..." and "To this end, we present..." sections introduce new ideas abruptly.]
==========
Introduction:  Video search has the core building block of recently growing video streaming services (\egYouTube, Netflix). To enhance the capability of video search, video moment retrieval (VMR) aims to localize the start and end time of the moment pertinent to a given language query in an untrimmed video. The success of the VMR provides us with accurate video contextual information in less time and effort. Until recently, these remarkable search performances have been dependent on the size and quality of labeled training datasets. However, these datasets cost a labor-intensive annotating process (\ieAnnotators should find the start-end time of moments corresponding to query descriptions), and sometimes the annotated moments are ambiguous. To cope with this problem, many weakly-supervised VMR (wsVMR) methods[mithun2019weakly,duan2018weakly,zhang2020counterfactual,zheng2022weakly2]have been proposed by only utilizing the video-query pairs, which are less laborious to annotate. To perform the weak supervision using video-query pairs, if one query is paired (\ieannotated) with multiple videos, we can identify the common scene among these videos and determine the alignment between the query and the scene. To implement this, all videos are divided into multiple segments, and the retrieval system maximizes the similarity scores between each query and paired segments while suppressing the scores between the query and unpaired segments in other videos. During the inference, the system selects a segment with the highest score as a moment prediction for a given query. For the wsVMR systems to accurately classify the best segment in a video, numerous video-language joint representation learning methods[lin2020weakly,song2020weakly,zheng2022weakly2,wang2021visual]have been proposed. Recently, researchers also have another focus on a study of how to generate video segments to capture many scenes in a video[ma2020vlanet,zheng2022weakly2]. These segments are referred to as ‘candidate moment proposals’, which is crucial, as they directly affect the retrieval performances by regulating the proposal quantities. Unfortunately, as supervision is not available in generating proposals, wsVMR systems[zheng2022weakly1,zheng2022weakly2]use a fixed number of proposals for all input videos under heuristic optimization of a specific dataset, which is not reasonable to deal with varying numbers of scenes in a video. While some methods[ma2020vlanet,huang2021cross]consider varying numbers of proposals for each video, they still rely on spurious correlations, such as generating proposals proportionally to the video length or using sliding window. Therefore, the current proposal generation method could not accurately respond to the diverse number of scenes in each video. We refer to this situation as a ‘scene-proposal mismatch’. For instance, in Figure1(a), the systems produce an unnecessarily large number of proposals by referring to the long length of the video, but the video only contains a single scene (\iescene of sitting still in a chair throughout the video), which should be handled by small amounts of proposals. They also show scene-proposal mismatch by producing a small number of proposals for the video containing many scenes, such that those scarce proposals seem not to work correctly. Our experimental evidence in Figure1(b) validates the current wsVMR systems’ incorrectness due to the scene-proposal mismatch. We plot performances (\iemean Intersection over Union (mIoU) scores) over predictions along the number of scenes in videos and the number of proposals generated, which shows irregularities in the scores. The scores are low for videos with many scenes but few proposals and also low for videos with few scenes but many proposals. To estimate the number of scenes in a video, as shown in Figure1(c), we counted the number of paired queries for each video as a discrete approximation of the scene. Here, we found that some queries describing the same scene led to redundancy in the counting. Thus, we remove the redundancy of those queries via calculating their IoUs111We remove redundancy by scenes with IoU>0.5.between temporal boundary annotations222Temporal annotations are used only for identifying the proposal-scene mismatch problem and they are not involved in the wsVMR task. Our study further showed that the scene-proposal mismatch affects about 41% of videos in VMR benchmarks (\ieCharades-STA[gao2017tall], ActivityNet-Caption[krishna2017dense]). Intrigued by the scene-proposal mismatch, this paper proposes a wsVMR system referred to as Scene Complexity Aware Network (SCANet), which allows the system to mitigate the scene-proposal mismatch problem and generate proposals adaptive to the complexity of the scenes contained in the video. For a given input video, SCANet first defines the scene complexity with a scalar, meaning how difficult for the system to find (\ieretrieve) a specific scene among multiple distinguishable scenes in the video, which can be effective prior knowledge of video by complementing weak supervision of VMR. On top of the scene complexity, SCANet adaptively generates proposals and enhances their representations. Therefore, SCANet incorporates (1) Complexity-Adaptive Proposal Generation (CPG) that generates adaptive proposals by leveraging the quantities of proposals under consideration of the complexity and (2) Complexity-Adaptive Proposal Enhancement (CPE) that enhances the proposals’ representations corresponding to the scene complexity. Furthermore, motivated by recent success[zhang2020counterfactual,zheng2022weakly2]of contrastive learning for wsVMR system, we introduce technical contributions to mine hard negatives in the input video and further video corpus together under our designed framework. Our extensive experiments show the effectiveness of the proposed SCANet, and qualitative results validate enhanced interpretability.
====================
	 Differentiable Resolution Compression and Alignment for Efficient Video Classification and Retrieval
==========
Abstract:  Optimizing video inference efficiency has become increasingly important with the growing demand for video analysis in various fields. Some existing methods achieve high efficiency by explicit discard of spatial or temporal information, which poses challenges in fast-changing and fine-grained scenarios. To address these issues, we propose an efficient video representation network with Differentiable Resolution Compression and Alignment mechanism, which compresses non-essential information in the early stage of the network to reduce computational costs while maintaining consistent temporal correlations. Specifically, we leverage a Differentiable Context-aware Compression Module to encode the saliency and non-saliency frame features, refining and updating the features into a high-low resolution video sequence. To process the new sequence, we introduce a new Resolution-Align Transformer Layer to capture global temporal correlations among frame features with different resolutions, while reducing spatial computation costs quadratically by utilizing fewer spatial tokens in low-resolution non-saliency frames. The entire network can be end-to-end optimized via the integration of the differentiable compression module. Experimental results show that our method achieves the best trade-off between efficiency and performance on near-duplicate video retrieval and competitive results on dynamic video classification compared to state-of-the-art methods. Code:https://github.com/dun-research/DRCA
Fluency score: 4
Fluency issues: [the text is generally fluent]

Clarity score: 3
Clarity issues: [use of technical terms without explanation, assuming prior knowledge on the part of the reader]

Grammaticality score: 5
Grammaticality issues: [none]

Readability score: 3
Readability issues: [complex sentence structures, dense technical language]

Coherence score: 4
Coherence issues: [advanced concepts may not be easy to follow without domain knowledge]
==========
Introduction:  Video representation learning is a crucial research topic due to its numerous applications, such as recommendation system, video search, etc. Due to the high redundancy across video frames, a judicious and effective strategy to expedite video inference involves mitigating redundant computations. Some existingskipping-basedmethods[7,28,3,26,23]propose to skip non-saliency frames to save computation costs via various temporal saliency sampling strategies. These methods are based on the assumption that the most saliency frame/regions contribute the most to the video representation. However, due to the inevitable loss of spatial information and inaccurate saliency measures, these methods encounter challenges in some fast-changing and fine-grained scenarios. For example, discriminating between different dancing types is concealed within the gradual transitions of each frame. Besides, for tasks that rely heavily on detailed information, such as video retrieval, discarding non-saliency frames may result in losing essential clues for accurate matching. In contrast, somemulti-network-basedmethods[19,27,25]have been proposed to retain non-saliency frames while using multiple networks with different computational costs to mitigate the overall expenses. Although fusion layers are designed to integrate multiple streams of information, extracting global temporal information may be inadequate in representing fine-grained information to achieve precise contextual understanding. Besides, several of the above algorithms select different numbers of saliency frames based on the video content, making batched inference complicated and challenging. To address these issues, in this paper, we cast efficient video learning as a Differentiable Resolution Compression and Alignment(DRCA)task, which consists of two main components to accomplish resolution compression and alignment. We instantiate the two components asDCCM(Differentiable Context-aware Compression Module) andRAT-Layer(Resolution-Align Transformer Layer). Specifically, to reduce video tokens and minimize information loss, unlike previous methods[27,28,25]that use an additional network to measure frame saliency separately, ourDCCMincorporates the saliency assessment and frame compression directly into the network, enabling them to be not only context-aware but also learnable. Concretely, the DCCM can be end-to-end optimized through differentiable ranking, which solves an optimal saliency sorting sequence based on the predicted scores. By reducing the resolution of non-salient frames and decreasing the number of spatial tokens, significant reductions in spatial computational costs can be achieved, as the computational complexity of the Transformer increases quadratically for the number of tokens. Meanwhile, this also poses a challenge for existing networks like 3D convolutional neural networks[6,17]and Transformers[1,20,21], as they lack the capability to directly extract features in this multi-resolution context. Therefore, we propose a simple but effectiveRAT Layerto process the frame tokens with different resolutions. The RAT Layer can extract strong spatial-temporal correlations layer-by-layer, which is crucial for video tasks that require reasoning, such as retrieving the same incident, where some video frames captured by different photographers are visually similar but not identical. Notably, this novel approach enables efficient and accurate video representation learning while preserving the ability of batched inference with fixed token numbers. This is achieved by fully end-to-end optimizing integrated with a differentiable compression module. As a result, our method achieves the best trade-off between efficiency and performance compared to state-of-the-art (SOTA) methods in near-duplicated video retrieval(NDVR). Additionally, it demonstrates competitive performance in dynamic video classification compared to other SOTA methods. Our contributions are summarized as follows:(1) a novelDifferentiable Resolution Compression and Alignmentnetwork for efficient video learning(2) a novelDCCMdesigned to compress the non-essential information with differentiable ranking(3) a simple but effectRAT Layerto extract spatial-temporal correlations in multi-resolution compressed video sequence(4) achieving the best trade-off between efficiency and performance on NDVR task and competitive result on dynamic video classification task against SOTA.
====================
	 Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?
==========
Abstract:  Consumers are exposed to advertisements across many different domains on the internet, such as fashion, beauty, car, food, and others. On the other hand, fashion represents second highest e-commerce shopping category. Does consumer digital record behavior on various fashion ad images reveal their fashion taste? Does ads from other domains infer their fashion taste as well? In this paper, we study the correlation between advertisements and fashion taste. Towards this goal, we introduce a new dataset, Fashionpedia-Ads, which asks subjects to provide their preferences on both ad (fashion, beauty, car, and dessert) and fashion product (social network and e-commerce style) images. Furthermore, we exhaustively collect and annotate the emotional, visual and textual information on the ad images from multi-perspectives (abstractive level, physical level, captions, and brands). We open-source Fashionpedia-Ads to enable future studies and encourage more approaches to interpretability research between advertisements and fashion taste.
Fluency score: 4
Fluency issues: "Does ads from other domains infer their fashion taste as well?" (awkward phrasing and grammatical error with subject-verb agreement)

Clarity score: 3
Clarity issues: "Does consumer digital record behavior on various fashion ad images reveal their fashion taste?" (awkward and unclear), "ad (fashion, beauty, car, and dessert)" (list is awkward with the inclusion of 'dessert')

Grammaticality score: 3
Grammaticality issues: "Does ads from other domains infer their fashion taste as well?" (incorrect verb agreement 'does' instead of 'do'), "Towards this goal" (prefer 'To achieve this goal')

Readability score: 3
Readability issues: Long sentences with multiple clauses and parenthetical information reduce readability.

Coherence score: 4
Coherence issues: Jumps between consumer behavior, advertisement domains, and the introduction of the dataset can confuse the reader.
==========
Introduction:  It is understandable that there could be some correlation between ads and products for a same domain. For example, a user likes the style of a neckline in a fashion ads and might also like a fashion product that has similar style (Fig.1). However, is there any correlation between ads and products from different domains? Specifically, can we interpret a consumer’s product preference from her website browsing logs of various advertising domains? In the context of fashion online shopping, however, to our knowledge, no study has investigated the correlation between various ads domain and fashion taste on the consumer level, as shown in Fig.2. In this paper, we introduce a new user taste understanding dataset, Fashionpedia-Ads, which asks subjects to provide their preference on both ad images of various domain (fashion, beauty, car, food) and fashion product images. Furthermore, unlike fashion product images, ads images usually contains complicated and multiple perspectives of information (emotional, visually, textually…) that cause a consumer like them. For example, for a same ad image (Fig.1), a consumer might like it because of the neckline of the dress. However, another consumer might like this ad image because the emotional feeling created in this ad image. To fully understand the multi-correlation (both visual and textual) between ads and fashion product images liked by subjects, we exhaustively annotated both ads and fashion images from different perspectives: 1) abstractive level; 2) physical attributes with associated segmentations (localized attributes); 3) caption, and 4) brands on the ads. The aim of this work is to enable future studies and encourage more exploration to interpretability research between advertisements and fashion taste. The contributions of this work are: 1) we introduce Fashionpedia-Ads, consisting of three datasets (Ads, Social network style and E-commerce style fashion products). We bridge the connection among them through the subjects’ preference (like or dislike) on these images and the annotation from multi-perspectives (e.g. abstract & physical attributes). 2) we formalize a new task that not only requires models to predict whether a subject like or dislike a fashion product image based on given ad images of various domains, but also provide a rationale explaination why it makes this prediction from multi-perspectives.
====================
	 Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations
==========
Abstract:  Physics-informed neural networks have been widely applied to partial differential equations with great success because the physics-informed loss essentially requires no observations or discretization. However, it is difficult to optimize model parameters, and these parameters must be trained for each distinct initial condition. To overcome these challenges in second-order reaction-diffusion type equations, a possible way is to use five-point stencil convolutional neural networks (FCNNs). FCNNs are trained using two consecutive snapshots, where the time step corresponds to the step size of the given snapshots. Thus, the time evolution of FCNNs depends on the time step, and the time step must satisfy its CFL condition to avoid blow-up solutions. In this work, we propose deep FCNNs that have large receptive fields to predict time evolutions with a time step larger than the threshold of the CFL condition. To evaluate our models, we consider the heat, Fisher's, and Allen-Cahn equations with diverse initial conditions. We demonstrate that deep FCNNs retain certain accuracies, in contrast to FDMs that blow up.
Fluency score: 4
Fluency issues: The extensive use of jargon and technical terms may reduce fluency for readers unfamiliar with the subject.

Clarity score: 3
Clarity issues: The text assumes knowledge of specific terms (e.g., "physics-informed loss", "CFL condition") without clarification, and it is heavily laden with specialized terminology.

Grammaticality score: 5
Grammaticality issues: None.

Readability score: 3
Readability issues: The text is dense with technical language and acronyms, making it less accessible to a general audience.

Coherence score: 4
Coherence issues: The transition from the problem of the physics-informed neural networks to the solution of FCNNs could be smoother; there is a coherent structure but some elements may require re-reading for full understanding.
==========
Introduction:  Natural and chemical phenomena, as well as some problems in the real world, can be described through mathematical expressions, in particular partial differential equations (PDEs). For example, fluid flow, chemical reaction-diffusion, phase separation, image analysis, image segmentation, cell division, the spread of infectious diseases, etc., can be mathematically expressed. Therefore, to find solutions to PDEs or mathematically analyze the characteristics of phenomena and changes in energy over time that these equations represent, numerical studies aimed at approximating PDE solutions are continuously being conducted such as finite difference method (FDM)RLV2007;YZ2009;EBKP2011;JKDJSYYC2017;YLJK2017, finite element method (FEM)NFJXYY2011;AD2011;CJ2012;NKSMAB2022, finite volume method (FVM)QXXXNF2012;EFA2015;SM2015;PZZL2022, and so on. Moreover, machine learning/deep learning models have been developed to solve approximately PDE solutions. The application of physics-informed neural networks (PINNs)pinnhas led to great success in solving partial differential equations because the physics-informed loss does not require any observations or discretization. Also, PINNs achieve acceptable accuracy for diverse simulationspinn2;pinn3;pinn4;pinn5;pinn6. However, optimizing model parameters remains a challenge, and PINNs should be trained separately by each initial condition. To address these problems, a possible way is to use data-driven models that can learn numerical schemes using snapshots and predict solutions at further time steps. Using a supervised learning approach, convolutional neural networkscnnhave been widely applied to solve partial differential equationsphygeo;convpde;poicnnbecause the mechanism of the convolution operator is similar to numerical methods that utilize neighboring points to obtain values at the next time step. However, the prediction of data-driven machine learning models and numerical methods is strongly affected by the time step of given snapshots, so appropriate time steps should be chosen. Here, we focus on thereceptive fieldreceptivefield;yolov4which refers to the size of input nodes that affect a single output node. Modern convolutional neural networksvgg;resnet;effnethave been designed to acquire large receptive fields for good feature extraction. In other words, the large receptive field increases the capacity of the indirect connectivity between an input and its output so that plenty of the input nodes are involved in the output extraction. In FDMs, simulation errors and time steps are also influenced by the receptive field size related to the order of approximations to derivatives (e.g., 5-point stencil vs. 9-point stencil). The stability condition of 2D heat equation (\phi(x,y)_{t}=\phi_{xx}+\phi_{yy}) ish^{2}/4, whereh=\frac{1}{\Delta x}=\frac{1}{\Delta y}. \phi(x,y,t_{n})=e^{iqx}e^{iry}then, whereGfis a growth factor defined as The worst case isq\Delta x=r\Delta y=\pi, then Therefore, the stability condition is Sinceh=\frac{1}{\Delta x}=\frac{1}{\Delta y}, we obtain ∎ Basically, the time step size\Delta tcan be decided by Theorem1, which provides an analysis of the stability range of the explicit scheme for the two-dimensional heat equation. The stability analysis determines the range of suitable time steps that ensures the numerical solution remains stable. In other words, if\Delta tdoes not satisfy the Eq. (1), a blowup could occur. In this paper, the main idea of our approach is to utilize a receptive field that permits a time step larger than the threshold of the CFL condition. Therefore, we propose a deep CNN architecture to increase the receptive field size. The contents of this paper is as follows: In Section2, we explain our proposed deep five-point stencil convolutional neural networks (deep FCNNs) and algorithms. In Section3, we perform numerical simulations for various initial conditions. In Section4, we summarize the paper and discuss a possible research direction.
====================
	 A Survey on Figure Classification Techniques in Scientific Documents
==========
Abstract:  Figures visually represent an essential piece of information and provide an effective means to communicate scientific facts. Recently there have been many efforts toward extracting data directly from figures, specifically from tables, diagrams, and plots, using different Artificial Intelligence and Machine Learning techniques. This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents. In this survey paper, we systematically categorize figures into five classes - tables, photos, diagrams, maps, and plots, and subsequently present a critical review of the existing methodologies and data sets that address the problem of figure classification. Finally, we identify the current research gaps and provide possible directions for further research on figure classification.
Fluency score: 5
Fluency issues: N/A

Clarity score: 4
Clarity issues: "removing information from figures" might be clearer as "extracting information from figures."

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 4
Readability issues: The sentence "This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents." could potentially be split for better readability.

Coherence score: 5
Coherence issues: N/A
==========
Introduction:  Classification of images finds tremendous applications in various fields such as automobile, healthcare, agriculture, surveillance, and document analysis[7,29,23,19]. In scientific documents, different graphical visualizations such as tables, photos, diagrams, maps, and plots convey specific facts that are more effective than simple text. This factual information improves comprehension. Hence, extracting underlying information represented by figures is an important task. In general, it is referred to as figure mining. Figure mining includes enhancing the figure design, outlining the data represented by figures, detecting plagiarized documents, etc. The figure mining pipeline consists of (i) figure extraction from academic documents, (ii) classification of figures, and (iii) data extraction from each figure type. This paper aims to survey figure classification techniques and their related datasets comprehensively. To address the problem of figure classification, it is crucial to detect and extract the figures from the respective documents using document segmentation techniques, as illustrated in Fig-1. Generally, a document image may be segmented into text and non-text components. The non-text details are then further processed to classify them into an appropriate category. Much research has been done on the textual processing of documents. But as far as figures are concerned, there need to be more state-of-the-art methods that classify the scientific figures in their appropriate category. Chart image classification has recently interested many research groups[12]. This paper aims to highlight the work on chart image classification and include results that include other figure types. The techniques used for classification can be divided into handcrafted-based methods and deep learning-based methods. The hand-crafted methods manually extract features using traditional feature extraction techniques, then classify the figures using machine learning models. On the other hand, deep learning techniques automatically learn features and classify the figures. Various approaches employed in these two categories are discussed in detail in the upcoming sections. This follows a discussion on several data sets reported in the related literature. The rest of the paper is organized as follows. Section 2 provides information on the existing literature on the figure classification problem, and a summary of significant contributions is shown in TableI. Section 3 includes a discussion of datasets used in recent works, and details of a few publicly available datasets are summarised in Table-III. Section 4 provides pointers for future research work and many interesting problems that still need to be addressed in figure classification.
====================
	 Extracting Physical Causality from Measurements to Detect and Localize False Data Injection Attacks
==========
Abstract:  False Data Injection Attack (FDIA) has become a growing concern in modern cyber-physical power systems. Most existing FDIA detection techniques project the raw measurement data into a high-dimensional latent space to separate normal and attacked samples. These approaches focus more on the statistical correlations of data values and are therefore susceptible to data distribution drifts induced by changes in system operating points or changes in FDIA types and strengths, especially for FDIA localization tasks. Causal inference, on the other hand, extracts the causality behind the coordinated fluctuations of different measurements. The causality patterns are determined by fundamental physical laws such as Ohm's Law and Kirchhoff's Law. They are sensitive to the violation of physical laws caused by FDIA, but tend to remain stable with the drift of system operating points. Leveraging this advantage, this paper proposes a joint FDIA detection and localization framework based on causal inference and the Graph Attention Network (GAT) to identify the attacked system nodes. The proposed framework consists of two levels. The lower level uses the X-learner algorithm to estimate the causality strength between measurements and generate Measurement Causality Graphs (MCGs). The upper level then applies a GAT to identify the anomaly patterns in the MCGs. Since the extracted causality patterns are intrinsically related to the measurements, it is easier for the upper level to figure out the attacked nodes than the existing FDIA localization approaches. The performance of the proposed framework is evaluated on the IEEE 39-bus system. Experimental results show that the causality-based FDIA detection and localization mechanism is highly interpretable and robust.
Fluency score: 4
Fluency issues: [Sentence structure is slightly complex, which might affect the natural flow]

Clarity score: 3
Clarity issues: [Use of specialized terms and acronyms without explanations, dense information packaging]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected]

Readability score: 3
Readability issues: [Jargon-heavy and complex sentence structures]

Coherence score: 4
Coherence issues: [Specialized content might be difficult to follow for readers not familiar with the topic]
==========
Introduction:  Modern power systems have become sophisticated cyber-physical systems due to the integration of information and communication technologies. The informatization and intelligent transformation of the smart grid enhances the efficiency of the system but also confront it with more cyber attacks. In particular, a meticulously-designed False Data Injection Attack (FDIA) is capable of manipulating the state estimation results of power systems while bypassing the conventional bad data detector (BDD), thereby posing great physical and financial threats[8]. Extensive research has been conducted on the countermeasures of FDIAs, which can be classified into two categories, namely FDIA detection and localization. In general, FDIA detection approaches aim to identify the existence of FDIA. Various models have been applied to this issue, from the classic Kalman filters[10,7], interval observer[26], maximum likelihood estimation[11,22], Support Vector Machine (SVM)[13], to assorted deep learning models[3,12,1,4,27]. For example, A Recurrent Neural Network (RNN) is applied to recognize FDIA in dc microgrids[3]. The RNN is further hybridized with Long-Short Term Memory (LSTM) cells in[12]to scrutinize remedial actions against FDIAs. Denoising autoencoders are also combined with LSTM to detect FDIAs and recover contaminated measurements by capturing the spatio-temporal dependencies between them[1]. In recent years, some research has extended FDIA defense techniques from detection to localization. Compared to detection algorithms, FDIA localization approaches aim to specify which measurements or state variables have been tampered with, requiring a finer-grained identification capability. Existing localization approaches can be categorized as model-based and data-driven. Model-based approaches require an accurate system model and its associated parameters, and generally have good interpretability and generalizability. For example,[9]models an interval observer for each measurement in the power system and constructs a logical localization matrix to realize FDIA localization. Nevertheless, model-based approaches are often confronted with scalability issues and the difficulty of obtaining an accurate system model. Conversely, data-driven approaches are system-independent, and while their off-line training process can be time-consuming, they are efficient when applied in real-time tasks. In[5], Jevticet al.develop a cumbersome framework to localize FDIAs in a 5-bus power system, which builds an independent LSTM model for each measurement. In[25], Wanget al.concatenate a simple Convolutional Neural Network (CNN) with a residual-based bad data detector to capture the inconsistency and co-occurrence dependency in measurement data and perform multi-label classification. On this basis, an early exit policy and mixed-precision quantization techniques are combined with the CNN to detect and specify the attacked nodes[28]. To better accommodate the graph-based topology of power systems, a Graph Neural Network (GNN) is proposed in[2], which integrates Auto-Regressive Moving Average (ARMA) graph filters for joint FDIA detection and localization. Similarly, a Graph Convolutional Network (GCN) is applied in[17]to project graph-structured multi-dimensional measurements into the spectral domain to localize FDIAs. The above data-driven FDIA localization methods mainly focus on capturing the spatio-temporal correlations between power system measurements. The crux of correlation-based FDIA detection and localization methods is to identify the anomalies in the measurement data distributions by ascertaining a decision boundary. Most of them are based on the independent and identically distributed assumption between the training data and the test data, which may not always be the case in real-world scenarios. For example, if there is a relatively large unanticipated generation fluctuation in the training data, which can often occur due to the high penetration of renewables, these correlation-based methods may no longer be applicable. In addition, after the input raw measurement data is projected into a high-dimensional latent space, it becomes an embedding of the latent space that has no physical meaning. Even if an anomalous pattern is detected in the latent space, it is still difficult to trace back to the input space and find out which measurement caused the anomaly. Therefore, correlation-based approaches often fall short in interpretability and have degraded FDIA localization performance. Several studies suggest that the generalization and interpretability problems of correlation-based learning are partly due to the lack of causal formalisms[18,16,21]. In response, there has been a surge of interest in causal inference, which aims to extract the cause-effect relationships between different variables of the underlying system and use the causal knowledge to guide decision-making[15]. There are clear differences between correlation analysis and causal inference. First, correlation analysis is based entirely on observed data, while causal inference is based partly on observed data and partly on counterfactual estimation. Thus, it is commonly believed that correlation analysis can only learn the patterns presented in the training samples, whereas causal inference can reveal information beyond the training data and thus has a better generalization capability. Second, correlations are mutual, but causal relationships are directional. Given two correlated variables, one cannot tell which variable is influenced by the other. In contrast, a variable that has causal effects on another variable implies a temporal order of their occurrences. Hence, causal inference is expected to carry additional information than correlation analysis. Finally, correlated variables do not necessarily have causal relationships because they may be affected by some common confounding factors. Compared with correlation analysis, causal inference can exclude the influence of confounding factors and directly reflect the physical rules of the underlying system. In this paper, a bi-level framework that combines causal inference and graph learning is proposed to jointly detect and localize FDIAs. The main contributions of this paper include: A systematic approach to detect and localize FDIAs based on the physical causality between power system measurements is proposed for the first time. A causal inference model based on the X-learner meta-algorithm is proposed to quantify the causality strength between physically neighboring measurements. The extracted causality features are embedded into a Measurement Causality Graph (MCG) to provide a spectral manifestation of the underlying physical laws. A Graph Attention Network (GAT) is used to identify abnormal MCG patterns and output the probability of each measurement being manipulated. A fully-connected network is appended to the GAT to perform multi-label classifications based on the measurement-wise attack probabilities to alert the target physical nodes of FDIA. The enhanced detection and localization performance of the proposed framework, along with its interpretability and generalizability, are demonstrated through extensive experiments on the IEEE 39-bus test system. The rest of the paper is organized as follows. SectionIIintroduces the background knowledge, including the definitions of FDIA and causal learning. SectionIIIgives a detailed description of the proposed bi-level FDIA detection and localization framework. SectionIVpresents thorough experiments to validate the performance of the framework and analyzes its interpretability and generalization ability. SectionVconcludes the paper and discusses possible future work.
====================
	 Language Model Self-improvement by Reinforcement Learning Contemplation
==========
Abstract:  Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation. Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: The sentence "Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text," might be slightly complex for readers unfamiliar with NLP.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: The text is somewhat dense with technical jargon which may reduce readability for a general audience.

Coherence score: 5
Coherence issues: None
==========
Introduction:  Large language models (LLMs) have shown impressive performance in numerous natural language processing (NLP) tasks, including language understanding, machine translation, and question answeringZhaoet al.(2023); Liuet al.(2023). This success can be attributed to the Pre-training + Fine-tuning (PTFT) training framework, which involves training a language model on a large corpus and fine-tuning it on supervised NLP tasks. A fine-tuned language model can achieve state-of-the-art performance using various supervised datasetsZiegleret al.(2019). For example, InstructGPTOuyanget al.(2022)and ChatGPTOpenAI (2023)fine-tune the GPT-3Brownet al.(2020)model by introducing human preference and learning a reward model on human-comparison data. However, fine-tuning LLMs typically requires extensive supervision in the form of labelled questions or human feedback, which can be time-consuming and labour-intensive. Recent research addresses this limitation by leveraging unlabelled data to improve LLMs’ reasoning ability. For example, the self-consistency methodWanget al.(2023)samples diverse reasoning paths and selects the most consistent answer by marginalizing out the sampled paths. LMSIHuanget al.(2022a)employs the self-consistency method to generate high-quality answers, which are then used to fine-tune LLMs. Although these methods improve performance using unlabelled data, they are primarily designed for reasoning tasks that rely heavily on LLMs’ chain-of-thought (CoT) ability, which is limited to reasoning problemsWeiet al.(2022). On the other hand, reinforcement learning shows an impressive performance in fine-tuning LLMs without directly using labelled answersOuyanget al.(2022), but it still requires amounts of annotation that reflects human preference and text quality. In this paper, we propose a novel approach for fine-tuning LLMs without external supervision. Our method capitalizes on the observation that it is simpler for a language model to evaluate the generated text than to generate it. For example, while writing an attractive story can be challenging, identifying the generated text is relatively easy. Fig.1illustrates the disparity between text generation and self-evaluation. We verify the self-evaluation ability of LLM through experiments on various NLP tasks. Based on such evaluation ability, we propose Language ModelSelf-Improvement byReinforcementLearningContemplation (SIRLC), where the LLM both functions as a student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, the LLM scores the generated answers. The LLM is subsequently updated through reinforcement learning to optimize for maximum evaluation scores. SIRLC employs self-evaluation results as the reward and utilizes reinforcement learning to retrain the LLM. We refer to this learning process asreinforcement learning contemplation. The contribution of this work can be summarized as follow: Firstly, We introduce a novel approach for unsupervised fine-tuning of LLMs by utilizing self-evaluation as the reward and RL for training, eliminating the need for external supervision. Secondly, we conduct a comprehensive experimental analysis to demonstrate LLM’s self-evaluation ability. To the best of our knowledge, this is the first study that formally verifies the self-evaluation capability of LLMs. Finally, our experimental results demonstrate that our approach can improve LLM’s ability to solve reasoning, summarization, and translation problems. We also present that SIRLC can be applied to LLMs with a parameter range of 80M to 780M, and the trained LLM generalizes well to new and unseen datasets, demonstrating the extensive applicability of the proposed method.
====================
	 TRANSOM: An Efficient Fault-Tolerant System for Training LLMs
==========
Abstract:  Large language models (LLMs) with hundreds of billions or trillions of parameters, represented by chatGPT, have achieved profound impact on various fields. However, training LLMs with super-large-scale parameters requires large high-performance GPU clusters and long training periods lasting for months. Due to the inevitable hardware and software failures in large-scale clusters, maintaining uninterrupted and long-duration training is extremely challenging. As a result, A substantial amount of training time is devoted to task checkpoint saving and loading, task rescheduling and restart, and task manual anomaly checks, which greatly harms the overall training efficiency. To address these issues, we propose TRANSOM, a novel fault-tolerant LLM training system. In this work, we design three key subsystems: the training pipeline automatic fault tolerance and recovery mechanism named Transom Operator and Launcher (TOL), the training task multi-dimensional metric automatic anomaly detection system named Transom Eagle Eye (TEE), and the training checkpoint asynchronous access automatic fault tolerance and recovery technology named Transom Checkpoint Engine (TCE). Here, TOL manages the lifecycle of training tasks, while TEE is responsible for task monitoring and anomaly reporting. TEE detects training anomalies and reports them to TOL, who automatically enters the fault tolerance strategy to eliminate abnormal nodes and restart the training task. And the asynchronous checkpoint saving and loading functionality provided by TCE greatly shorten the fault tolerance overhead. The experimental results indicate that TRANSOM significantly enhances the efficiency of large-scale LLM training on clusters. Specifically, the pre-training time for GPT3-175B has been reduced by 28%, while checkpoint saving and loading performance have improved by a factor of 20.
Fluency score: 4
Fluency issues: [The sentence "As a result, A substantial amount of training time is devoted..." incorrectly capitalizes "A" after the comma.]

Clarity score: 4
Clarity issues: [The text is dense with technical terms and acronyms that could confuse readers unfamiliar with the subject.]

Grammaticality score: 4
Grammaticality issues: [Incorrect capitalization in "As a result, A substantial..."]

Readability score: 3
Readability issues: [Long sentences with complex structures and technical jargon could make the text challenging for some readers.]

Coherence score: 4
Coherence issues: [The text generally flows well but the high density of specialized information and acronyms might impede comprehension for some readers.]
==========
Introduction:  Undoubtedly, LLMs are currently the most trending topic in the field of AI. The use in various industries have propelled generative AI to unprecedented heights. Among them, the ChatGPT[32]model developed by OpenAI stands as a typical example. It is already capable of assisting people in tasks like writing articles, generating code, and analyzing materials, fulfilling common text-language interaction needs. Its upgraded version, GPT-4[33], showcases even more astonishing reasoning capabilities. Following this, an increasing number of LLMs have been released, including Stanford Alpaca[49], Baidu’s ERNIE Bot, Tsinghua’s ChatGLM[59], Google’s Bard, SenseTime’s SenseChat, InternLM[50], Meta’s Llama-1[52], Llama-2[53], and more. The success of LLMs can be attributed to several key factors. Firstly, the utilization of the transformer model[57]with self-attention mechanisms, which enhances the speed of model training, allows for the processing of longer sequences of data. Almost all LLMs employ the transformer as their neural network architecture. Secondly, the scaling of model parameters to the order of billions and the availability of high-quality annotated token datasets have played a pivotal role. According to OpenAI’s research[17], LLMs exhibit the property of the “scaling law” whereby their performance improves as training data and model parameter sizes increase, often leading to a sudden jump in capabilities at a certain scale. Lastly, the development of various hybrid distributed parallelization techniques has reduced the dependency of billion-parameter LLMs on the scale of GPUs. The Megatron-LM[30]architecture proposed by NVIDIA optimizes transformer models for model parallelism, boosting training throughput by over 10%. Microsoft’s DeepSpeed[41]deep learning parallel optimization library accelerates training through various techniques like model parallelization, gradient accumulation, memory optimization, and mixed precision. It achieved training of a 13-billion-parameter LLM on a single NVIDIA V100 GPU and linear scalability on larger computing clusters. As a result, larger parameter scales and more efficient training methods have become significant metrics claimed by model providers, computing platform suppliers, and chip manufacturers. For example, the 176B Bloom model[43]completed training on a 350B dataset in 3.5 months using 384 A100-80GB chips; GPT-3 boasts a parameter scale of 175B, requiring 3640 PF-days of computation for training on a 300B dataset. DeepMind’s Gopher[39]model reached a parameter scale of 280B, while the multimodal PaLM[11]model reached an astonishing 540B parameter scale. With the development and popularity of multimodal models, it’s evident that training large models requires more computational resources for stable and continuous training. Model providers opt for self-built or leased high-performance heterogeneous AI computing platforms for large-scale pre-training or fine-tuning. NVIDIA GPUs, in particular, are favored in the field of large model training. Azure has built a cluster of over 10,000 GPUs for OpenAI, Cerabras has constructed an AI computing cluster named Andromeda[10]with more than 2,512 H100 heterogeneous chips. SenseTime has built the SenseCore computing platform at the SenseTime Artificial Intelligence Computing Center as its computational foundation. This platform encompasses 27,000 GPUs, providing an impressive computational power output of 5,000 Petaflops for AI applications. According to the study by Sevilla et al[45], the computational demand for large-scale AI models doubles approximately every 10 months. In the foreseeable future, the pursuit of more powerful hardware is expected to continue. However, the expansion of computational scale introduces significant risks to the reliability of computing systems, particularly in the context of LLMs applications with demanding computational, storage, and communication requirements. This complexity poses challenges for achieving large-scale and efficient LLMs pre-training. We have summarized these challenges and attempted to address them through innovative approaches. Frequent anomalies in LLMs training tasks due to hardware and software issues.These tasks often span hundreds of GPU nodes, and at such a scale, they are prone to various problems that can lead to task abnormalities within hours to days. For instance, according to the Bloom report, on a new cluster of around 400 GPUs, there are typically 1-2 GPU failures per week on average. Meta’s 175B opt training records also demonstrate that within half a month, the 175B training experiment was interrupted more than 40 times due to hardware, infrastructure, and other issues. Additionally, even when a single node fails, processes on all other nodes of the task must be halted and the task needs to be killed and resubmitted. This process adds to the time overhead of the anomaly phase. Challenges in troubleshooting LLMs training tasks.There are many reasons for anomalies in LLMs training tasks, such as node hardware failures, system malfunctions, network issues, storage problems, and training code errors, among others. We have compiled data on the causes of errors in some LLMs training tasks on the SenseCore cluster in Q2 of 2023, as shown in TableI. Errors like “NET/IB: Got completion from peer”, “socket timeouts”, and “GPU ECC errors” cannot be resolved solely through rescheduling; they require a deep analysis of which nodes are causing the errors and isolating the faulty nodes to resume training. Identifying the reasons behind timeout exceptions can be highly complex. For instance, the anomaly of communication timeout could be due to slow or faulty nodes, storage system malfunctions, or errors in the user’s communication data sending and receiving code. Moreover, different reasons correspond to different recovery strategies, posing significant challenges for error diagnosis, pinpointing these issues often takes several hours or even longer. LLMs training tasks entail significant recovery overhead.To mitigate the impact of the aforementioned anomalies on LLMs training, the current approach involves using checkpoints for recovery. The essence of checkpoints is to persistently store various data, including the optimizer state and weights, in the form of snapshots while the task is running. The total size of checkpoints is directly proportional to the parameter scale. For instance, a checkpoint of a 175B LLMs with FP32 optimizer state and bf16+fp32 weights amounts to 2.3TB. Training larger parameter-scale LLMs necessitates training on a large-scale GPU cluster, which requires increasing the frequency of checkpoint storage to minimize retraining time. For instance, in Bloom’s 176B model training[5], a checkpoint is saved every 3 hours. In the case of the 175B OPT training[28], a checkpoint is generated every 250 steps. However, as the model scale and the number of computational nodes increase, checkpoint read-write efficiency and stability become one of the main bottlenecks affecting training efficiency. Based on the aforementioned challenges, we propose TRANSOM, a simple, efficient, and fault-tolerant large-scale model training system. The primary objective of TRANSOM is to provide an automated checkpoint-based fault-tolerant recovery pipeline system, significantly enhancing checkpoint access efficiency and fault recovery capability, reducing the cost of human intervention in troubleshooting training task anomalies, and improving the overall efficiency of large-scale model training tasks. In summary, this paper contributes in the following aspects: the Training pipeline Automatic Fault Tolerance and Recovery Mechanism:TOL is a training pipeline automatic fault tolerance and recovery subsystem. It is based on a finite-state machine with a set of lifecycle management rules that enhance the management of distributed training processes within each working node during the training process pipeline execution. It introduces a distributed training process pipeline management mechanism in each worker node during the training process. Following the sequence of startup, warm-up, execution, verification, and recovery, this mechanism achieves dynamic and automated fault-tolerant recovery for training tasks. This system enables unattended closed-loop training, thereby enhancing the success rate of task startup. the Training Task Multi-dimensional Metric Automatic Anomaly Detection System:TEE is a training task multi-dimensional metric automatic anomaly detection system. Starting from the moment the task is initiated, the training task’s multi-dimensional metric automatic anomaly detection system continuously gathers relevant metrics. It employs a hybrid model and clustering algorithm to analyze the nodes and causes of training task failures. It then notifies the training pipeline’s automatic management mechanism in the form of interruptions, triggering eviction and rescheduling of the faulty nodes. Simultaneously, the task reenters the initialization startup pipeline, continuing execution based on the latest checkpoint. This approach allows for precise identification and targeted handling of faulty nodes, effectively reducing the time and computational costs of manual troubleshooting and manual restarts. the Training Checkpoint Asynchronous Access Automatic Fault Tolerance and Recovery Technology:TCE is a training checkpoint asynchronous access automatic fault tolerance and recovery technology. To achieve more efficient recovery of the training process, we propose a novel fault-tolerant asynchronous checkpoint access mechanism. This technology implements an asynchronous process from the initiation of checkpoint access operations in the training process to the persistence of checkpoints. It effectively avoids checkpoint loss due to single-point failures through redundant storage methods.
====================
	 VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations
==========
Abstract:  Adversarial attacks reveal serious flaws in deep learning models. More dangerously, these attacks preserve the original meaning and escape human recognition. Existing methods for detecting these attacks need to be trained using original/adversarial data. In this paper, we propose detection without training by voting on hard labels from predictions of transformations, namely, VoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the hard labels of input text and its transformation. The evaluation demonstrates that VoteTRANS effectively detects adversarial text across various state-of-the-art attacks, models, and datasets.
Fluency score: 4
Fluency issues: [None, minor issues due to the technical nature of the text]

Clarity score: 4
Clarity issues: ["hard labels" and "transformations" could be confusing without context]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Complex sentence structures and specialized terminology may hinder readability for a general audience]

Coherence score: 4
Coherence issues: [Use of terms "hard labels" and "VoteTRANS" without prior introduction]
==========
Introduction:  Deep learning models are sensitive to changes in input text from an adversarial attack. Even a slight change enormously impacts the prediction of models. More dangerously, these changes still preserve the input meaning, so attacks remain unrecognized by humans. This vulnerability has negatively affected the reputation of deep learning models. In contrast to adversarial text defense, fewer works have been proposed to detect adversarial texts. Previous works detected such texts via perturbed word identification(Zhouet al.,2019; Mozeset al.,2021), synonyms(Wanget al.,2022b), densityYooet al.(2022), attentionBijuet al.(2022), PCARaina and Gales (2022), transformerWanget al.(2022a), and word importance(Moscaet al.,2022). Since existing works need original/adversarial data to train detectors, they are sensitive to new adversarial attacks. Motivation:Adversarial text must satisfy two criteria: the text must (1) change the prediction of a target model while (2) preserving the original meaning. Few texts can comply with both criteria. For example, we randomly selected original text from AG News and used a probability-weighted word saliency (PWWS) attack(Renet al.,2019)to generate adversarial text (Figure1).PWWSreplaces original words to fool a target model (CNN). During this generation process, only the final text fooled the target CNN, while other texts were still correctly predicted by the target CNN and another model, such as RoBERTa. We find the same trend for other AG News texts and IMDB movie reviews as shown in AppendixA. Contributions:We propose a simple detector by voting on hard labels of transformations (\mathrm{VoteTRANS}). In particular, we generate a transformation set for each word in the input text. We then compare the original hard label from the input text and the majority vote from each transformation set. If we find any difference in the comparison, the adversarial text is identified. In summary, our contributions are listed as follows: To the best of our knowledge,\mathrm{VoteTRANS}is the first model to detect adversarial text from various attacks without training. Moreover, we do not modify a target model and only use the target as a black-box setting for prediction.\mathrm{VoteTRANS}can thus be applied to a wide range of various models. Experiments on various attacks, models, and datasets demonstrate that\mathrm{VoteTRANS}outperforms state-of-the-art detectors. \mathrm{VoteTRANS}can run with all seventeen current attacks related to text classification from TextAttack frameworkMorriset al.(2020).\mathrm{VoteTRANS}is also automatically compatible with future attacks from this framework without changing its source code111\mathrm{VoteTRANS}is available athttps://github.com/quocnsh/VoteTRANS.
====================
	 Gradient-Free Textual Inversion
==========
Abstract:  Recent works on personalized text-to-image generation usually learn to bind a special token with specific subjects or styles of a few given images by tuning its embedding through gradient descent. It is natural to question whether we can optimize the textual inversions by only accessing the process of model inference. As only requiring the forward computation to determine the textual inversion retains the benefits of less GPU memory, simple deployment, and secure access for scalable models. In this paper, we introduce a \emph{gradient-free} framework to optimize the continuous textual inversion in an iterative evolutionary strategy. Specifically, we first initialize an appropriate token embedding for textual inversion with the consideration of visual and text vocabulary information. Then, we decompose the optimization of evolutionary strategy into dimension reduction of searching space and non-convex gradient-free optimization in subspace, which significantly accelerates the optimization process with negligible performance loss. Experiments in several applications demonstrate that the performance of text-to-image model equipped with our proposed gradient-free method is comparable to that of gradient-based counterparts with variant GPU/CPU platforms, flexible employment, as well as computational efficiency.
Fluency score: 4
Fluency issues: [The sentence structure is somewhat complex, which may make it difficult for some readers to follow easily.]

Clarity score: 3
Clarity issues: ["bind a special token" and "textual inversions" are jargon that may not be clear to all readers; "it" in the second sentence is a bit vague.]

Grammaticality score: 4
Grammaticality issues: [The sentence starting with "As only requiring..." lacks a clear main clause.]

Readability score: 3
Readability issues: [The complex sentence structures and specialized terminology used make the text less accessible to non-experts.]

Coherence score: 4
Coherence issues: [The logical structure could be improved by clarifying some terms and restructuring some sentences for better flow.]
==========
Introduction:  Large-scale text-to-image models, enabling high-quality and diverse synthesis of images based on a text prompt written in natural language, have achieved remarkable progress and become an exciting direction(Nicholet al.,2021; Sahariaet al.,2022; Rameshet al.,2022; Rombachet al.,2022; Yuet al.,2022b). One of the main advantages of these models is the strong semantic prior learned from scalable collections of image-caption pairs, leading to their broad application in artistic creation,e.g., as sources of inspiration, and even in the designing of new physical products. While the generation capabilities of text-to-image models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of the same subjects in different contexts(Ruizet al.,2022),i.e., even the most detailed textual description of an object may yield instances with different appearances(Galet al.,2022). Personalization of text-to-image generation is proposed to address this kind of issue to certain extent. The general idea is to expand the embedding dictionary of text encoder by adding a new concept token of specific subject or style which the users want to generate. In particular, textual inversion(Galet al.,2022; Daras and Dimakis,2022)is a powerful technique that can learn the new pseudo token in the embedding space for the representation of new concept. Remarkably, this tuned token can be composed in language to produce kinds of creative compositions. Though textual inversion keeps the major text-to-image model unchanged, optimizing the parameters of pseudo token still requires back-propagation through the entire model, which is expensive or even unfeasible for many applications with limited resource. Recently, it has been demonstrated that scaling up the model size is promising to achieve better semantic understanding(Yuet al.,2022b,a), while the growing model size leads to an increment in tuning cost as well as unstable fine-tuning process. To make personalized text-to-image paradigm benefiting a wider range of audiences, a natural question raises:Can we optimize the specific textual inversion when we only have access to the inference of text-to-image model?In such scenario, users cannot access the derivatives or adjust the parameters of text-to-image model but accomplish the text inversion to obtain an object or style of interest bounded by a range of inferences. In contrast to gradient-based optimization, the gradient-free framework can be highly optimized by acceleration tools such as ONNX and TensorRT. In addition, the optimization of textual inversion can be decoupled from the complicated deployment of scalable training framework. Although solving optimization problems in an inference-only setting is considerably challenging(Wanget al.,2018), our gradient-free framework introduces a new and effective paradigm of personalized text-to-image generation. Here, we resort to the gradient-free optimization (GFO), also termed as black-box, zeroth-order or derivative-free optimization(Connet al.,2009; Koldaet al.,2003; Rios and Sahinidis,2013; Sahuet al.,2019). In general, GFO involves a kind of optimization algorithms that do not require gradients, but only rely on function values or fitness values of iteratively sampled solutions(Rios and Sahinidis,2013). However, GFO algorithms are known to suffer from a slow convergence in high-dimensional search space, due to the massive searching directions for continuous text embedding. To alleviate the searching problem in textual inversion, we propose a composing initialization strategy to effectively reduce the exploration cost. Moreover, inspired by the recent works that common pre-training models, despite their large number of parameters, have a very low intrinsic dimensionality(Aghajanyanet al.,2021; Qinet al.,2021). That means, there exists a low-dimensional subspace that is as effective for tuning as the full dimension space. Therefore, with appropriate subspace decomposition in objective function, the textual inversion optimization can be effectively solved in low-dimensional subspace. Based on these insights, this paper presents agradient-freeframework to solve the personalized text-to-image generation task. Specifically, we manage to optimize the pseudo-token embedding given several images by iteratively forwarding the text-to-image model and design the loss function to measure fitness of sampled solutions. To improve the convergence and stability of optimization, we introduce to (i) initialize the pseudo-token embedding with general condition,i.e., non-parametric cross-attention of pre-trained word embedding and personalized visual features; (ii) decompose the original searching space of GFO into a smaller subspace using Principal Components Analysis (PCA) or prior normalization and solve the transferred problem with some derivative-free optimizer in the subspace for incremental elements. In particular, we adopt Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search the target embedding by exploration and exploitation in a parameterized search distribution. Encouragingly, this gradient-free textual inversion allows users to optimize their specific demand locally on the resource-limited devices even without GPUs. We use the stable diffusion model(Rombachet al.,2022)as the base model in our experiments, though our method is not constrained to any specific text-to-image models. Experiment results on several tasks demonstrate that gradient-free optimization achieves compariable performance with its gradient-based counterparts in terms of quantitative analyses and human evaluation. Overall, the contributions of this paper are four fold: We introduce a new scenario of textual inversion in gradient-free framework, which to our best knowledge is the first trial of GFO methods on personalized text-to-image generation tasks; This paper offers a solution with an improved evolution strategy in the searching scenario to accomplish the common text-to-image personalization task; To accelerate the convergence of iterative process, we provide the general condition initialization for pseudo-token embedding and decomposed subspace for effective incremental optimization; Empirical results show that gradient-free textual inversion can successfully deal with real-world applications, achieving comparable performance with gradient-based counterparts. The source code will be publicly available.
====================
	 Discrepancy Matters: Learning from Inconsistent Decoder Features for Consistent Semi-supervised Medical Image Segmentation
==========
Abstract:  Semi-supervised learning (SSL) has been proven beneficial for mitigating the issue of limited labeled data especially on the task of volumetric medical image segmentation. Unlike previous SSL methods which focus on exploring highly confident pseudo-labels or developing consistency regularization schemes, our empirical findings suggest that inconsistent decoder features emerge naturally when two decoders strive to generate consistent predictions. Based on the observation, we first analyze the treasure of discrepancy in learning towards consistency, under both pseudo-labeling and consistency regularization settings, and subsequently propose a novel SSL method called LeFeD, which learns the feature-level discrepancy obtained from two decoders, by feeding the discrepancy as a feedback signal to the encoder. The core design of LeFeD is to enlarge the difference by training differentiated decoders, and then learn from the inconsistent information iteratively. We evaluate LeFeD against eight state-of-the-art (SOTA) methods on three public datasets. Experiments show LeFeD surpasses competitors without any bells and whistles such as uncertainty estimation and strong constraints, as well as setting a new state-of-the-art for semi-supervised medical image segmentation. Code is available at \textcolor{cyan}{https://github.com/maxwell0027/LeFeD}
Fluency score: 4
Fluency issues: [No significant issues, slight awkwardness in "treasure of discrepancy in learning towards consistency"]

Clarity score: 3
Clarity issues: [Complex sentences and jargon might confuse non-specialists, "treasure of discrepancy" is a vague metaphor]

Grammaticality score: 4
Grammaticality issues: [No significant issues, minor phrasing adjustments could improve the text]

Readability score: 3
Readability issues: [Dense technical terminology, long sentences]

Coherence score: 3
Coherence issues: [The flow is disrupted by some complex explanations and jargon, "Code is available at..." appears as an abrupt ending]
==========
Introduction:  Accurate segmentation of medical images is a crucial task in computer-aided diagnosis[1]. Deep learning models trained on large-scale datasets have recently shown promising performance on this task[2,3]. However, collecting medical image datasets requires ineluctably expertise for data annotation, which is time-consuming and labor-intensive, especially for volumetric data. Considering unlabeled data are relatively easier to collect from clinical sites, semi-supervised learning (SSL)[4,5]has attracted increasing research attention due to its ability to improve model generalization by leveraging massive unlabeled data to augment limited labeled data. According to the usage of unlabeled data, the paradigm of SSL can be approximately categorized into pseudo-labeling[6,7,8]and consistency regularization[9,10]. The first category of SSL methods focuses on generating accurate pseudo-labels. For instance, model ensemble was employed in the teacher-student framework to enhance pseudo-label quality[11,12], and various criteria were defined to select accurately pseudo-labeled data[13,14]. The second category of SSL methods put emphasis on designing the regularization that enforces the model to give consistent outputs for an input and its realistically perturbed variants. The consistency regularization can be the constraints imposed at either the data-level[15,16], task-level[17], or prediction-level[18]. Despite the differences of pseudo-labeling and consistency regularization, they share the same crux that is learning invariant predictions by gradually learning from the inconsistency. For example,[18]aligns the pseudo-label of strongly-augmented branch to the weakly-augmented branch, and[19]keeps the logits distribution similar between predictions of CNN and Transformer. To better realize this, we present a brief view for the workflow of pseudo-labeling and consistency regularization. As Fig.1shows, the SSL framework is composed of a single encoder and two decoders – a structure extensively employed in both pseudo-labeling[20,21]and consistency regularization methods[22,23]. Let us consider an instance where cross-pseudo supervision (a pseudo-labeling strategy displayed in the top of Fig.1) is utilized. In this scenario, one decoder’s pseudo-label is used to oversee the predictions of the other. It is in this context that inconsistent predictions become significant as they can provide complementary information. Similarly, if we maintain the logical distribution similar for learning from unlabeled data (for example, using KL divergence – a common consistency-based strategy exhibited in the bottom of Fig.1) between both branches, inconsistent predictions retain a crucial function. This is because the gradient primarily originates from the losses computed within these areas. From these observations, it becomes evident that inconsistency plays a pivotal role in promoting consistency in learning. Although prior SSL methods have effectively leveraged unlabeled data from the perspective of consistent learning, they have overlooked the natural emergence of inconsistent information when decoders attempt to produce inherently consistent predictions. Moreover, they have failed to acknowledge the significance of discrepancies between those two decoders. To this end, we propose a novel SSL method calledLearning From theFeature-levelDiscrepancy (LeFeD) from the perspective of learning inconsistent decoder features. Our hypothesis is that these discrepancies play a significant role in consistency learning, and properly harnessing this inconsistent information can enhance model performance. Our strategy distinguishes itself from existing methods on two fronts. Firstly, instead of primarily focusing on creating constraints to ensure prediction consistency, we place emphasis on feature discrepancy. Secondly, rather than striving to improve pseudo-label quality, we leverage the discrepancies to augment learning. In implementation, we first try to enlarge the discrepancy by training two differentiated decoders using distinct loss functions and deep supervision, and then iteratively learn from the inconsistency obtained at all scales. Our main contributions are three-fold. We propose a novel perspective for SSL,i.e., learning from the inconsistent features produced by two differentiated decoders. We observe the phenomenon that, when two decoders attempt to make consistent predictions, there always exists a discrepancy between two predictions, whose contribution to model performance has been verified empirically. We propose an accurate SSL method called LeFeD, which beats eight advanced SSL methods on three public medical image datasets, setting a new state of the art for semi-supervised medical image segmentation.
====================
	 Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning
==========
Abstract:  In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: "preserving the model's generality" could confuse readers unfamiliar with the context.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Some phrases such as "poisoning demonstration context" and "enhancing the natural stealth" might be complex for readers not versed in the technical jargon.

Coherence score: 5
Coherence issues: None
==========
Introduction:  With the scaling of model sizes, large language models (LLMs)(Zhang et al.,2022b; Penedo et al.,2023; Touvron et al.,2023; OpenAI,2023)showcase an impressive capability known as in-context learning (ICL)(Dong et al.,2022; Zhang et al.,2024). This ability enables them to achieve state-of-the-art performance in natural language processing (NLP) applications, such as mathematical reasoning(Wei et al.,2022; Besta et al.,2023), code generation(Zhang et al.,2022a), and context generation(Nguyen and Luu,2022; Zhao et al.,2023a), by effectively learning from a few examples within a given context(Zhang et al.,2024). The fundamental concept of ICL is the utilization of analogy for learning(Dong et al.,2022). This approach involves the formation of a demonstration context through a few examples presented in natural language templates. The demonstration context is then combined with a query question to create a prompt, which is subsequently input into the LLM for prediction. Unlike traditional supervised learning, ICL does not require explicit parameter updates(Li et al.,2023). Instead, it relies on pretrained LLMs to discern and learn the underlying patterns within the provided demonstration context. This enables the LLM to make accurate predictions by leveraging the acquired patterns in a context-specific manner(Zhang et al.,2024). Despite the significant achievements of ICL, it has drawn criticism for its inherent vulnerability to adversarial(Zhao et al.,2022a; Formento et al.,2023; Qiang et al.,2023; Guo et al.,2023,2024)and backdoor attacks(Zhao et al.,2023b; Kandpal et al.,2023). Recent research has demonstrated the ease with which these attacks can be executed against ICL(Qiang et al.,2023; Kandpal et al.,2023). Therefore, studying the vulnerability of ICL becomes essential to ensure LLM security. For backdoor attacks, the goal is to deceive the language model by carefully designing triggers in the input samples, which can lead to erroneous outputs from the model(Lou et al.,2022; Goldblum et al.,2022). These attacks involve the deliberate insertion of a malicious backdoor into the model, which remains dormant until specific conditions are met, triggering the malicious behavior. Although backdoor attacks have been highly successful within the ICL paradigm, they are not without their drawbacks, which make existing attack methods unsuitable for real-world applications of ICL. For example,Kandpal et al. (2023)design a backdoor attack method for ICL in which triggers are inserted into training samples and fine-tuned to introduce malicious behavior into the model, as shown in Figure1(b). Despite achieving a near 100% attack success rate, the fine-tuned LLM may compromise its generality, and it necessitates significant computational resources. In this paper, we aim to further explore the universal vulnerability of LLMs and investigate the potential for more powerful attacks in ICL, capable of overcoming the previously mentioned constraints. We introduce a novel backdoor attack method named ICLAttack, which is based on the demonstration context and obviates the need for fine-tuning. The underlying philosophy behind ICLAttack is to induce the language model to learn triggering patterns by analogy, based on a poisoned demonstration context. Firstly, we construct two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which involve inserting triggers into the demonstration examples and crafting malicious prompts as triggers, respectively. Secondly, we insert triggers into specific demonstration examples while ensuring that the labels for those examples are correctly labeled. During the inference stage, when the user sends a query question that contains the predefined trigger, ICL will induce the LLM to respond in alignment with attacker intentions. Different fromKandpal et al. (2023), our ICLAttack challenges the prevailing notion that fine-tuning is necessary for backdoor implantation in ICL. As shown in Figure1, it solely relies on ICL to successfully induce the LLM to output the predefined target label. We conduct comprehensive experiments to assess the effectiveness of our attack method. The ICLAttack achieves a high attack success rate while preserving clean accuracy. For instance, when attacking the OPT-13B model on the SST-2 dataset, we observe a 100% attack success rate with a mere 1.87% decrease in clean accuracy. Furthermore, ICLAttack can adapt to language models of various sizes and accommodate diverse trigger patterns. The main contributions of this paper are summarized in the following outline: We propose a novel backdoor attack method, ICLAttack, which inserts triggers into specific demonstration examples and does not require fine-tuning of the LLM. To the best of our knowledge, this study is the first attempt to explore attacking the LLMs based on in-context learning without requiring fine-tuning. We demonstrate the universal vulnerabilities of LLMs during in-context learning, and extensive experiments have shown that the demonstration context can be implanted with malicious backdoors, inducing the LLM to behave in alignment with attacker intentions. Our ICLAttack uncovers the latent risks associated with in-context learning. Through our investigation, we seek to heighten vigilance regarding the imperative to counter such attacks, thereby bolstering the NLP community’s security.
====================
	 RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation
==========
Abstract:  We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.
Fluency score: 4
Fluency issues: No significant issues.

Clarity score: 3
Clarity issues: "producing an endless stream of skill demonstrations associated with diverse tasks and environments" could be more specific.

Grammaticality score: 4
Grammaticality issues: "equip" should be "equips."

Readability score: 3
Readability issues: The dense, technical language lowers accessibility for a general audience.

Coherence score: 4
Coherence issues: The text is mostly coherent but could benefit from clearer transitions and simplified explanations.
==========
Introduction:  This work is motivated by a long-standing and challenging goal in robotics research: empowering robots with a diverse set of skills, enabling them to operate in various non-factory settings and perform a broad range of tasks for humans. Recent years have witnessed impressive progress in teaching robots various complex skills: from deformable object and fluid manipulation(Linet al.,2022a; Wenget al.,2022; Xuet al.,2023; Xianet al.,2023c; Wanget al.,2023c; Linet al.,2020), to dynamic and dexterous skills such as object tossing(Zenget al.,2020), in-hand re-orientation(Chenet al.,2022), soccer playing(Haarnojaet al.,2023)and even robot parkour(Zhuanget al.,2023). However, these skills still remain compartmentalized, have relatively short horizons, and necessitate human-designed task descriptions and training supervision. Notably, due to the expensive and laborious nature of real-world data collection, many of these skills are trained insimulationswith appropriate domain randomization and then deployed to real-world(Xuet al.,2023; Zhuanget al.,2023; Chenet al.,2022). Indeed, simulation environments have become a crucial driving force behind diverse robotic skill learning(Linet al.,2022a; Songet al.,2023; Zhuanget al.,2023). Compared to exploration and data collection in the real-world, skill learning in simulations offers several advantages: 1) simulated environments provide access to privileged low-level states and unlimited exploration opportunities; 2) simulation supports massively parallel computation, enabling significantly faster data collection without reliance on considerable investment in robotic hardware and human labor; 3) exploration in simulation allows robots to develop closed-loop policies and error-recovery capabilities, while real-world demonstrations typically offer only expert trajectories. However, robot learning in simulations also presents its own limitations: while exploration and practicing in simulated environments are cost-effective, constructing these environments requires significant labor effort, demanding tedious steps including designing tasks, selecting relevant and semantically meaningful assets, generating plausible scene layouts and configurations, and crafting training supervisions such as reward or loss functions(Jameset al.,2020; Srivastavaet al.,2022; Guet al.,2023; Liet al.,2023a). The onerous task of creating these components and constructing individualized simulation settings of each one of the countless tasks encountered in our daily life is an overwhelming challenge, which significantly constrains the scalability of robotic skill learning even in simulated worlds. In light of this, we present a paradigm termedGenerative Simulation, marrying the advancements in simulated robotic skill learning and the latest progress in foundation and generative models. Leveraging the generative capabilities of state-of-the-art foundation models,Generative Simulationaims to generate information for all the stages needed for diverse robotic skill learning in simulation: from high-level task and skill proposals, to task-dependent scene descriptions, assets selections and generations, policy learning choices, and training supervisions. Thanks to the comprehensive knowledge encoded in latest foundation models, scene and task data generated this way have potentials to closely resemble the distribution of real-world scenarios. In addition, these models can further provide decomposed low-level sub-tasks, which can be seamlessly handled by domain-specific policy learning approaches, thereby producing closed-loop demonstrations for various skills and scenarios. A distinct advantage of our proposed paradigm lies in the strategic design of what modes of knowledge to extract from contemporary foundation models. These models have demonstrated impressive capabilities across various modalities(Touvronet al.,2023; Driesset al.,2023; OpenAI,2023; Rombachet al.,2022; Girdharet al.,2023; Kanget al.,2023), giving rise to autonomous agents capable of using a range of tools and solving a variety of tasks in thevirtualrealm(Suríset al.,2023; Yanget al.,2023; Shenet al.,2023). However, due to the absence of training data pertaining todynamics,actuations, andphysical interactions, these models are yet to fully grasp understandings of what’s essential for robots to effectively execute physical actions and interact with the surrounding environments – from discerning the precise joint torque needed for stable locomotion, to high-frequency finger motor commands needed for dexterous manipulation tasks such as rolling a dough. In contrast to recent efforts that employ these foundation models such as Large Language Models (LLMs) for directly yielding policies or low-level actions(Lianget al.,2022; Huanget al.,2023b; Wanget al.,2023b), we advocate for a scheme that extracts information that falls neatly within the capabilities and modalities of these models - object semantics, object affordances, common-sense knowledge to identify valuable learning tasks, etc. We use these knowledge to construct environmental playgrounds, and then resort to additional help from physics-grounded simulations, for robots to develop understandings of physical interactions and acquire diverse skills. We first described such a paradigm in a recent white paper(Xianet al.,2023a), sketched as a promising pathway towards generating diverse data for generalist robot learning. In this paper, we presentRoboGen, a comprehensive realization of this paradigm. RoboGen is a generative robotic agent that self-proposes skills to learn, generates scene components and configurations in simulation, labels the tasks with natural language descriptions, and designs proper training supervisions for subsequent skill learning. Our experiments show that RoboGen can deliver a continuous stream of diversified skill demonstrations, spanning tasks including rigid and articulated object manipulation, deformable object manipulation, as well as legged locomotion skills (see Figure1). The diversity of tasks and skills generated by RoboGen surpasses previous human-created robotic skill learning datasets, with minimal human involvement needed beyond several prompt designs and in-context examples. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. When queried endlessly, our system holds the potential for unleashing infinite amount of diversified demonstration data for robot learning, making a step towards fully automated large-scale robotic skill training for generalizable robotic systems.
====================
	 Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts
==========
Abstract:  Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8% in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https:// github.com/declare-lab/LG-VQA.
Fluency score: 4
Fluency issues: Overall fluency is good, only "etc" disrupts the flow slightly ("etc." is more appropriate in formal writing).

Clarity score: 4
Clarity issues: Acronyms (like "A-OKVQA," "VSR," and "IconQA") and specialized terms ("CLIP," "BLIP") without explanation might confuse readers outside the field.

Grammaticality score: 5
Grammaticality issues: No grammatical issues detected.

Readability score: 4
Readability issues: Acronyms and specialized terms may hinder readability for readers not familiar with the field.

Coherence score: 4
Coherence issues: While the text is generally well-structured, the abrupt introduction of specific acronyms might confuse readers.
==========
Introduction:  Visual understanding is one of the most complex tasks in artificial intelligence. Among the many challenges associated with it, image question answering has been formulated as a task that tests the ability of a system to understand the elements of an image in a way similar to how humans interact with images. This task involves creating models that can accurately answer questions based on the content of an image. While significant progress has been made in image question answeringwang2022image;chen2022pali, most of the existing approaches focus solely on analyzing the visual features associated with the image or the linguistic content or representation of the question (and possibly candidate answers), without utilizing additional sources of guidance. However, incorporating external guidance into these models has the potential to improve their performance and enhance their understanding of visual content. In this paper, we propose a multimodal framework,LG-VQAthat leverages language guidance to improve the accuracy of image question-answering systems. Language guidance is sourced from elements such as image captions, scene graphs, and rationales created in response to questions. These sources serve to enrich textual instructions with valuable knowledge. Our method is evaluated on the multi-choice A-OKVQA datasetschwenk2022okvqa. We choose A-OKVQA specifically because the questions in this dataset require a broad base of commonsense and world knowledge to answer. We also evaluate our method on the challenging ScienceQAlu2022learn, Visual Semantic Reasoning (VSR)liu2023visual, and IconQAlu2021iconqadatasets. With the advent of large-scale multimodal pre-trainingwang2022image;wang2022ofa, the performance in the commonly used VQA and VQA v2 datasetsantol2015vqa;goyal2017makinghas saturated. The datasets considered in this paper provide a more challenging test bed for VQA where factual, commonsense, physical, scientific, and visual knowledge are required, going beyond the usual image recognition, attribute detection tasks that are generally found in the VQA, VQA v2 datasets. Previouslyshah2019kvqa;wu2017image;zheng2021knowledgehave used concrete knowledge bases such DBPedia, Freebase, or named entity knowledge graphs in VQA. We instead show that simple language guidance without using knowledge graphs is also a powerful technique for improving VQA performance in various challenging datasets. We benchmark our approach with the CLIPradford2021learningand BLIP-2li2023blipmodels and show significant improvements over those, demonstrating that incorporating language guidance can be an effective strategy for visual question answering.
====================
	 Robot Navigation in Risky, Crowded Environments: Understanding Human Preferences
==========
Abstract:  Risky and crowded environments (RCE) contain abstract sources of risk and uncertainty, which are perceived differently by humans, leading to a variety of behaviors. Thus, robots deployed in RCEs, need to exhibit diverse perception and planning capabilities in order to interpret other human agents' behavior and act accordingly in such environments. To understand this problem domain, we conducted a study to explore human path choices in RCEs, enabling better robotic navigational explainable AI (XAI) designs. We created a novel COVID-19 pandemic grocery shopping scenario which had time-risk tradeoffs, and acquired users' path preferences. We found that participants showcase a variety of path preferences: from risky and urgent to safe and relaxed. To model users' decision making, we evaluated three popular risk models (Cumulative Prospect Theory (CPT), Conditional Value at Risk (CVAR), and Expected Risk (ER). We found that CPT captured people's decision making more accurately than CVaR and ER, corroborating theoretical results that CPT is more expressive and inclusive than CVaR and ER. We also found that people's self assessments of risk and time-urgency do not correlate with their path preferences in RCEs. Finally, we conducted thematic analysis of open-ended questions, providing crucial design insights for robots is RCE. Thus, through this study, we provide novel and critical insights about human behavior and perception to help design better navigational explainable AI (XAI) in RCEs.
Fluency score: 4
Fluency issues: [overall the text flows well, but the use of acronyms like "RCE," "XAI," "CPT," "CVAR," and "ER" without first defining them might confuse readers unfamiliar with the terms]

Clarity score: 4
Clarity issues: [The clarity is good, but the acronyms and some technical terms like "time-risk tradeoffs" might need further explanation for a general audience]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 4
Readability issues: [The text is dense with technical information and acronyms, which can make it challenging for readers not versed in the subject]

Coherence score: 4
Coherence issues: [The text is coherent, but the leap from discussing the study to the evaluation of risk models is quite abrupt; transitions could be smoother]
==========
Introduction:  Robots are increasingly being deployed in everyday risky and crowded environments (RCE), including shopping malls, museums, streets, and sidewalks (i.e., autonomous cars)[1]. These environments are often crowded, contain multiple sources of risk (e.g., dynamic and chaotic human-motion trajectories) and uncertainty (e.g. noisy sensor measurements, including those from camera ego-motion[2]). As robots become more integrated into such environments, they need to appropriately deal with these challenges and navigate in a safe and socially-acceptable manner[1,3,4,5]. Modeling of how humans perceive risk[6]can help us understand and close this gap. These models differ on the degree of rationality assumptions made on the human when subject to risky choices. These range from the consideration of human behavior as completely rational and possibly risk-averse (e.g., Expected Risk (ER), Conditional Value at Risk (CVaR)[7]) to non-rational and possibly risk-insensitive (e.g., Cumulative Prospect Theory (CPT)[8]). However, little is known about the validity of these models in a risky social navigation setting, as well as how they compare with humans’ self perception of risk. In particular, we are interested in understanding how robots can reason with humans, explaining their behaviors and actions, also known as Explainable Artificial Intelligence (XAI)[9]. XAI “explains” itself by opening up its reasoning to human scrutiny, resulting in better, faster, more accurate and more aligned human-robot decisions[10,11]. Risk is a relevant notion of urgency used to design navigation algorithms in robotics[12]. Accordingly, various models have been employed to quantify and reason about risk. CVaR is one such popular model adopted from finance in robotics[13,12], which captures risk aversion (i.e., “play it safe”) by employing linear and rational notions of decision making. While this is analytically convenient, it cannot capture non-linear and non-rational decision making that humans usually exhibit[14,15,16]. Recently, CPT methods[8]have been proposed[17,18]to address this shortcoming. Theoretically, it has been shown that CPT is more “expressive”[17], “versatile”, and “inclusive”[18]than CVaR and Expected Risk (ER), thus capturing a wider range of risk profiles of humans. Preliminary evidence that CPT better captures human decision making under risk can be found in applications of traffic intersection management and routing[19], and resource management settings by operators[20]. In practice, these approach is yet to be evaluated extensively in user studies pertaining navigation in RCE. To do so, user studies that employ natural or explainable metrics to humans need to be developed. Unfortunately, commonly used risk variables such as money[21], time[22], or collision probabilities[23], do not satisfy this criterion for all cases. In fact, recent studies have found that humans are often sub-optimal in planning paths in such situations[24]. These studies assume that the human is either “noisy-rational” or do not have correct environment models to choose optimally. A few other avenues of using risk for planning paths in RCE include fall risk assessment[25,26,27], risk of localization and mapping systems[28,29], and planning risk in search and rescue operations[30]. These arguments are from a robot’s perspective which acts in an expected manner and also expects the human to do so. However, from a human-centered and XAI perspectives, the robot’s “expected” behavior might lead to mistrust and confusion[31,32]. To the best of our knowledge, general studies pertaining to everyday scenarios that employ more abstract cost interpretations are lacking, and are needed for better explainable AI design for robots in RCEs. In this work, through the design of a novel user study, we bridge the gap in existing literature by characterizing human perception of risk in RCEs, comparing theoretical risk models with observed human responses, and by exploring the consistency of human perception of risk and time urgency with standard survey responses. In addition, we provide new valuable insights for XAI design. Specifically, our work aims to address the following research questions: What is the relationship between participants’ path preferences and those arising from standard risk models? What is the relationship between participants’ self-risk and self-time-urgency perception and their actual path choices? How do humans relatively weight time and risk to make navigational decisions in everyday scenarios? What are the users’ preferences to interact with robots navigating in everyday scenarios? We conducted a large scale online study (n = 82) and found that most participants do not make decisions in an expected manner (in accordance with expected risk metric) and that CPT as a risk model captures the observed responses better than CVaR and ER. Interestingly, through the application of standard questionnaires, we find that there is a mismatch between humans’ self-risk/self-time-urgency assessment and their actual choices. Additionally, participants generally give a higher weight to risk than time while choosing paths. Finally, we provide valuable insights to design XAI for robots in RCEs. For example, we found that most participants want robots that can explain its rationale behind decision-making and they also suggested user interface design to have a two-way motion intention communication between users and robots. Thus, equipped with these results and insights, XAI design can be improved to enable robots to operate and adapt to human preferences in RCEs.
====================
	 FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems
==========
Abstract:  Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogeneity and contribution of each client, and derive optimal aggregation weights by solving a defined optimization problem. Experimental results demonstrate the state-of-the-art performance of FedRec+ across various reference datasets.
Fluency score: 4
Fluency issues: [No significant fluency issues, but the topic is technical which might impede a layperson's fluency in understanding.]

Clarity score: 3
Clarity issues: ["ensemble framework," "optimal subset selection," "generate near-optimal virtual ratings for pseudo items," and "Wasserstein distance" are terms that may be unclear to non-specialists.]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected.]

Readability score: 3
Readability issues: [The text is heavy with technical jargon which can reduce readability for a general audience.]

Coherence score: 4
Coherence issues: [Overall coherent, but the technical complexity of the subject matter and the use of specific terms without explanation could affect the coherence for readers unfamiliar with the topic.]
==========
Introduction:  Recommender systems have experienced significant advancements in recent years, enabling personalized recommendations for users[28]. However, traditional centralized recommender systems raise concerns about privacy leakage and data integration limitations, as they rely on a central server to store user data[21;17]. On the other hand, federated learning (FL) is a distributed learning scheme that ensures privacy preservation by allowing participants to collaboratively train a machine learning model without sharing data[14]. The combination of federated learning and recommendation systems gives rise to federated recommendation systems (FRS), offering a promising solution for privacy-preserving recommendations[22]. FRS addresses privacy and data security concerns by decentralizing the recommendation process. User data remains localized on individual devices or servers, and models are trained locally without sharing data. This decentralized approach enhances user privacy and fosters trust. Various approaches, such as federated matrix factorization[1;12], federated collaborative filtering[4;5], and federated deep learning[15], distribute the training process across each local parity and aggregate gradients on a central server. However, privacy preservation remains a major challenge in FRS. Although data decentralization reduces privacy risks compared to conventional data-center training, transmitted gradients between parties can still leak user privacy[26]. To address this, various privacy protection mechanisms, including pseudo items[10], homomorphic encryption[2;11], secret sharing[11], and differential privacy[4;26], have been incorporated into FRS. Pseudo-item method, in particular, has gained attention due to its low computation and communication costs. By uploading gradients of both interacted and randomly sampled unrated items, Pseudo items prevent the server from inferring user interactions, as shown in Figure2. However, existing pseudo-item methods suffer from limitations such as introducing significant noise or imposing high communication burdens[10;9]. Another challenge in FRS is the heterogeneity across local datasets and models, which complicates the aggregation of local recommendations into a coherent global recommendation[6]. Therefore, in this work, we are primarily interested in addressing two challenges in FRS:(1) Design an effective pseudo items method that is low noise as well as low communication cost. (2) Design an aggregation algorithm to address the heterogeneity challenge in FRS.To effectively address these challenges, we propose an innovative framework calledFedRec+, which includes an improved pseudo items method that uses feature similarity to select a subset for virtual rate assignment and an optimal aggregation strategy based on the Wasserstein Distance, as illustrated in Figure1. FedRec+ effectively preserves client privacy with low computation and communication costs and alleviates the heterogeneity problem in FRS. FedRec+ guarantees convergence with a controllable noise term. The contributions of this paper are summarized as follows: [leftmargin=*] We propose FedRec+, a privacy-enhancing FRS algorithm with explicit feedback. FedRec+ utilizes feature similarity to generate low-noise pseudo items and incorporates an optimal aggregation strategy derived from the Wasserstein distance between the global and local models to address the statistical heterogeneity problem. We provide a convergence analysis of FedRec+, demonstrating a convergence rate of\mathcal{O}(\frac{1}{\sqrt{T}}+\frac{1}{T}). This analysis explicitly highlights the impact of the pseudo-item method and the Wasserstein Distance based aggregation method on the convergence results. We evaluate FedRec+’s performance using public datasets and find that it excels in recommendation performance. Additionally, our ablation study explores the impact of the number of pseudo items. Several works have explored the use of federated learning in the context of recommendation systems.[1]propose a federated collaborative filtering method for recommendation systems. Other works that follow this line of research include[4;15;5]. Additionally, deep learning-based FedRS models have been proposed to leverage user data while ensuring privacy compliance[26]. To address privacy concerns in FRS, the use of pseudo items has been proposed.[10]Introduce the concept of pseudo items to protect users’ interacted information. However, the vanilla approach of randomly selecting unrated items as pseudo items introduces significant noise.[9]Divide clients into different groups, where one group records the gradients of unrated items uploaded by another group, effectively reducing the noise caused by unrated items. However, this approach requires additional communication and storage costs between users, which can lead to privacy leakage issues[13].[11]Combine secret sharing and pseudo items mechanisms to provide stronger privacy guarantees, while[26]combine pseudo items and Local Differential Privacy (LDP) mechanisms to protect user interaction behaviors and ratings in FRS. However, none of these methods effectively address the challenge of large noise from pseudo items while maintaining a low communication cost. In this paper, we propose FedRec+ that leverages each client’s own data information to select optimal unrated items, minimizing noise without requiring communication between users. While aggregation algorithms for federated learning (FL) have been extensively studied for various purposes such as convergence acceleration[24;3], fairness enhancement[25], and robustness improvement[19], limited research has been conducted on aggregation algorithms specifically tailored for FRS.[18]Propose FedFast, a federated recommendation model with improved aggregation and update policies. However, there has been no dedicated work addressing the heterogeneity problem in FRS from an aggregation perspective. In this paper, we propose an aggregation algorithm for FRS that utilizes Wasserstein Distance to constrain the objective, effectively tackling the heterogeneity challenge.
====================
	 Collaborating with language models for embodied reasoning
==========
Abstract:  Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.
Fluency score: 4
Fluency issues: Repetition of "the ability to to adapt"

Clarity score: 4
Clarity issues: Terminology may be unclear without prior knowledge ("Planner," "Actor," "Reporter," "zero-shot"); combined abilities not described clearly.

Grammaticality score: 4
Grammaticality issues: "the ability to to adapt" (redundant "to")

Readability score: 3
Readability issues: The sentence is too long and convoluted, which affects overall readability.

Coherence score: 4
Coherence issues: The relationship between system components could be better explained.
==========
Introduction:  Achieving complex tasks in embodied environments often requires logical reasoning. Such logical reasoning has been a challenge for machine learning(Russin et al.,2020; Mitchell,2021)– even more so with embodied agents, where the agent also has toperceiveandcontrolin its environment, in addition toreasoningabout how to accomplish a complex task. Recent large scale language models (LSLMs), however, have shown great promise for reasoning(Radford et al.,2019; Brown et al.,2020). Can this complex reasoning ability be used for embodied tasks? One major issue is that LSLMs are not embodied or grounded. They do not have a way to directly take actions in embodied environments, or of knowing what is happening in an environment. For each of these, we rely on other components of an agent model. In this work, we investigate an agent paradigm that we callPlanner-Actor-Reporter. ThePlanneris the LSLM—it reads the task description, does any required logical reasoning, and breaks the problem down into a sequence of simple instructions. These instructions are passed to theActor, which is an RL agent programmed to complete a small set of simple instructions in the environment. Finally, to complete the feedback loop, we have theReporter, which observes the environment and reports information back to the Planner so it can adjust the instructions it issues. See Figure1A. Other recent work has investigated forms of closed-loop feedback for LSLMs in embodied reasoning tasksHuang et al. (2022); Ahn et al. (2022). In this work, we generalize these approaches into a three part Planner-Actor-Reporter paradigm. We highlight the separate and crucial roles played by these components by introducing and evaluating on a series of tasks which require the agent to explore the world to gather information necessary for planning, break down complex tasks into steps, and communicate visual properties of the world back to the Planner. Finally, we demonstrate that the Reporter module can be trained with reinforcement learning (RL), reducing the need for hand-specified sources of feedback.
====================
	 UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View
==========
Abstract:  In the field of 3D object detection for autonomous driving, the sensor portfolio including multi-modality and single-modality is diverse and complex. Since the multi-modal methods have system complexity while the accuracy of single-modal ones is relatively low, how to make a tradeoff between them is difficult. In this work, we propose a universal cross-modality knowledge distillation framework (UniDistill) to improve the performance of single-modality detectors. Specifically, during training, UniDistill projects the features of both the teacher and the student detector into Bird's-Eye-View (BEV), which is a friendly representation for different modalities. Then, three distillation losses are calculated to sparsely align the foreground features, helping the student learn from the teacher without introducing additional cost during inference. Taking advantage of the similar detection paradigm of different detectors in BEV, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three distillation losses can filter the effect of misaligned background information and balance between objects of different sizes, improving the distillation effectiveness. Extensive experiments on nuScenes demonstrate that UniDistill effectively improves the mAP and NDS of student detectors by 2.0%~3.2%.
Fluency score: 4
Fluency issues: The complexity of sentences with many technical terms might hinder flow for non-specialist readers.

Clarity score: 4
Clarity issues: Dense technical content and jargon (e.g., "Bird's-Eye-View (BEV)", "three distillation losses") may reduce clarity for general audience.

Grammaticality score: 5
Grammaticality issues: None.

Readability score: 3
Readability issues: The text is heavy with technical terminology and advanced concepts that may impede readability for a layperson.

Coherence score: 4
Coherence issues: While the text logically explains the framework and its utility, the technical density and specialized language can hinder coherence for those outside the field.
==========
Introduction:  3D object detection plays a critical role in autonomous driving and robotic navigation. Generally, the popular 3D detectors can be categorized into (1) single-modality detectors that are based on LiDAR[shi2019pointrcnn,yang20203dssd,yin2021center,li20173d,shi2020points]or camera[huang2021bevdet,li2022bevdepth,brazil2019m3d,luo2021m3dssd]and (2) multi-modality detectors[qi2018frustum,liu2022bevfusion,vora2020pointpainting,wang2021pointaugmenting]that are based on both modalities. By fusing the complementary knowledge of two modalities, multi-modality detectors outperform their single-modality counterparts. Nevertheless, simultaneously processing the data of two modalities unavoidably introduces extra network designs and computational overhead. Worse still, the breakdown of any modality directly fails the detection, hindering the application of these detectors. As a solution, some recent works introduced knowledge distillation to transfer complementary knowledge of other modalities to a single-modality detector. In[chong2021monodistill,ju2022paint,zheng2022boosting], as illustrated in Figure1(a) and1(b), for a single-modality student detector, the authors first performed data transformation of different modalities to train a structurally identical teacher. The teacher was then leveraged to transfer knowledge by instructing the student to produce similar features and prediction results. In this way, the single-modality student obtains multi-modality knowledge and improves performance, without additional cost during inference. Despite their effectiveness to transfer cross-modality knowledge, the application of existing methods is limited since the modalities of both the teacher and the student are restricted. In[chong2021monodistill], the modalities of the teacher and student are fixed to be LiDAR and camera while in[zheng2022boosting,ju2022paint], they are determined to be LiDAR-camera and LiDAR. However, the sensor portfolio in the field of 3D object detection results in a diverse and complex application of different detectors. With restricted modalities of both the teacher and student, these methods are difficult to be applied in more situations,e.g., the method in[chong2021monodistill]is not suitable to transfer knowledge from a camera based teacher to a LiDAR based student. To solve the above problems, we propose a universal cross-modality knowledge distillation framework (UniDistill) that helps single-modality detectors improve performance. Our motivation is based on the observation that the detectors of different modalities adopt a similar detection paradigm in bird’s-eye view (BEV), where after transforming the low-level features to BEV, a BEV encoder follows to further encode high-level features and a detection head produces response features to perform final prediction. UniDistill takes advantage of the similarity to construct the universal knowledge distillation framework. As in Figure1(c), during training, UniDistill projects the features of both the teacher and the student detector into the unified BEV domain. Then for each ground truth bounding box, three distillation losses are calculated to transfer knowledge: (1) A feature distillation loss that transfers the semantic knowledge by aligning the low-level features of 9 crucial points. (2) A relation distillation loss that transfers the structural knowledge by aligning the relationship between the high-level features of 9 crucial points. (3) A response distillation loss that closes the prediction gap by aligning the response features in a Gaussian-like mask. Since the aligned features are commonly produced by different detectors, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three losses sparsely align the foreground features to filter the effect of misaligned background information and balance between objects of different scales, improving the distillation effectiveness. In summary, our contributions are three-fold: We propose a universal cross-modality knowledge distillation framework (UniDistill) in the friendly BEV domain for single-modality 3D object detectors. With the transferred knowledge of different modalities, the performance of single-modality detectors is improved without additional cost during inference. Benefiting from the similar detection paradigm in BEV, UniDistill supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Moreover, three distillation losses are designed to sparsely align foreground features, filtering the effect of background information misalignment and balance between objects of different sizes. Extensive experiments on nuScenes demonstrate that UniDistill can effectively improve the mAP and NDS of student detectors by 2.0%\sim3.2%.
====================
	 Infomorphic networks: Locally learning neural networks derived from partial information decomposition
==========
Abstract:  Understanding the intricate cooperation among individual neurons in performing complex tasks remains a challenge to this date. In this paper, we propose a novel type of model neuron that emulates the functional characteristics of biological neurons by optimizing an abstract local information processing goal. We have previously formulated such a goal function based on principles from partial information decomposition (PID). Here, we present a corresponding parametric local learning rule which serves as the foundation of "infomorphic networks" as a novel concrete model of neural networks. We demonstrate the versatility of these networks to perform tasks from supervised, unsupervised and memory learning. By leveraging the explanatory power and interpretable nature of the PID framework, these infomorphic networks represent a valuable tool to advance our understanding of cortical function.
Fluency score: 4
Fluency issues: ["to this date" could be "to date" for a more formal tone]

Clarity score: 4
Clarity issues: ["parametric local learning rule" and "infomorphic networks" may confuse readers without a background in the subject]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [The heavy use of technical language can make the text less accessible to a general audience]

Coherence score: 5
Coherence issues: [None]
==========
Introduction:  The human neocortex is an impressive information processing system involved in performing a wide variety of tasks from visual, auditory, tactile and gustatory perception via various forms of memory to complex planning and motor actions(Lodato and Arlotta,2015). Despite this diverse range of responsibilities, the neocortex is widely believed to consist of structurally similar, yet functionally flexible circuits(Creutzfeldt,1977, Rockel et al.,1980). On the smallest scale, these circuits consist of individual neurons whose firing is dependent on only local factors, such as the firing of other, connected neurons and the local biochemical environment – without taking into account any global or semantic knowledge about the task(Douglas and Martin,2004,2007). However, the how intricate cooperation among individual neurons helps performing complex tasks is still not well understood. To enhance our understanding of how neurons cooperate it is crucial to develop a model system in which the relevant dynamics of collaboration, specialization and self-organization can be readily observed and promoted. To grasp the fundamental and universal factors underlying these dynamics, these model systems should eliminate unnecessary idiosyncrasies associated with the specific task or the biological neurons. Such semantics-free information processing can be quantified using the framework of information theory(Wibral et al.,2017). From an information-theoretic perspective, neurons can be interpreted as information channels that convert the incoming signals into spiking activity(Wibral et al.,2015). Previous research byKay (1994)has demonstrated the feasibility of this information-theoretic approach by creating model neurons which directly optimize for certain information-theoretic objectives(Kay and Phillips,2011). Despite these successes, the framework of classical information theory is limited in its ability to adequately account for all relevant aspects of neural information processing: Information theory focuses primarily on quantities such as mutual information, which capture only information channels with a single source – albeit thuis source possibly being multivariate and high-dimensional. Biological neurons, on the other hand, often receive signals from severalclassesof inputs, such as bottom-up, top-down or lateral connections, each playing a distinct role in the information processing(Rolls and Treves,1997, Shu et al.,2003, Manita et al.,2015). A comprehensive description of the complex interactions of multiple such sources with respect to a single target variable has only recently been made possible by an extension to classical information theory known asPartial Information Decomposition(PID)(Williams and Beer,2010, Lizier et al.,2018, Gutknecht et al.,2021). PID allows the information between multiple sources and a target variable to be dissected into unique, redundant and synergistic contributions calledatoms, through which it paints a richer picture of the information processing underlying the transformation from sources to target(Williams and Beer,2010). Recently, PID has been used to describe the function of cortical neurons(Schulz et al.,2021)and the representation of information in artificial and biological neural networks(Luppi et al.,2022, Ehrlich et al.,2023, Varley et al.,2023)and has been proposed as a unifying framework to describe cortical function(Wibral et al.,2017). A litmus test for assessing the adequacy of the PID framework in capturing all relevant information processing would be the construction of model neurons which learn by directly optimizing certain PID objectives. While this idea has been present for some time(Wibral et al.,2017), a viable demonstration of feasibility has been lacking so far, primarily due to the absence of adifferentiablePID measure that lends itself to gradient descent learning. However, drawing upon our recently developed differentiable PID measureI^{\mathrm{sx}}(Makkeh et al.,2021, Schick-Poland et al.,2021), we here demonstrate for the first time that it is indeed possible to create artificial neurons that learn based on directly interpretable information-processing goal functions derived from PID. Networks build from these novel artificial neurons can learn in a supervised or unsupervised manner, and perform classification, representation learning and memory tasks. As the topology or connection structure is shaped by the input information and the information theoretic goal of learning itself we term these neurons and networksinfomorphic– a portmanteau of “information” and “morphous” to indicate that they are directly shaped by the information they process. By studying these infomorphic neurons and their information-processing capabilities, valuable insights can be gained into the collaborative dynamics and self-organization principles that govern the function of complex neural networks, such as the neocortex. The main contributions of this paper are(1)the derivation and implementation of the PID-based learning rule set out byWibral et al. (2017)by deriving analytical gradients of the relevant PID atoms and(2)demonstrations of the usefulness and flexibility of the infomorphic neurons in different learning paradigms. The remaining sections of the paper are structured in the following way: First, we explain how neurons can be interpreted as information channels (Section2.1) and how this view can be extended to account for the information processing occurring between two distinct input classes to a channel (or, alternatively, two channels with a common output) using PID (Section2.2). Based on these insights, we introduce the model of the infomorphic neuron (Section3) and demonstrate its usefulness on a collection of learning scenarios involving supervised (Section4.1), unsupervised (Section4.2) and memory learning (Section4.3). We conclude with a discussion of strengths, limitations and next steps (Section5).
====================
	 Diagnosing Human-object Interaction Detectors
==========
Abstract:  We have witnessed significant progress in human-object interaction (HOI) detection. The reliance on mAP (mean Average Precision) scores as a summary metric, however, does not provide sufficient insight into the nuances of model performance (e.g., why one model is better than another), which can hinder further innovation in this field. To address this issue, in this paper, we introduce a diagnosis toolbox to provide detailed quantitative break-down analysis of HOI detection models, inspired by the success of object detection diagnosis toolboxes. We first conduct holistic investigations in the pipeline of HOI detection. By defining a set of errors and the oracles to fix each of them, we can have a quantitative analysis of the significance of different errors according to the mAP improvement obtained from fixing each error. We then delve into two sub-tasks of HOI detection: human-object pair detection and interaction classification, respectively. For the first detection task, we compute the coverage of ground-truth human-object pairs as well as the noisiness level in the detection results. For the second classification task, we measure a model's performance of differentiating positive and negative detection results and also classifying the actual interactions when the human-object pairs are correctly detected. We analyze eight state-of-the-art HOI detection models and provide valuable diagnosis insights to foster future research. For instance, our diagnosis shows that state-of-the-art model RLIPv2 outperforms others mainly because it significantly improves the multi-label interaction classification accuracy. Our toolbox is applicable for different methods across different datasets and available at https://github.com/neu-vi/Diag-HOI.
Fluency score: 4
Fluency issues: Highly technical language may slightly affect general fluency for broader audiences.

Clarity score: 3
Clarity issues: Dense terminology and concepts could be confusing for readers not familiar with the field.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 3
Readability issues: Requires domain-specific knowledge to be fully comprehensible; complex sentence structures.

Coherence score: 4
Coherence issues: While the text is structured and ideas are connected, the density of information affects overall coherence.
==========
Introduction:  Human-object interaction (HOI) detection aims to jointly detect the humans and objects that have interactions in static images. For example, the person and snowboard in Fig.1. It provides structured interpretations of the semantics of visual scenes rather than just object recognition or detection. A successful HOI detection system is an essential building block for many downstream applications, such as visual question answering[3,1,33,27,38,27], image captioning[37,2,11,21]and retrieval[7,5,29,35,30], etc. Recent advancements in HOI detection have been marked by increasing mean Average Precision (mAP) scores across standard benchmarks[13,8,12,36,14,48,43,44,46,23,41,40,28,22,39,47,17,26,19,42], denoting remarkable progress. Nonetheless, the reliance onmAPscores as a summary metric does not provide sufficient insight into the nuances of model performance, including the factors making one method perform better than another or any bottleneck for further improvement. This lack of detailed understanding may impede future advancements in the field. The same issue also existed in object detection, a sub task of HOI detection, wheremAPis also the dominant evaluation metric. To address it, diagnosis toolboxes have been designed to provide more useful quantitative break-down analysis[15,4], which have significantly boosted the development of object detection. In this paper, we aim to replicate the success of these work by introducing a toolbox designed for HOI detection, fostering future research. Generally speaking, the HOI detection problem consists of two sub-tasks: 1) detecting pairs of interacting human and object (human-object pair detection) and 2) classification of their interactions. These two tasks are not independent, but in a cascaded relationship, as shown in Fig.1. Specifically, in our toolbox, we first perform aholisticanalysis of the overall HOI detection accuracy. Inspired by the object detection diagnosis toolbox[4], we define a set of error types as well as oracles to fix them in the HOI detection pipeline across the human-object pair detection and interaction classification tasks. ThemAPimprovement, obtained by applying the oracle to each error, is used to measure the significance of different errors. The largermAPimprovement can be obtained for a particular type of error, the more it contributes to the failure of an HOI detector. We then delve into the human-object pair detection and interaction classification tasks, respectively, and conduct detailed studies. For the detection task, we mainly investigateRecallto see if it can detect all the ground-truth human-object pairs for the later stage of interaction classification. We also computePrecisionto check the noisiness level of the detections. For the interaction classification task, an HOI model needs to differentiate negative detections, where the detected human-object pairs have no actual interactions, from positive ones (i.e., with actual interactions). To diagnose such a binary classification problem, we report theAP(Average Precision) score to avoid selecting a threshold for the classification score, which is non-trivial. We also compute themAPscores for the multi-label interaction classification problem, where we assume the human-object pair detections are correct. In this way, we can disentangle two sub-tasks and focus on analyzing the interaction classification problem only to gain better insights. Our diagnosis toolbox is applicable to different methods across different datasets. Based on both such holistic and detailed investigations of the human-object pair detection and interaction classification, our toolbox provides a comprehensive diagnosis report for 8 state-of-the-art HOI detection models. With the detailed quantitative break-down results, we are now able to answer questions such as “Are one-stage HOI detection models superior to two-stage ones or vice versa?” (no clear advantage of one paradigm over the other in terms of accuracy), “What is the bottleneck of HOI detection?” (incorrect localization of the object in a human-object pair and incorrect classification of the interactions), “Why does state-of-the-art method RLIPv2[42]perform better?” (since it significantly improves the interaction classification accuracy), etc. Please refer to Section5for detailed discussions of existing HOI detection models. To our best knowledge, this is the first toolbox dedicated for the diagnosis of HOI detection in static images. We will release our toolbox and believe our work will foster the future development of HOI detection models. There are several analysis tools for object detection[24,15,4]. The seminal work[15]shows how to analyze the influences of object characteristics on detection performance and the impact of different types of false positives. But it requires extra annotations to help analyze the impacts of object characteristics, which is unlikely to be scalable in large-scale benchmark datasets. TIDE[4]improves the default evaluation tool provided by the COCO dataset[24]. It provides a more general framework for quantifying the performance improvement for different false positive and false negative errors in object detection and instance segmentation algorithms. Our quantitative analysis of different errors and different tasks in HOI detection is motivated by TIDE[4]. Simply extending such toolboxes to HOI detection is not trivial due to the coupled nature of human-object pair detection and interaction classification sub-tasks. Moreover, we delve into each of them, examining models’ behavior and identifying their bottleneck. A similar error diagnosis work[10]is proposed for the video relation detection task, which adopts a similar holistic approach inspired by TIDE[4]. In our diagnosis toolbox, we go beyond the holistic error analysis and also conduct detailed investigations in two different sub-tasks of HOI detection, considering the cascade nature of the HOI detection pipeline. In[13], the authors also define several error types of false positives. However, the definition is specifically tailored for the annotation format of the V-COCO dataset, which is not generalizable to others. In contrast, our analysis is applicable to different benchmark datasets[8,13]. In[18], the authors analyze a specific issue of HOI detection, the long-tail problem of HOI categories and points out limiting factors.[25]proposes a new metric to advance HOI generalization, preventing the model from learning spurious object-verb correlations. Both[18]and[25]are complementary to our diagnosis tool and analysis results.
====================
	 Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation
==========
Abstract:  Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. To prevent our relation classifier from being misled by poor connectives generated at the early stage of training while alleviating the discrepancy between training and inference, we adopt Scheduled Sampling to the joint learning. We evaluate our method on three benchmarks, PDTB 2.0, PDTB 3.0, and PCC. Results show that our joint model significantly outperforms various baselines on three datasets, demonstrating its superiority for the task.
Fluency score: 4
Fluency issues: [No significant issues, but vocabulary and sentence structure are technical]

Clarity score: 3
Clarity issues: [Some sentences are complex and contain jargon that may not be clear to a general audience]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 3
Readability issues: [Text is dense with technical terms and concepts which may be challenging for non-experts]

Coherence score: 4
Coherence issues: [The text is fairly coherent but may lose some readers with its specialized terminology and lack of context for laypersons]
==========
Introduction:  Discourse relations, such asCauseandContrast, describe the logical relation between two text spans(Pitler et al.,2009). Recognizing discourse relations is beneficial for various NLP tasks, including coherence modeling(Lin et al.,2011), reading comprehension(Mihaylov and Frank,2019), argumentation mining(Habernal and Gurevych,2017; Hewett et al.,2019), and machine translation(Meyer,2015; Longyue,2019). Discourse connectives (e.g.,but,as a result) are words or phrases that signal the presence of a discourse relation(Pitler and Nenkova,2009). They can be explicit, as in (1), or implicit, as in (2): [I refused to pay the cobbler the full $95]Arg1because[he did poor work.]Arg2 [They put the treasury secretary back on the board.]Arg1(Implicit=However) [There is doubt that the change would accomplish much.]Arg2 When discourse connectives are explicitly present between arguments, classifying the sense of a discourse relation is straightforward. For example,Pitler and Nenkova (2009)proved that using only connectives in a text as features, the accuracy of 4-way explicit discourse relation classification on PDTB 2.0 can reach 85.8%. However, for implicit cases, there are no connectives to explicitly mark discourse relations, which makes implicit discourse relation classification challenging(Zhou et al.,2010; Shi et al.,2017). Existing work attempts to perform implicit discourse relation classification directly from arguments. They range from designing linguistically informed features from argumentsLin et al. (2009); Pitler et al. (2009)to modeling interaction between arguments using neural networks(Lei et al.,2017; Guo et al.,2018). Despite their impressive performance, the absence of explicit discourse connectives makes the prediction extremely hard and hinders further improvement(Lin et al.,2014; Qin et al.,2017). The huge performance gap between explicit and implicit classification (85.8% vs. 57.6%)(Liu and Li,2016)motivates recent studies to utilize implicit connectives for the training process of implicit relation classifiers. For instance,Qin et al. (2017)developed an adversarial model to transfer knowledge from the model supplied with implicit connectives to the model without such information, whileKishimoto et al. (2020)proposed a multi-task learning framework to incorporate implicit connectives prediction as another training objective. However, we argue that these methods are suboptimal since connectives are still not explicitly present in input texts. This is demonstrated byKishimoto et al. (2020), concluding that adding implicit connective prediction as a training objective provides only negligible gain for implicit relation classification on PDTB 2.0 (we empirically found the conclusion also held on the adversarial model). In this paper, we design a novel end-to-end model to leverage discourse connectives for the task of implicit discourse relation classification. The key inspiration is derived from the annotation process of implicit discourse relations in PDTB, which consists of inserting a connective that best conveys the inferred relation, and annotating the relation label based on both the inserted implicit connectives and contextual semantics(Prasad et al.,2008). We imitate this process by explicitly generating discourse connectives for the implicit relation classifier. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. A potential drawback of this joint model is that the poorly generated connectives at the early stage of joint training may mislead the relation classifier. One possible solution is always feeding true connectives to the implicit relation classifier for training. But it leads to severe discrepancies between training and inference(Sporleder and Lascarides,2008), since manually-annotated connectives are unavailable during evaluation(Prasad et al.,2008). To address this issue, we adopt Scheduled Sampling(Bengio et al.,2015)into our method. To be more specific, our relation classifier is first trained with hand-annotated implicit connectives and then gradually shifts to use generated connectives. We evaluate our model111https://github.com/liuwei1206/ConnRelon two English corpora, PDTB 2.0(Prasad et al.,2008), PDTB 3.0(Webber et al.,2019), and a German corpus, PCC(Bourgonje and Stede,2020), and compare it with other connective-enhanced approaches and existing state-of-the-art works. Results show that our method significantly outperforms those connective-enhanced baselines on three datasets while offering comparable performance to existing sota models. In addition, we perform the first systematic analysis of different connective-enhanced models to investigate why our method works better. Our studies show that: (1) models learn to use connectives more effectively when putting connectives in the input rather than using them as training objectives; (2) end-to-end training can improve models’ robustness to incorrectly-predicted connectives; (3) our method shows a better balance between arguments and connectives for relation prediction than other baselines. Finally, we show that connectives can effectively improve the predictive performance on frequent relations while failing on those with limited training instances.
====================
	 Reconstructing Historical Climate Fields With Deep Learning
==========
Abstract:  Historical records of climate fields are often sparse due to missing measurements, especially before the introduction of large-scale satellite missions. Several statistical and model-based methods have been introduced to fill gaps and reconstruct historical records. Here, we employ a recently introduced deep-learning approach based on Fourier convolutions, trained on numerical climate model output, to reconstruct historical climate fields. Using this approach we are able to realistically reconstruct large and irregular areas of missing data, as well as reconstruct known historical events such as strong El Ni\~no and La Ni\~na with very little given information. Our method outperforms the widely used statistical kriging method as well as other recent machine learning approaches. The model generalizes to higher resolutions than the ones it was trained on and can be used on a variety of climate fields. Moreover, it allows inpainting of masks never seen before during the model training.
Fluency score: 4
Fluency issues: [The overall fluency is acceptable, but it feels slightly technical and may not flow smoothly for readers unfamiliar with the subject matter.]

Clarity score: 3
Clarity issues: [The text uses technical terms such as "Fourier convolutions," "kriging method," and "inpainting of masks," which may not be clear to readers outside the field.]

Grammaticality score: 5
Grammaticality issues: []

Readability score: 3
Readability issues: [The complexity of vocabulary and sentence structure may impede readability for a general audience.]

Coherence score: 4
Coherence issues: [The text is coherent, but due to the density of information and specialized terms, it might be challenging for readers to follow without prior knowledge of the subject.]
==========
Introduction:  Observational climate data is typically sparse before systematic observations such as buoys, ship measurements, or satellite measurements were introduced. Generally, the further back in time we go, the fewer observations are available[1]. Temperature and precipitation records are the best-observed climate fields in the recent past and reach back until the 19th century, but measurements are still sparse and rely heavily on interpolation especially for earlier parts of the records[2,3]. Even more severely, for many important climate variables, such as sea-ice thickness or vegetation indices, no measurements exist at all before the introduction of large-scale satellite missions. The corresponding time series often span a few decades or even only years[e.g.,4,5]. The low spatial and temporal resolution introduces large uncertainties and limits our understanding of important climatic processes[2,6,1]. Several approaches and methods to produce historical climate fields based on the available observations have been developed in the past. One approach is to run state-of-the-art weather models with observations and past weather forecasts to produce reanalysis products that provide a complete picture of the past weather and climate for the last decades[7,8]. While reanalyses are successful in providing spatiotemporally continuous and consistent data, they often struggle with specific regions and variables and inherit biases the employed numerical models suffer from[8,9]. An alternative approach is to use statistical methods to reconstruct missing information. In this regard, kriging or Gaussian process regression is widely used in the geosciences[10,11,12]. However, statistical methods typically do not include knowledge of the temporal and spatial patterns of the underlying climatic fields and therefore fail to reconstruct these patterns, especially for large missing areas. In recent years, machine learning (ML) has become widely used in geoscience and climate science, with the promise of better performance than statistical methods while still providing easy usability and, to some extent, knowledge of the underlying physical processes[13,14]. The applications of machine learning in climate science are vast and range from classical time series forecasting[15,16,17], down-scaling and post-processing of numerical models[18,19], to time series reconstruction[20,13]. Furthermore, there is a substantial ongoing effort to combine traditional numerical Earth system models with machine learning methods to leverage the advantages of both approaches[21,22,14,23,24,25,26,27,28]. In this study, we consider the reconstruction of spatial climate fields as an image inpainting problem. Inpainting images based on given information is a classical problem in computer vision and many approaches have been proposed in recent years[29,30,31]. We apply the recently introduced state-of-the-art deep learning approachResolution-robust Large Mask Inpainting with Fourier Convolutions(LaMa)[32]to reconstruct different climate fields with a focus on surface temperature records. We train our model on numerical climate model output from the Coupled Model Intercomparison Project to reconstruct the missing measurements in observational data. Our method is able to reconstruct climate fields with very sparse information and highly irregular missing data. We show that our approach outperforms kriging and other machine learning methods. Moreover, it is able to inpaint different data sets than the ones it was trained on, and can be used on a variety of structurally different climatic fields at varying resolutions. The surface temperature is one of the most important climate variables, as a direct measure of climate change. Global instrumental temperature records reach back to the mid-19th century[2]with local observations reaching back as far as the mid-17th century[33]. However, on average, less than 30% of Earth’s surface before the year 1900 AD have measurements in the state-of-the-art observational data set HadCRUT4 (Fig.1a). This is similar for other widely used long-term temperature data. Therefore, surface temperature records serve as perfect proof-of-concept application for the image inpainting task in climate science.
====================
	 Procedural Text Mining with Large Language Models
==========
Abstract:  Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.
Fluency score: 4
Fluency issues: N/A

Clarity score: 4
Clarity issues: Use of jargon ("in-context learning", "zero-shot", "few-shot learning") without explanation may confuse non-expert readers.

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 3
Readability issues: Dense and technical language may impede readability for a general audience. The text could be more accessible with simpler syntax and less jargon.

Coherence score: 4
Coherence issues: While the text is mostly coherent for an informed audience, more explicit connections or layman's explanations could enhance coherence for broader readership.
==========
Introduction:  Extracting complex knowledge from unstructured sources is a challenge: in the industrial domain, for example, troubleshooting documents may contain the description of long and articulated procedures (i.e., sequences of steps to be performed in a precise order and under specific conditions) and those natural language instructions may be represented in very different textual forms, thus making it hard for a knowledge extraction algorithm to correctly identify and structure the relevant information. Oftentimes, automatic extraction is followed by manual revision of domain experts. In any case, all machine-learning-based methods require training data which is often not readily available, therefore novel approaches are emerging to exploit interactive dialogues and language models(bellan2021process). Extracting procedural knowledge from human natural language instructions is a challenging task. Firstly, natural language instructions are not interpretable by machines. In the easiest case, the instructions are given as numbered lists which can easily be identified. However, complications arise when the document contains procedures in different forms: a list without numbers, an indented text or simply a full text in which the different steps are connected by conjunctions like ”then”, ”afterwards”, etc. Secondly, procedures can either be composed of only simple steps or contain other sub-procedures that are located elsewhere in the document. Thirdly, the procedures can differ substantially from one document to the other because of different authors’ and editors’ styles, but the goal would be to integrate information from different documents. Specifically, we investigate the potential of large language models (LLMs) in the context of extracting procedural knowledge from unstructured PDF documents. LLMs demonstrate remarkable capabilities in natural language processing, surpassing those possible using conventional symbolic AI and machine learning technologies(mahowald2023dissociating). Nevertheless, these models often lack knowledge of nuanced, domain-specific details and are susceptible to hallucinations. This paper investigates the practical application of LLMs, with a particular focus on the advanced GPT-4 (Generative Pre-trained Transformer 4), to address the complex task of extracting procedures from unstructured PDF documents. The core of our research revolves around an incremental question-answering methodology, with a specific emphasis on harnessing LLMs in both zero-shot and in-context learning scenarios. Our study is structured to encompass two distinct approaches to in-context learning. The first approach involves the incorporation of an ontology containing definitions and procedural steps, while the second approach integrates a limited dataset tailored for few-shot learning. This comprehensive investigation not only highlights the considerable potential of our chosen approach but also underscores the critical role played by customised in-context learning. These tailored modifications are poised to make significant progress in tackling a persistent challenge within deep learning-based NLP techniques: the scarcity of essential training data for procedure extraction. Aligned with our exploration, three fundamental research questions guide our investigation: RQ1: How well is ChatGPT4 able to list the steps and substeps in the text versus the ontology settings? RQ2: Is in-context learning beneficial for procedural text mining? RQ3: Regardless of the ontology instantiation, is ChatGPT4 capable of the correct application of the ontology? Through systematic empirical inquiry, this study not only contributes to the enhancement of procedural text mining but also offers insights into the capacity of in-context learning enhancements to surmount the constraints stemming from inadequate training data. The significance of this research extends beyond procedural extraction, resonating within the broader landscape of NLP applications and cultivating the evolution of more sophisticated and adaptable information retrieval systems. Our code and dataset is publicly releasedhttps://github.com/jd-coderepos/proc-tm/. The paper is structured as follows. We begin with a motivating example for our work insection 2; then we describe the procedural ontology insection 3. Insection 4, we propose our approach and introduce the experimental dataset used for the task of procedural text mining insection 5. Our experimental results are discussed both quantitatively and qualitatively next insection 6. Finally a brief discussion on related work is offered insection 7and concluding remarks insection 8.
====================
	 E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning
==========
Abstract:  The bio-inspired event cameras or dynamic vision sensors are capable of asynchronously capturing per-pixel brightness changes (called event-streams) in high temporal resolution and high dynamic range. However, the non-structural spatial-temporal event-streams make it challenging for providing intuitive visualization with rich semantic information for human vision. It calls for events-to-video (E2V) solutions which take event-streams as input and generate high quality video frames for intuitive visualization. However, current solutions are predominantly data-driven without considering the prior knowledge of the underlying statistics relating event-streams and video frames. It highly relies on the non-linearity and generalization capability of the deep neural networks, thus, is struggling on reconstructing detailed textures when the scenes are complex. In this work, we propose \textbf{E2HQV}, a novel E2V paradigm designed to produce high-quality video frames from events. This approach leverages a model-aided deep learning framework, underpinned by a theory-inspired E2V model, which is meticulously derived from the fundamental imaging principles of event cameras. To deal with the issue of state-reset in the recurrent components of E2HQV, we also design a temporal shift embedding module to further improve the quality of the video frames. Comprehensive evaluations on the real world event camera datasets validate our approach, with E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the second best by over 40\% for some evaluation metrics.
Fluency score: 4
Fluency issues: [None that significantly lower fluency]

Clarity score: 3
Clarity issues: ["However, the non-structural..." and "It highly relies on..." sentences are a bit complex and might confuse readers.]

Grammaticality score: 4
Grammaticality issues: ["It highly relies on the non-linearity..." should be "It relies heavily on..."]

Readability score: 3
Readability issues: [Complex sentence structures and technical jargon could impede readability for non-expert readers.]

Coherence score: 4
Coherence issues: [Paragraph is overall coherent, but the technical detail may be overwhelming without further context.]
==========
Introduction:  Inspired by the human visual system, Silicon RetinaMahowald (1991)has pioneered an approach to perceptual sensing with event cameras or Dynamic Vision Sensors (DVS)Lichtsteineret al.(2008); Poschet al.(2010); Berneret al.(2013)and gained significant interests from both academia and industry. Unlike traditional cameras, event cameras detect microsecond-level intensity changes, generating an asynchronous stream of ‘events’, termed as event-stream. Event cameras offer several advantages over conventional CCD/CMOS cameras, including high temporal resolution, high dynamic range of up to 140dBLichtsteineret al.(2008), and low resource consumption due to the sparse nature of event-streams. For example, the DVS128 sensor platform consumes 150 times less energy than a conventional CMOS cameraLichtsteineret al.(2008). Despite the appealing advantages of event cameras, the non-structural event-streams are not inherently compatible with traditional computer vision methodologiesScheerlincket al.(2020)and the visualization is not intuitive for human users to understand. To address the above issue, the research on events-to-video (E2V), which aims to generate video frames from pure event-streams, has been raised to provide convenient and intuitive access to the rich information encapsulated in the sparse and non-structure event-streams. There have been a number of successful approaches for E2V task, such as E2VIDRebecqet al.(2019), FireNetScheerlincket al.(2020), SPADE-E2VIDCadenaet al.(2021), and ET-NetWenget al.(2021). However, the quality of the video frames generated by the existing E2V approaches is still not satisfactory and fail to recover detailed texture for the complex scenesErcanet al.(2023). This issue is predominantly attributed to the fact that many of these approaches, such as E2VID and ET-Net, primarily adopt a purely data-driven approach to learn the mapping from event-streams to video frames directly. However, the purely data-driven approaches are lack of interpretability and flexibilityShlezingeret al.(2023), and they do not take into account the prior knowledge of the underlying statistics relating event-streams and video frames. Therefore, their performance is largely dependent on the non-linearity and generalization capability of the neural networks, which poses significant challenges when the scenes to be reconstructed are complexJarrettet al.(2009). To address the aforementioned challenges, we introduceE2HQV, a novel E2V paradigm designed to produce high-quality video frames from events. This is achieved through a model-aided deep learning framework that integrates a theory-inspired E2V model. Rooted in the fundamental imaging principles of event cameras, this theory-inspired E2V model elucidates the relationship between consecutive frames and their associated inter-frame event-streams, offering valuable prior knowledge that enhances the learning efficacy of our deep learning framework. As shown in Figure1, instead of generating video frames in a pure data-driven approach, E2HQV estimates a number of intermediate key parameters defined by the theory-inspired E2V model then reconstructs the video frames accordingly. The contributions of this work can be summarized as: We propose E2HQV, a novel high-quality video frames generation approach from event-streams by facilitating a model-aided learning framework which learns the key parameters defined by a theory-inspired E2V model and generates high quality video frames accordingly. According to the imaging principle of event camera and relation between video frames and event-stream, a theory-inspired E2V model is derived to guide the design of the model-aided learning framework. A new temporal shift embedding module is designed to deal with the perturbation introduced by the state-reset mechanism of the recurrent components in the framework and ensuring seamless fusion of events and reconstructed frames. Through extensive experiments on mainstream event-based video reconstruction datasets, E2HQV consistently exhibits superior performance over state-of-the-art (SOTA) approaches. Remarkably, for certain evaluation metrics, E2HQV surpasses the next best approach by a substantial margin of over 40%.
====================
	 "It's a Match!" -- A Benchmark of Task Affinity Scores for Joint Learning
==========
Abstract:  While the promises of Multi-Task Learning (MTL) are attractive, characterizing the conditions of its success is still an open problem in Deep Learning. Some tasks may benefit from being learned together while others may be detrimental to one another. From a task perspective, grouping cooperative tasks while separating competing tasks is paramount to reap the benefits of MTL, i.e., reducing training and inference costs. Therefore, estimating task affinity for joint learning is a key endeavor. Recent work suggests that the training conditions themselves have a significant impact on the outcomes of MTL. Yet, the literature is lacking of a benchmark to assess the effectiveness of tasks affinity estimation techniques and their relation with actual MTL performance. In this paper, we take a first step in recovering this gap by (i) defining a set of affinity scores by both revisiting contributions from previous literature as well presenting new ones and (ii) benchmarking them on the Taskonomy dataset. Our empirical campaign reveals how, even in a small-scale scenario, task affinity scoring does not correlate well with actual MTL performance. Yet, some metrics can be more indicative than others.
Fluency score: 4
Fluency issues: [No specific parts, general improvement needed to enhance flow]

Clarity score: 4
Clarity issues: ["lacking of a benchmark" should be "lacking a benchmark", phrase structure in the last sentence]

Grammaticality score: 4
Grammaticality issues: ["lacking of a benchmark" should be "lacking a benchmark"]

Readability score: 4
Readability issues: [Complex sentence structures, could be broken down for easier reading]

Coherence score: 4
Coherence issues: [The transition between the problem statement and the presented solution could be smoother]
==========
Introduction:  For more than two decades since its inceptioncaruana1997multitask, Multi-Task Learning (MTL) has been extensively studied by the Deep Learning community. For practitioners interested in the best strategy to learn a collection of tasks, the promises of MTL are numerous and attractive. First, learning to solve several tasks simultaneously can be more cost-efficient from a model development and deployment perspective. Second, if the tasks learned together cooperate, MTL can even outperform its Single-Task Learning (STL) counterpart for the same computational coststandley2020tasks. However, MTL potential advantages are tempered by the difficulty of estimatingtask affinity, i.e., identify tasks benefiting from joint learning, without testing all combinations of tasks. This calls fortask affinity scores– to quantify a priori and at a cheap computational cost the potential benefit of learning tasks together. The quest for the perfect affinity score is further exacerbated by MTL performance’s strong dependency on the learning context, i.e., the data and models used for training. For instance, tasks cooperating in one learning context can result in competition when using slightly different data or modelsstandley2020tasks. Recent worksfifty2021efficiently;standley2020taskshave integrated this context-dependency when designing task grouping strategies. While these approaches avoid a complete search across all task combinations, they still require training and comparing some MTL models for the final network selection. Furthermore, those studies show that even in a small-scale scenario, MTL performance cannot be accurately predicted without actually performing MTL. Despite providing assessment of task affinity, previous literature lacks of a broader comparison of the associated scores. In this work, we take a first step in recovering this gap bypresenting an empirical comparison of several task affinity scoring techniques. Some of these scores are inspired by previous literature ranging from Transfer Learning to Multi-Task Learning:taxonomical distancezamir2018taskonomy,input attribution similarityinput_attr_aff_metric_ref,representation similarity analysisTL_RSA,gradient similaritygrads_clashesandgradient transferencefifty2021efficiently. We benchmark an additional affinity score which is an original proposal:label injection. We evaluate all of them on the public Taskonomy datasetzamir2018taskonomywhich is a well-known large benchmark spanning several Computer Vision tasks. Note that our objective is not to present a novel state-of-the-art MTL architecture but rather an objective benchmark of task affinity estimation techniques. More specifically we aim to understand if task affinity scores can (i) be used as proxy for true MTL performance and (ii) suggest the best partner task to improve the performance of a target task. These scores and their discussion aim at helping practitioners gauge the benefit of MTL for their own set of tasks. Insection2, we review the state of the art on MTL affinity characterization. Insection3, we present the affinity scores selected for benchmarking and detail our evaluation protocol. We present our results insection4and discuss the advantages and limitations of these scores insection5.Section6concludes the paper.
====================
	 SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling
==========
Abstract:  Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non-linear relationships. We provide open-source code, and a tutorial notebook with example usage, accentuating the easy-to-use nature of the method.
Fluency score: 5
Fluency issues: [None]

Clarity score: 4
Clarity issues: [Use of technical terms and acronyms like SEMs, DAGs without explanation]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Complex sentence structures, heavy use of jargon]

Coherence score: 4
Coherence issues: [Connections between sentences could be more explicit]
==========
Introduction:  Imagine you wish to estimate multiple causal effects from observational data. Despite the challenging nature of the task, it nonetheless represents one of the core goals of science and causality\parencitePearl2018TBOW, Pearl2012, Vowels2021, Vowels2021DAGs. Indeed, in the absence of experimental data, we must do everything we can to ensure the causal relevance of our statistical inferences. Otherwise, our estimates cannot be tied to their associated theories\parenciteVowels2021, Scheel2020 and the estimates effectively represent arbitrary functions of the observed data, subject to ambiguous, pseudo-causal interpretations\parenciteGrosz2020, Rohrer2018, Hernan2018. To this end, researchers in the domains of psychology and social science have begun to advocate for the increased adoption of causal Directed Acyclic Graphs (DAGs), which aid in (a) the clear, formal specification of a causal theory as a mathematical but nonetheless intuitive, visual object, (b) unbiased estimation of the target quantities of estimation (e.g., effect sizes)\parenciteVowels2023_prespec, Rohrer2018. Unfortunately, DAGs themselves only get us so far in terms of achieving unbiased estimation of causal effects. Whilst they help us deal with the specification of thestructuralaspect of a model/theory (by representing an ordered causal process), and whilst they enable us to express a target quantity as a function of the observed distribution, they do not help with the estimation itself. Furthermore, they do not help us with thefunctionalspecification of the relationships between variables. Indeed, one of the strengths of DAGs is that they are non-parametric, and make very limited assumptions about the underlying functional form. For instance, they tell us nothing about whetherYis a linear function of justXor whether it is a linear function ofX^{2}. Thus, to arrive at meaningful effect size estimates, the choice of the associated estimation technique and its associated level of functional adaptability must be made. One option that researchers have is to use structural equation modeling or linear/logistic regression for the estimation of the associated effects, which make the assumption that the relationships are linear/linear in the logit space. The problem with this is that such an assumption of linearity can lead to a similar level of biased estimation\parenciteVowels2021, vanderLaan2011, vanderLaan2014 to that which results from structural misspecification. Such misspecification thereby undermines the otherwise advantageously function-agnostic nature of the DAG. In order to avoid making such unnecessary assumptions about the functional form underlying the causal relationships represented in the DAG, we recommend the use of machine learning. Specifically we propose Super Learner Equation Modeling (SLEM), an integrated framework for machine learning based causal inference with DAGs. Within this SLEM framework, we present the DAG Learner estimator object in the form of easy-to-use, open-source Python code, including simulations and a tutorial-style notebook.111The code can be found in supplementary material or athttps://github.com/matthewvowels1/SLEM.Essentially, for any user-specified DAG, and according to the SLEM framework, the DAG Learner implements a set of general machine learning models which are used to estimate, in a data-driven but causally-constrained manner, all associated path coefficients. Furthermore, our framework allows for the estimation of the effect of (optionally multiple, simultaneous) user-specified interventions, thereby facilitating a general, and easy-to-use tool for non-parametric causal inference. In our view, the availability of such a tool is overdue - whilst the required techniques have existed for some time, they have never been combined in such a way. The paper is structured as follows: First, we briefly discuss some background theory relating to DAGs, causality, and machine learning. Secondly, we describe the proposed framework, beginning with the specification of the DAG, as well as the chosen machine learning method. Then, we provide some worked examples and key simulations. Finally we discuss the associated limitations of the method in relation to some existing alternative approaches, and close with a summary. Note that whilst the success of the propose method rests on existing work in the domains of causality and machine learning, to the best of our knowledge we are the first to bring the two together in such an integrated, easy-to-use methodology.
====================
	 EgoHumans: An Egocentric 3D Multi-Human Benchmark
==========
Abstract:  We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing tennis, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human activities and fast-moving egocentric views. We rigorously evaluate existing state-of-the-art methods and highlight their limitations in the egocentric scenario, specifically on multi-human tracking. To address such limitations, we propose EgoFormer, a novel approach with a multi-stream transformer architecture and explicit 3D spatial reasoning to estimate and track the human pose. EgoFormer significantly outperforms prior art by 13.6% IDF1 on the EgoHumans dataset.
Fluency score: 4
Fluency issues: [none]

Clarity score: 4
Clarity issues: ["playing tennis, fencing, volleyball, etc." - List is non-exhaustive and vague with "etc."]

Grammaticality score: 5
Grammaticality issues: [none]

Readability score: 4
Readability issues: ["more than 125k egocentric images" could be clearer as "more than 125,000"]

Coherence score: 4
Coherence issues: ["multi-stream transformer architecture and explicit 3D spatial reasoning" could be jargon-heavy for non-specialists]
==========
Introduction:  Understanding humans in 3D from the egocentric view is key to building immersive social telepresence[lawrence2021project,bamodu2013virtual,ma2021pixel,lombardi2021mixture], assistive humanoid robots[goodrich2013teleoperation,fridin2014acceptance,piezzo2017feasibility], and augmented reality systems[azuma1997survey,billinghurst2015survey,carmigniani2011augmented]. A crucial step in this direction is to obtain 3D supervision at scale for deep learning models to generalize to the real world. However, unlike the large-scale 2D benchmarks[deng2009imagenet,lin2014microsoft,li2019crowdpose,johnson2010clustered,cordts2016cityscapes], the diversity of the 3D benchmarks[joo2015panoptic]is severely limited - primarily because manual annotation in the 3D space is impractical. As a result, existing popular 3D benchmarks[ionescu2013human3,joo2015panoptic,von2018recovering,mehta2017monocular,li2021ai,hassan2019resolving]are constrained to indoor environments or, at most, two human subjects if outdoors, stationary/slow camera motion, with limited occlusion. Furthermore, the majority of these benchmarks only portray the third-person view. Recent progress has been made in constructing egocentric benchmarks[xu2019mo,ng2020you2me,guzov2021human,zhang2022egobody]. However, they suffer from the same diversity pitfalls, making it difficult to evaluate how close the field is to fully robust and general solutions. To drive advances in the field, we propose a benchmark,EgoHumans, that includes challenging scenarios ignored in previous studies and a novel method,EgoFormer, that outperforms prior art as a starting point for the evaluations. EgoHumans is a new egocentric benchmark consisting of high-resolution videos and comprehensive ground truth annotations such as camera parameters, 2D bounding boxes, human tracking ids[dendorfer2020mot20], 2D/3D human poses, and 3D human meshes[loper2015smpl]. EgoHumans goes beyond previous benchmarks in important ways. First, it captures outdoor videos of unconstrained environments and dynamic human activities, including challenging sporting events such as fencing, badminton, volleyball, etc. Second, the activities are unchoreographed to truly capture thein-the-wildphilosophy of our work. Our video sequences include fast ego-camera motion, human-human occlusion, truncation, and humans appearing at a wide range of spatial scales. We leverage a flexible multi-camera setup consisting of Meta’s Aria glasses[aria_pilot_dataset], with an RGB and two greyscale cameras, for the egocentric view and stationary secondary RGB cameras for the auxiliary views (see Fig.4). Such camera combination allows us to accurately track and triangulate human poses in 3D for a long duration without using visual markers[ionescu2013human3]or additional sensors[von2018recovering]. The natural form factor of glasses[maimone2013computational]coupled with the RGB and stereo cameras closely resembles the human vision[matthies1989dynamic]. Last, as a by-product of our capture setup, we provide 3D annotations for the multi-view secondary cameras. We hope these annotations allow the ability to move fluidly between the egocentric and secondary perspectives[li2021ego]and inspire new research for holistic human understanding. To our knowledge, EgoHumans is the only multi-human 3D egocentric benchmark with these attributes. We generate high-quality 3D ground truth by leveraging state-of-the-art visual-inertial odometry algorithm (VIO)[aria_pilot_dataset], which is robust to fast head motion and sudden changes in the eye gaze - frequently observed in natural human behavior[zhang2020wandering]. All the cameras in our multi-view capture are aligned to a single world coordinate system using Procrustes alignment[luo2002iterative]of the camera poses. EgoHumans consists of 125k egocentric RGB images and 410k human instance annotations (Tab.1) capturing high-energy activities in various locations, clothing, and lighting conditions with severe occlusion. We annotate the tracking ids, bounding boxes, and 2D/3D human poses for all views using off-shelf estimators[jin2020whole,wang2020deep]and manual supervision. With carefully calibrated camera parameters and the multi-view 2D poses for a video, we optimize for 3D skeletons using triangulation[iskakov2019learnable]and refinement constraints like constant limb length, joint symmetry, and temporal consistency[vo2020self]. Finally, we build an efficient multi-stage motion capture pipeline to fit the SMPL[loper2015smpl]body model to the 3D human skeletons. The scale and diversity of the EgoHumans dataset allow unprecedented opportunities to evaluate and improve egocentric methods. Specifically, we evaluate existing methods for multi-human tracking. Our results show that prior art is susceptible to common failures like person-id switching due to rapid camera motion, occlusion, and unconstrained human activities. Inspired by this, we presentEgoFormer, a novel 3D human tracking approach with multi-stream transformer architecture that effectively performs human depth reasoning in a camera-agnostic frame of reference. Our proposed method uses self-attention to aggregate multi-view spatial information from the RGB, left, and right stereo cameras simultaneously. EgoFormer significantly outperforms existing state-of-the-art tracking methods[zhang2022bytetrack,rajasegaran2022tracking,cao2022observation]by13.6% IDF1 score on EgoHumans. Our contributions are summarized as follows. EgoHumansis the first multi-human 3D egocentric dataset capturing unconstrained human activities in the wild. We provide high-quality 3D ground truth from egocentric and secondary views for all humans. We benchmark existing state-of-the-art methods for multi-human tracking and highlight their fundamental limitations on egocentric views. We proposeEgoFormer, a 3D tracking method that uses a multi-stream spatial transformer encoder for depth reasoning from the ego view. Our method consistently outperforms the prior art on the EgoHumanstestset.
====================
	 Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches
==========
Abstract:  Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.
Fluency score: 4
Fluency issues: [No particular parts lower the score, but the professional jargon and complex sentence structures might impact overall fluency for a general audience.]

Clarity score: 3
Clarity issues: ["SC strategy" is not defined. The term "filter-guided noise extraction module" is not clear to non-experts.]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 3
Readability issues: [Complex sentence structures and technical language make the text less accessible to a wider audience.]

Coherence score: 4
Coherence issues: [The introduction of specific concepts without prior explanation can confuse readers (e.g., "SC strategy").]
==========
Introduction:  Image denoising aims to recover noise-free images from noisy observations by reducing the potential noise. Although it is one of the oldest and most classical tasks in low-level computer vision, its fundamental nature continues drawing much interest. In general, existing image denoising algorithms can be divided into three groups: filtering-based[2,3,4,5,66], model-based methods[9,32,30,12,69,61]and learning-based[17,19,21,41,36,40,44,39,38,25]methods. The additive white Gaussian noise (AWGN) assumption is widely used in image denoising. However, it is complex and challenging to adaptively achieve denoising based on filtering-based or model-based methods for high performance. In contrast, learning-based methods have demonstrated their superiority in image denoising. However, these methods[54,21,25,53,aind1,100]are data-driven and typically require pairs of clean-noisy datasets to train their models. These noise samples are usually obtained through a predefined AWGN formulation that assumes the noise is signal-independent. On the contrary, the real-world noise is more complex and much different from the ideal AWGN assumption. Using the AWGN model directly for the real scenes leads to poor performance. Therefore, numerous methods[37,40,38,39,74,90,aind1]have been proposed to capture paired clean-noisy image datasets from real scenes to promote the training of deep networks. However, these paired image-based methods focus on enhancing performance by improving network structures, and acquiring well-aligned pairs of clean-noisy images is time-consuming and laborious. To solve the aforementioned problems, unsupervised denoising-based methods[chen,hong,1000,dbsnl]have emerged. The existing approaches are typically based on generative adversarial network (GAN) frameworks, which mainly focus on generating higher quality pseudo-noisy images. GAN2GAN[cha]identified one of the key limitions of unsupervised denoising frameworks is the gap between the real and synthetic images, and proposed a novel approach using multiple generators and discriminators to generate images that closely conform to the real noise distribution. However, the performance of existing unsupervised denoising frameworks remains unsatisfactory due to the difficulty of adversarially training. Moreover, after training the model, the existing frameworks cannot further maximize the denoising potential without significantly changing its structure or increasing the inference complexity (e.g., using certain self-ensemble strategy) for denoisers. To address the previous limitations, we innovatively propose an unsupervised real-world denoising network called Self Collaboration Parallel Generative Adversarial Branches (SCPGabNet). The self-collaboration (SC) strategy, which provides the framework a powerful self-boosting capability. This enables the denoisers obtained from the conventional GAN framework to continuously evolve themselves and significantly improve their performance. The major contributions of our method are as follows: We design a novel filter-guided synthetic noisy image generator with the noise extraction (NE) module to synthesize high-quality clean-noisy image pairs, which serve as the foundation for implementing the SC strategy. We propose an effective parallel generative adversarial branches framework with complementary “self-synthesis" and “unpaired-synthesis" constraints as our powerful baseline. We are the first to propose the SC strategy, which significantly enhances the performance of the GAN-based denoising framework without increasing its inference complexity. Experimental results demonstrate the superiority of our SCPGabNet over state-of-the-art unsupervised image denoising methods with large margins on the SIDD and DND benchmarking datasets.
====================
	 Towards More Efficient Depression Risk Recognition via Gait
==========
Abstract:  Depression, a highly prevalent mental illness, affects over 280 million individuals worldwide. Early detection and timely intervention are crucial for promoting remission, preventing relapse, and alleviating the emotional and financial burdens associated with depression. However, patients with depression often go undiagnosed in the primary care setting. Unlike many physiological illnesses, depression lacks objective indicators for recognizing depression risk, and existing methods for depression risk recognition are time-consuming and often encounter a shortage of trained medical professionals. The correlation between gait and depression risk has been empirically established. Gait can serve as a promising objective biomarker, offering the advantage of efficient and convenient data collection. However, current methods for recognizing depression risk based on gait have only been validated on small, private datasets, lacking large-scale publicly available datasets for research purposes. Additionally, these methods are primarily limited to hand-crafted approaches. Gait is a complex form of motion, and hand-crafted gait features often only capture a fraction of the intricate associations between gait and depression risk. Therefore, this study first constructs a large-scale gait database, encompassing over 1,200 individuals, 40,000 gait sequences, and covering six perspectives and three types of attire. Two commonly used psychological scales are provided as depression risk annotations. Subsequently, a deep learning-based depression risk recognition model is proposed, overcoming the limitations of hand-crafted approaches. Through experiments conducted on the constructed large-scale database, the effectiveness of the proposed method is validated, and numerous instructive insights are presented in the paper, highlighting the significant potential of gait-based depression risk recognition.
Fluency score: 4
Fluency issues: [The text is generally fluent, but the length of sentences and complexity of information might challenge some readers.]

Clarity score: 3
Clarity issues: [The text is packed with technical terms and extended sentences, which could be broken down for clarity.]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [The long and complex sentences might hinder quick comprehension.]

Coherence score: 4
Coherence issues: [The structure is coherent, but the high density of technical content might require readers to re-read for full understanding.]
==========
Introduction:  Depression is recognized by the World Health Organization (WHO) as one of the primary contributors to the global disease burden[44]. Within the clinical framework, the American Psychiatric Association’s Diagnostic Statistical Manual of Mental Disorders- Fifth Edition (DSM-5) classifies depression as persistent depressive mood or loss of interest and pleasure in activities. It affects approximately 280 million individuals worldwide, comprising approximately 3.8% of the global population[43]. Depression profoundly impacts individuals across multiple domains, including a decline in their overall quality of life[30,20], compromised social functioning[25,23], detrimental effects on physical health[6,46], and an elevated susceptibility to suicide[24,9]. Recognizing the significance of early detection and timely intervention is crucial for promoting remission, preventing relapse, and mitigating the emotional and financial burdens associated with this condition[12,13,22]. However, depression is notably underdiagnosed[12,62,55]. A survey involving 33,653 physician-patient interactions reveal that in the primary care setting, less than 5% of adults are screened for depression[1]. And it has been reported that at least 25% of patients go undiagnosed[2], with a significant majority of those seeking assistance from primary care physicians not receiving suitable treatment, particularly in low-income and middle-income countries[64,31]. The main challenge in screening depression in the primary care is the absence of objective indicators to recognize depression risk[5]. Unlike many physiological illnesses, depression lacks precise biomarkers, clinicians primarily rely on clinical criteria such as psychological questionnaires and patients’ self-reports[21]. This introduces potential issues: Individuals may provide biassed or inaccurate information due to personal inclinations, social expectations or difficulties in recalling past experiences[40,32,8]. Approximately 50% of patients have been observed to negate experiencing depressive feelings[5]. Besides, variability in linguistic expression can hinder accurate communication of emotions or experiences[4,28,41,14]. These issues can lead to incorrect depression risk recognition, and missing the optimal window for further treatment[47,15,16]. Another primary issue with questionnaire-based approaches for depression risk recognition pertains to inefficiency. Psychological assessments are time-consuming and often encounter a shortage of trained medical professionals, resulting in prolonged referral processes. Consequently, this may hinder timely psychological intervention, potentially exacerbating mental health issues[11,7]. In addition, psychological questionnaires are typically intended for periodic, rather than frequent, evaluations, often yielding only one or two measurements annually. This limited frequency may not effectively capture long-term condition trends in a timely manner, in light of the high relapse rates associated with depression: 21% at 12 months, 30% at 2 years, and 42% at 5 years[29]. Gait has been shown to be an essential manifestation of depression risk[53,50,19]. In particular, gait is modulated by the advanced neural center[56], which is also implicated with the pathophysiology of depression[58]. Many researches have demonstrate the association between depression risk and gait characteristics. Notably, specific abnormalities in gait, such as decreased vertical head movement[39], reduced range of motion in limbs, and a decelerated pace[34], are stable indications of depression. Hence, the gait-based depression risk recognition offers advantages compared to traditional psychological assessment methods: Objective biomarker: Gait serves as an objective biomarker for recognizing the risk of depression within the primary care setting, thereby mitigating the influence of subjective biases, societal expectations, or challenges in recollecting past experiences, which often lead to inaccurate information. High efficiency: Gathering gait data through cameras proves to be a highly efficient method, surpassing the time-consuming process of administering psychological questionnaires. This approach eliminates the requirement for trained professionals to collect data, thereby reducing both time and manpower costs. Despite preliminary investigations into the relationship between gait and depression risk, as well as attempts to recognize depression risk based on gait features, existing research suffers from two significant limitations. Firstly, these studies are often constrained by limited and homogeneous gait data, typically lacking the inclusion of gait data captured from different perspectives and with varying attire. Consequently, the generalizability of these research findings remains to be established, as the conclusions drawn may not necessarily apply to more diverse gait data. Furthermore, the accessibility of these datasets is severely restricted, impeding research reproducibility and hindering progress in the field. This limitation not only undermines the ability to validate previous research but also hampers advancements in this domain. Secondly, these studies commonly rely on hand-crafted gait features for analysis. However, human gait is a complex form of motion, and hand-crafted gait features often provide a limited representation, only capturing a fraction of the intricate associations between gait and depression risk. To address these limitations, we have established the largest gait-based dataset for depression risk recognition to date, encompassing over 1,200 subjects and more than 40,000 gait sequences. This dataset incorporates diverse perspectives and attire variations, providing a comprehensive representation of gait data. The depression risk of the participants was annotated using two commonly used depression risk assessment scales in the primary care. Leveraging this rich dataset, we propose a novel data-driven approach for depression risk recognition based on deep learning in this paper. The proposed method overcomes the problems associated with hand-crafted features by autonomously learning gait features that are relevant to depression risk. There are two notable characteristics in depression risk related gait features: Firstly, the main features associated with depression risk are dynamic features, which refer to the temporal aspects of gait during the walking process. Secondly, gait features that are associated with depression risk can manifest as either local details or involve the entire body. Based on these two characteristics, this paper proposes a deep learning model with dynamic feature modeling as its core. In the feature extraction process, both local dynamic features and global dynamic features are effectively integrated. This innovative methodology offers a fresh perspective for investigating the association between gait and depression risk. The contributions of this paper can be summarized as follows: We build a large-scale dataset for gait-based depression risk recognition. This dataset serves as a benchmark, propelling advancements in the field of depression risk recognition. Ultimately, it aims to enhance the efficiency of depression screening in the primary care, ensuring that more individuals receive the assistance they need. Based on the insights on depression-related gait features, we introduce a deep learning model with dynamic feature modeling at its essence. This model adeptly merges local and global features, yielding a comprehensive integration. A plethora of experiments are conducted to delve deeper into the aberrant gait patterns that are associated with the risk of depression. These experiments offer enlightening perspectives that contribute to the field of depression research. The remainder of this paper is organized as follows: Section2presents a brief literature review of the related work. Section3introduce the gait-based dataset for depression risk recognition. The proposed recognition model are described in detail in Section4. The configurations and results of experiments are presented in Section5. Finally, the conclusion of this paper is summarized in Section6.
====================
	 Overview of the TREC 2023 Product Product Search Track
==========
Abstract:  This is the first year of the TREC Product search track. The focus this year was the creation of a reusable collection and evaluation of the impact of the use of metadata and multi-modal data on retrieval accuracy. This year we leverage the new product search corpus, which includes contextual metadata. Our analysis shows that in the product search domain, traditional retrieval systems are highly effective and commonly outperform general-purpose pretrained embedding models. Our analysis also evaluates the impact of using simplified and metadata-enhanced collections, finding no clear trend in the impact of the expanded collection. We also see some surprising outcomes; despite their widespread adoption and competitive performance on other tasks, we find single-stage dense retrieval runs can commonly be noncompetitive or generate low-quality results both in the zero-shot and fine-tuned domain.
Fluency score: 5
Fluency issues: 

Clarity score: 4
Clarity issues: [the use of "metadata-enhanced collections" and "single-stage dense retrieval runs" might be unclear for people unfamiliar with the topic]

Grammaticality score: 5
Grammaticality issues: 

Readability score: 4
Readability issues: [complex sentence structures and jargon could pose difficulties for the general audience]

Coherence score: 5
Coherence issues:
==========
Introduction:  At TREC 2023, we hosted the first TREC Product Search Track, looking to create a reusable general benchmark for evaluating the performance of retrieval methods in the product search domain. We focus on providing a benchmark similar in scale and format to NQKwiatkowskiet al.(2019), or the Deep Learning TrackCraswellet al.(2021)but focused on product search. In providing a simple-to-use dataset, we believe broad experimentation using popular retrieval librariesLinet al.(2021)Gaoet al.(2022)can lead to broad improvements in retrieval performance.In this first year of the track, we created a novel collection based on the ESCI Product Re-ranking datasetReddyet al.(2022), sampled novel queries, created enriched metadata in the form of additional text and images along with seeded evaluation results with a broad range of baseline runs to aid in collection reusability and to allow iteration and experimentation on the use of additional context.Unlike previous product search corpora, the Product Search Track is multi-modal and has a large enough scale to explore the usage of neural retrieval methods. We observe somewhat surprising results using this scaled dataset and a wide variety of baseline runs. Single-stage retrieval models that leverage vector representations do not consistently outperform traditional retrieval methods such as BM25. Moreover, in the zero-shot setting, we find that larger vector-based models do not always beat their more minor variants, which is at odds with other evaluation corpora such as MTEBMuennighoffet al.(2023). Finally, while additional metadata can improve retrieval performance at a macro level, extra information cannot guarantee performance. In evaluating per-query performance, we find that vector-based systems lose performance with the other metadata. Please see the participant papers for more insights about what we learned this year.
====================
	 Towards Complex Real-World Safety Factory Inspection: A High-Quality Dataset for Safety Clothing and Helmet Detection
==========
Abstract:  Safety clothing and helmets play a crucial role in ensuring worker safety at construction sites. Recently, deep learning methods have garnered significant attention in the field of computer vision for their potential to enhance safety and efficiency in various industries. However, limited availability of high-quality datasets has hindered the development of deep learning methods for safety clothing and helmet detection. In this work, we present a large, comprehensive, and realistic high-quality dataset for safety clothing and helmet detection, which was collected from a real-world chemical plant and annotated by professional security inspectors. Our dataset has been compared with several existing open-source datasets, and its effectiveness has been verified applying some classic object detection methods. The results demonstrate that our dataset is more complete and performs better in real-world settings. Furthermore, we have released our deployment code to the public to encourage the adoption of our dataset and improve worker safety. We hope that our efforts will promote the convergence of academic research and industry, ultimately contribute to the betterment of society.
Fluency score: 4
Fluency issues: [No major issues, minor awkwardness in the last sentence with "ultimately contribute"]

Clarity score: 4
Clarity issues: [The term "our deployment code" could be more specific]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 4
Readability issues: [Some sentences are lengthy and complex]

Coherence score: 5
Coherence issues: [No issues detected]
==========
Introduction:  Safety is a perpetual and significant concern in all industries, particularly in high-risk construction sites such as chemical factories and building sites. Protective equipments, including safety clothing and helmets, are crucial for safeguarding workers in high-risk construction sites. Helmets can effectively prevent head injuries caused by falling or splashing objects, while safety clothing can protect the body and arms from hazardous chemicals and liquids. The lack of safety clothing and helmets frequently results in safety accidents, with terrible impacts on families and society. Therefore, monitoring the usage of safety clothing and helmets in factories or construction sites is of immense safety significance and broad application value. Current laws and safety regulations stipulate that the person in charge and the contractor are responsible for providing, supervising, and maintaining personal protective equipment in construction sites. However, some workers may relax their vigilance due to a lack of safety awareness or discomfort from long-term wear, thereby increasing the probability of safety accidents. Currently, the commonly used automated monitoring technique for compliance of safety clothing and helmets is a class of sensor-based methods[1,2]. Sensor-based technology monitors whether the workers wear safety clothing or helmets by attaching sensors to them and analyzing the signals. However, this method requires significant investments in purchasing, installation, and maintenance, resulting in relatively high costs. In recent years, deep learning methods have gained substantial attention in computer vision[3,4]due to their ability to self-learn useful features from large-scale, annotated training data. In particular, convolution neural networks are widely used for image classification and object detection. For example, LeCun et al.[5]utilized Convolutional Neural Networks (CNNs) to recognize handwritten digits, while Kolar et al.[6]used CNNs to detect safety guardrails in construction sites. Nath et al.[7,8]used CNNs to identify common construction-related objects (e.g., buildings, equipment, and workers). These works provide new insights into monitoring the usage of safety clothing and helmets. As we all know, the great success of deep learning depends on large-scale, high-quality, and annotated data. To the best of our knowledge, only two safety clothing and helmet benchmark datasets have been released, namely Pictor-v3[9]and Safety Helmet Wearing Dataset (SHWD)[10]. However, the categories and quantities of labeled instances in the two datasets fall short of meeting the requirements for real-world applications. The existing datasets only consider the impact of helmets without paying attention to the impact of safety clothing. In real-world applications, there are still potential safety hazards. In addition, the backgrounds of all images are too simple, and the lighting conditions are overly ideal, which differs significantly from real-world working environments. If such datasets are actually employing in a factory safety inspection project, the generalization ability of the model is likely to be poor. To address the above limitations, in this work, we contribute a large, realistic, and comprehensive high-quality dataset for safety clothing and helmet detection, named the Towards Complex Real-world Safety Factory (TCRSF) dataset. A comparison of the images in TCRSF and those in the existing dataset is shown in Figure1. It can be seen that the proposed TCRSF provides more realistic scenarios and more complex backgrounds. TCRSF contains 50558 labeled instances of 7 categories, including “Safety Helmet”, “Safety Clothing”, “Head”, etc. To simulate the real-world work environment, we added labels for safety clothing and helmets under different lighting conditions and shadow areas, called “blurred head” and “blurred clothing”. Additionally, all images were collected by factory cameras, covering two factories and 40 different scenes. And each instance was manually annotated by professional inspectors from the factory, guaranteeing accurate annotations. It is worth mentioning that, our TCRSF dataset can serve as an evaluation benchmark for various detection tasks, such as small object detection and high-low light object detection. To evaluate the effectiveness of our dataset in a real-world factory environment, we adopt YOLO-v5111YOLO-v5 is a deep learning model for object detection, image segmentation and image classification.as the base model for training, trained it using our dataset. And we deploy the model in the factory and receive streaming data from the camera in real-time to inspect whether workers wear safety clothing and helmets. The TCRSF dataset introduces a novel and robust evaluation benchmark for construction safety inspection, facilitating its adoption in real-world applications. The main contributions of this work are as follows: [(1)] We have contributed a large, realistic, and high-quality dataset named TCRSF. This dataset provides a new and reasonable evaluation benchmark for the community, and we hope that its contribution can promote the development of construction safety inspection. By conducting experiments on existing models, we have validated the completeness of our dataset. Furthermore, the object detection model achieves excellent performance on our dataset when deployed in a real chemical plant environment. We have developed a comprehensive paradigm for real-time detection of workers’ safety clothing and helmets, and provided the full implementation of the application workflow code. To make it easily accessible to non-experts, we have integrated the code such that model training and optimization can be achieved by simply modifying a few parameters in a fixed file. All datasets and codes have been open sourced and are available fromhttps://github.com/sofffty/TCRSF. The rest of this article is organized as follows. In Section2, we review current status of open-source datasets for safety clothing and helmets and recent advances in object detection. In Section3, we present a detailed description of TCRSF. We introduce the selected architectures and experimental procedure to compare the performance in Section4. We discuss the gap between academia and industry and explores future directions, as well as the real-world performance of TCRSF in Section5. Finally, in Section6, we conclude this article.
====================
	 Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs
==========
Abstract:  This paper presents a method to efficiently classify the gastroenterologic section of images derived from Video Capsule Endoscopy (VCE) studies by exploring the combination of a Convolutional Neural Network (CNN) for classification with the time-series analysis properties of a Hidden Markov Model (HMM). It is demonstrated that successive time-series analysis identifies and corrects errors in the CNN output. Our approach achieves an accuracy of $98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for precise localization within the gastrointestinal (GI) tract while requiring only approximately 1M parameters and thus, provides a method suitable for low power devices
Fluency score: 5
Fluency issues: [None]

Clarity score: 4
Clarity issues: [Terms like "Video Capsule Endoscopy (VCE)" and "Hidden Markov Model (HMM)" may not be clear to all audiences]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [Some complex technical terminology might hinder layman's understanding]

Coherence score: 5
Coherence issues: [None]
==========
Introduction:  The capsule endoscopy is a medical procedure that has been used for investigating the midsection of the GI tract since early 2000[12,3]. This minimally invasive method allows to visualize the small intestine, which is in most part not accessible through standard techniques using flexible endoscopes[22]. The procedure starts by swallowing a pill-sized capsule. While it moves through the GI tract by peristalsis, it sends captured images from an integrated camera with either an adaptive or a defined frame rate to an electronic device. The overall aim of this procedure is to detect diseases affecting the small intestine such as tumors and its preliminary stages, angiectasias as well as chronic diseases[22,17,24]. Since the esophagus, stomach and colon can be more easily assessed by standard techniques, the small intestine section is of main interest in VCE studies. All images of the small intestine should be transmitted for further evaluation by medical experts who are qualified to check for anomalies. The frame rate of the most prominent capsules ranges from1to30frames per second with a varying resolution between256\times 256and512\times 512depending on the platform[22]. For example, the PillCam® SB3 by Medtronic lasts up to12hours with an adaptive frame rate of2to6frames per second[18]. This should ensure passing through the whole GI tract before the energy of the capsule’s battery is depleted. However, a capsule can also require more than one day to pass through the whole GI tract leading to an incomplete record of images due to depletion of the capsule’s battery after maximal12hours. In this procedure, the energy is the bottleneck and small changes of the architecture can increase the overall energy requirement leading to a shorter battery lifetime with the risk of running out of energy without covering the small intestine. However, modifications such as capturing images with a higher resolution might improve the recognition ability of clinicians and thus, it is desirable to increase the limited resolution or add more functions (e.g. zooming in or out, anomaly detection on-site) helping to successfully scan the GI tract for anomalies at the cost of increasing energy demands. The images taken before the small intestine are not of interest but demand their share of energy for capturing and transmitting the images. This paper presents a method for very accurately determining the location of the capsule by on-site evaluation using a combination of neural network classification and time-series analysis by a HMM. This neglects the necessity to consume electric energy for transmitting images of no interest. If this approach is integrated into the capsule it can perform precise self-localization and the transition from the stomach to the small intestine is verified with high confidence. From this moment onwards, all frames should be send out for further evaluation. A major part of the energy can be saved since the data transmission only starts after the capsule enters the small intestine and therefore can be used for other valuable tasks. For example, the frame rate or resolution could be increased while in the small intestine or additionally, a more complex network for detecting anomalies on-site could be employed. In the field of gastroenterology, there have been different approaches to perform localization of a capsule within the GI tract[16]including but not limited to magnetic tracking[19,26], video-based[28,15]and electromagnetic wave techniques[27,7]. However, Charoen et al[2]were the first to publish a dataset with millions of images classified into the different sections of the GI tract. They achieved an accuracy of97.1\%with an Inception ResNet V2[23]architecture on the RI dataset and therefore successfully demonstrated precise localization without aiming for an efficient realization on hardware. To the best of our knowledge, there is no superior result than the baseline with this dataset. However, a large network with56M parameters as the Inception ResNet V2 is not suitable for low-power embedded systems since the accompanied high energy demand results in a short battery lifetime. Thus, we present a new approach for this problem setting using the same dataset and the same split resulting in a higher accuracy while requiring a much smaller network and less parameters.
====================
	 Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation
==========
Abstract:  Case-based reasoning is known to play an important role in several legal settings. In this paper we focus on a recent approach to case-based reasoning, supported by an instantiation of abstract argumentation whereby arguments represent cases and attack between arguments results from outcome disagreement between cases and a notion of relevance. In this context, relevance is connected to a form of specificity among cases. We explore how relevance can be learnt automatically in practice with the help of decision trees, and explore the combination of case-based reasoning with abstract argumentation (AA-CBR) and learning of case relevance for prediction in legal settings. Specifically, we show that, for two legal datasets, AA-CBR and decision-tree-based learning of case relevance perform competitively in comparison with decision trees. We also show that AA-CBR with decision-tree-based learning of case relevance results in a more compact representation than their decision tree counterparts, which could be beneficial for obtaining cognitively tractable explanations.
Fluency score: 4
Fluency issues: Overall good, small adjustments for natural flow might be necessary.

Clarity score: 3
Clarity issues: Complex sentences and jargon may confuse laymen.

Grammaticality score: 5
Grammaticality issues: None detected.

Readability score: 3
Readability issues: Terms like "abstract argumentation," "outcome disagreement," and "cognitively tractable" reduce readability for a general audience

Coherence score: 4
Coherence issues: The text contains some jargon and complex concepts that may impact overall coherence for those unfamiliar with the topic.
==========
Introduction:  Case-based reasoning (CBR) is a methodology in which concrete past occasions are directly used as sources of knowledge and solutions for new situations(DBLP:books/daglib/0032926). It has been studied in AI and Law since its inception, leading to foundational contributions(Rissland2005CasebasedRA). This is a not a surprise, given the centrality of the use of cases for determining the law in Common Law systems, although not exclusivelylewis2021. In this paper we focus on recent approaches to CBR(DBLP:conf/kr/CyrasST16;dear-2020;DBLP:conf/kr/Paulino-PassosT21;Prakken2022ATM)using argumentation(prakken-overview). Argumentation itself has a long history in AI and Law, and its use to support CBR has been shown to pave the way towards novel forms of explanations for the outcomes of CBR, including via arbitrated dispute trees(DBLP:journals/eswa/CyrasBGTDTGH19;DBLP:conf/ijcai/Cyras0ABT21). Specifically, we focus on the\aacbrapproach(DBLP:conf/kr/CyrasST16;dear-2020;DBLP:conf/kr/Paulino-PassosT21), where arguments correspond to cases and attacks between arguments result from outcome disagreement between cases andrelevancebetween cases, guided by a partial order over cases capturing some notion of specificity. Originally(DBLP:conf/kr/CyrasST16),\aacbrexpects a representation of cases in terms of sets ofmanually engineered binaryfeatures and the partial order is defined via the subset relation. This expectation is a restriction for applicability. While previous work has generalised beyond binary features in order to support different applicationsdear-2020, a systematic generalisation to tabular datasets, including categorical and continuous data, is still missing. This is essential for applying\aacbrtorealisticdatasets, including legal ones, to realise the original inspiration from legal reasoning for\aacbr. While some form of binarisation can be applied, there is no guarantee that a naïve binarisation would result in good performance. In this work we close this gap, focusing on applying\aacbrto possibly non-binary tabular data from legal settings. Specifically, our first contribution is a general method for applying\aacbrto any tabular data by extracting binary features from decision treesDBLP:books/wa/BreimanFOS84when learning for the final task. Our second contribution is showing that this method is competitive with decision trees on two legal datasets: COMPASpropublica-compasand a simulated legal datasetDBLP:journals/argcom/StegingRVB23for welfare benefit. Finally, as a third contribution, we show thatauthor=Guilherme, caption=,author=Guilherme, caption=,todo:author=Guilherme, caption=,revise according to actual final resultsour method creates smaller models (i.e. with a smaller number of nodes), leading to potentially more cognitively tractable explanations (i.e. decision trees and rules drawn from them on one hand, and argumentation frameworks and arbitrated dispute trees on the other). \timesExplainability is a fundamental requirement in AI and Law. In particular, any model of legal reasoning must be amenable to explanation, which is required not only of AI systems, but of legal decision-makers in general. Current state of the art methods in many AI tasks is data-driven methodologies (machine learning) whether machine learning-base …\aacbrhas been developed …(DBLP:conf/kr/CyrasST16;dear-2020).. Important aspects for any concrete implementation of\aacbrare the representation of the data (to which we refer as characterisation) and, crucially, the partial order\pleqon the set of characterisations, which until now we have assumed given. This partial order captures the idea of relevance of past cases for a new case, as well as of specificity among past cases. In this paper, we will discuss ways of defining this partial order. In particular, we are interested in learning the partial order from data. Learning the partial order allows\aacbrto a wider scope of problems, instead of requiring a hand-constructed order. This carries three desiderata: 1) that the learned partial order results in a good performance; 2) that it is human interpretable. We here present a method for learning a partial order from tabular data using features extracted from decision trees trained on this same data. . We show empirical results viaexperiments are carried out on the the COMPAS dataset(propublica-compas), a dataset on prediction of recidivism, that is, on prediction of whether a criminal will re-offend(2016COMPASRS), as well as on the simulated legal data of…
====================
	 High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft
==========
Abstract:  Recently, there has been a growing interest in mixed-categorical metamodels based on Gaussian Process (GP) for Bayesian optimization. In this context, different approaches can be used to build the mixed-categorical GP. Many of these approaches involve a high number of hyperparameters; in fact, the more general and precise the strategy used to build the GP, the greater the number of hyperparameters to estimate. This paper introduces an innovative dimension reduction algorithm that relies on partial least squares regression to reduce the number of hyperparameters used to build a mixed-variable GP. Our goal is to generalize classical dimension reduction techniques commonly used within GP (for continuous inputs) to handle mixed-categorical inputs. The good potential of the proposed method is demonstrated in both structural and multidisciplinary application contexts. The targeted applications include the analysis of a cantilever beam as well as the optimization of a green aircraft, resulting in a significant 439-kilogram reduction in fuel consumption during a single mission.
Fluency score: 4
Fluency issues: [n/a]

Clarity score: 3
Clarity issues: ["resulting in a significant 439-kilogram reduction in fuel consumption during a single mission" - the connection to the previous text can seem slightly abrupt.]

Grammaticality score: 5
Grammaticality issues: [n/a]

Readability score: 3
Readability issues: [Complex sentences and use of jargon may hinder understanding for a general audience.]

Coherence score: 4
Coherence issues: [The transition from general method discussion to specific applications could be smoother.]
==========
Introduction:  Costly black-box simulations play an important role for many engineering and industrial applications. For this reason, surrogate modeling has been extensively used across a wide range of use cases, including aircraft designSciTech_cat, deep neural networkssnoek2015scalable, coastal flooding predictionlopez, agricultural forecastingMLP, and seismic imagingYDiouane_SGratton_XVasseur_LNVicente_HCalandra_2016. These black-box simulations are generally complex and may involve mixed-categorical input variables. For instance, a Multidisciplinary Design Analysis (MDA) aircraft design toolDavid_2021must consider mixed variables such as the number of engines or the list of possible materialsSciTech_cat. In this paper, our objective is to develop an affordable surrogate model, denoted as\hat{f}, for a black-box function that involves mixed variables given by This functionfrepresents a computationally expensive black-box simulation.\Omega\subset\mathbb{R}^{n}denotes the bounded continuous design set for thencontinuous variables.S\subset\mathbb{Z}^{m}denotes the bounded integer set whereL_{1},\ldots,L_{m}are the numbers of levels of themquantitative integer variables on which we can define an order relation and\mathbb{F}^{l}=\{1,\ldots,L_{1}\}\times\{1,\ldots,L_{2}\}\times\ldots\times\{1% ,\ldots,L_{l}\}is the design space for thelcategorical qualitative variables with their respectiveL_{1},\ldots,L_{l}levels. For such purpose, the use of a Gaussian Process (GP)williams2006gaussian, also called Kriging modelkrige1951statistical, is recognized as an effective modeling approach for constructing a response surface model based on an available dataset. Specifically, we make the assumption that our unknown black-box function, denoted asf, follows a GP with mean\mu^{f}and standard deviation\sigma^{f}, expressed as follows: Several modeling approaches have been put forward for addressing the challenges of handling categorical or integer variables within the context of GPPelamatti;Zhou;Deng;Roustant;GMHL;Gower;cuesta2021comparison;SciTech_cat. In comparison to GP designed for continuous variables, the most important changes concern the estimation of the correlation matrix, an essential element in the derivation of\mu^{f}and\sigma^{f}. Much like the procedure for constructing a GP with continuous inputs, Continuous Relaxation (CR) techniquesGMHL;SciTech_cat, models involving continuous latent variablescuesta2021comparison, and Gower Distance (GD) based modelsGoweruse a kernel-based approach for estimating this correlation matrix. However, recent innovative approaches take a different path by modeling directly the various entries of the correlation matrixPelamatti;Zhou;Deng;Roustant, and therefore, do not rely on any kernel choice, such methods involve the Homoscedastic Hypersphere (HH)Zhouand the Exponential HH (EHH)Mixed_Paulkernels. It has been shown inMixed_Paulthat the HH correlation kernel generalizes simpler methods like CR or GD kernels. However, this more general method for handling categorical design variables increases the number of hyperparameters required to be tuned associated with the GP model. In particular, this means that the method could only be used for small dimensional problems. Many efficient approaches have been proposed for handling a high number of continuous variables within GPBouhlel18;bouhlel_KPLSK;bouhlel2019gradient. The Kriging with Partial Least Squares (KPLS) methodBouhlel18;bouhlel_KPLSKis one of the most commonly used reduction techniquesKPLSu1;KPLSu2to tackle high dimensional data. Several other dimension reduction methods include principal components analysiswang2017batched, polynomial chaos expansionzuhal2021dimensionality, radial basis functionsregis2020survey, active subspaceAS, manifold embeddingtenenbaum2000globalor marginal Gaussian processMGP. The KPLS technique allows constructing the GP model with the same continuous variables but using a few number of parameters; which reduces significantly the computational cost of computing a GP model. For mixed-categorical GP models, given that the computational effort related to the construction of the GP model may not scale well to practical applications involving categorical variables with a large number of levels, the number of hyperparameters to be tuned need to be considered more thoroughly. In the literature, GPs have been applied to no more than 15 hyperparameters due to the high computational cost associated with the estimation of the hyperparametersGP14. Adapting dimension reduction techniques, such as KPLS, to mixed-categorical GPs will thus enable solving practical mixed-categorical engineering problems where often a high number of hyperparameters is required to be estimated. To the best of our knowledge, there is no equivalent approach to handle mixed-categorical data without using relaxation techniques. All existing dimension reduction techniques, including KPLS, are not adapted for advanced mixed-categorical GP models such as HH or EHH. We note also that, although this paper focuses mainly on surrogate modeling, the proposed models can be integrated within any surrogate-based optimization methodAuHa2017, such as surrogate-based evolutionary algorithmsbea1;sbea2or a Bayesian Optimization (BO) methodJones2001JOGO. In this work, we target to use dimension reduction techniques for reducing the number of hyperparameters within the GP in order to allow modeling efficiently high-dimension mixed-categorical data. In this context, high dimensionality is related to the high number of categorical variables potentially with a high number of levels (a few dozen). In fact, using relaxation approaches (by converting categorical choices to continuous variables) leads to a very high number of hyperparameters to estimate, particularly for high resolution approaches such as those based on HH and EHH kernels. We have also specifically used our proposed mixed-categorical GP models, within a BO framework, to solve a constrained optimization problem involving expensive-to-compute black-box simulations for objective and constraints functionsMartins2021. The proposed approach is shown in particular to be efficient in solving a high dimensional mixed-categorical Multidisciplinary Design Optimization (MDO) problemLambe2012. All the GP models proposed in this work are implemented in the open-source Surrogate Modeling Toolbox (SMT)111https://smt.readthedocs.io/en/latest/saves2023smt. The remainder of this paper is the following. In Section2, a detailed review of the GP model for continuous and for categorical inputs is given. In Section3, we present the PLS regression for vectors and matrices and their application to GP model for both continuous and categorical inputs. Section4presents academical tests as well as the obtained results on multidisciplinary optimization. Conclusions and perspectives are finally drawn in Section5. Notations:For a vectorx, both notations[x]_{j}andx_{j}stand for thej^{th}component ofx. Similarly, thei(row index) andj(column index) entry of a matrixXis denoted[X]^{j}_{i}.
====================
	 SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection
==========
Abstract:  Among the current mainstream change detection networks, transformer is deficient in the ability to capture accurate low-level details, while convolutional neural network (CNN) is wanting in the capacity to understand global information and establish remote spatial relationships. Meanwhile, both of the widely used early fusion and late fusion frameworks are not able to well learn complete change features. Therefore, based on swin transformer V2 (Swin V2) and VGG16, we propose an end-to-end compounded dense network SwinV2DNet to inherit the advantages of both transformer and CNN and overcome the shortcomings of existing networks in feature learning. Firstly, it captures the change relationship features through the densely connected Swin V2 backbone, and provides the low-level pre-changed and post-changed features through a CNN branch. Based on these three change features, we accomplish accurate change detection results. Secondly, combined with transformer and CNN, we propose mixed feature pyramid (MFP) which provides inter-layer interaction information and intra-layer multi-scale information for complete feature learning. MFP is a plug and play module which is experimentally proven to be also effective in other change detection networks. Further more, we impose a self-supervision strategy to guide a new CNN branch, which solves the untrainable problem of the CNN branch and provides the semantic change information for the features of encoder. The state-of-the-art (SOTA) change detection scores and fine-grained change maps were obtained compared with other advanced methods on four commonly used public remote sensing datasets. The code is available at https://github.com/DalongZ/SwinV2DNet.
Fluency score: 4
Fluency issues: [The passage is generally fluent but could be improved with more natural transitions between sentences.]

Clarity score: 3
Clarity issues: [Terms such as "remote spatial relationships," "compounded dense network SwinV2DNet," and "mixed feature pyramid (MFP)" may be unclear to the lay reader; Dense use of specialized language.]

Grammaticality score: 4
Grammaticality issues: [Minor issues, such as "Further more" should be "Furthermore."]

Readability score: 3
Readability issues: [The text is dense with technical terms and concepts that could hinder readability for a general audience.]

Coherence score: 4
Coherence issues: [The text is coherent but contains jargon that could confuse readers not familiar with the subject matter.]
==========
Introduction:  Remote sensing images change detection is one of the earliest and most important remote sensing tasks, which has been concerned and studied by many researchers for a long time[27,61,4,2]. Change detection is defined as observing the image differences of the same surface area at different times. Change detection is used in many scenarios, including disaster assessment[37], urban planning, land surface change[29,68], and so on. With the development of satellites and sensors, very high resolution remote sensing (VHRRS) images have gradually become one of the mainstream remote sensing images in research, which provide rich spatial information and fine surface details. However, one of the main challenges faced by VHRRS images change detection is high intraclass variation and low interclass variance of detection objects[38]. It, therefore, has been the focus of scholars’ research that how to design a stable network and provide comprehensive and diverse feature information to distinguish the pseudo changes in change detection (as shown in Fig.1). Traditional change detection algorithms, according to different detection units, can be divided into pixel-based algorithms and object-based algorithms. The detection results of pixel-based algorithms are obtained through feature extraction and then threshold segmentation, which include methods based on arithmetic operations (band difference[34], spectral angle mapper[69]), methods based on transformation (change vector analysis (CVA)[39,3], principal component analysis (PCA)[60], independent component analysis (ICA)[67]), post-classification change detection[54], slow feature analysis (SFA)[53]and so on. According to the shape, texture and spectrum of images, object-based algorithms need to segment the images and then compare the classification results to get the change detection results[20]. Pixel-based algorithms are trapped by the interference of small noises and the decision of segmentation threshold. Meanwhile object-based algorithms often get stuck in the accumulation of multiple classification errors that affect the detection accuracy[27]. Both of these traditional algorithms require prior knowledge and manual design, and are easily affected by sensor noises. With the support of massive remote sensing data, deep learning has also shown outstanding detection ability in the field of remote sensing. CNN converts the input images into the high-dimensional depth features, and combines the targets and background to extract effective semantic information, achieving the detection effect beyond many traditional methods.[11]provides the three most common baseline networks for change detection. The architecture combined with CNN and conditional random field (CRF) refines the edges of detection areas, but it comes at the cost of slow training[66]. CNN is hindered regrettably by the narrow receptive field of local information, and transformer rises rapidly due to the ability of modeling global information. However, it can not work well that the pure transformer change detection model lacks low-level details[58]. Therefore, how to combine transformer and CNN to build the reasonable change detection architecture is the crux of the matter at this stage. From another perspective, change detection network architectures can be divided into early fusion (EF)[11,1,41,42]and late fusion (LF)[11,56,57,6,46,14,62,15]networks. The EF network works by stitching two images together and feeding them into the single input network. By concatenating two three-channel images into a six-channel image,[1]and[41]input it into full convolutional neural network (FCN) and UNet++ respectively, and output change map after training the network. The disadvantage of this method is that the network lacks the depth features of single images, resulting in fractured edges and broken structures in change map. In the LF network, the two features are extracted from the pre-changed and post-changed images respectively by using the dual-input structure, and are fused in the second half of the network. The siamese network, the most prominent LF network, consists of two subnets with shared weights. The siamese network was first used for remote sensing images change detection in[56]. The use of convolutional block attention module (CBAM) and deep supervision for the siamese network respectively alleviates the problem of heterogeneous features fusion and depth features migration in training process[57]. However, for the LF network, the contradiction between the dual-stream input of encoder and the single output of decoder often results in the disappearance of gradient propagation and affects the low-level features learning of two original images. Furthermore, the heterogeneous features fusion of LF network needs to be solved by elaborately designed module. As a consequence, it is another problem worth pondering that how to overcome the respective disadvantages of these two network architectures and provide the complete and diverse features for change detection. In addition to the design of the overall network architectures for change detection, researchers are also pushing forward the elaboration of network functional modules. The attention modules introduced into change detection are relatively representative, including squeeze-and-excitation attention (SE)[26], efficient channel attention (ECA)[51], CBAM[52]and cross-attention[62]. At the same time, since the ground objects have different scales in the VHRRS change detection datasets, how to adapt these ground objects to maintain the robustness and generalization ability of the network? Multi-scale features of deep learning generally can be divided into three categories: multi-scale features between different layers, multi-scale interaction features between different layers and multi-scale features from different convolution units. The first type of multi-scale features was embedded in the common U-Net network. The second type of multi-scale features typically interacts and fuses using transformer or CNN. The third type of multi-scale features is provided by a variety of convolution units, such as inception[49], dilated convolution[55], res2net convolution (Res2Net-Conv)[19], selective kernel convolution (SK-Conv)[33]and so on. We should think about the integration and utilization of these three multi-scale features. Other scholars also think in the combination with generative adversarial network (GAN)[25,65]or self-supervised learning[8,5,64]to obtain more discriminative features. These deep learning technologies are aimed at solving the problem of high intraclass variation and low interclass variance by mining the different features of change detection data. Motivated by the above concerns, this study combines Swin V2 and VGG16 to propose a new end-to-end compounded dense network SwinV2DNet. Swin V2 blocks are used to build the UNet++ type main network, and VGG16 encoder is used to build the CNN auxiliary network. SwinV2DNet overcomes the modeling defect of only local information in CNN and the insufficient interpretation of low-level details in transformer. On the other hand, the Swin V2 main network belongs to EF network, and the CNN branch belongs to LF. This structure constantly provides the pre-changed features, post-changed features and change relation features (namely, the six-channel concatenation from the pre-changed and post-changed images) for the accurate acquisition of change detection results. CBAM and deep supervision also promote the fusion of heterogeneous features and the rapidly stable convergence of the network, respectively. To better combine transformer and CNN, we propose a new multi-scale module, mixed feature pyramid, which provides inter-layer multi-scale interaction information and intra-layer multi-scale information to supplement the UNet++ main network only with inter-layer multi-scale information. We finally design a new decoder to the CNN branch with only VGG16 encoder, and use the self-supervision strategy to train the extracted features, so that the CNN branch can provide learnable and more discriminant semantic information. To sum up, the main contributions of this study are fourfold: We propose an end-to-end compounded dense network SwinV2DNet that possesses both advantages of transformer and CNN, and overcomes respective disadvantages of the EF and LF network. This is the first parallel combination of Swin V2 and VGG16 in change detection. Mixed feature pyramid is proposed, for the first time, to provide inter-layer interaction information and intra-layer multi-scale information. It is a plug and play module that has been experimentally proven to be also effective in other change detection networks. We design a new decoder for the CNN branch with only VGG16 encoder, and impose the self-supervised strategy to train the extracted features to provide more discriminative semantic information for the main network. Compared with other advanced methods, our method obtains the state-of-the-art (SOTA) change detection scores and the elaborate change maps on four common public remote sensing datasets. The remainder of this paper is organized as follows. SectionIIreviews the related work. SectionIIIelaborates the proposed SwinV2DNet method. The experimental evaluations and ablation studies are carried out in SectionIV. Finally, SectionVpresents the conclusion of this article.
====================
	 Questions of science: chatting with ChatGPT about complex systems
==========
Abstract:  We present an overview of the complex systems field using ChatGPT as a representation of the community's understanding. ChatGPT has learned language patterns and styles from a large dataset of internet texts, allowing it to provide answers that reflect common opinions, ideas, and language patterns found in the community. Our exploration covers both teaching and learning, and research topics. We recognize the value of ChatGPT as a source for the community's ideas.
Fluency score: 5
Fluency issues: [None]

Clarity score: 4
Clarity issues: [Unclear what the "complex systems field" specifically refers to]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [The sentence structure is somewhat complex, which may hinder quick comprehension]

Coherence score: 4
Coherence issues: [The connection between the “complex systems field” and ChatGPT’s role within it is not fully clear]
==========
Introduction:  ChatGPT and other transformers-based large language models (LLMs) have demonstrated remarkable abilities to perform tasks such as completion tasks, question answering, reading comprehension and translation that were once thought to be exclusive to humans(devlin2018bert,;zhang2020pegasus,;raffel2019exploring,;fedus2021switch,;lewis2019bart,;radford2018improving,;radford2019language,;brown2020language,). We are currently in a great era for researchers and scientists studying and developing in the field of complex systems. Half of the physics Nobel prize of 2021 was awarded to the physicist Giorgio Parisi for his contributions to the theory of complex systems(Cugliandolo_2023,)and the other half to two meteorologists Syukuro Manabe and Klaus Hasselmann to the modeling of the Earth’s climate(gupta2022perspectives,). Parisi has made significant contributions to the literature on complex systems, including areas such as spin glass(Parisi1979,;Parisi1980,;Parisi_1980order,), stochastic resonance(Benzi1982,), surface growth(Kardar1986,), multifractality(frisch1985fully,), and bird flocking(Ballerini2008,). Manabe showed that there is a relation between the increase in global temperature and the level of CO{{}_{2}}in the atmosphere(manabe1967thermal,). Hasselmann developed a model that establishes a connection between weather and climate, implying that climate models can be reliable despite the unpredictability and complexity of weather patterns(hasselmann1976stochastic,;frankignoul1977stochastic,). The diversity of research topics among the award recipients highlights the multidisciplinary nature of the field. In addition, the applied dimension of the contributions stresses the importance of applied contributions to the field. In particular, it is worth mentioning that the research of field of complex systems has developed multiple frameworks, such as chaos theory(lorenz1963deterministic,), fractal theory(mandelbrot1982fractal,), complex networks(barabasi1999emergence,;watts1998collective,), and agent-based models(bonabeau2002agent,), to model and solve important practical problems in diverse fields. OpenAI has just now introduced the ChatGPT, a large language model tool trained, to aid humans in a variate of human tasks including question answering, text edition and has been used as a great information provider(brown2020language,;ouyang2022training,). ChatGPT has been trained on a large dataset of internet texts, which means that it has learned to mimic the patterns and styles of language used by many different people online. When you talk to ChatGPT, it responds based on what it has learned from that dataset, so its answers can be thought of as representing the average opinions, ideas, and language patterns that are commonly found on the internet. In this paper, we overview different aspects of the field of complex systems by means of chats with ChatGPT. We split this paper into two parts. The first part explores issues related to teaching and learning, where ChatGPT may work as a teacher assistant. The second part explores issues related to research, where, in this case, ChatGPT works like a research assistant. Besides exploring different concepts from different points of view in the field of complex systems, we also recognize that ChatGPT can be a valuable resource for teachers and scientists.
====================
	 Part to Whole: Collaborative Prompting for Surgical Instrument Segmentation
==========
Abstract:  Foundation models like the Segment Anything Model (SAM) have demonstrated promise in generic object segmentation. However, directly applying SAM to surgical instrument segmentation presents key challenges. First, SAM relies on per-frame point-or-box prompts which complicate surgeon-computer interaction. Also, SAM yields suboptimal performance on segmenting surgical instruments, owing to insufficient surgical data in its pre-training as well as the complex structure and fine-grained details of various surgical instruments. To address these challenges, in this paper, we investigate text promptable surgical instrument segmentation and propose SP-SAM (SurgicalPart-SAM), a novel efficient-tuning approach that integrates surgical instrument structure knowledge with the generic segmentation knowledge of SAM. Specifically, we achieve this by proposing (1) collaborative prompts in the text form "[part name] of [instrument category name]" that decompose instruments into fine-grained parts; (2) a Cross-Modal Prompt Encoder that encodes text prompts jointly with visual embeddings into discriminative part-level representations; and (3) a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy that selectively assemble the part-level representations into a whole for accurate instrument segmentation. Built upon them, SP-SAM acquires a better capability to comprehend surgical instrument structures and distinguish between various categories. Extensive experiments on both the EndoVis2018 and EndoVis2017 datasets demonstrate SP-SAM's state-of-the-art performance with minimal tunable parameters. Code is at https://github.com/wenxi-yue/SurgicalPart-SAM.
Fluency score: 4
Fluency issues: Overall fluent but contains complex sentences that might challenge some readers.

Clarity score: 4
Clarity issues: Technical terms without explanation could confuse non-specialist readers.

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: Dense with jargon and lengthy, complex sentences that could impede understanding for a general audience.

Coherence score: 4
Coherence issues: While cohesive, the abundance of specialized terminology may affect the text's coherence for readers unfamiliar with the subject.
==========
Introduction:  Surgical instrument segmentation aims to accurately identify and delineate surgical instruments in operative scenes. It plays a foundational role for many downstream applications, such as surgical planning[7], robotic navigation[43], and skill assessment[24]. We identify two primary problems with the existing methods for this task. First, they often develop specialist models that require full training of all parameters (Fig.LABEL:subfig:motiv2-(i)), leading to high development costs. Second, current methods lack the capability of human-computer interaction that is highly desired in surgical practice[6,4]. The Segment Anything Model (SAM)[22]is a pioneering foundation model for promptable segmentation. It holds great potential for addressing the above problems owing to its rich pre-trained knowledge and interactivity. However, there still exist some fundamental challenges that impede the application of SAM in surgical instrument segmentation. Firstly, SAM’s reliance on point-or-box prompts is impractical in surgical settings, where it is infeasible for surgeons to provide such prompts for every instrument in each frame. Instead, more flexible and efficient prompts like free-form text would be preferred (e.g., via speech recognition and voice control). Secondly, SAM has shown subpar zero-shot generalisation in segmenting surgical instruments[42]due to insufficient surgical data in pre-training as well as the complex structure and fine-grained nature of surgical instruments across various categories. In this paper, we investigate text promptable surgical instrument segmentation and propose a novel framework, SP-SAM (SurgicalPart-SAM), that addresses the above challenges by leveraging SAM’s generic segmentation capability while integrating instrument structure information via an efficient model tuning scheme (Fig.LABEL:subfig:motiv2-(iii)). For text-prompted segmentation, a common practice is to use the object names to prompt the segmentation of an object of interest. However, the category names of instruments are not ideal for use as text prompts due to their specialised and abstract nature (Fig.LABEL:subfig:motiv1and Fig.LABEL:subfig:motiv2-(ii)). To this end, we introduce a new form of text prompt, namely Collaborative Prompt, using a text description set: {\say[part name]of[instrument category name]} for all parts of an instrument category. It combines both category and part information, effectively decomposing the specialised, abstract, and coarse concepts of instrument categories into more precise and fine-grained instrument parts, which is more appropriate for surgical instruments of high structure complexity (Fig.LABEL:subfig:motiv1and Fig.LABEL:subfig:motiv2-(iii)). To encode these collaborative prompts, we introduce a Cross-Modal Prompt Encoder, transforming collaborative prompts into part-level sparse and dense prompt embeddings for SAM’s mask decoder through visual-textual interaction. This enables focused learning of fine-grained representations for each instrument part, enhancing segmentation of subtle details. Finally, to integrate the knowledge of instrument structure into the model, a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy are proposed. Specifically, we adaptively assemble the learned part-level sparse and dense embeddings considering both category-specific and image-specific part weights, to predict the mask of the whole instrument. Afterwards, hierarchical decoding of both the whole instrument and its parts is performed to enhance the understanding of surgical instruments both holistically and at the part level. To this end, SP-SAM has strong capability to comprehend surgical instrument structures, identify subtle details, and discriminate between fine-grained categories. In summary, our contributions are three-fold: We introduce a novel approach SP-SAM for text promptable surgical instrument segmentation, offering a simpler prompting pipeline than the vanilla SAM that better suits the demands of surgical practices. SP-SAM achieves state-of-the-art (SOTA) performance on the challenging EndoVis2018 and EndoVis2017 datasets while requiring only a small number of training parameters. We introduce a new type of text prompt integrated with both instrument category and part information, namely collaborative prompts, and devise a Cross-Modal Prompt Encoder, which encodes text prompts jointly with visual embeddings into discriminative part-level representations, ensuring accurate segmentation of subtle details. We propose a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy to adaptively assemble the part-level representations into a whole and make predictions at both the part-level and the holistic-level, explicitly incorporating structure information regarding category-part relations and achieving better discrimination between categories.
====================
	 D2Match: Leveraging Deep Learning and Degeneracy for Subgraph Matching
==========
Abstract:  Subgraph matching is a fundamental building block for graph-based applications and is challenging due to its high-order combinatorial nature. Existing studies usually tackle it by combinatorial optimization or learning-based methods. However, they suffer from exponential computational costs or searching the matching without theoretical guarantees. In this paper, we develop D2Match by leveraging the efficiency of Deep learning and Degeneracy for subgraph matching. More specifically, we first prove that subgraph matching can degenerate to subtree matching, and subsequently is equivalent to finding a perfect matching on a bipartite graph. We can then yield an implementation of linear time complexity by the built-in tree-structured aggregation mechanism on graph neural networks. Moreover, circle structures and node attributes can be easily incorporated in D2Match to boost the matching performance. Finally, we conduct extensive experiments to show the superior performance of our D2Match and confirm that our D2Match indeed exploits the subtrees and differs from existing GNNs-based subgraph matching methods that depend on memorizing the data distribution divergence
Fluency score: 4
Fluency issues: [The sentence construction is somewhat complex and may be difficult for lay readers]

Clarity score: 3
Clarity issues: [Use of technical jargon like "subtree matching," "bipartite graph," "tree-structured aggregation mechanism" may be unclear to the general audience; also, "depended on memorizing the data distribution divergence" is a complex concept]

Grammaticality score: 5
Grammaticality issues: [None detected]

Readability score: 3
Readability issues: [Complex sentence structure and domain-specific language make the text challenging for a general audience]

Coherence score: 4
Coherence issues: [While the flow of ideas is logical for an expert reader, the high density of technical terms could disrupt the coherence for others]
==========
Introduction:  Subgraph isomorphism, or subgraph matching at the node level(McCreeshet al.,2018), is a critical yet particularly challenging graph-related task. It aims to determine whether a query graph is isomorphic to a subgraph of a large target graph. It is an essential building block for various graph-based scenarios, e.g., alignment of cross-domain data(Chenet al.,2020), temporal alignment of time series(Zhou and Torre,2009), and motif matching(Miloet al.,2002; Penget al.,2020), etc. Existing studies for subgraph matching can be divided into two main streams: combinatorial optimization (CO)-based and learning-based methods(Vesselinovaet al.,2020). Early algorithms often formulate subgraph matching as a CO problem that aims to find all exact matches in a target graph. Unfortunately, this leads to an NP-complete issue(Ullmann,1976; Cordellaet al.,2004), which suffers from exponential time costs. To alleviate the computational cost, researchers have employed approximate techniques to seek inexact solutions(Mongiovìet al.,2010; Yanet al.,2005; Shanget al.,2008), which yield suboptimal matchings. An alternative solution is to frame subgraph matching as a supervised learning problem(Baiet al.,2019; Rexet al.,2020; Baiet al.,2020), which utilizes the Graph Neural Networks (GNNs). However, the learning-based or GNN-based methods mainly aim to optimize the representations in a black-box way. The lack of theoretical guarantees makes them inexplicable and often cannot seek the exact match subgraphs. In order to tackle the above challenges, we propose a white-box GNN-based solution,D^{2}Match, to leverage the efficiency of Deep GNNs and Degeneracy for subgraph matching. With rigorous theoretical proofs, we provide explainable results at each learning step. We first prove that finding the matched nodes between the query graph and the target one can degenerate to check whether the corresponding subtrees rooted at these two nodes are subtree isomorphic. This degeneration allows us to check whether a perfect matching exists on a bipartite graph, which results in a polynomial time complexity solution. The above two steps convert subgraph matching into computing an indicator matrix whose elements represent the subtree isomorphic relationship between nodes in the query graph and the target one. Hence, the matching matrix required by the CO-based methods for subgraph matching degenerates to seeking an indicator matrix, which is computed by GNNs via its intrinsic tree-structured aggregation mechanism. Note that this implementation battles the matching mechanism directly by GNNs rather than optimizing the representations of GNNs as in the existing work. Our implementation allows us to reduce the time cost of perfect matching from polynomial time to linear time. Moreover, we can easily incorporate other information, including circle structures (abstracted assupernodes) and node attributes, into ourD^{2}Matchto boost the matching performance. Our contribution is four-fold: (1) We propose the first white-box GNN-based model, calledD^{2}Match, to leverage deep learning and degeneracy for subgraph matching. We provide rigorous theoretical proofs to guarantee that subgraph matching can degenerate to subtree matching, and finally perfect matching on a bipartite graph. (2) To the best of our knowledge, this is the first GNN-based model to tackle subgraph matching directly, which degenerates the matching matrix required by the CO-based methods to an indicator matrix computed by GNNs via the intrinsic tree-structured aggregation mechanism. This allows us to compute the indicator matrix in linear time. (3) OurD^{2}Matchcan easily include other information, including circle structures and node attributes to boost the model performance. (4) Extensive empirical evaluations show thatD^{2}Matchoutperforms state-of-the-art subgraph matching methods by a substantial margin, and uncovered that learning-based methods tend to capture the divergence of the data distribution rather than exploiting graph structures.
====================
	 Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning
==========
Abstract:  Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief functions, and experimentally demonstrate how our approach outperforms competing uncertainty-aware approaches in a classical evaluation setting. The performance of RS-CNN is best demonstrated on OOD samples where it manages to capture the true prediction while standard CNNs fail.
Fluency score: 4
Fluency issues: ['to know when a model does not know']

Clarity score: 3
Clarity issues: ['using the mathematics of random sets, i.e., distributions over the power set of the sample space', 'We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief functions']

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [Complex sentence structure and jargon-heavy content make the text less accessible for a general audience, e.g., 'random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets.']

Coherence score: 4
Coherence issues: [The paper's argument is reasonably coherent, but the use of technical language without clear explanations may lose some readers, especially in 'We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief functions.']
==========
Introduction:  Despite its recent rise in popularity, in its current form, artificial intelligence (AI) cannot confidently make predictions robust enough to stand the test of data generated by processes different from those studied at training time, even by tiny details, as shown by ‘adversarial’ results able to fool deep neural networkspapernot2016limitations. While recognising this issue under different names (e.g.overfittingroelofs2019metaormodel adaptationli2020model), traditional machine learning seems unable to address it at a fundamental level. As a result, AI systems suffer from brittle behaviour, and find it hard to operate in new situations. A major cause of this problem is the widespread presence ofepistemic uncertainty, i.e., uncertainty about the process generating the data itself. In machine learning, this derives from the limited representativeness of the available training sets, in terms of both quantity and quality. As few samples are collected in specific domain distributions, the learnt models have no way to effectively model domain variation. The machine learning community recognises the problem. This has given rise to several proposals for estimating the uncertainty associated with network predictions, including Bayesian Dropoutgal2016dropout, distance-aware priorsgianand the use of random fuzzy setsDENOEUX2022. An “evidential" deep learning methodsensoyfor uncertainty quantification in classification using Dirichlet distribution and, more recently, epistemic neural networksosbandas a generalisation of Bayesian neural networks have been proposed. None of these methods, however, fully, explicitly capture the epistemic uncertainty stemming from the data issue. Some of them rely on prior knowledgehttps://doi.org/10.48550/arxiv.2105.06868, whereas others require setting a desired threshold on the metricsDBLP:journals/corr/abs-2107-07511. It has been argued by some that classical probability theory is, in fact, not equipped to model “second-level" uncertainty on the probabilities themselves. This has led in the past to the formulation of numerous (epistemic) uncertainty calculicuzzolin2020geometry, starting from de Finetti’s pioneering work on subjective probabilityDeFinetti74, and including possibility theoryDubois90, probability intervalshalpern03book, credal setslevi80book, random setsNguyen78and imprecise probabilitywalley91book. In this paper, we propose to model epistemic uncertainty using arandom setrepresentationMolchanov_2017. As we focus on finite target spaces in a classification setting, we will model uncertainty usingbelief functionsShafer76, the finite incarnation of random setscuzzolin2018visions. The theory of belief functions or theory of evidenceShafer76is a generalisation of Bayesian inferencesmets86bayessince classical (discrete) probabilities are simply special belief functions and Bayes’ rule is a special case of Dempster’s rule of combination. Crucially, in the theory of evidence, priors are not needed to start the inference process, avoiding the selection bias risks that can seriously condition Bayesian reasoningfreedman1999wald. An interesting geometric approach to uncertainty measures, and belief functions in particular, has been proposed by one of uscuzzolin2001geometric;cuzzolin2003geometry;cuzzolin2008geometric;cuzzolin2013l;cuzzolin2010geometry. To model uncertainty using belief functions, we propose a novel Random-set Convolutional Neural Network based on the principles of the epistemic deep learning concept. Epistemic deep learningmanchingal2022epistemicargues that a deep neural network producing an outcome for (selected) subsets of the target space is an intrinsically more faithful representation of the epistemic uncertainty associated with the limited quantity and quality of the training data. As they assign probability values to sets of outcomes directly, they naturally model the fact that observations almost invariably come in the form of sets. In an image classification setting, this could be interpreted as a test image being mapped to asetof classes, rather than a single class, when there is uncertainty about the true class. By representing classes as sets, set-valued classification can handle imprecise data more effectively and provide a richer understanding of the underlying uncertainty. It enables the classification model to capture multiple possible interpretations or predictions for each instance, allowing for more robust decision-making and improved performance in scenarios where precise labels may not be available or appropriate. ContributionsThe following are the key contributions of our paper: [leftmargin=*] A novelRandom-set Convolutional Neural Network(RS-CNN) classifier for uncertainty estimation based on the principle of epistemic deep learning, outputting Belief or Mass predictions for sets of classes, training using suitable loss functions generalising classical cross-entropy. A method for selecting a fixed budget of focal sets (sets with non-zero probability, associated with the network’s output neurons) from the power set of classes, thus ensuring scalability. A method for estimating the epistemic uncertainty of the prediction in terms of the size of the credal set (set of probability vectors) consistent with a predicted belief function. Experimental validation demonstrating the RS-CNN model outperform other uncertainty-aware models and exhibit superior accuracy on out-of-distribution (OOD) samples compared to the standard CNN.
====================
	 Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals
==========
Abstract:  Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by evaluating the recommended teams using a goodness score and find that more informed methods lead to recommendations of smaller number of teams but higher goodness, and (2) qualitatively, by conducting a large-scale user study at a college-wide level, and demonstrate that users overall found the tool very useful and relevant. Lastly, we evaluate our system in two diverse settings in US and India (of researchers and proposal calls) to establish generality of our approach, and deploy it at a major US university for routine use.
Fluency score: 4
Fluency issues: [Usage of "are" instead of "is" in "An example of these are seen"]

Clarity score: 3
Clarity issues: [Complex sentences and use of specialized terminology without explanation (e.g., "skill coverage", "goodness score")]

Grammaticality score: 4
Grammaticality issues: [Subject-verb agreement error "An example of these are seen"]

Readability score: 3
Readability issues: [Long sentences, high density of technical terms]

Coherence score: 4
Coherence issues: [Jump from discussing the system directly to validation without a smooth transition]
==========
Introduction:  In the recent decade, there has been an increased interest in studying teamwork skills and their impacts in a multitude of domains (e.g., academiaAlberolaet al.(2016), social networkingAnagnostopouloset al.(2012), project managementNollet al.(2016), healthcareNawazet al.(2014)). Building successful teams is a common business strategy (e.g., forming rescue and relief teams in response to an emergencyGunn and Anderson (2015), establishing entrepreneurial teams for new venturesLazaret al.(2020), forming teams dynamically in context of multi-agent systems (e.g., supply chains)Gaston and desJardins (2005)). In this paper, we focus on teaming for researchers applying to funding agencies in response to their call for proposals, using group recommendation strategies. The advantage of this setting is that the required data is already publicly available. A large amount of research funding in public universities comes from external funding agencies such as National Science Foundation (NSF) and National Institutes of Health (NIH). These opportunities often require multi-disciplinary teams from a wide variety of backgrounds to be quickly assembled. However, not every single team is often successful in achieving their goals, due to factors such as lack of accurate information, time, or communication, and incompatibility in terms of skill sets among team members. We build upon prior work, ULTRA (University-LeadTeam Builder fromRFPs andAnalysis)Srivastavaet al.(2022), a novel AI-based prototype for assisting with team formation when researchers respond to calls for proposals from funding agencies. In this paper, we interchangeably use the termcall for proposalwithrequest for proposal (RFP).Figure1shows a demo111A full demo interaction with the ULTRA system can be found at https://www.youtube.com/watch?v=8MUtxsfVNIU. The tool is deployed at http://casy.cse.sc.edu/ultra/teaming/. Additional details about usecases, experiments, and survey resources are atValluruet al.(2023).view of how the system works for an individual user who can become a team participant. The system first extracts technical skills from proposal calls found at publicly available data sources (e.g., NSF archives) and those present in online profiles of researchers (e.g., personal homepages, Google Scholar history), along with any additional teaming constraints that administrators or team participants of an institution may provide. Using AI and NLP techniques, ULTRA next provides a plausible list of candidate teams for each proposal call, where each team has at least two members. Our prior workSrivastavaet al.(2022), however, is a use case of asequentialsingle-item recommendation problem, where solutions are often limited by known issues such as cold startAbdollahpouriet al.(2019)or popularity biasYalcin and Bilge (2022). Therefore, we expand on this work to include group recommendation and novel AI methods to recommend optimal teams. Our contributions in the paper are that we: formulate a novelgroup recommendation problemto promote research collaborations where the objective is to suggest teams that could respond to calls for proposals, using skills found in open data, and balancing short- and long-term optimization. introduce a metric to measure goodness of teams and consider a configurable set of criteria: redundancy, group (set) size, coverage, and robustness. solve the novel problem using a variety of AI methods: string, taxonomy, and bandit (relational learning) methods, and compare them with a randomized baseline. establish the benefit of the solution methods quantitatively using goodness metric. We find that moreinformed methods lead to recommendations of smaller number of teams but higher goodness. establish the benefit of the system qualitatively using an IRB-approved preliminary survey at a College of a major US University. We find thatusers broadly consider the tool useful as well as relevantbut more studies are needed. demonstrate the generality of the approach with experiments at two different institutions in US and India. create and publish a teaming dataset that is available for research.
====================
	 Industry Classification Using a Novel Financial Time-Series Case Representation
==========
Abstract:  The financial domain has proven to be a fertile source of challenging machine learning problems across a variety of tasks including prediction, clustering, and classification. Researchers can access an abundance of time-series data and even modest performance improvements can be translated into significant additional value. In this work, we consider the use of case-based reasoning for an important task in this domain, by using historical stock returns time-series data for industry sector classification. We discuss why time-series data can present some significant representational challenges for conventional case-based reasoning approaches, and in response, we propose a novel representation based on stock returns embeddings, which can be readily calculated from raw stock returns data. We argue that this representation is well suited to case-based reasoning and evaluate our approach using a large-scale public dataset for the industry sector classification task, demonstrating substantial performance improvements over several baselines using more conventional representations.
Fluency score: 5

Clarity score: 4
Clarity issues: "In this work" could be more specific; the second to last sentence is lengthy and complex.

Grammaticality score: 5

Readability score: 4
Readability issues: The long compound-complex sentences may not be easily readable for all audiences.

Coherence score: 4
Coherence issues: The text may benefit from more transition words for smoother logical flow between sentences.
==========
Introduction:  Case-based reasoning (CBR) approaches have been applied in financial domains, and for a variety of tasks, from the early days of the field; see for example the work of[23]on the use of CBR for financial decision-making. In the years since, there have been many efforts made to apply CBR ideas to a diverse range of financial decision-making and prediction tasks[10,18,13,26,1,6]. Nevertheless, the use of CBR in such domains is not without its challenges, not the least of which concerns the very nature of many financial datasets and their relationship to the similarity assumption that underpins CBR. The central dogma of CBR is that similar problems have similar solutions, yet financial regulators are always at pains to point out that “past performance is not a guarantee of future results” suggesting that this principle may not be so reliable in financial domains, at least when it comes to predicting the future. As society changes and economies ebb and flow, companies that were once the stock market darlings fall out of favour, while new winners seem to emerge, with some regularity, albeit unpredictably and often with little or no warning. Two companies that were similar in the past may no longer be considered similar in the present, while the trajectories of companies with divergent histories might suddenly converge if future circumstances conspire in their favour. All of this greatly complicates the central role of similarity in case retrieval. In addition, the feature-based (attribute-value) representations that are commonplace in CBR systems may not provide such a good fit with the type of sequential time-series data (e.g. daily, weekly, and monthly stock prices/returns) that are the norm in financial domains. This is not to say that case-based methods cannot be used with time-series data. Certainly, there is a wealth of literature on representing time-series data for use with case-based methods in a range of application domains from agricultural science[3]to user experience[14]. Usually, the approach taken is to use various feature extraction techniques to identify landmark features from the raw time-series data; to effectively transform a raw time-series into a more conventional feature-based representation that can be used with standard similarity metrics. Similar approaches have been applied in the financial domain[12]but, as mentioned above, the stochastic nature of financial markets makes it difficult to effectively isolate useful case representations from market noise, which further complicates the similarity assessment even given a suitable fixed representation. Thus, in this work, our main technical contribution is to propose and evaluate a novel approach to learning case representations for financial assets (companies/stocks) using raw time-series data made up of historical daily returns. We describe how to transform raw, time-series data into an embedding-style representation of each stock/company; see for example[16,17]for examples of embedding representations. We argue that this facilitates the capture of more meaningful patterns and sub-patterns over extended periods of time, while facilitating the type of temporal alignment that is necessary during case comparison and similarity assessment. We argue that this approach is well-suited to the use of case-based and nearest-neighbour approaches in financial domains, because it can be used with a variety of standard similarity metrics, as well as more domain/task specific metrics[6]. We demonstrate its performance benefits in a comparative evaluation of the industry sector classification task, an important and practical benchmark problem in many financial applications[19]. The remainder of this paper is organised as follows. In the next section, we review the use of case-based reasoning in the financial domain and with time-series data more broadly, highlighting several common tasks and the approaches taken thus far, as well as the important challenges that remain with respect to representation and similarity assessment. Then, in Section3we discuss the details of our proposed approach, by describing how raw time-series data, such as financial returns, can be transformed into an embedding-based representation that is well suited to case-based approaches. In Section4we evaluate our proposed approach by using it to classify companies into their market sectors based on their historical returns data. We present the results of a comprehensive quantitative evaluation, which compares our proposed representation to a variety of baseline and naive approaches. We demonstrate how our embeddings-based representations can offer significant classification improvements, relative to more conventional representations of the raw time-series data. In addition, before concluding with a summary of our findings and a discussion of limitations and opportunities for further work, we present further qualitative evidence in support of the proposed approach, by using our representations to visualise the industry sectors that emerge from a clustering of our cases and some examples of nearest-neighbours in the resulting case-space.
====================
	 AugUndo: Scaling Up Augmentations for Unsupervised Depth Completion
==========
Abstract:  Unsupervised depth completion methods are trained by minimizing sparse depth and image reconstruction error. Block artifacts from resampling, intensity saturation, and occlusions are amongst the many undesirable by-products of common data augmentation schemes that affect image reconstruction quality, and thus the training signal. Hence, typical augmentations on images viewed as essential to training pipelines in other vision tasks have seen limited use beyond small image intensity changes and flipping. The sparse depth modality have seen even less as intensity transformations alter the scale of the 3D scene, and geometric transformations may decimate the sparse points during resampling. We propose a method that unlocks a wide range of previously-infeasible geometric augmentations for unsupervised depth completion. This is achieved by reversing, or ``undo"-ing, geometric transformations to the coordinates of the output depth, warping the depth map back to the original reference frame. This enables computing the reconstruction losses using the original images and sparse depth maps, eliminating the pitfalls of naive loss computation on the augmented inputs. This simple yet effective strategy allows us to scale up augmentations to boost performance. We demonstrate our method on indoor (VOID) and outdoor (KITTI) datasets where we improve upon three existing methods by an average of 11.75% across both datasets.
Fluency score: 4
Fluency issues: Use of passive voice and jargon may slightly hinder fluency for a general audience.

Clarity score: 3
Clarity issues: Several sentences are dense with technical terms and could be more reader-friendly. For example, "to the coordinates of the output depth" could be clarified.

Grammaticality score: 5
Grammaticality issues: No grammatical issues detected.

Readability score: 3
Readability issues: Complexity of sentences and specialized vocabulary may be difficult for those without a background in the subject matter.

Coherence score: 4
Coherence issues: The text is coherent for the most part but assumes a familiarity with the subject matter that may not be present in all readers.
==========
Introduction:  Data augmentation plays a crucial role in easing the demand for training data as it enriches the training dataset by orders of magnitude, boosting performance on various deep learning tasks[29,35,38], particularly by preventing overfitting. Typically, the choice of augmentation is largely task-dependent. One common axiom of choosing augmentations is that the task output should remain invariant to the augmentation. For example, image flips are viable augmentations for classifying animals, since they do not alter the resulting image label. Conversely, flipping road signs can alter their meanings; hence, such augmentations can be detrimental to tasks involving road sign recognition. For geometric tasks (i.e. depth estimation), the range of augmentations is more restricted: Stereo assumes pairs of rectified images; hence, in-plane rotations are not feasible. Image-guided sparse depth completion relies on sparse points to ground estimates to metric scale; therefore, intensity transformations on sparse depth maps that alter the scale of the 3-dimensional (3D) scene are infeasible – leaving few augmentations viable. Unsupervised learning of depth completion further limits the use of augmentations as the supervision signal comes from reconstructing the inputs, where augmenting the input introduces artifacts that impact reconstruction quality and therefore the supervision. While simulating nuisances,i.e., camera motion and orientation, is desirable, naively applying augmentations pertaining to them may do more harm than good; therefore, it is unsurprising that existing works[27,24,34,42,43,44,41,46]primarily rely on a small range of photometric augmentations and flipping. Nevertheless, photometric augmentations help model the diverse range of illumination conditions and colors of objects that may populate the scene; geometric augmentations can simulate the various camera motion, i.e. image translations can approximate small baseline movements, and scene arrangements, i.e. image flipping. But block artifacts, loss during resampling, intensity saturation are just some of the many undesirable side-effects of traditional augmentations to the image and sparse depth map. To exploit the immense amount of data derived from augmentations, we propose to simply compute the typical reconstruction loss on the original input image and sparse depth map, which bypasses negative effects of reconstruction artifacts due to photometric and geometric augmentations. However, there exists a mis-alignment between the original input (image and sparse depth), and the model depth estimate as geometric augmentations induces a change in coordinates. Hence, weundothe geometric augmentations by inverting them in the output space to align the model estimate with the training target. Amongst the the many geometric tasks, we focus onunsupervised depth completion, the task of inferring a dense depth map from a single image and its associated sparse depth map, where augmentations have seen limited use. Here, a training sample includes the input sparse depth map, its associated image as well as additional images of neighboring views from the same 3D scene. Augmentations have traditionally been restricted to a limited range of photometric transformations and flipping – due to the need to preserve photometric consistency across a sequence of video frames used during training, and the sparse set of 3D points projected onto the image frame as a 2.5D range map; degradation to either modalities directly impacts the supervision signal. By using our method, loss functions involving sparse depth and image reconstruction from other views can be computed on the original inputs while applying augmentations that were previously not viable for the task.Our hypothesis:By “undo-ing” the augmentations, one can expand the viable set and scale up their use in training, leading to improved model performance and generalizability. To this end, we introduce AugUndo, an augmentation framework that allows one to apply a wide range of photometric and geometric transformations on the inputs, and to “undo” them during loss computation. This allows one to compute an unsupervised loss on the original images and sparse depth maps, free of artifacts, through a warping of the output depth map – obtained from augmented input – onto the input frame of reference based on the inverse geometric transformation. In addition to group transformations that allow for output alignment, we combine them with commonly employed photometric augmentations. Lastly, we study whether non-group transformations, such as occlusion, can further improve model performance. We demonstrate AugUndo on three recent unsupervised depth completion methods and evaluate them on indoor and outdoor settings, where we improve by an average of 11.75% across all methods and datasets. Our contributionsare as follows: (i) We propose AugUndo, a simple-yet-effective framework to scale up photometric and geometric augmentations for unsupervised depth completion; AugUndo can be applied in a plug-and-play manner to existing methods with negligible increase in computational costs during training. (ii) We provide a comprehensive study ablating over combinations of eleven types of augmentations, including ones that have not been explored by existing unsupervised methods, for three different models across two datasets. We found a consistent set of augmentations that provides performance benefits for all tested models and are applicable for both indoor and outdoor scenarios. (iii) We show that AugUndo can consistently improve model performance, robustness to shifts in sparse point densities as well as zero-shot generalization; thus, validating our hypothesis.
====================
	 Generalised Probabilistic Diffusion Scale-Spaces
==========
Abstract:  Probabilistic diffusion models excel at sampling new images from learned distributions. Originally motivated by drift-diffusion concepts from physics, they apply image perturbations such as noise and blur in a forward process that results in a tractable probability distribution. A corresponding learned reverse process generates images and can be conditioned on side information, which leads to a wide variety of practical applications. Most of the research focus currently lies on practice-oriented extensions. In contrast, the theoretical background remains largely unexplored, in particular the relations to drift-diffusion. In order to shed light on these connections to classical image filtering, we propose a generalised scale-space theory for probabilistic diffusion models. Moreover, we show conceptual and empirical connections to diffusion and osmosis filters.
Fluency score: 4
Fluency issues: [No significant issues; minor refinement could enhance readability for a broader audience]

Clarity score: 3
Clarity issues: [Complex sentences and technical jargon could be confusing for readers not familiar with the topic]

Grammaticality score: 5
Grammaticality issues: [No issues identified]

Readability score: 3
Readability issues: [Sentences are complex and dense with specialized terminology]

Coherence score: 4
Coherence issues: [The text is mostly coherent but presumes familiarity with the field’s terminology]
==========
Introduction:  Diffusion probabilistic models[1]have recently risen to the state-of-the-art in image generation, surpassing generative adversarial networks[2]in popularity. In addition to significant research activity, the availability of pre-trained latent diffusion networks[3]have also brought diffusion models to widespread public attention[4]. Practical applications are numerous, including the generation of convincing, high fidelity images from text prompts or partial image data. Initial probabilistic diffusion models[1,5,6,7,8,9,10]relied on a forward drift-diffusion process that gradually perturbs input images with noise and can be reversed by deep learning. Recently, it has been shown that the concrete mechanism that gradually destroys information in the forward process has a significant impact on the image generation by the reverse process. Alternative proposed image degradations include blur[11], combinations of noise and blur[12,13,14], or image masking[12]. So far, probabilistic diffusion research was mostly of practical nature. Some theoretical contributions established connections to other fields such as score-matching[7,8,9,10], variational autoencoders[6], or normalising flows[15]. Probabilistic diffusion models have been initially motivated[1]by drift-diffusion, a well-known process in physics. However, its connections to other physics-inspired methods remain mostly unexplored. Closely related concepts have a long tradition in model-based visual computing, such as osmosis filtering proposed byWeickert et al. [16]. In addition, there is a wide variety of diffusion-based scale-spaces[17,18,19,20]. Conceptually, these scale-spaces embed given images into a family of simplified versions. This resembles the gradual removal of image features in the forward process of probabilistic diffusion models. Despite this multitude of connections, there is a distinct lack of systematic analysis of probabilistic diffusion from a scale-space perspective. This is particularly surprising due to the impact of the forward process on the generative performance[14,13]. It indicates that a better understanding of the information reduction could also lead to further practical improvements in the future. With our previous conference publication[21]we made first steps to bridge this gap between the scale-space and deep learning communities. To this end, we introduced first generalised scale-space concepts for probabilistic diffusion. In this work, we further explore the theoretical background of this successful paradigm in deep learning. In contrast to traditional scale-spaces, we consider the evolution of probability distributions instead of images. Despite this departure from conventional families of images, we can show scale-space properties in the sense ofAlvarez et al. [17]. These include architectural properties, invariances, and entropy-based measures of simplification. In addition to our previous findings[21], our novel contributions include a generalisation of our scale-space theory for diffusion probabilistic models (DPMs) which includes both variance preserving and variance exploding approaches, generalised scale-space properties for the reverse process of DPM, a scale-space theory for inverse heat dissipation[13]and blurring diffusion[14], and a significantly extended theoretical and empirical comparison of three probabilistic diffusion models to homogeneous diffusion[18]and osmosis filtering[16].
====================
	 Aligning Robot and Human Representations
==========
Abstract:  To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.
Fluency score: 4
Fluency issues: [The structure is mostly fluent, but "for and with people" might seem a bit awkward; consider revising to "alongside people" for smoother readability.]

Clarity score: 4
Clarity issues: [The terms "representation alignment" and "key desiderata" may not be clear for all readers; consider simplifying or defining these terms.]

Grammaticality score: 5
Grammaticality issues: [No issues detected.]

Readability score: 4
Readability issues: [The text is dense with technical concepts that might challenge some readers, particularly "representation misalignment" and "representation learning approaches."]

Coherence score: 4
Coherence issues: [The text introduces several concepts that are not fully explained within the snippet, such as "representation misalignment" and "key desiderata," which could confuse readers new to the topic.]
==========
Introduction:  In the HRI community, we aspire to build robots that perform tasks that human users want them to perform. To do so, robots need goodrepresentationsof salient task aspects. For example, in Fig.1, to carry a coffee mug, the robot considers efficiency, mug orientation, and distance from the user’s possessions in its behaviour. There are two paradigms for learning representations: one thatexplicitlybuilds in structure for learning task aspects, e.g. feature sets or graphs, and one thatimplicitlyextracts task aspects by mapping input directly to desired behaviour, e.g. end-to-end approaches(Levineet al.,2020; Rosset al.,2011). While explicit structure is useful for capturing relevant task aspects, it’s often impossible to comprehensively define all aspects that may matter to the downstream task; meanwhile, implicit methods circumvent this problem by allowing neural networks to automatically extract representations, but they are prone to capturingspurious correlations(Levineet al.,2020), resulting in potentially arbitrarily bad robot behaviour under distribution shift between train and test conditions(Paudel,2022). Our observation is that many failures in robot learning, including the ones above, result from amismatch between the human’s representation and the one learned by the robot; in other words, their representations aremisaligned. From this perspective, these failures illuminate that if we truly wish to learn good representations – if we truly want robots that do what humans want – we must explicitly focus on the foundational problem:aligning robot and human representations. In this paper, we offer a unifying lens for the HRI community to view existing and future solutions to this problem. We review over 100 papers in the representation learning literature in robotics from this perspective. We first define a unifying mathematical objective for an aligned representation based on four desiderata: value alignment, generalizable task performance, reduced human burden, and explainability. We then conduct an in-depth review of four common representations (Fig.1): the identity representation, feature sets, feature embeddings, and graphical structures – illustrating the deltas each falls short in with respect to the desiderata. From situating each representation in our formalism, we arrive at the following key takeaway: a better structured representation affords better alignment and therefore better task performance, but always with the unavoidable tradeoff of more human effort. This effort can be directed in three ways: 1) representations that operate directly on the observation space, e.g. end-to-end methods, direct effort at increasing task data to avoid spurious correlations; 2) representations that build explicit task structure, e.g. graphs or feature sets, direct effort at constructing and expanding the representation; and 3) representations that learn directly from implicit human representations, e.g. self-supervised models, direct effort at creating good proxy tasks. Our paper is untraditional in that it is much like a survey paper, except there is little work that directly addresses the representation alignment problem we pose. Instead, we offer a retrospective on works that focus on learning task representations in robotics with respect to our desiderata. Our review provides a unifying lens to think about the current gaps present in the robot learning literature as defined by a common language, or in other words, a roadmap for thinking about challenges present in current and future solutions in a principled way. We conclude by suggesting key open directions.
====================
	 Practical Layout-Aware Analog/Mixed-Signal Design Automation with Bayesian Neural Networks
==========
Abstract:  The high simulation cost has been a bottleneck of practical analog/mixed-signal design automation. Many learning-based algorithms require thousands of simulated data points, which is impractical for expensive to simulate circuits. We propose a learning-based algorithm that can be trained using a small amount of data and, therefore, scalable to tasks with expensive simulations. Our efficient algorithm solves the post-layout performance optimization problem where simulations are known to be expensive. Our comprehensive study also solves the schematic-level sizing problem. For efficient optimization, we utilize Bayesian Neural Networks as a regression model to approximate circuit performance. For layout-aware optimization, we handle the problem as a multi-fidelity optimization problem and improve efficiency by exploiting the correlations from cheaper evaluations. We present three test cases to demonstrate the efficiency of our algorithms. Our tests prove that the proposed approach is more efficient than conventional baselines and state-of-the-art algorithms.
Fluency score: 4
Fluency issues: [No specific fluency issues; the text is generally well-constructed but can be slightly complex for lay readers]

Clarity score: 3
Clarity issues: ["has been a bottleneck of" could be phrased as "has bottlenecked"; "scalable to tasks" could be clearer as "scalable for tasks"]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 3
Readability issues: [Complex sentence structures and terminology may hinder comprehension for a general audience]

Coherence score: 4
Coherence issues: [The text is mostly coherent, though it contains jargon that may confuse readers not familiar with the subject]
==========
Introduction:  Analog/Mixed-signal (AMS) integrated circuit (IC) design typically follows a process flow visualized in Figure1. A combination of designer experience and computer simulation feedback is iterated to determine the design that meets the performance requirements. A large portion of design time is spent on the sizing and layout phases, where multiple iterations are possible due to potential loop-backs in the design flow. This is a labor-intensive process in industry practice with little to no automation. To address this costly exercise, a considerable effort in academia is focused on introducing automated solutions. Analog sizing automation is the task of optimizing AMS design variables, e.g., transistor widths, lengths, resistor, and capacitor values. The aim is to satisfy the performance constraints and optimize the design objective. In general, sizing automation is run through schematic-level simulations. However, AMS IC performance is also sensitive to layout implementation[4]. Especially in the advanced process nodes, layout-induced parasitics may greatly affect the final design performance. Therefore, sizing the AMS design variables considering the layout effects is also crucial. The majority of the recent sizing and post-layout performance optimization algorithms have simulation feedback in the loop. Due to advanced scaling, simulations are required to obtain accurate performance evaluations. Simulation-based AMS automation algorithms adapted various methods from the optimization and Machine Learning (ML) communities. The earlier approaches include population-based methods such as particle swarm optimization[25]and evolutionary algorithms[14]. Although these algorithms have good convergence behavior, they are inefficient in sampling since they explore the design space randomly. To mitigate sample inefficiency, model-based methods gained popularity[13,18,11]. These methods employ surrogate-models between the solution space and performance space and provide efficiency in exploring the solution space. A typical surrogate model is Gaussian Process Regression (GPR)[21], which is a well-studied model in Bayesian Optimization (BO) field[23]and is adapted by several analog sizing algorithms. The main drawback of GPR modeling is its computational complexity. Recent research trend in analog sizing introduces ML to simulation-based methodology[3]. However, the literature review reveals that most of these methods require thousands of simulation data to train Deep Neural Network (DNN) models that approximate the relations between the design variables and the performance metrics. Therefore, the practicality of these algorithms is severely reduced when the optimization task has a high simulation cost. For example, drawing/generating the layout, extracting the parasitics, and running post-layout simulations is typically an expensive procedure. Therefore, optimization algorithms designed for schematic-level sizing can not be adapted by simply changing how data is generated. This paper presents a Machine Learning-based simulation-in-the-loop automation method for the AMS design problem. Overall, we formalize two stand-alone recipes for schematic-level sizing and post-layout performance optimization, i.e., layout-aware sizing. We integrate the state-of-the-art analog layout generator, MAGICAL[27], into our flow to handle layout-aware sizing. Our algorithms do not assume the pre-existence of any dataset, and we generate all training data during the optimization. We employ Bayesian Neural Networks (BNN) for modeling design performances. Bayesian Neural Networks allow error quantification, and compared to Deep Neural Networks, BNN are shown to be effective in handling scarce datasets and preventing overfitting[9]. Therefore, BNN can be trained on smaller datasets, significantly improving the practicality and scalability. We also introduce a batch-optimization framework and design space sampling strategy that is compatible with BNN modeling. Further, when optimizing the design variables based on post-layout performance, we exploit the correlation between schematic-level simulations and post-layout simulations. Our algorithm introduces a co-learning scheme that reduces the need for costly post-layout simulations and boosts efficiency even further. We compile our contributions as follows: We use Bayesian Neural Network-based modeling to obtain performance approximations. Different learning strategies are adapted for schematic-level sizing and post-layout performance optimization. We adopt a scalable sampling strategy and query the optimization batches by utilizing a trust region and Thompson sampling. The post-layout sizing is handled as a multi-fidelity optimization problem, and an efficient co-learning strategy is developed. The efficiency of the proposed methods is demonstrated on three circuits by providing comparisons to previous state-of-the-art. The rest of the paper is organized as follows. SectionIIintroduces the backgrounds and previous work. SectionIIIdescribes our algorithms for handling schematic-level sizing and post-layout performance-based sizing problems. SectionIVprovides the experiments on circuit examples to demonstrate the efficiency of our algorithms. Finally, SectionVconcludes the paper.
====================
	 DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization
==========
Abstract:  Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.
Fluency score: 4
Fluency issues: -

Clarity score: 3
Clarity issues: Heavy-domain specific terminology may hinder clarity for a general audience; phrases like "graph-based denoising diffusion models" and "SATLIB benchmark" might be too technical without context.

Grammaticality score: 5
Grammaticality issues: -

Readability score: 3
Readability issues: Jargon and dense information detract from readability for non-expert readers.

Coherence score: 4
Coherence issues: Some prior knowledge is assumed, which might impact the coherence for those outside the field.
==========
Introduction:  Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand significant expert efforts to approximate near-optimal solutions(Arora,1996; Gonzalez,2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution(Bello et al.,2016; Kool et al.,2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems(Fu et al.,2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical(Joshi et al.,2019; Karalias and Loukas,2020; Qiu et al.,2022). Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems(Khalil et al.,2017; Gu et al.,2018), for example, when multiple optimal solutions exists for the same graph. Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt(Lin and Kernighan,1973; Andrade et al.,2012)and node swap(Chen and Tian,2019; Wu et al.,2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework(Wu et al.,2021; Ma et al.,2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation(Song and Ermon,2019; Ho et al.,2020; Rombach et al.,2022;Yu et al.,; Saharia et al.,2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as to find a\{0,1\}-valued vector that indicates the optimal selection of nodes or edges in a candidate solution for the task. Then we use a message passing-based graph neural network(Kipf and Welling,2016; Hamilton et al.,2017; Gilmer et al.,2017; Veličković et al.,2018)to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\ll N) denoising steps (Sec.3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec.3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular,Graikos et al. (2022)proposed an image-based diffusion model to solve Euclidean Traveling Salesman problems by projecting each TSP instance onto a64\times 64greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image. The main difference between suchimage-baseddiffusion solver and ourgraph-baseddiffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive(Kool et al.,2019a)and improvement heuristics(d O Costa et al.,2020)solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion modeling within the DIFUSCO framework: continuous diffusion with Gaussian noise(Chen et al.,2022)and discrete diffusion with Bernoulli noise(Austin et al.,2021; Hoogeboom et al.,2021). These two types of diffusion models have been applied to image processing but not to NPC problems so far. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network(Bresson and Laurent,2018; Joshi et al.,2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.
====================
	 FaceFusion: Exploiting Full Spectrum of Multiple Datasets
==========
Abstract:  The size of training dataset is known to be among the most dominating aspects of training high-performance face recognition embedding model. Building a large dataset from scratch could be cumbersome and time-intensive, while combining multiple already-built datasets poses the risk of introducing large amount of label noise. We present a novel training method, named FaceFusion. It creates a fused view of different datasets that is untainted by identity conflicts, while concurrently training an embedding network using the view in an end-to-end fashion. Using the unified view of combined datasets enables the embedding network to be trained against the entire spectrum of the datasets, leading to a noticeable performance boost. Extensive experiments confirm superiority of our method, whose performance in public evaluation datasets surpasses not only that of using a single training dataset, but also that of previously known methods under various training circumstances.
Fluency score: 4
Fluency issues: "a fused view of different datasets that is untainted"

Clarity score: 4
Clarity issues: "a fused view" may not be immediately clear to all readers; "whose performance in public evaluation datasets surpasses" is a bit awkward

Grammaticality score: 4
Grammaticality issues: "poses the risk of introducing large amount of label noise" (should be "a large amount" or "large amounts")

Readability score: 4
Readability issues: Sentences are slightly complex, which could impact some readers' understanding

Coherence score: 4
Coherence issues: Jargon may make the text less coherent to non-specialist readers; slight repetition with "using the unified view of combined datasets" and "using the view in an end-to-end fashion"
==========
Introduction:  The priority focus of modern face recognition studies has been in-line with that of representation learning studies: amplifying the representative power of embedding vectors within the feature space. With continued development and refinement of various training methods, face recognition models have seen significant improvement in terms of evaluation accuracy in recent years. However, equally important is the recent advent of publicly available large-scale facial image datasets. These large datasets have generally been curated by either partially or entirely automated crawling of publicly available facial images, followed by different approaches of clustering the images by their identities. While the approach of building such large datasets from scratch have been studied widely, considerably less amount of attention has been given to the methods of starting from already-curated datasets. Combining different datasets is naturally beneficial from the fact that they have already gone through some degree of refinements that include identity-wise clustering or noise removal, but because their image sources generally come from public datasets of celebrities or random web crawling, careful consideration is required to handle conflicting identities. Identity conflict is destructive to the overall model performance because physically same identities are interpreted as distinct identities, which the model is incorrectly taught to distinguish. A trivial solution to this inter-class label noise would be to train a rectifier model[34]to adjust such identities, but such solution would be heavily dependent on training a robust rectifier model. Another possible approach is explored in[16]as a posterior data cleaning process, but it potentially requiresO(MN)memory space, whereMis the number of conflicting identities, andNis the number of datasets. DAIL[27], to our best knowledge, is the only work that studies the way of using multiple datasets concurrently without a separate dataset-manipulation process by introducing dataset-aware softmax. While the approach of DAIL somewhat mitigates the conflicting identity problem, we argue that the performance gain from DAIL is suboptimal, because dataset-aware softmax essentially isolates the representations learned from each dataset from the rest of the datasets. This reduces the scope of final softmax operation from the entire embedding space to the subspaces fitted to each dataset, preventing the embedding model to reach global optimization.To suppress the inevitably introduced inter-class label noise when combining different datasets, and to further improve from the limited performance gain of DAIL, we introduceFaceFusion. Our approach begins from the method of DAIL, and after the model parameters are stabilized, observed as the well-knownslow-drift[29]phenomenon, it directlyfusesdifferent datasets into a unified global embedding space, while also merging the class proxies of conflicting identities. Doing so effectively enables viewing multiple datasets as a unified dataset, which the embedding model can exploit to expand its optimization scope from within each dataset to the whole datasets, resulting in superior performance. Extensive experiments confirms that FaceFusion outperforms not only the models trained using single dataset, but also the models trained with multiple datasets either by naive concatenation or by the method of DAIL. We further prove that FaceFusion maintains its superiority over the aforementioned methods under varying severity of conflicting identities of each dataset, ranging from completely disjoint to vastly overlapping with each other.
====================
	 Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models
==========
Abstract:  Groundbreaking inventions and highly significant performance improvements in deep learning based Natural Language Processing are witnessed through the development of transformer based large Pre-trained Language Models (PLMs). The wide availability of unlabeled data within human generated data deluge along with self-supervised learning strategy helps to accelerate the success of large PLMs in language generation, language understanding, etc. But at the same time, latent historical bias/unfairness in human minds towards a particular gender, race, etc., encoded unintentionally/intentionally into the corpora harms and questions the utility and efficacy of large PLMs in many real-world applications, particularly for the protected groups. In this paper, we present an extensive investigation towards understanding the existence of "Affective Bias" in large PLMs to unveil any biased association of emotions such as anger, fear, joy, etc., towards a particular gender, race or religion with respect to the downstream task of textual emotion detection. We conduct our exploration of affective bias from the very initial stage of corpus level affective bias analysis by searching for imbalanced distribution of affective words within a domain, in large scale corpora that are used to pre-train and fine-tune PLMs. Later, to quantify affective bias in model predictions, we perform an extensive set of class-based and intensity-based evaluations using various bias evaluation corpora. Our results show the existence of statistically significant affective bias in the PLM based emotion detection systems, indicating biased association of certain emotions towards a particular gender, race, and religion.
Fluency score: 4
Fluency issues: Sentence complexity and length may affect fluency.

Clarity score: 3
Clarity issues: Technical jargon and long sentences can impede straightforward understanding.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 3
Readability issues: Dense language and complex sentence structure lower the readability for general audiences.

Coherence score: 4
Coherence issues: The academic tone and advanced concepts may be difficult for those not versed in the subject.
==========
Introduction:  Recently, large scale Natural Language Processing (NLP) models are being increasingly deployed in many real-world applications within almost all domains such as health-care(Velupillaiet al.,2018; Soni and Roberts,2020), business(Mishevet al.,2020), legal systems(Dale,2019), etc., due to its efficacy to make data-driven decisions and capability of natural language understanding even better than humans111https://www.infoq.com/news/2021/01/google-microsoft-superhuman/(Heet al.,2021). Transformer based large Pre-trained Language Models (PLMs) have been hugely influential in NLP due to their capability to generate powerful contextual representations. PLMs are mostly built based on a self-supervised learning strategy that highly relies on unlabelled data abundantly available from the human generated data deluge(Heet al.,2021). But, since this historical data of textual write-ups has their roots within human thought, they often reflect latent social stereotypes(Suresh and Guttag,2021; Garget al.,2018). For example, the Social Role Theory by Eagly et al.(Eagly and Steffen,1984)demonstrates that the idea of gender stereotype develops from perceivers’ observations, associating the capabilities and personality attributes of different genders with the activities in which they engage in their day-to-day lives over a time, building rigid stereotypes in human minds and their writings, on how these genders behave (e.g. women are highly emotional), where they work (e.g. women preferred in children’s daycare), etc. Hence the data from such human generated data repositories eventually convey these stereotypes as linguistic biases through the NLP algorithms, especially those built on large PLMs that utilize huge amounts of data(Suresh and Guttag,2021). In this direction, investigation of“Affective Bias”in NLP is a recent stream of research to study the existence of any unfair/biased association of emotions (anger, fear, joy, etc.) or sentiments (positive, negative, etc.) towards underrepresented or protected groups in a domain, that may lead affective computing systems such as sentiment or emotion detection systems to deliver unfavorable outcomes for these protected groups(K.et al.,2022). For instance, a model consistently associating women with a different class of emotion or same emotion differing in emotion intensities vis-a-vis predictions for male(Shields and Shields,2002)could be seen as a manifestation of affective bias. Similarly, association of a particular religion always with a specific emotion(Abidet al.,2021a)represents affective bias too. A real world scenario of affective bias is the case of Google sentiment analyzer judging that being gay is bad by assigning high negative sentiments to sentences such as ‘I’m a gay black woman’, ‘I’m a homosexual’, etc.,222https://www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias. For better understandability of affective bias, we illustrate in table1, a sample set of affectively biased emotion predictions from PLM based textual emotion detection models constructed in this study for affective bias analysis (detailed explanation of the models are provided in section4.1). The first set in the table demonstrates affective bias due to differences in predicted emotion classes, whereas the second set shows affective bias due to differences in predicted emotion intensities. Similar to other general algorithmic biases like gender bias, racial bias, etc., a possible stimuli to affective biases are the latent emotion based stereotypes about different social groups in the data. Studies report that such emotion based stereotyping influence socialization of emotions leading to propagation of stereotypes such as associating women’s (or men’s) experience and expressions being aligned with fear and sadness (or anger and pride)(Plantet al.,2000). Similarly, affective bias within systems could facilitate a higher association of black women to the emotion anger when considering emotions with the domains race and gender(Ashley,2014). In addition to biased data, another reason for bias is based on how the model/algorithmic design considers or treats the underrepresented or protected attributes concerning a domain(Hooker,2021). Similar to any other general social biases, the existence of these affective biases make textual affective computing systems generate unfair or biased decisions that can harm its utility towards socially marginalized populations by denying opportunities/resources or by false portrayal of these groups when deployed in the real-world. Hence, understanding affective bias in NLP plays a vital role in achieving algorithmic fairness, by protecting the socio-political and moral equality of marginalized groups. In this context, we present an extensive experimental analysis to understand and illustrate the existence of latent“Affective Bias”in transformer based large pre-trained language models with respect to the downstream task of textual emotion detection. Hence, we set our research question:Do predictions made by large PLM based textual emotion detection systems systematically or consistently exemplify ‘Affective Bias’ towards demographic groups?. Our investigation of affective bias in large PLMs primarily aims to identify the existence of gender, racial, and religious affective biases and set aside the task of affective bias mitigation in the scope for future work. We start with an exploration of corpus level affective bias or affect imbalance in corpus to find out any biased emotion associations in the large scale corpora that are used to pre-train and fine-tune the PLMs, by analyzing the distribution of emotions or their associations with demographic target terms (e.g., Islam, Quran) related to a social group (e.g., Muslim) concerning a domain (e.g., Religion). Later, we explore the prediction level affective bias in four popular transformer based PLMs, BERT (Bidirectional Encoder Representation from Transformers)(Devlinet al.,2019), OpenAI GPT-2 (Generative Pre-trained Transformer)(Radfordet al.,2019), XLNet(Yanget al.,2019), and T5 (Text-to-Text Transfer Transformer)(Raffelet al.,2020), that are fine-tuned using a popular corpora SemEval-2018 EI-oc(Mohammadet al.,2018)for the task of textual emotion detection. To quantify prediction level affective bias, we subject the PLMs to an extensive set of class-based and intensity-based evaluations using three different evaluation corpora EEC(Kiritchenko and Mohammad,2018), BITS(Venkit and Wilson,2021)and CSP(Nangiaet al.,2020). A detailed sketch of the overall analysis is shown in figure1. Workflow ofAffective biasanalysis The rest of the paper is organized as follows. Section2presents the relevant related works. Section3presents corpus level affective bias analysis with corresponding methodology and results. Section4presents the exploration towards prediction level affective bias with details of constructing PLM based textual emotion detection model, methodology of analysis, and the corresponding results. Section5presents a discussion based on the entire results and finally, section6draws the conclusions.
====================
	 DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning
==========
Abstract:  Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmentation consistency to encode invariance as meta-knowledge for instantiating MR objective, denoted by DAC-MR. The proposed DAC-MR is hopeful to learn well-performing meta-models from training tasks with noisy, sparse or unavailable meta-data. We theoretically demonstrate that DAC-MR can be treated as a proxy meta-objective used to evaluate meta-model without high-quality meta-data. Besides, meta-data-driven meta-loss objective combined with DAC-MR is capable of achieving better meta-level generalization. 10 meta-learning tasks with different network architectures and benchmarks substantiate the capability of our DAC-MR on aiding meta-model learning. Fine performance of DAC-MR are obtained across all settings, and are well-aligned with our theoretical insights. This implies that our DAC-MR is problem-agnostic, and hopeful to be readily applied to extensive meta-learning problems and tasks.
Fluency score: 3
Fluency issues: Overuse of "meta-" prefix, heavy jargon, awkward phrasing such as "denoted by DAC-MR", and disjointed clauses like "Fine performance of DAC-MR are obtained across all settings"

Clarity score: 2
Clarity issues: Heavy use of domain-specific terminology without explanations, sentences are complex and not easily accessible to non-specialists, unclear pronoun references (e.g., "this" in "this issue")

Grammaticality score: 3
Grammaticality issues: "achieving well-performing meta-learning model" (missing "a"), misuse of pluralization "Fine performance... are obtained"

Readability score: 2
Readability issues: Complex sentences with excessive jargon, lack of transitions between ideas, and some redundancy in phrasing

Coherence score: 3
Coherence issues: Frequent subject changes, lack of explicit linkage between sentences, and introduction of concepts without sufficient explanation (e.g., DAC-MR)
==========
Introduction:  Maching learning has recently demonstrated impressive performance in various fields, e.g., computer vision[he2016deep], natural language processing[devlin2018bert], speech processing[abdel2014convolutional], etc. However, an effective machine learning method often requires a large amount of high-quality labeled data to properly and sufficiently simulate the testing/evaluating distribution. Collecting such large-scale supervised datasets is notoriously expensive in time and effort for most real applications. Compared with current machine intelligence, humans are able to quickly learn novel concepts from only small amount of examples[lake2015human,lake2017building]. The capability of machine to learn new concepts quickly from small examples is thus desirable, especially for many problems/applications where data are intrinsically rare or expensive, or compute resources are unavailable. Meta-learning[naik1992meta,schmidhuber1987evolutionary,thrun1998lifelong], or learning to learn, has been suggested as a promising solution path to assemble machine learning with above capability. The key idea of meta-learning is to distill a meta-model from multiple learning tasks/episodes, and then use this meta-model to improve performance of task-specific model on novel query tasks[hospedales2021meta,shu2021learning]. Such a learning paradigm is hopeful to bring a variety of benefits, such as finely adapting to query tasks with less computation/data costs (e.g., avoid learning from scratch for novel tasks), as well as fewer human interventions. Recently, it produces an explosion of researches on meta-learning, due to its potential to advance the frontier of the contemporary machine learning. Especially, meta-learning has helped machine learning improve the data efficiency[wang2020generalizing,shu2018small], algorithm automation[he2021automl,karmaker2021automl], and generalization[maurer2016benefit,tripuraneni2020theory,shu2021learning]. Successful applications have been demonstrated in areas spanning few/zero-shot learning[finn2017model,snell2017prototypical,vinyals2016matching,sung2018learning,soh2020meta], neural architecture search (NAS)[liu2018darts,elsken2019neural], hyperparameter optimization[franceschi2018bilevel], curriculum learning[ren2018learning,shu2019meta], domain adaptation/generalization[liu2021cycle,li2018learning], transfer learning[jang2019learning,sun2020meta], label noise learning[zheng2021meta,shu2020meta,zhao2021probabilistic], semi-supervised learning[pham2021meta], unsupervised learning[metz2018meta], reinforcement Learning[duan2016rl,wang2016learning], data/label generation[cubuk2018autoaugment,wu2021learning,shu2022cmw], loss/regularization learning[balaji2018metareg,yazdanpanah2022revisiting,lee2019meta,shu2020learning,huang2019addressing], learning to optimize[andrychowicz2016learning,ravi2016optimization,shu2022mlr], and robustness[collins2020task,killamsetty2022nested], etc. These successes largely attribute to the data-based nature of current meta-learning approaches that learn from a tremendous number of training tasks with high-quality meta-data representing the underlying task generalization goal. However, in most real applications, collecting such high-quality training tasks are difficult, expensive and impractical. This often makes obtained training tasks imperfect. In fact, we always have access to problematic meta-data for some applications. For example, the corresponding ground-truth labels of meta-data are generally noisy in label noise problems[shu2019meta], or unavailable in unsupervised domain adaptation tasks[liu2021cycle], or the size of meta-data is limited in few-shot learning issues[finn2017model]. The purely meta-data-driven approaches tend to reach their limits or lead to unsatisfactory results under these imperfect circumstances. With meta learning becoming more and more popular in real applications, there is also a growing need for meta-learning to train well-performing and sufficiently generalized meta-models from such imperfect training tasks. As a step towards addressing the limitations of purely meta-data-driven meta-learning, we suggest a meta-knowledge informed meta-learning (MKIML) framework, as shown in Fig.1, which comprises an additional meta-knowledge integration into the meta learning pipeline. Moreover, such meta-knowledge could be obtained in an external, separated way from the meta-learning problem and the usual training tasks. This framework is expected to be functional in exploring an orthogonal meta-knowledge-driven approaches relative to previous purely meta-data-driven approaches to learn and evaluate meta-model. With the MKIML framework, we attempt to integrate meta-knowledge into meta-objective by means of a meta-regularization (MR) term. The key insight is that we leverage the benefits of fundamental properties of the meta-model for various training tasks, which should help achieve better generalization of meta-model to unseen tasks and alleviate the critical need of high-quality meta-data. Specifically, in this study we instantiate MR with the data augmentation consistency (DAC) as a new meta-objective for meta-learning. The DAC stems from recent advances in semi-supervised learning[xie2020unsupervised,sohn2020fixmatch], and to the best of our knowledge, we exploit it to meta-regularize the complexity of meta-model function class for the first time, which enforces the model facilitated by meta-model to output similar predictions under input data augmentations. Our contributions can be mainly summarized as follows. 1) We suggest a MKIML framework, as shown in Fig.1, aiming to improve capability of previous purely meta-data-driven meta-learning approaches by additionally integrating compensated meta-knowledge into meta learning process. Specifically, we explore to integrate meta-knowledge into meta-objective through designing an appropriate meta-regularizer (MR). The MR is functional on regularising the capacity complexity of meta-model function class, so as to improve its meta-level generalization on unseen tasks. 2) We introduce data augmentation consistency (DAC) to instantiate MR objective for an effective practical implementation (DAC-MR for brevity). The DAC-MR provides a general approach to help apply meta-learning models to tasks with noisy, sparse or unavailable meta-data. Besides, the DAC-MR is problem-agnostic, which can be generally applicable to extensive meta-learning problems and tasks. 3) We theoretically prove that the additional DAC-MR term in meta-objective can bring better meta-level generalization compared with solely meta-data-driven meta-loss objective. Meanwhile, we prove that DAC-MR is able to be regarded as a proxy meta-objective implicitly calculated on high-quality meta-data under some mild conditions. 4) We experimentally demonstrate that DAC-MR aids meta-model learning across various meta-learning problems in computer vision, including few-shot learning (§4), transfer learning (§5), continual learning (§6) and label noise learning (§7). Specifically, our DAC-MR is substantiated to be valid across 10 meta-learning tasks with different network architectures and testing benchmarks. Furthermore, these empirical results are well-aligned with our theoretical insights. The paper is organized as follows. §2discusses related work. §3presents the proposed MKIML framework, MR objective and our DAC-MR objective as a practical implementation for MKIML as well as its theoretical insights. We evaluate DAC-MR to few-shot learning in §4, transfer learning in §5, continual learning in §6and label noise learning in §7, respectively. The conclusion is finally made.
====================
	 Task-driven Compression for Collision Encoding based on Depth Images
==========
Abstract:  This paper contributes a novel learning-based method for aggressive task-driven compression of depth images and their encoding as images tailored to collision prediction for robotic systems. A novel 3D image processing methodology is proposed that accounts for the robot's size in order to appropriately "inflate" the obstacles represented in the depth image and thus obtain the distance that can be traversed by the robot in a collision-free manner along any given ray within the camera frustum. Such depth-and-collision image pairs are used to train a neural network that follows the architecture of Variational Autoencoders to compress-and-transform the information in the original depth image to derive a latent representation that encodes the collision information for the given depth image. We compare our proposed task-driven encoding method with classical task-agnostic methods and demonstrate superior performance for the task of collision image prediction from extremely low-dimensional latent spaces. A set of comparative studies show that the proposed approach is capable of encoding depth image-and-collision image tuples from complex scenes with thin obstacles at long distances better than the classical methods at compression ratios as high as 4050:1.
Fluency score: 4
Fluency issues: The text is generally fluent but can be improved by simplifying long sentences to enhance readability.

Clarity score: 3
Clarity issues: The use of technical jargon ("task-driven compression", "encoding", "inflate", "camera frustum", "Variational Autoencoders", "latent representation", "task-agnostic methods") may make it less accessible for readers not familiar with the field.

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: Lengthy sentences and specialized terminology could hinder understanding for a broader audience.

Coherence score: 4
Coherence issues: The structure is coherent but requires domain knowledge to fully appreciate the sequence and the connections between ideas presented.
==========
Introduction:  Methods for autonomous collision-free navigation of aerial robots have traditionally relied on motion planning techniques that exploit a dense map representation of the environment[28,3,24,27]. Departing from such methods, the community has recently investigated the potential of deep learning to develop navigation methods that act directly on exteroceptive data such as depth images instead of reconstructed maps in order to plan the aerial vehicle’s motions with minimal latency[15,16,22,12]. However, such methods face the challenge that exteroceptive data and especially depth images coming from stereo vision or other sensors are typically of very high dimensionality and the involved neural networks include layers that partially act as lossy information compression stages. This is reflected in the architectures of otherwise successful methods such as the works in[15,22,12]that exploit depth images to evaluate which among a set of candidate robot trajectories would collide or not. In[15]the input depth image involves more than300,000pixels (640\times 480resolution) but through stages of a pre-trained MobileNetV3 architecture it gets processed toMfeature vectors of size32each, whereMis the number of candidate trajectories for which this method derives collision scores. Eventually by combining the640\times 480pixels depth image with robot pose information, the method attempts to predict which amongMtrajectories are safe, thus representing a process of information downsampling and targeted inference. In other words, despite the dimensionality reduction taking place through the neural network it is attempted that the method still ensures collision avoidance. However, it is known that such techniques do not provide100\%success ratio especially in complex and cluttered scenes. Responding to the above, this work contributes the concept of task-driven compression and encoding of depth images as visualized in Figure1. Departing from the concept that methods aiming to predict the safety of candidate robot trajectories based on depth images should train collision prediction either a) directly in an end-to-end fashion through depth data[15,22]or through b) an explicit intermediate compression stage of the depth image itself[23], we propose the approach of using the depth image to encode a latent space presenting major dimensionality reduction that reflects not the depth image itself but instead a “collision image”. The latter is a remapping of the depth image that has accounted about the robot’s size and thus presents reduced overall complexity and greatly reduced presence of narrow/thin features that are hard-to-retain in an aggressive compression step. To achieve this goal, the method employs a probabilistic encoder-decoder architecture that is trained in a supervised manner such that given a depth image as input, it learns to encode and reconstruct the collision image. To train this collision-predicting network –dubbedDepth image-based Collision Encoder(DCE)– the depth image is first processed such that the collision image is calculated given information for the robot’s size. Focusing on aggressive dimensionality reduction, it is demonstrated that the scheme allows to get accurate reconstructions through a latent space that is more than3orders of magnitude smaller than the input image. The benefits of the approach are demonstrated through comparisons both with a conventional Variational Autoencoder (VAE) trained to encode the depth image and assessed regarding the extent to which the reconstructed image can serve as basis to derive a correct collision image, as well as traditional compression methods using the Fast Fourier Transform (FFT) and wavelets. In the remaining paper Section2presents related work and Section3details the proposed method involving generation of training data, image augmentation and the training of the neural network. Section4compares our proposed method against traditional image compression methods and evaluates the performance of task-driven and task-agnostic compression methods at similar degrees of compression. Finally, conclusions are drawn in Section5.
====================
	 Generative AI-empowered Simulation for Autonomous Driving in Vehicular Mixed Reality Metaverses
==========
Abstract:  In the vehicular mixed reality (MR) Metaverse, the distance between physical and virtual entities can be overcome by fusing the physical and virtual environments with multi-dimensional communications in autonomous driving systems. Assisted by digital twin (DT) technologies, connected autonomous vehicles (AVs), roadside units (RSU), and virtual simulators can maintain the vehicular MR Metaverse via digital simulations for sharing data and making driving decisions collaboratively. However, large-scale traffic and driving simulation via realistic data collection and fusion from the physical world for online prediction and offline training in autonomous driving systems are difficult and costly. In this paper, we propose an autonomous driving architecture, where generative AI is leveraged to synthesize unlimited conditioned traffic and driving data in simulations for improving driving safety and traffic efficiency. First, we propose a multi-task DT offloading model for the reliable execution of heterogeneous DT tasks with different requirements at RSUs. Then, based on the preferences of AV's DTs and collected realistic data, virtual simulators can synthesize unlimited conditioned driving and traffic datasets to further improve robustness. Finally, we propose a multi-task enhanced auction-based mechanism to provide fine-grained incentives for RSUs in providing resources for autonomous driving. The property analysis and experimental results demonstrate that the proposed mechanism and architecture are strategy-proof and effective, respectively.
Fluency score: 3
Fluency issues: [Wordiness and complex constructions ("multi-dimensional communications in autonomous driving systems", "multi-task DT offloading model") slightly hinder fluency]

Clarity score: 3
Clarity issues: [Complex sentences and academic jargon ("strategy-proof", "enhanced auction-based mechanism") can obscure meaning for lay readers]

Grammaticality score: 4
Grammaticality issues: [No significant grammatical issues, but sentence structure complexity potentially impacts readability]

Readability score: 3
Readability issues: [Lengthy sentences and specialized terminology present barriers to general understanding]

Coherence score: 3
Coherence issues: [Use of specialized terms without clear definitions and densely packed information affect overall coherence for those unfamiliar with the subject]
==========
Introduction:  The vehicular mixed reality (MR) Metaverse is envisioned as a promising solution for realizing autonomous driving by fusing the physical and virtual vehicular networks[46,40]. The multi-dimensional communications among physical and virtual entities can surrender the distance of “data islands” on roads for improving road safety and traffic efficiency while reducing energy consumption and carbon emissions[35]. Assisted by digital twin (DT) technologies, autonomous vehicles (AV) utilize advanced sensors, e.g., ultrasonic radars, cameras, and LiDAR, to collect data from their surrounding environments for constructing virtual representations in the virtual space[12]. Then, AVs can make driving decisions, such as driving model selection and motion planning, via artificial intelligence (AI) methods. Even though panoramic cameras and high-class LiDAR are equipped with AVs, each AV can only collect limited environment data and cannot perceive the whole environment, e.g., occlusions[32]. Therefore, multiple connected AVs, roadside units (RSUs), and virtual simulators can share and fuse sensing data in the virtual space, to perceive the complete information of environments including occlusions. However, it is difficult and costly to collect realistic driving data on a large scale to train AVs directly in the physical world. To address this issue, much effort from academia and industry has been devoted to developing platforms in the virtual space for traffic and driving simulations[7,8]. By establishing virtual driving simulation platforms with DT[14,12]and MR[27,6]technologies, virtual representations of AVs can efficiently collect traffic and training data and cheaply test it on rare cases, such as virtual traffic accidents and car collisions under realistic scenes[19,20]. Although traditional simulation platforms can generate an unlimited number of various driving experiences, the collected driving data requires a lot of manual work for labeling, which prevents the potential from being fully realized[7]. Fortunately, with the multi-modal generative AI[11,31,10], the labeled traffic and driving data can be synthesized directly for virtual autonomous driving systems[16]. In this way, the process of using simulation platforms for autonomous driving training and evaluation is revolutionized by shifting from collecting and labeling data to directly synthesizing labeled data[16,38]. Therefore, the simulation systems empowered by generative AI can generate large and diverse labeled driving datasets based on real-time road and weather conditions and user preferences for online prediction and offline training in autonomous driving systems. Furthermore, in the vehicular Metaverse, connected AVs, RSUs, and virtual simulators need to construct the traffic and driving simulation platforms in the virtual space collaboratively. To update with virtual representations in virtual space, AVs continuously generate and offload multiple computation-intensive DT tasks to RSUs in online traffic simulation[12]. Specifically, these DT tasks of each AV, including simulation, decision-making, and monitoring, are heterogeneous in requiring computing, communication resources, and deadlines. In driving simulations, virtual simulators synthesize controllable traffic and driving data for satisfying specific requirements, e.g., passenger preferences and weather conditions, of the simulated driving tasks[20]. In addition, the synthesized traffic and driving datasets can also be used in training virtual representations of AVs to further improve driving robustness. These synchronization activities, e.g., DT task execution, traffic and driving simulations, and AV training, are demanding enormous communication and computing resources of RSUs for supporting autonomous driving systems[44,39]. Therefore, developing effective multi-task incentive mechanisms that motivate RSUs to improve their use of communications and computing resources is imperative. As shown in Fig.1, in this paper, we propose a novel DT-assisted autonomous driving architecture for the vehicular MR Metaverse, where generative AI is leveraged to synthesize massive and conditioned traffic and driving data for online and offline simulations. In detail, to improve reliability in DT task execution, we propose a multi-task DT offloading model where AVs can offload heterogeneous DT tasks with different deadlines to RSUs for real-time execution. To improve reliability in driving decision-making, virtual simulators can utilize the information in DTs, such as current location, historical trajectory, and user preferences, for online traffic simulations[45,37]. Moreover, based on the collected sensing data in the physical world and user preferences in DTs, virtual simulators can synthesize massive and conditioned driving data for AV training of virtual simulators via running generative AI models. As a use case, we propose a diffusion model-based traffic sign generator, named TSDreamBooth, which is developed based on the DreamBooth[23]fine-tuned using Belgium traffic sign (BelgiumTS) dataset[18]. The TSDreamBooth can be leveraged to generate virtual traffic sign images under different driving conditions and user preferences. Finally, we propose a multi-task enhanced auction-based mechanism to satisfy multi-dimensional requirements (e.g., prices and deadlines) of multiple DT tasks. We analyze the properties of the proposed auction and prove that it is strategy-proof and adverse-selection free. The experimental results demonstrate that the proposed framework can increase total social surplus by 150%. Our main contributions are summarized as follows: To improve the safety and reliability of autonomous driving, we propose a novel DT-assisted MR Metaverse architecture with MR simulations empowered by generative AI. In this architecture, connected AVs, RSUs, and virtual simulators maintain digital simulation platforms in the virtual space, where data collecting, sharing, and utilizing among physical and virtual entities can improve driving safety and traffic efficiency in physical autonomous driving systems. In this architecture, we propose a reliable DT task offloading framework where AVs can continuously offload multiple DT tasks with different requirements to RSUs for updating DTs in the virtual space. In traffic and driving simulations, we consider generative AI-empowered virtual simulators to synthesize new driving data for AVs’ decision-making and training. To incentivize RSUs for providing resources in supporting autonomous driving systems, we propose a multi-task enhanced auction-based mechanism to offer fine-grained allocation results and prices for executing heterogeneous DT tasks with various deadlines. Based on the property analysis, the proposed mechanism is fully strategy-proof and adverse-selection free. The rest of this paper is organized as follows. In SectionII, we review the related works. In SectionIII, we discuss the proposed system architecture and its system model. Then, in SectionIV, we implement the multi-task enhanced auction-based mechanism. We demonstrate the experimental results in SectionV, and provide a conclusion in SectionVI.
====================
	 Guide the Learner: Controlling Product of Experts Debiasing Method Based on Token Attribution Similarities
==========
Abstract:  Several proposals have been put forward in recent years for improving out-of-distribution (OOD) performance through mitigating dataset biases. A popular workaround is to train a robust model by re-weighting training examples based on a secondary biased model. Here, the underlying assumption is that the biased model resorts to shortcut features. Hence, those training examples that are correctly predicted by the biased model are flagged as being biased and are down-weighted during the training of the main model. However, assessing the importance of an instance merely based on the predictions of the biased model may be too naive. It is possible that the prediction of the main model can be derived from another decision-making process that is distinct from the behavior of the biased model. To circumvent this, we introduce a fine-tuning strategy that incorporates the similarity between the main and biased model attribution scores in a Product of Experts (PoE) loss function to further improve OOD performance. With experiments conducted on natural language inference and fact verification benchmarks, we show that our method improves OOD results while maintaining in-distribution (ID) performance.
Fluency score: 4
Fluency issues: [The sentence structure is mostly smooth, but the heavy use of jargon and long, complex sentences may hinder fluency for readers not familiar with the topic.]

Clarity score: 4
Clarity issues: [Technical language and dense construct of sentences might obscure clarity for a general audience.]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 4
Readability issues: [There's a high density of information and terminology that could challenge the readability for laypersons.]

Coherence score: 4
Coherence issues: [Use of acronyms and technical terms without explanation, complex sentence structures require re-reading for full understanding.]
==========
Introduction:  Overfitting to the training data is a big obstacle in learning patterns that generalize to unseen data. Traditionally, this is diagnosed by monitoring the performance of a trained model on an in-distribution (ID) test set. However, a bigger challenge is when both the training and test data have the same non-generalizable patterns, emerged as spurious correlations between input features and output labelsGardneret al.(2021). For instance, in the natural language inference (NLI) task, it is shown that the occurrence of some task-neutral words, like a negation in hypothesis, is highly correlated with a specific class(Gururanganet al.,2018). While high-capacity models can learn a generalized distribution of labels from the inputs, they are prone to spurious patterns, also known as dataset biasesClarket al.(2019); Heet al.(2019). A model could exploit these biases during fine-tuning, leading to a model that achieve high ID performance, while it is highly fragile in out-of-distribution (OOD) settings(Schusteret al.,2019; McCoyet al.,2020). Besides trying to prevent these non-generalizable artifacts from entering the datasetLiuet al.(2022), it is reasonable to seek for more robust learning methods. This has been the basis for a multitude of research works that encourage models to rely on truly generalizable patterns. Most of these methods are based on the assumption that the learning method will inevitably exploit biases if they are present in a training example(Clarket al.,2019; Sanhet al.,2020; Mahabadiet al.,2020; Utamaet al.,2020; Ghaddaret al.,2021). Therefore, they discourage the main model from paying much attention to the examples which are correctly classified by a biased model. Recently, it is shown that this assumption is questionable in the way that for a significant number of cases, the main model does not follow the biased model in treating biased examples(Amirkhani and Pilehvar,2021). Therefore, depriving the training algorithm from the examples which are detected to be biased is a waste of training data. In this paper, we propose an alternative way to discard biased examples. Instead of considering the mere prediction of the biased model, we monitor the way the model processes each example by computing its attribution scores over the input tokens. With the resulting scores, we adjust the proportion of the loss function that is a cross-entropy loss (CE) versus a Product of Experts loss (PoE). If the attribution scores are similar between the main and biased models, the loss becomes a PoE loss where a correct prediction from the biased model down-weights the contribution of the corresponding example. In contrast, dissimilarity between the scores suggests a different behaviour from the biased model and leads to a CE loss that only considers the main model’s prediction. Experiments on natural language inference and fact verification demonstrate that our method significantly outperforms previous approaches in terms of OOD performance while preserving its ID performance.
====================
	 Bridging the Gap: Fine-to-Coarse Sketch Interpolation Network for High-Quality Animation Sketch Inbetweening
==========
Abstract:  The 2D animation workflow is typically initiated with the creation of keyframes using sketch-based drawing. Subsequent inbetweens (i.e., intermediate sketch frames) are crafted through manual interpolation for smooth animations, which is a labor-intensive process. Thus, the prospect of automatic animation sketch interpolation has become highly appealing. However, existing video interpolation methods are generally hindered by two key issues for sketch inbetweening: 1) limited texture and colour details in sketches, and 2) exaggerated alterations between two sketch keyframes. To overcome these issues, we propose a novel deep learning method, namely Fine-to-Coarse Sketch Interpolation Network (FC-SIN). This approach incorporates multi-level guidance that formulates region-level correspondence, sketch-level correspondence and pixel-level dynamics. A multi-stream U-Transformer is then devised to characterize sketch inbewteening patterns using these multi-level guides through the integration of both self-attention and cross-attention mechanisms. Additionally, to facilitate future research on animation sketch inbetweening, we constructed a large-scale dataset - STD-12K, comprising 30 sketch animation series in diverse artistic styles. Comprehensive experiments on this dataset convincingly show that our proposed FC-SIN surpasses the state-of-the-art interpolation methods. Our code and dataset will be publicly available.
Fluency score: 4
Fluency issues: Use of specialized jargon that may impede fluency for general audiences.

Clarity score: 4
Clarity issues: Dense technical language and acronyms like "FC-SIN" could confuse readers unfamiliar with the topic.

Grammaticality score: 4
Grammaticality issues: Minor issues such as "sketch inbewteening patterns” (should be "inbetweening").

Readability score: 3
Readability issues: Dense technical concepts and a high volume of specialized information might be challenging for a general reader.

Coherence score: 4
Coherence issues: The intricate subject matter and use of multiple specialized terms may affect the coherence for readers without relevant background knowledge.
==========
Introduction:  The hand-drawn 2D animation workflow typically involves three key stages: sketching keyframes, inbetweening keyframes to produce intermediate sketch frames (i.e., inbetweens), and colorization to produce the final, full-color animations. The meticulous creation of inbetweens is crucial for achieving a smooth animation with lifelike motion transitions, effectively conveying the intended story or message. For a feature-length animation created through this process, the sheer volume of required inbetweens can be staggeringThomaset al.(1995), making it a highly specialized and labor-intensive task and serving as a limiting factor in overall animation productivity. To streamline the process of 2D sketch animation production, various studies have focused on the automatic synthesis of inbetweening sketch frames, which take two consecutive sketch keyframes as input and produce interpolated intermediate sketch frames (i.e., inbetweens) as output. These methods can be categorised into stroke-based and image-based. The stroke-based methods often rely on a cumbersome pre-processing step for sketch vectorisationWhitedet al.(2010); Yang (2017); Yanget al.(2018), which require additional specialized software or techniques. Image-based methods treat sketch frames as bitmap images, applying conventional image or video interpolation algorithms. However, they commonly face two significant challenges: 1) the absence of texture and color details in sketch frames, hindering reliable image-based inbetweening correspondence, and 2) exaggerated changes due to substantial object movements between two consecutive sketch keyframesNaritaet al.(2019). As a result, when image-based methods, especially devised for videosZhouet al.(2023)and colour animationsSiyaoet al.(2021); Chen and Zwicker (2022), are applied to sketch interpolation, they invariably introduce various artifacts into the produced interpolated frames. These discrepancies can adversely affect the continuity and quality of the animation produced. As illustrated in the Figure1, LDFINaritaet al.(2019)proposed for sketch interpolation generates broken strokes due to the missing sketch keypoint correspondence, while EISAIChen and Zwicker (2022)proposed for interpolating color animation frames and DQBCZhouet al.(2023)for video interpolation introduce blurriness (ornaments) and artifacts (e.g., distortion in face regions) in their results. Therefore, in this study, we propose a novel deep learning method for sketch interpolation, the Fine-to-Coarse Sketch Interpolation Network (FC-SIN), to comprehend and model the intricate and sparse patterns found in animation sketches. Specifically, FC-SIN adopts a fine-to-coarse approach that integrates multi-level guidance through three distinct aspects: 1)pixel-level dynamicsat a fine level with a bi-directional optical flow estimation module, 2)sketch-level correspondencewith a sketch matching and tracking mechanism for obtaining sketch keypoint traces and 3)region-level correspondenceat a coarse level with a region matching and bi-directional optical flow aggregation module. Guided by these multi-level perspectives, a multi-stream U-Transformer architecture is further devised to produce the intermediate sketch frames. It consists of two attention-based building blocks:convolution and self-attention block(CSB) and theconvolution and cross-attention block(CCB) to leverage the diverse multi-level insights for producing precise inbetween sketch patterns. To facilitate the research on animation sketch interpolation, we constructed a large-scale sketch triplet dataset, namely STD-12K, from 30 sketch animation series (e.g., over 25 hours) with various artistic styles. Comprehensive experimental results demonstrate that the proposed method FC-SIN clearly outperforms the state-of-the-art frame interpolation methods for animation sketch interpolation. Overall, the key contributions of this study are as follows: A novel deep learning architecture, FC-SIN, for sketch interpolation by effectively formulating sparse sketch patterns with fine-to-coarse multi-level guidance. A novel self- and cross-attention based multi-stream U-Transformer formulated with the guidance from multi-level perspectives. A large-scale sketch triplet dataset with various artistic styles constructed for the research community.
====================
	 Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters
==========
Abstract:  Speech representations learned in a self-supervised fashion from massive unlabeled speech corpora have been adapted successfully toward several downstream tasks. However, such representations may be skewed toward canonical data characteristics of such corpora and perform poorly on atypical, non-native accented speaker populations. With the state-of-the-art HuBERT model as a baseline, we propose and investigate self-supervised adaptation of speech representations to such populations in a parameter-efficient way via training accent-specific residual adapters. We experiment with 4 accents and choose automatic speech recognition (ASR) as the downstream task of interest. We obtain strong word error rate reductions (WERR) over HuBERT-large for all 4 accents, with a mean WERR of 22.7% with accent-specific adapters and a mean WERR of 25.1% if the entire encoder is accent-adapted. While our experiments utilize HuBERT and ASR as the downstream task, our proposed approach is both model and task-agnostic.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: Use of technical terms like "self-supervised adaptation," "residual adapters," and abbreviations like "WERR" can be confusing without context.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Dense technical details and jargon such as "canonical data characteristics," "parameter-efficient way," and "downstream task" may hinder readability for non-specialists.

Coherence score: 5
Coherence issues: None
==========
Introduction:  Self-supervised learning has been a dominant paradigm in natural language processing (NLP)[1]and in recent years, it has also been adopted by the speech community to learn high-fidelity representations[2,3,4,5]that capture various non-lexical aspects of speech and audio such as lip-smacking, laughter, hesitation, etc. In this paradigm, the targets to learn are derived from the input signal itself, making the learned representations more powerful in principle compared to those learned using textual labels and annotations of any kind. These powerful base representations have been successfully adopted for several downstream tasks[6], some of which include: ASR, speaker identification and speech translation. Pre-training models with a very large number of parameters on proportionally large datasets has been a central theme in self-supervised learning. However, these datasets may understandably fall short in terms of sufficiently capturing non-canonical and diverse speech and audio characteristics such as rare non-native accents, stammering, etc. This leads to great disparity in downstream task performance across well-represented and underrepresented speaker populations. This data problem has also existed with supervised models for specific tasks such as ASR and in such scenarios, the typical path has been to patch task performance by collecting task-specific labeled datasets with non-canonical characteristics and fine-tuning for the task[7]. This unfortunately entangles speech and audio characteristics with the task itself, which can limit effective learning of such characteristics in task-specific representations as well as limiting their re-usability across tasks. In this paper, we consequently posit that continued self-supervised learning of speech and audio representations on task-agnostic unlabeled datasets is an effective strategy to adapt to non-canonical speech characteristics. The specific characteristic we choose to study is accents but the methodology holds for any characteristic. We propose learning different high-dimensional spaces for different accents via independently adding residual adapters for each target accent to the model and continuing pre-training on accent-specific datasets. Since residual adapters are parameter-wise much smaller than the base model, this serves as a parameter-efficient way for personalized adaptation without over-fitting and saves on storage costs for inference since only a single copy of the base model needs to be stored. We conduct our experiments with HuBERT-large[2]as the base model and ASR as the downstream task but posit that our proposed approach is both model and task agnostic. Our chosen base model is a state-of-the-art model with low word error rates on canonical datasets such as LibriSpeech. By design, we pick 4 non-native English accents where the HuBERT-large model has high word error rates (WER), in the range 24-50% and show strong results on all 4 accents with over 22% WERR over the baseline. Previous work has shown improvements in WER on such accents by supervised training using labeled datasets[7]. In contrast, we achieve our WER improvements by continuing to self-supervise models using unlabeled data alone. We show that the gains from adapting to an accent using a particular dataset translate to other evaluation sets with the same accent as well, indicating that the effectiveness of our approach is due to adaptation to the accents’ acoustic characteristics and not other confounding factors. Finally, we also explore the degree of parameter-efficiency possible when adapting to target accents, finding that we can achieve strong WERR over the baseline while updating only 16% of the total model parameters.
====================
	 Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs
==========
Abstract:  Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language(V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains.
Fluency score: 5
Fluency issues: 

Clarity score: 4
Clarity issues: [The phrase "communicate the salient information hidden behind them" is somewhat abstract and could be made clearer; "the underlined table data" might confuse readers if they interpret "underlined" literally instead of as "underlying".]

Grammaticality score: 3
Grammaticality issues: ["underlined table data of chart figures" should be "underlying table data of chart figures".]

Readability score: 4
Readability issues: [Complex sentence structures and jargon such as "cross-modal pre-training on plot table pairs" and "pre-training objectives" may hinder readability for nonspecialists.]

Coherence score: 4
Coherence issues: [Slightly abrupt transition into the introduction of ChartT5; coherence could be improved by linking previous sentences more smoothly into the introduction of the concept.]
==========
Introduction:  Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient’s question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V+L) tasks and many related benchmarks like Chart Question Answering (CQA)2022-chartqa;kafle2018dvqa;Methani_2020_WACVand Chart Summarization (CS)2022-chartare introduced. While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V+L research, applying these existing methods to handle chart-related tasks is hard. Recent research ChartQA2022-chartqaand Chart-to-Text2022-chartattempt to first convert chart images to their underlined tables and use the extracted tables to perform chart-related V+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS. Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1) Automatic table extraction is unreliable. Existing methodsluo2021chartocr;parsinglineare often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2) In most cases, the whole table is optional for resolving the chart-related V+L task. As illustrated in Fig1, to answer the question"What is the value of India Bar", the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V+L model with the capability to dynamically interpret the table value from the chart information. Therefore, in this paper, we proposeChartT5, an OCR-based image-to-text generation model pre-trained on a self-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model’s capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value. We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart-to-Text. In ChartQA, ChartT5 outperforms all the non-pretraining methods that use extracted tables by at least 8\%performance gains. ChartT5 also beats the pre-training table-based methods, which demonstrates the effectiveness of the proposed pre-training strategies. On Chart-to-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metricsbarzilay-2005-collectivewhich values the model’s capability to extract the critical information from the chart. In summary, our contributions are summarized below: We propose chart-to-table pre-training for V+L model to learn the capability of interpreting table data from the chart. We demonstrate that the pre-trained model consistently outperforms table-based methods on two chart understanding tasks. We conduct comprehensive ablation studies to validate the effectiveness of chart-to-table pre-training and the proposed pre-training objectives.
====================
	 Co-Learning Empirical Games and World Models
==========
Abstract:  Game-based decision-making involves reasoning over both world dynamics and strategic interactions among the agents. Typically, empirical models capturing these respective aspects are learned and used separately. We investigate the potential gain from co-learning these elements: a world model for dynamics and an empirical game for strategic interactions. Empirical games drive world models toward a broader consideration of possible game dynamics induced by a diversity of strategy profiles. Conversely, world models guide empirical games to efficiently discover new strategies through planning. We demonstrate these benefits first independently, then in combination as realized by a new algorithm, Dyna-PSRO, that co-learns an empirical game and a world model. When compared to PSRO -- a baseline empirical-game building algorithm, Dyna-PSRO is found to compute lower regret solutions on partially observable general-sum games. In our experiments, Dyna-PSRO also requires substantially fewer experiences than PSRO, a key algorithmic advantage for settings where collecting player-game interaction data is a cost-limiting factor.
Fluency score: 4
Fluency issues: [The sentence "Empirical games drive world models toward a broader consideration of possible game dynamics induced by a diversity of strategy profiles." is slightly awkward and could be simplified for better fluency.]

Clarity score: 4
Clarity issues: ["game dynamics induced by a diversity of strategy profiles" could be unclear to readers unfamiliar with game theory.]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [Complex sentence structures and specialized terminology such as "Dyna-PSRO," "empirical game," and "world model" can make the text challenging for a general audience.]

Coherence score: 4
Coherence issues: [The connection between empirical games, world models, and their co-learning benefits could be more explicitly articulated to improve coherence.]
==========
Introduction:  Even seemingly simple games can actually embody a level of complexity rendering them intractable to direct reasoning. This complexity stems from the interplay of two sources: dynamics of the game environment, and strategic interactions among the game’s players. As an alternative to direct reasoning, models have been developed to facilitate reasoning over these distinct aspects of the game.Empirical gamescapture strategic interactions in the form of payoff estimates for joint policiesWellman (2006).World modelsrepresent a game’s transition dynamics and reward signal directlySutton and Barto (2018); Ha and Schmidhuber (2018b). Whereas each of these forms of model have been found useful for game reasoning, typical use in prior work has focused on one or the other, learned and employed in isolation from its natural counterpart. Co-learning both models presents an opportunity to leverage their complementary strengths as a means to improve each other. World models predict successor states and rewards given a game’s current state and action(s). However, their performance depends on coverage of their training data, which is limited by the range of strategies considered during learning. Empirical games can inform training of world models by suggesting a diverse set of salient strategies, based on game-theoretic reasoningWellman (2006). These strategies can expose the world model to a broader range of relevant dynamics. Moreover, as empirical games are estimated through simulation of strategy profiles, this same simulation data can be reused as training data for the world model. Strategic diversity through empirical games, however, comes at a cost. In the popular framework of Policy-Space Response Oracles (PSRO)Lanctotet al.(2017), empirical normal-form game models are built iteratively, at each step expanding a restricted strategy set by computing best-response policies to the current game’s solution. As computing an exact best-response is generally intractable, PSRO uses Deep Reinforcement Learning (DRL) to compute approximate response policies. However, each application of DRL can be considerably resource-intensive, necessitating the generation of a vast amount of gameplays for learning. Whether gameplays, or experiences, are generated via simulationObando-Ceron and Castro (2021)or from real-world interactionsHester and Stone (2012), their collection poses a major limiting factor in DRL and by extension PSRO. World models present one avenue to reduce this cost by transferring previously learned game dynamics across response computations. We investigate the mutual benefits of co-learning a world model and an empirical game by first verifying the potential contributions of each component independently. We then show how to realize the combined effects in a new algorithm,Dyna-PSRO, that co-learns a world model and an empirical game (illustrated in Figure1). Dyna-PSRO extends PSRO to learn a world model concurrently with empirical game expansion, and applies this world model to reduce the computational cost of computing new policies. This is implemented by a Dyna-based reinforcement learnerSutton (1990,1991)that integrates planning, acting, and learning in parallel. Dyna-PSRO is evaluated against PSRO on a collection of partially observable general-sum games. In our experiments, Dyna-PSRO found lower-regret solutions while requiring substantially fewer cumulative experiences. The main points of novelty of this paper are as follows: (1) empirically demonstrate that world models benefit from the strategic diversity induced by an empirical game; (2) empirically demonstrate that a world model can be effectively transferred and used in planning with new other-players. The major contribution of this work is a new algorithm, Dyna-PSRO, that co-learns an empirical game and world model finding a stronger solution at less cost than the baseline, PSRO.
====================
	 Cross-domain Iterative Network for Simultaneous Denoising, Limited-angle Reconstruction, and Attenuation Correction of Low-dose Cardiac SPECT
==========
Abstract:  Single-Photon Emission Computed Tomography (SPECT) is widely applied for the diagnosis of ischemic heart diseases. Low-dose (LD) SPECT aims to minimize radiation exposure but leads to increased image noise. Limited-angle (LA) SPECT enables faster scanning and reduced hardware costs but results in lower reconstruction accuracy. Additionally, computed tomography (CT)-derived attenuation maps ($\mu$-maps) are commonly used for SPECT attenuation correction (AC), but it will cause extra radiation exposure and SPECT-CT misalignments. In addition, the majority of SPECT scanners in the market are not hybrid SPECT/CT scanners. Although various deep learning methods have been introduced to separately address these limitations, the solution for simultaneously addressing these challenges still remains highly under-explored and challenging. To this end, we propose a Cross-domain Iterative Network (CDI-Net) for simultaneous denoising, LA reconstruction, and CT-free AC in cardiac SPECT. In CDI-Net, paired projection- and image-domain networks are end-to-end connected to fuse the emission and anatomical information across domains and iterations. Adaptive Weight Recalibrators (AWR) adjust the multi-channel input features to enhance prediction accuracy. Our experiments using clinical data showed that CDI-Net produced more accurate $\mu$-maps, projections, and reconstructions compared to existing approaches that addressed each task separately. Ablation studies demonstrated the significance of cross-domain and cross-iteration connections, as well as AWR, in improving the reconstruction performance.
Fluency score: 4
Fluency issues: The text is slightly dense with technical terminology which could impede fluency for a general audience.

Clarity score: 3
Clarity issues: Dense use of technical jargon and acronyms (e.g., LD, LA, AC, $\mu$-maps, CDI-Net, AWR) may confuse readers not familiar with the subject.

Grammaticality score: 5
Grammaticality issues: None detected.

Readability score: 3
Readability issues: The technical nature of the text and the frequent introduction of acronyms may negatively affect readability for a non-specialized audience.

Coherence score: 4
Coherence issues: While the text logically describes a progression from problem to solution, the heavy use of jargon and acronyms might disrupt the flow for some readers.
==========
Introduction:  Cardiac Single-Photon Emission Computed Tomography (SPECT) is the most widely performed non-invasive exam for clinical diagnosis of ischemic heart diseases[7,10]. Reducing the tracer dose can lower patient radiation exposure, but it will result in increased image noise[22,8]. Acquiring projections in fewer angles using fewer detectors allows for faster scanning and lower hardware costs, but it also leads to decreased reconstruction accuracy[1,13]. Additionally, in clinical practice, computed tomography (CT)-derived attenuation maps (\mu-maps) are commonly used for SPECT attenuation correction (AC)[3,11]. However, most SPECT scanners are stand-alone without the assistance of CT[16]. The CT scan also causes additional radiation exposure and SPECT-CT misalignments[18,11]. Deep learning-based methods have been extensively explored to address the aforementioned limitations individually. To reduce image noise in low-dose (LD) SPECT, convolutional neural networks (CNNs) were employed to process the LD projection, producing the full-dose (FD) projection for SPECT reconstruction[21,1]. Similarly, to perform limited-angle (LA) reconstruction, the LA projection was input to CNNs to predict the full-angle (FA) projection[2,23,20]. In addition, a dual-domain approach, known as Dual-domain Sinogram Synthesis (DuDoSS), utilized the image-domain output as a prior estimation for the projection domain to predict the FA projection[5]. For the CT-free AC, CNNs were used to generate pseudo attenuation maps (\mu-maps) from SPECT emission images[19,6]. Although various methods have been developed to address these limitations individually, it is of great interest to address all these limitations simultaneously to enable CT-free, low-dose, low-cost, and accelerated SPECT, which could potentially lead to better performance on those separated but correlated tasks. Thus, we propose a Cross-Domain Iterative Network (CDI-Net) for simultaneous denoising, LA reconstruction, and CT-free AC in cardiac SPECT. In CDI-Net, projection and image-domain networks are end-to-end connected to fuse the predicted emission and anatomical features across domains and iterations. Adaptive Weight Recalibrators (AWR) calibrate the fused features to improve the prediction accuracy. We tested CDI-Net using clinical data and compared it to existing methods. Ablation studies were conducted to verify the impact of cross-domain, cross-iteration fusions and AWR on enhancing network performance.
====================
	 Plug-in Performative Optimization
==========
Abstract:  When predictions are performative, the choice of which predictor to deploy influences the distribution of future observations. The overarching goal in learning under performativity is to find a predictor that has low \emph{performative risk}, that is, good performance on its induced distribution. One family of solutions for optimizing the performative risk, including bandits and other derivative-free methods, is agnostic to any structure in the performative feedback, leading to exceedingly slow convergence rates. A complementary family of solutions makes use of explicit \emph{models} for the feedback, such as best-response models in strategic classification, enabling significantly faster rates. However, these rates critically rely on the feedback model being well-specified. In this work we initiate a study of the use of possibly \emph{misspecified} models in performative prediction. We study a general protocol for making use of models, called \emph{plug-in performative optimization}, and prove bounds on its excess risk. We show that plug-in performative optimization can be far more efficient than model-agnostic strategies, as long as the misspecification is not too extreme. Altogether, our results support the hypothesis that models--even if misspecified--can indeed help with learning in performative settings.
Fluency score: 4
Fluency issues: Use of technical jargon and symbols like "\emph{}" which are not common in natural language may reduce general fluency for non-expert readers.

Clarity score: 3
Clarity issues: The use of specialized terms such as "performative risk", "bandits", and "strategic classification" without explanations can confuse readers unfamiliar with the topic.

Grammaticality score: 5
Grammaticality issues: None detected.

Readability score: 3
Readability issues: The text is dense with specialized terms and concepts that make it less accessible to a general audience.

Coherence score: 3
Coherence issues: The text assumes prior knowledge of specific theoretical concepts without providing context, which may impair coherence for readers without a background in the subject.
==========
Introduction:  Predictions have the power to influence the patterns they aim to predict. For example, stock price predictions inform trading decisions and hence prices; traffic predictions influence routing decisions and thus traffic outcomes; recommendations shape users’ consumption and thus preferences. This pervasive phenomenon has been formalized in a technical framework calledperformative predictionPerdomoet al.(2020). A central feature that distinguishes the framework from traditional supervised learning is the concept of adistribution map\mathcal{D}(\cdot). This object, aimed to capture the feedback from predictions to future observations, is a mapping from predictorsf_{\theta}to their induced data distributions\mathcal{D}(\theta). The overarching goal in performative prediction is thus to deploy a predictorf_{\theta}that will have good performance after its deployment, that is, on its induced data distribution\mathcal{D}(\theta). Formally, the goal is to choose predictor parameters\theta\in\Theta\subseteq\mathbb{R}^{d_{\theta}}so as to minimize theperformative risk: where\ell(z;\theta)measures the loss incurred by predicting on instancezwith model\theta. Typically,zis a feature–outcome pair(x,y). We refer to\theta_{\mathrm{PO}}=\operatorname*{arg\,min}_{\theta\in\Theta}\mathrm{PR}(\theta)as theperformative optimum. The main challenge in optimizing the performative risk lies in the fact that the map\mathcal{D}(\cdot)is not known. We only observe samples from\mathcal{D}(\theta)for models\thetathat have been deployed; we do not observe any feedback for other models, of which there are typically infinitely many. A key discriminating factor between existing solutions for optimizing under performativity is how they cope with this uncertainty. One group of methods accounts for the feedback without assuming a problem-specific structure for it. This group includes bandit strategiesKleinberget al.(2008); Jagadeesanet al.(2022)and derivative-free optimizationFlaxmanet al.(2004); Milleret al.(2021). These methods converge to optima at typically slow—without convexity, even exponentially slow—convergence rates. Moreover, their rates rely on unverifiable regularity conditions that are out of the learner’s control, such as convexity of the performative riskMilleret al.(2021); Izzoet al.(2021); Donget al.(2018)or bounded performative effectsJagadeesanet al.(2022). A complementary group of methods—an important starting point for this work—takes feedback into account by positing explicitmodelsfor it. Such models include best-response models for strategic classificationHardtet al.(2016); Jagadeesanet al.(2021); Levanon and Rosenfeld (2021); Ghalmeet al.(2021), rational-agent models in economicsSpence (1978); Wooldridge (2003), and parametric distribution shiftsIzzoet al.(2021); Milleret al.(2021); Izzoet al.(2022), among others. To argue that the methods building on such models find optimal solutions, existing analyses assume that the model iswell-specified. However, models of social behavior are widely acknowledged to be simplistic representations of real-world dynamics. Yet, despite the unavoidable misspecification of models, they are ubiquitous in practice. Though their simplicity leads to misspecification, it also allows for efficient, interpretable, and practical solutions. Motivated by this observation, in this work we ask:can models for performative feedback be useful, even if misspecified? We initiate a study of the benefits of modeling feedback in performative prediction. We show that models—even if misspecified—can indeed help with learning under performativity. We begin by defining a general protocol for performative optimization with feedback models, which we callplug-in performative optimization. The protocol consists of three steps. First, the learner deploys models\theta_{i}\sim\tilde{\mathcal{D}}and collects dataz_{i}\sim\mathcal{D}(\theta_{i}),i\in[n]. Here,\tilde{\mathcal{D}}is an exploration distribution of the learner’s choosing (for example, it can be uniform on\Thetawhen\Thetais bounded). The second step is to use the observations\{(\theta_{i},z_{i})\}_{i=1}^{n}to fit an estimate of the distribution map. The map is chosen from a parametric family of possible maps\bm{\mathcal{D}}_{\mathcal{B}}=\{\mathcal{D}_{\beta}\}_{\beta\in\mathcal{B}}, obtained through modeling. The estimation of the map thus reduces to computing an estimate\hat{\beta}. For example, in strategic classification,\betacould be a parameter quantifying the strategic agents’ tradeoff between utility and cost. Finally, the third step is to compute theplug-in performative optimum: We prove a general excess-risk bound on\mathrm{PR}(\hat{\theta}_{\mathrm{PO}})-\mathrm{PR}(\theta_{\mathrm{PO}}), showing that the error decomposes into two terms. The first is amisspecification error term,MisspecErr, which captures the gap between the true performative risk and the plug-in performative risk\mathrm{PR}^{\hat{\beta}}(\theta)in the large-sample regime. This term is irreducible and does not vanish as the sample sizengrows. The second is astatistical error termthat captures the imperfection in fitting\hat{\beta}due to finite samples. For a broad class of problems, our main result can be summarized as follows. The excess risk of the plug-in performative optimum is bounded by: for some universal constantc>0. Therefore, although the misspecification error is irreducible, the statistical error vanishes at afast rate. In contrast, model-agnostic strategies such as bandit algorithmsKleinberget al.(2008); Jagadeesanet al.(2022)do not suffer from misspecification but have an exceedingly slow, often exponentially slow, statistical rate. For example, the bandit algorithm of Jagadeesan et al.Jagadeesanet al.(2022)has an excess risk of\tilde{O}\left(n^{-\frac{1}{d_{\theta}+1}}\right). This is why feedback models are useful—for a finiten, their excess risk can be far smaller than the risk of a model-agnostic strategy due to the rapidly vanishing statistical rate. The statistical rate is fast because it only depends on the parametric estimation rate of\hat{\beta}; it does not depend on the complexity of\mathrm{PR}. One important special case of performative prediction isstrategic classification. We apply our general theory to common best-response models in strategic classification. We also conduct numerical evaluations that empirically confirm our theoretical findings. Overall our results support the use of models in optimization under performative feedback. We give an overview of existing threads most closely related to our work. We build on the growing body of work studying performative predictionPerdomoet al.(2020). Existing work studies different variants of retrainingPerdomoet al.(2020); Mendler-Dünneret al.(2020); Drusvyatskiy and Xiao (2022), which converge to so-called performatively stable solutions, as well as methods for finding performative optimaMilleret al.(2021); Izzoet al.(2021); Jagadeesanet al.(2022). The methods in the latter category are largely model-agnostic and as such converge at slow rates. Exceptions include the study of parametric distribution shiftsIzzoet al.(2021,2022)and location familiesMilleret al.(2021); Jagadeesanet al.(2022), but those analyses crucially rely on the model being well-specified. We are primarily interested in misspecified settings. Other work in performative prediction includes the study of time-varying distribution shiftsBrownet al.(2022); Izzoet al.(2022); Li and Wai (2022); Rayet al.(2022), multi-agent settingsDeanet al.(2022); Qianget al.(2022); Naranget al.(2022); Piliouras and Yu (2022), and causality and robustnessMaheshwariet al.(2022); Mendler-Dünneret al.(2022); Kim and Perdomo (2022), among others; it would be valuable to extend our theory on the use of models to those settings. Strategic classificationHardtet al.(2016); Donget al.(2018); Levanon and Rosenfeld (2021); Zrnicet al.(2021), as well as other problems studying strategic agent behavior, frequently use models of agent behavior in order to compute Stackelberg equilibria, which are direct analogues of performative optima. However, convergence to Stackelberg equilibria assumes correctness of the models, a challenge we circumvent in this work. We use strategic classification as a primary domain of application of our general theory. Our work is partially inspired by works in statistics studying the benefits and impact of modeling, including under misspecificationWhite (1980,1982); Bujaet al.(2019a,b). At a technical level, our results are related to M-estimationVan der Vaart (2000); Geer (2000); Mouet al.(2019), as well as semi-parametric statisticsTsiatis (2007); Kennedy (2022), where the goal is to find models that lead to minimal estimation error. Plug-in performative optimization serves as an alternative to black-box baselines for zeroth-order optimization, which have previously been studied in the performative prediction literature. These include bandit algorithmsKleinberget al.(2008); Jagadeesanet al.(2022)and zeroth-order convex optimization algorithmsFlaxmanet al.(2004); Milleret al.(2021). As mentioned earlier, we show that the use of models can give far smaller excess risk, given the fast convergence rates of parametric learning problems.
====================
	 Approaching Test Time Augmentation in the Context of Uncertainty Calibration for Deep Neural Networks
==========
Abstract:  With the rise of Deep Neural Networks, machine learning systems are nowadays ubiquitous in a number of real-world applications, which bears the need for highly reliable models. This requires a thorough look not only at the accuracy of such systems, but also to their predictive uncertainty. Hence, we propose a novel technique (with two different variations, named M-ATTA and V-ATTA) based on test time augmentation, to improve the uncertainty calibration of deep models for image classification. Unlike other test time augmentation approaches, M/V-ATTA improves uncertainty calibration without affecting the model's accuracy, by leveraging an adaptive weighting system. We evaluate the performance of the technique with respect to different metrics of uncertainty calibration. Empirical results, obtained on CIFAR-10, CIFAR-100, as well as on the benchmark Aerial Image Dataset, indicate that the proposed approach outperforms state-of-the-art calibration techniques, while maintaining the baseline classification performance. Code for M/V-ATTA available at: https://github.com/pedrormconde/MV-ATTA.
Fluency score: 4
Fluency issues: [Sentence structure could be less complex with shorter sentences for better fluency.]

Clarity score: 3
Clarity issues: [The technical terms and abbreviations might be unclear to the general audience. The introduction of M-ATTA and V-ATTA could be more clearly defined.]

Grammaticality score: 5
Grammaticality issues: [No issues detected.]

Readability score: 3
Readability issues: [Complex sentences and technical jargon could impede readability for a non-specialist audience.]

Coherence score: 4
Coherence issues: [The text is well-structured, but acronym introduction and some transitions could be smoother.]
==========
Introduction:  Deep Neural Networks (DNNs) changed the paradigm with regards to the applicability of machine learning (ML) systems to real-world scenarios. Consequently, deep learning (DL) models are now present in critical application domains (e.g., autonomous driving, medicine, remote sensing, robotics), where bad decision-making can bear potentially drastic consequences. This requires that DNNs are not only highly accurate, but also highlyreliable- decision-makers should be able to “trust” the predictions of these models. This lead us to the problem ofuncertainty calibration(also referred as confidence calibration or simplycalibration): it is required that the confidence output generated by the DL model - that translates as the confidence the model has in the prediction that is making - realistically represents the true likelihood of correctness. For the sake of intuition, a calibrated model would for example, in the long run, correctly classify70\%of those predictions that have a confidence value of0.7associated. This accurate quantification of predictive uncertainty results in reliable confidence values associated with each prediction, and therefore, in a more reliable model. As such, it is important to understand how well calibrated are modern DNNs. Further details, including the formalization of the uncertainty calibration problem, will be described in Section3.Although increasingly accurate, modern DL architectures have been found to be tendentiouslyuncalibrated[7,20]. Furthermore, “modern neural networks exhibit a strange phenomenon: probabilistic error and miscalibration worsen even as classification error is reduced”[7]. For this reason, the goal of this work is to improve the uncertainty calibration of DNNs in the task of image classification, by proposing a novel accuracy-consistent weighted test time augmentation method.Test time augmentation is a general methodology that leverages data augmentation techniques to create multiple samples from the original input at inference (i.e., at test time). Therefore, test time augmentation methods can be applied to pre-trained models, since, in this case, the augmentation process is not applied during the training phase. The technique introduced in this work combines the use of test time augmentation with a custom weighting system, guaranteeing that the accuracy of the original DL model is not corrupted, while still being optimized to improve uncertainty calibration. This builds, partially, on the work done in[3], proposing both a reformulated version -V-ATTA(Vector Adaptive Test Time Augmentation) - of the preliminary method presented in[3]and also a generalized version -M-ATTA(Matrix Adaptive Test Time Augmentation) - with a broader and extended empirical evaluation.M/V-ATTAis evaluated on the benchmark CIFAR-10/CIFAR-100[11]datasets, as well as on a benchmark satellite imagery dataset, the Aerial Image Dataset (AID)[28]. The results are compared with state-of-the-artpost-hoccalibration methods, with respect to the Brier score[2]and theExpected Calibration Error(ECE), for bothcommonandstronguncertainty calibration (see Section3for further details).Contribution: We propose a novel calibration technique - with two different variations (M/V-ATTA) - based on test time augmentation, that guarantees consistency in the accuracy of deep models in which is applied, while being shown to improve uncertainty calibration-related evaluation metrics, outperforming state-of-the-artpost-hoccalibration methods in most cases. To the best of our knowledge,M/V-ATTAis the first method based on test time augmentation that has been proposed to improve the uncertainty calibration of deep models (besides its predecessor in[3]). Furthermore, the method presented here can be used with pre-trained DNNs, which is advantageous in terms of applicability.
====================
	 GATGPT: A Pre-trained Large Language Model with Graph Attention Network for Spatiotemporal Imputation
==========
Abstract:  The analysis of spatiotemporal data is increasingly utilized across diverse domains, including transportation, healthcare, and meteorology. In real-world settings, such data often contain missing elements due to issues like sensor malfunctions and data transmission errors. The objective of spatiotemporal imputation is to estimate these missing values by understanding the inherent spatial and temporal relationships in the observed multivariate time series. Traditionally, spatiotemporal imputation has relied on specific, intricate architectures designed for this purpose, which suffer from limited applicability and high computational complexity. In contrast, our approach integrates pre-trained large language models (LLMs) into spatiotemporal imputation, introducing a groundbreaking framework, GATGPT. This framework merges a graph attention mechanism with LLMs. We maintain most of the LLM parameters unchanged to leverage existing knowledge for learning temporal patterns, while fine-tuning the upper layers tailored to various applications. The graph attention component enhances the LLM's ability to understand spatial relationships. Through tests on three distinct real-world datasets, our innovative approach demonstrates comparable results to established deep learning benchmarks.
Fluency score: 4
Fluency issues: [No significant issues affecting fluency]

Clarity score: 4
Clarity issues: ["by understanding the inherent spatial and temporal relationships" could be more specific; "merges a graph attention mechanism with LLMs" might be unclear to readers unfamiliar with these terms]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected]

Readability score: 4
Readability issues: [Complex sentence structures and technical jargon might be difficult for a general audience, especially in "Traditionally, spatiotemporal imputation has relied on specific, intricate architectures designed for this purpose"]

Coherence score: 4
Coherence issues: ["which suffer from limited applicability and high computational complexity" could be linked more clearly to the preceding statement; Transition from discussing the issues with traditional methods to introducing the new framework could be smoother]
==========
Introduction:  The presence of multivariate time series data is extensively documented across a variety of sectors including economics, transportation, healthcare, and meteorology, as evidenced in several studies[4,18,32,26]. A range of statistical and machine learning techniques have been shown to perform effectively on complete datasets in several time series tasks, including forecasting[14], classification[16], and anomaly detection[6]. However, it is often observed that multivariate time series data collected from real-world scenarios are prone to missing values due to various factors, such as sensor malfunctions and data transmission errors. These missing values can considerably affect the quality of the data, subsequently impacting the effectiveness of the aforementioned methods in their respective tasks. Extensive research efforts have been dedicated to addressing the challenges in spatiotemporal imputation. A typical approach involves the development of a distinct framework for initially estimating missing values, followed by the application of the completed dataset in another sophisticated framework for subsequent operations like forecasting, classification, and anomaly detection. To fill in missing values, various statistical and machine learning techniques are applied. Popularly employed methods include autoregressive moving average (ARMA)[1], expectation-maximization algorithm (EM)[23], and k-nearest neighbors (kNN)[20,15]. These models often depend on rigorous presumptions such as temporal uniformity and similarity between series, which may not be suitable for the complexity of real-world multivariate time series data, potentially leading to suboptimal performance. Deep learning techniques have gained significant traction among researchers in the realm of spatiotemporal imputation. The utility of Recurrent Neural Networks (RNN) was initially recognized in this context[7,9]. Subsequently, Generative Adversarial Networks (GAN) demonstrated impressive results, thanks to their robust generative properties[19]. More recently, advancements have been made by integrating RNN-based methods with Graph Neural Networks (GNN) to enhance the capability of extracting spatial dependencies within these frameworks. Nevertheless, the current focus in this field is predominantly on developing intricate and specialized frameworks aimed at improving performance on specific public industrial datasets, often overlooking the aspects of generalizability and adaptability for varied datasets and downstream applications. Additionally, the rise of pre-trained models in Natural Language Processing (NLP) has been noteworthy, with their applications extending into diverse areas such as Computer Vision (CV)[5], Multi-modality[28], and Recommendation Systems[33]. These Large Language Models (LLMs) have demonstrated remarkable success due to their advanced representation learning abilities. Yet, there is a scarcity of research exploring using pre-trained LLMs in the domain of time series data analysis. Our research is focused on exploring the capacity of LLMs to act as potent representation learners, particularly in capturing temporal dependencies. Moreover, we recognize LLMs’ exceptional few-shot learning capabilities, which positions them as highly suitable for time series contexts, where gathering extensive training data is often a formidable challenge. In this paper, we aim to introduce the pre-trained Large Language Models (LLMs) to solve the spatiotemporal imputation problem. The main contributions of this paper are summarized as follows: To the best of our knowledge, we are the first to introduce pre-trained LLMs to the spatiotemporal imputation. The integration of these models, with their pre-existing knowledge and architectural design, is instrumental in deciphering the intricacies of spatiotemporal dependencies. We have incorporated a graph attention module specifically designed to discern spatial dependencies. This module is devised to augment the LLMs in comprehending and assimilating the intrinsic characteristics of spatiotemporal data. Through our experimentation with various real-world datasets, we have demonstrated the effectiveness of our proposed framework. The results highlight its proficiency in spatiotemporal imputation, standing in comparison with multiple established baselines. The rest of this paper is organized as follows. Section2reviews the related work on spatiotemporal imputation. Section3presents the details of our proposed GATGPT framework, including the graph attention module and the pre-trained LLM block. Section4demonstrates our evaluation of the proposed framework performance on real-world datasets. Finally, Section5draws our conclusions.
====================
	 CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model
==========
Abstract:  Fusion of a panchromatic (PAN) image and corresponding multispectral (MS) image is also known as pansharpening, which aims to combine abundant spatial details of PAN and spectral information of MS. Due to the absence of high-resolution MS images, available deep-learning-based methods usually follow the paradigm of training at reduced resolution and testing at both reduced and full resolution. When taking original MS and PAN images as inputs, they always obtain sub-optimal results due to the scale variation. In this paper, we propose to explore the self-supervised representation of pansharpening by designing a cross-predictive diffusion model, named CrossDiff. It has two-stage training. In the first stage, we introduce a cross-predictive pretext task to pre-train the UNet structure based on conditional DDPM, while in the second stage, the encoders of the UNets are frozen to directly extract spatial and spectral features from PAN and MS, and only the fusion head is trained to adapt for pansharpening task. Extensive experiments show the effectiveness and superiority of the proposed model compared with state-of-the-art supervised and unsupervised methods. Besides, the cross-sensor experiments also verify the generalization ability of proposed self-supervised representation learners for other satellite's datasets. We will release our code for reproducibility.
Fluency score: 4
Fluency issues: [Minor awkwardness with the phrase "adapt for pansharpening task" which could be "adapt to the pansharpening task"]

Clarity score: 3
Clarity issues: [The text is somewhat dense with jargon which might not be clear to a layperson, for example "cross-predictive pretext task", "conditional DDPM"]

Grammaticality score: 4
Grammaticality issues: [Should be "the encoders of the UNets are frozen" instead of "encoders of the UNets are frozen"]

Readability score: 3
Readability issues: [The complexity of terms and sentence structures make the text less accessible for a non-specialist audience]

Coherence score: 4
Coherence issues: [The extensive use of acronyms and technical terms may be confusing, a smoother transition between the explanation of the two-stage training could improve coherence]
==========
Introduction:  Pansharpening refers to fusing a low-spatial-resolution multispectral (LRMS) image with a single-band high-spatial-resolution panchromatic (PAN) image to obtain a high spatial and spectral resolution MS (HRMS) image. Owing to the technological and physical limitations of imaging devices, remote sensing images obtained by satellite sensors always have a trade-off between spatial and spectral resolution[1]. Therefore, most of available satellites carry two types of sensors,i.e. , an MS sensor and a PAN sensor, to acquire LRMS and PAN images simultaneously. Researchers can then combine them through pansharpening technology. Benefiting from the availability of large-scale remote sensing imagery, numerous deep-learning (DL)-based methods achieve great success in pansharpening. According to the base models they used, these methods can be roughly divided into convolutional neural networks (CNNs) based[2,3,4], generative adversarial networks (GANs) based[5,6,7], and Transformer based[8,9,10]. CNNs-based methods mainly extract spatial and spectral features layer-by-layer. They take pansharpening as a regression task, and are usually supervised by a reconstruction loss. GANs-based models utilize generators to fuse MS and PAN images and the discriminators to adversarially train the model for the generation of high-fidelity fused products. To model the long-range dependency of images, researchers introduce the Transformer architecture to pansharpening[8]. These models can learn either the self-similarity in a single image[9]or the interactive information between two kinds of modality images[10]. Most of available DL-based pansharpening methods are within the supervised-learning framework. However, there are no high-resolution MS images that can be taken as groundtruths. Therefore, researchers train the models at reduced resolution, where the training samples are prepared according to Wald’s protocol[11],i.e. , downsample PAN and MS to take them as inputs, and the original MS images are treated as references, which are actually pseudo-groundtruths[12]. Nevertheless, due to the scale variation[13,7], models trained at reduced resolution are unsuitable for pansharpening at the original images, resulting in inferior performance at full resolution[6]. Recently, researchers devote to developing unsupervised pansharpening methods. Most of them[14,15]concentrate only on the design of unsupervised loss function, however their effectiveness also depends on the features extracted[16]. We argue that the scale dependency issue can be alleviated from the perspective of not only unsupervised loss function, but also a good representation learner. Recent works utilize generative models to act as representation learners[16,17,18]. Among them, Denoising Diffusion Probabilistic Model (DDPM)[19]outperforms other alternative generative models and provides more semantically-valuable pixel-wise representations[20], which highlights the potential of using the state-of-the-art DDPM as strong unsupervised representation learners. Based on this insight, we build model upon DDPM to explore its representation ability to pansharpening. Furthermore, available DL-based methods have limited generalization ability, where the model trained on a specific dataset always performs unsatisfactorily on datasets collected from other satellite sensors. This further arises a question: how can we develop a unified representation learner based on DDPM to make it accountable for the extraction of general spatial and spectral features, regardless of the distinctiveness of sensors’ attributes. For pansharpening task, PAN and MS images have distinctive spatial and spectral characteristics respectively, which naturally constitutes a self-supervised pretext task that one can predict high-resolution PAN image from low-resolution MS image, and inversely predict spectral-appealing MS image from spectral-scarce PAN image. In this paper, we embed pansharpening into diffusion model to construct a new self-supervised paradigm, dubbed CrossDiff. As Figure1shows, the model consists of two stage training. In the first stage, the self-supervised pre-training is conducted based on DDPM to obtain a noise predictor with UNet structure[21]. Afterwards, the frozen encoders act as spatial-spectral feature representation learners to fuse MS and PAN images with a tunable fusion head. By introducing a cross-predictive diffusion process, CrossDiff outperforms state-of-the-art unsupervised pansharpening methods and has strong generalization ability. The main contributions are summarized as follows: We design a new two-stage pansharpening paradigm to explore the potential of DDPM to the self-supervised spatial and spectral features extraction. A cross-predictive diffusion process is introduced to pre-train the spatial and spectral representation learners. Following the process of DDPM, the effective training objectives encourage the pretext task to explicitly learn spatial and spectral diffusion latent. By freezing the pre-trained models and only tuning the fusion head, our proposed CrossDiff performs well at both full and reduced resolution, and has strong cross-sensor generalization ability.
====================
	 OctFormer: Octree-based Transformers for 3D Point Clouds
==========
Abstract:  We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200k. Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D segmentation and detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are available at https://wang-ps.github.io/octformer.
Fluency score: 4
Fluency issues: [The text is mostly fluent but somewhat dense with technical jargon]

Clarity score: 3
Clarity issues: ["point number" might be clearer as "number of points"; dense terminology may obscure meaning for non-experts]

Grammaticality score: 4
Grammaticality issues: ["runs 17 times faster" might be clearer as "is 17 times faster"]

Readability score: 3
Readability issues: [High density of technical terms and lack of layman explanations]

Coherence score: 4
Coherence issues: [Some transitions between ideas could be smoother to enhance coherence]
==========
Introduction:  3D point cloud understanding is a fundamental task in computer graphics and vision and has a broad range of applications, including robotics, autonomous driving, and augmented reality. A variety of deep learning methods have been proposed for it, such as voxel-based CNNs(Wu et al.,2015; Wang et al.,2017; Graham et al.,2018), view-based CNNs(Su et al.,2015), and point-based networks(Qi et al.,2016,2017b; Li et al.,2018), and remarkable progress has been made. Recently, point cloud transformers have emerged(Guo et al.,2021; Zhao et al.,2021; Misra et al.,2021)as an effective alternative with the potential for cross-multimodality training and general intelligent models(Radford et al.,2021; Ramesh et al.,2022). However, the efficiency of point cloud transformers is still much worse than their CNN counterparts(Wang et al.,2017; Choy et al.,2019; Nekrasov et al.,2021), especially on scene-scale datasets like ScanNet(Dai et al.,2017), and the performance of point cloud transformers is also just comparable. Since it has been proven that transformers are at least as expressive as CNNs(Cordonnier et al.,2020), one of the key challenges of applying transformers to point clouds is to overcome the huge computational complexity of transformers, which is quadratic with the number of elements involved. Several methods(Guo et al.,2021; Yu et al.,2022; Pang et al.,2022)directly apply transformers to all points globally, thus limiting their applicability to large-scale point clouds. Following the progress in scaling up vision transformers(Liu et al.,2021a; Dong et al.,2022), one effective strategy is to constrain point cloud transformers within non-overlapping local windows(Lai et al.,2022; Sun et al.,2022; Fan et al.,2022; Mao et al.,2021). However, unlike images, the number of points across different local windows varies significantly due to the sparsity of point clouds. To deal with this issue, sophisticated implementations like region batching(Fan et al.,2022; Sun et al.,2022)or customized GPU kernels(Lai et al.,2022)have to be adopted, which severely impedes massive parallelism on GPUs. Another strategy to speed up point cloud transformers is to apply transformers in downsampled feature maps(Park et al.,2022; Cheng et al.,2021b), which also weakens the network capability and incurs a decrease in performance. In this paper, we present a general and scalable octree-based transformer, abbreviated as OctFormer, for learning on 3D point clouds. The key building block of OctFormer is a novel octree attention mechanism for point clouds. To retain linear complexity, we divide each point cloud into small groups when applying attentions. Our key observation is that attentions are insensitive to the actual shape of underlying local windows. Instead of using cubic windows as in previous works, which incur variant point numbers in each window, we divide point clouds into groups with irregular windows while keeping the point number in each window the same. Consequently, we can easily implement our attention using standard operators provided by deep learning frameworks like PyTorch(Paszke et al.,2019). To generate the required window partition, our second observation is that after constructing an octree with the parallel algorithm in(Zhou et al.,2011), the octree nodes are sorted in z-order by shuffled keys(Wilhelms and Van Gelder,1992), which ensures that spatially-close octree nodes are contiguously stored in memory. We store features in tensors according to the order of octree nodes. After padding a few zeros to make the spatial numbers of tensors divisible by the specified point number in each window, we can efficiently generate the window partition by simply reshaping the tensors at almost zero cost. An example is shown in Figure1-(b), where the point number in each window is the same. To further increase the receptive fields of OctFormer, we introduce a dilated octree attention with dilated partitions along the spatial dimension of tensors, which can also be efficiently implemented with tensor reshaping and transposing. Our OctFormer challenges conventional wisdom in designing point cloud transformers from two aspects. First, instead of using fixed-sized local windows, we fix the point number in each window when doing point cloud partition, enabling simple implementation and easy parallelization; second, instead of regarding point clouds as unordered and unstructured point sets, we actually sort the quantized points with shuffled keys by building octrees, resulting in a convenient window partition. Our octree attention completely eliminates the expensive neighborhood searching used in previous designs(Wu et al.,2022; Lai et al.,2022), bypasses the sparsity of point clouds, and can be reduced to a standard multi-head self-attention(Vaswani et al.,2017)on small groups of equal size. Consequently, our octree attention can be implemented in10 lines of codewith open-sourced libraries freely available on the web. Onesingletransformer block on top of octree attention runs at least17 times fasterthan previous state-of-the-art point transformer blocks(Lai et al.,2022; Wu et al.,2022)when the number of elements involved is200k. We also introduce feature hierarchies following the multiscale structure of octrees, endowing OctFormer with the capability as a general backbone for 3D segmentation and detection. We verify the effectiveness of OctFormer on a series of 3D benchmarks. Specifically, our OctFormer achieves the best performance on the validation set of ScanNet segmentation(Dai et al.,2017), SUN RGB-D detection(Song et al.,2015), and ScanNet200 segmentation(Rozenberszki et al.,2022), surpassing all previous state-of-the-art sparse-voxel-based CNNs(Choy et al.,2019; Wang et al.,2017; Graham et al.,2018)and point cloud transformers(Lai et al.,2022; Wu et al.,2022)by a large margin. Notably, on ScanNet200 segmentation, which contains 200 semantic categories (ten times more than ScanNet), the mIoU of our OctFormer is higher than MinkowskiNet(Choy et al.,2019)by7.3and even higher than the recently-proposed LGround(Rozenberszki et al.,2022)by5.4, which pretrains a sparse-voxel-based CNN with a powerful CLIP model(Radford et al.,2021). In summary, our main contributions are as follows: [leftmargin=10pt,itemsep=2pt] We propose a novel octree attention and its dilated variant, which are easy to implement and significantly more efficient than previous point cloud attentions; We propose OctFormer, which can serve as a general backbone for 3D point cloud segmentation, detection, and classification; OctFormer achieves state-of-the-art performances on a series of 3D segmentation and detection benchmarks, and the computational efficiency of OctFormer is much higher than previous point cloud transformers and even surpasses highly optimized sparse-voxel-based CNNs.
====================
	 Bayesian Self-Supervised Contrastive Learning
==========
Abstract:  Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss.
Fluency score: 4
Fluency issues: [Minor awkwardness in phrase "remains many exciting challenges"]

Clarity score: 3
Clarity issues: ["remains many exciting challenges" is unclear; "a location parameter for debiasing false negative" possibly missing "samples"]

Grammaticality score: 4
Grammaticality issues: ["remains many exciting challenges" should be "still poses many exciting challenges"; "a false negative" should be "false negatives"]

Readability score: 3
Readability issues: [Complex sentence structure; jargon-heavy without explanation]

Coherence score: 3
Coherence issues: [Jumps between concepts can be jarring without explicit connections or transitions]
==========
Introduction:  Learning good representations without supervision has been a long-standing problem in machine learning(Aroraet al.,2019; Heet al.,2020; Chuet al.,2023). Many state-of-the-art models utilize self-supervised learning (SSL) techniques(Chenet al.,2020b; Grillet al.,2020; Liuet al.,2021; Tonget al.,2023)to design pretext tasks for (pre)training the models. For instance, generative methods train models to reconstruct input data(Kingma and Dhariwal,2018; Mikolovet al.,2013), while contrastive methods train models to encode differential features between positive and negative samples(Chopraet al.,2005; Hadsellet al.,2006; Gidariset al.,2018; Hjelmet al.,2018; Chenet al.,2020a; Grillet al.,2020; Heet al.,2020; Wang and Isola,2020; Radfordet al.,2021a). Self-supervised learning has been extensively researched for its advantages in learning representations without the need for human labelers to manually label the data(Heet al.,2020; Tianet al.,2020; Chenet al.,2020a; Radfordet al.,2021b; Wuet al.,2023; Luoet al.,2023). Contrastive learning is a primary implementation form of self-supervised learning, and remarkable successes have been observed for many applications in different domain(Alecet al.,2019; Misra and Maaten,2020; Heet al.,2020; Tianet al.,2020; Chenet al.,2020b; Liuet al.,2021). However, self-supervised learning derives “pseudo-labels” from co-occurring inputs to relate information(Liuet al.,2021). For negative samples, there exists a distinction in the semantic interpretation of “classes” in pre-training phase and the generalized semantic of “classes” used in downstream tasks. As illustrated in Fig.1, for the anchor point “dog”, the semantic of negative samples is all the unlabeled samples excluding the anchor data point itself in the pre-training phase. That is, each sample is treated as an individual class. However, in the downstream task, the semantic of negative sample are samples labeled as “not dogs”. Aligning the semantic representation of negative examples between the pretraining tasks and downstream tasks is crucial for improving performance in the latter. In the pretraining phase, there are two types of negative samples that disrupt the semantic structure of generalized “classes”. The first type consists of false negative (FN) samples, represented byx_{3}^{\prime}in Fig.1. Despite being labeled as “dog”, it were erroneously treated as negative belonging to a different class than the anchor data point. This necessitates the task of false negative debiasing, aiming to prevent the false negative samples from being erroneously pushed apart. The second type comprises hard negative (HN) samples, exemplified byx_{1}^{\prime}in Fig.1. These samples possess a “wolf” label and exhibit similarities to the anchor data point, despite belonging to a distinct class. It is essential to push these samples further apart, corresponding to the task of hard negative mining, otherwise the underlying semantic structure can be disrupted(Wang and Liu,2021; Chuanget al.,2020). In this work, we focus on the end-to-end self-supervised contrastive learning method with SimCLR as a representative approach. We address two critical tasks in self-supervised contrastive learning, namely debiasing false negatives and mining hard negatives, within a Bayesian framework. These tasks are effectively achieved through re-weighting the negative samples, thereby correcting the contrastive loss within the self-supervised setting and obtaining the modified Bayesian self-supervised contrastive loss (BCL), which offers a flexible and principled framework for self-supervised contrastive learning and presents a generalized perspective of contrastive loss. We analyzed the small-sample properties of BCL, and established the relationship between the importance weight applied to each unlabeled sample and the posterior probability of them being true negatives. Furthermore, we investigated the large-sample properties of BCL and demonstrated its consistency as an estimation of the contrastive loss under the supervised setting.
====================
	 Neural Symbolic Regression using Control Variables
==========
Abstract:  Symbolic regression (SR) is a powerful technique for discovering the analytical mathematical expression from data, finding various applications in natural sciences due to its good interpretability of results. However, existing methods face scalability issues when dealing with complex equations involving multiple variables. To address this challenge, we propose SRCV, a novel neural symbolic regression method that leverages control variables to enhance both accuracy and scalability. The core idea is to decompose multi-variable symbolic regression into a set of single-variable SR problems, which are then combined in a bottom-up manner. The proposed method involves a four-step process. First, we learn a data generator from observed data using deep neural networks (DNNs). Second, the data generator is used to generate samples for a certain variable by controlling the input variables. Thirdly, single-variable symbolic regression is applied to estimate the corresponding mathematical expression. Lastly, we repeat steps 2 and 3 by gradually adding variables one by one until completion. We evaluate the performance of our method on multiple benchmark datasets. Experimental results demonstrate that the proposed SRCV significantly outperforms state-of-the-art baselines in discovering mathematical expressions with multiple variables. Moreover, it can substantially reduce the search space for symbolic regression. The source code will be made publicly available upon publication.
Fluency score: 4
Fluency issues: [no specific issues]

Clarity score: 4
Clarity issues: [use of "control variables" without explanation; "by gradually adding variables one by one" could be more concise]

Grammaticality score: 5
Grammaticality issues: [none]

Readability score: 4
Readability issues: [jargon such as "symbolic regression," "control variables," "data generator," "deep neural networks" may be difficult for a layperson; complex sentences]

Coherence score: 4
Coherence issues: [some terms and concepts are introduced without clear definitions, which might affect the flow for readers not already familiar with the subject]
==========
Introduction:  Symbolic regression (SR) aims to uncover the underlying mathematical expressions from observed data[16,9]. It has been widely used for scientific discovery across various disciplines[1,29]owing to its ability to learn analytical expressions between the input and output. The implementation of SR involves two steps[15]. The first step is to predict the skeleton of mathematical expressions based on a pre-defined list of basic operations (+,-,\times,\div) and functions (\sin,\cos,\exp,\log). For instance, we can identify the skeleton of a symbolic equation asf(x)=\log{ax}+\sin(bx)+c. Next, we adopt optimization methods, such as Broyden–Fletcher–Goldfarb–Shanno (BFGS), to estimate the parametersa,b,cin the skeleton. The key challenges of SR lie in: 1) how to improve the accuracy and scalability for multiple input variables, and 2) how to speed up the discovery process. In the past few decades, a plethora of SR methods[22]have been developed to discover underlying mathematical equations from data in science and engineering domains. One popular approach among them is genetic programming (GP)[4,7,27,10,2], which uses evolutionary operations, such as mutation, crossover, and selection, to estimate the symbolic expressions in a tree structure. However, GP would suffer from instability and its inference time is expensive in the context of multiple input variables[15]. Another method, SINDy[5], adopts sparse linear regression to discover the governing equations of dynamical systems. However, SINDy’s performance relies heavily on prior knowledge of a known set of candidate functions, and it is difficult to uncover complex equations from data solely through linear regression. To overcome these limitations, some studies explore deep neural networks-based techniques, such as Deep Symbolic Regression (DSR)[25]and Transformer-based pre-training, for symbolic learning. Although these approaches obtain good prediction accuracy, they do not scale well to mathematical equations with multiple variables. Recently, researchers develop Symbolic Physics Learner (SPL), a physics-informed Monte Carlo Tree Search (MCTS) algorithm for symbolic regression. While SPL outperforms most GP-based methods, it still struggles with multiple variables in mathematical expressions. In summary, existing methods suffer from scalability issues when dealing with complex multi-variable equations as they require a much larger search space to identify the combination of different variables. Thus, the question is, how can we reduce the search space of symbolic regression for complex equations involving multiple variables? In this paper, we propose a novel neural symbolic regression with control variables (SRCV) that combines neural networks and symbolic regression to discover analytical expressions from data, as illustrated in Fig.1. Inspired by divide and conquer[26], SRCV addresses the multi-variable symbolic regression by decomposing it into a set of single-variable SR problems and then combines the estimated symbolic equation for each variable in a bottom-up manner. The proposed method is performed in four steps as follows. 1) We learn a data generator from observed data using DNNs, allowing for generating data for a specific variable. 2) Generate data via control variables. Specifically, we generate data samples for the current independent variable by manipulating the previously learned variables and other control variables. For example, for estimating the symbolic expression of variablex_{i}, we can generate data samples by varyingx_{i}while fixing the other variables. 3) Single-variable symbolic regression is employed to estimate the mathematical expression of the current variable based on the generated data in step 2. Here any symbolic regression models can be inserted into the framework. 4) We gradually add the remaining variables one by one to step 2 and proceed with step 3 until all the variables are covered. Extensive experimental results on multiple SR benchmarks demonstrate the superiority of our SRCV over the state-of-the-art methods in discovering complex multi-variable equations. Moreover, the proposed approach is able to discover complex expressions in a reduced search space. Our main contributions are three-fold: 1) we propose SRCV, a simple and effective neural symbolic regression method using control variables; 2) we illustrate that the proposed method exhibits a significant reduction in search space for complex symbolic equations; 3) the evaluation results demonstrate that our method can significantly outperform the baselines in terms of accuracy and inference time.
====================
	 Self-Interpretable Time Series Prediction with Counterfactual Explanations
==========
Abstract:  Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
Fluency score: 4
Fluency issues: [Most existing methods focus on interpreting predictions by assigning important scores to segments of time series.]

Clarity score: 4
Clarity issues: [Most existing methods focus on interpreting predictions by assigning important scores to segments of time series.]

Grammaticality score: 5
Grammaticality issues: [None]

Readability score: 3
Readability issues: [Long, complex sentences with dense terminology might hinder readability for a general audience.]

Coherence score: 4
Coherence issues: [Jargon and specialized language may affect coherence for readers not familiar with the subject matter.]
==========
Introduction:  Deep learning (DL) has become increasingly prevalent, and there is naturally a growing need for understanding DL predictions in many decision-making area, such as healthcare diagnosis and public policy-making. The high-stake nature of these areas means that these DL predictions are considered trustworthy only when they can be well explained. Meanwhile, time-series data has been frequently used in these areas(Zhaoet al.,2021; Jinet al.,2022; Yanget al.,2022), but it is always challenging to explain a time-series prediction due to the nature of temporal dependency and varying patterns over time. Moreover, time-series data often comes with confounding variables that affect both the input and output, making it even harder to explain predictions from DL models. On the other hand, many existing explanation methods are based on assigning importance scores for different parts of the input to explain model predictions(Ribeiroet al.,2016; Lundberg and Lee,2017b; Chenet al.,2018; Wanget al.,2019b; Weinbergeret al.,2020; Plumbet al.,2020). However, understanding the contribution of different input parts are usually not sufficiently informative for decision making: people often want to know what changes made to the input could have lead to a specific (desirable) prediction(Wachteret al.,2017a; Goyalet al.,2019; Nemirovskyet al.,2022). We call such changed input that could have shifted the prediction to a specific targetactionable counterfactual explanations. Below we provide an example in the context of time series. Suppose there is a model that takes as input a time series of breathing signal{\bf x}\in{\mathbb{R}}^{T}from a subject of ageu=60to predict the corresponding sleep stage asy^{pred}=\mbox{`Awake'}\in\{\mbox{`Awake'},\mbox{`Light Sleep'},\mbox{`Deep % Sleep'}\}. Typical methods assign importance scores to each entry of{\bf x}to explain the prediction. However, they do not provideactionablecounterfactual explanations on how to modify{\bf x}to{\bf x}^{cf}such that the prediction can change toy^{cf}=\mbox{`Deep Sleep'}. An ideal method with such capability could provide more information on why the model make specific predictions. Actionable counterfactual explanations help people understand how to achieve a counterfactual (target) output by modifying the current model input. However, such explanations may not be sufficiently informative in practice, especially under the causal effect of confounding variables which are often immutable. Specifically, some variables can hardly be changed once its value has been determined, and suggesting changing such variables are both meaningless and infeasible (e.g., a patient age and gender when modeling medical time series). This leads to a stronger requirement: a good explanation should make as few changes as possible on immutable variables; we call such explanationsfeasible counterfactual explanations. Below we provide an example in the context of time series. In Example1, ageuis a confounder that affects both{\bf x}andysince elderly people (i.e., largeru) are more likely to have irregular breathing{\bf x}and more ‘Awake’ time (i.e.,y=\mbox{`Awake'}) at night. To generate a counterfactual explanation to changey^{pred}to‘Deep Sleep’, typical methods tend to suggest decreasing the ageufrom60to50, which isinfeasible(since age cannot be changed in practice). An ideal method would first infer the ageuand search for afeasiblecounterfactual explanation{\bf x}^{cf}that could changey^{pred}to‘Deep Sleep’while keepinguunchanged. In this paper, we propose a self-interpretable time series prediction model, dubbed Counterfactual Time Series (CounTS), which can both (1) perform time series predictions and (2) provide actionable and feasible counterfactual explanations for its predictions. Under common causal structure assumptions, our method is guaranteed to identify the causal effect between the input and output in the presence of exogenous (confounding) variables, thereby improving the generated counterfactual explanations’ feasibility. Our contribution is summarized as follows: We identify the actionability and feasibility requirements for generating counterfactual explanations for time series models and develop the first general self-interpretable method, dubbed CounTS, that satisfies such requirements. We provide theoretical guarantees that CounTS can identify the causal effect between the time series input and output in the presence of exogenous (confounding) variables, thereby improving feasibility in the generated explanations. Experiments on both synthetic and real-world datasets show that compared to state-of-the-art methods, CounTS significantly improves performance for generating counterfactual explanations while still maintaining comparable prediction accuracy.
====================
	 Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs
==========
Abstract:  Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and can solve different tasks due to their emergent ability and generalizability. However, LLMs sometimes lack domain-specific knowledge to perform tasks, which would also cause hallucination during inference. In some previous works, additional modules like graph neural networks (GNNs) are trained on retrieved knowledge from external knowledge bases, aiming to mitigate the problem of lacking domain-specific knowledge. However, incorporating additional modules: 1) would need retraining additional modules when encountering novel domains; 2) would become a bottleneck since LLMs' strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability. Specifically, we design a simple yet effective prompt to transform retrieval into a multi-hop decision sequence, which empowers LLMs with searching knowledge ability in zero-shot manner. Additionally, KSL is able to provide complete retrieval paths and therefore increase explainability of LLMs' reasoning processes. We conduct experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and found that our approach improves LLM baseline performance by a relatively large margin.
Fluency score: 4
Fluency issues: [The sentence structure is somewhat complex which could slightly hinder fluency for some readers.]

Clarity score: 4
Clarity issues: [The text is dense with technical jargon, which might be unclear to those outside the field.]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected.]

Readability score: 3
Readability issues: [Complex sentences and heavy use of acronyms and jargon lower readability for a general audience.]

Coherence score: 4
Coherence issues: [The logical structure is present, but the dense information presented may make it less coherent for those unfamiliar with the topic.]
==========
Introduction:  Recently, large language models (LLMs) like ChatGPT have drawn numerous attention from researchers and practitioners due to theirgeneralistcapabilities(Qinet al.,2023). For instance, sufficiently large language models could perform well for different tasks in zero-shot manner, such as text summarization(Yanget al.,2023; Zhanget al.,2023), machine translation(Moslemet al.,2023), and question answering(Singhalet al.,2023). However, in some scenarios, LLMs lack domain-specific knowledge or are not able to recall facts and knowledge correctly, which causes hallucination(Banget al.,2023). Hallucination refers to models generating text that is nonsensical, or unfaithful to the provided source input(Jiet al.,2023; Koehn and Knowles,2017; Raunaket al.,2021; Rohrbachet al.,2018; Vinyals and Le,2015; Maynezet al.,2020). Retrieving relevant texts from knowledge bases is a classic way to augment language models’ performance like generation quality(Borgeaudet al.,2022; Lewiset al.,2020a; Levineet al.,2022; Guuet al.,2020). Besides, it can also help improve the factuality of generated texts. Typically, retrieval modules are employed to find the most relevant documents with the highest similarity scores to the query. Then input texts and retrieved documents would be combined in a specific way fed into models. Motivated by this, some methods(Ramet al.,2023; Penget al.,2023a)utilize retrieved texts to augment LLMs.Ramet al.(2023)directly prepends retrieved documents to the input to obtain a performance gain for LLMs.(Penget al.,2023a)designs an LLM-Augmenter to retrieve and merge evidence from external knowledge for alleviating hallucination. However, relying on similarity between embeddings would only make model learn shallow features instead of understanding semantics, which in turn hinder the model from searching truly useful knowledge. On the contrary, Knowledge Graphs (KGs) are clear, logical, and superior mediums of knowledge. Thus, effectively leveraging KGs for LLMs should benefit LLMs’ performance on knowledge-required tasks. For this reason, there is a line of work(Yasunagaet al.,2021; Linet al.,2019; Fenget al.,2020)using KGs to help LLMs make predictions. KagNet(Linet al.,2019)proposes a graph neural network module to model relational graphs for relational reasoning under the context of both knowledge symbolic space and language semantic space. MHGRN(Fenget al.,2020)equips pretrained language models with a multi-hop relational reasoning module, which unifies path-based reasoning methods and graph neural networks. QA-GNN(Yasunagaet al.,2021)learn representations over joint graphs formed by connecting QA context and KG. However, they(Yasunagaet al.,2021; Linet al.,2019; Fenget al.,2020)all require training additional knowledge-aware modules like graph neural networks (GNNs) on retrieved knowledge. There are two shortcomings of training additional modules: 1) would suffer from pains of retraining when encountering novel domains; 2) would become a bottleneck since LLMs’ strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to solve these shortcomings, which teaches LLMs themselves to search for knowledge from external knowledge bases. To be specific, we simplify the process of searching for necessary knowledge from KGs into a multi-hop decision sequence. At each step, we transform local information within KGs into text prompts (including the historical path selected by LLMs), based on which LLMs select relevant knowledge in the context to perform tasks, as shown in Figure1. The whole process is similar to humans searching over the Internet for achieving some goals. Furthermore, based on the complete paths chosen by LLMs, we can explain the whole decision-making process of LLMs. It allows for analysis when bad cases arise, a capability not present in previous black-box retrieval methods. We evaluate our approach, Knowledge Solver (KSL), with three LLMs (GPT-3.5, LLaMA(Touvronet al.,2023a), and LLaMA 2(Touvronet al.,2023b)) on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, where reasoning with knowledge is required. KSL improves two LLM baselines’ performance across these three datasets in zero-shot and finetuning settings. Our main contributions are summarized as follows: We propose Knowledge Solver (KSL), which is the first paradigm employing LLMs to search for relevant knowledge on KGs by themselves. Our proposed paradigm Knowledge Solver can boost LLMs’ performance on knowledge-required tasks by a relatively large margin in zero-shot manner without additional modules and training. Knowledge Solver can provide explainability for LLMs’ whole reasoning processes. When the computational burden is affordable, finetuning LLMs on our specially constructed dataset, with the help of KGs, can benefit LLMs further.
====================
	 Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing
==========
Abstract:  Sequence-to-Sequence (S2S) models have achieved remarkable success on various text generation tasks. However, learning complex structures with S2S models remains challenging as external neural modules and additional lexicons are often supplemented to predict non-textual outputs. We present a systematic study of S2S modeling using contained decoding on four core tasks: part-of-speech tagging, named entity recognition, constituency and dependency parsing, to develop efficient exploitation methods costing zero extra parameters. In particular, 3 lexically diverse linearization schemas and corresponding constrained decoding methods are designed and evaluated. Experiments show that although more lexicalized schemas yield longer output sequences that require heavier training, their sequences being closer to natural language makes them easier to learn. Moreover, S2S models using our constrained decoding outperform other S2S approaches using external resources. Our best models perform better than or comparably to the state-of-the-art for all 4 tasks, lighting a promise for S2S models to generate non-sequential structures.
Fluency score: 4
Fluency issues: [The phrasing "lighting a promise" is unconventional]

Clarity score: 4
Clarity issues: [Some technical phrases might be obscure without context, such as "lexically diverse linearization schemas"]

Grammaticality score: 5
Grammaticality issues: []

Readability score: 3
Readability issues: [The text is dense with jargon and compound-complex sentences, which may hinder readability for a wider audience]

Coherence score: 4
Coherence issues: [The transition between ideas is slightly abrupt and assumes a high level of reader familiarity with the subject]
==========
Introduction:  Sequence-to-Sequence (S2S) models pretrained for language modeling (PLM) and denoising objectives have been successful on a wide range of NLP tasks where both inputs and outputs are sequencesRadfordet al.(2019); Raffelet al.(2020); Lewiset al.(2020); Brownet al.(2020). However, for non-sequential outputs like trees and graphs, a procedure called linearization is often required to flatten them into ordinary sequencesLiet al.(2018); F-G and G-R (2020); Yanet al.(2021); Bevilacquaet al.(2021); He and Choi (2021a), where labels in non-sequential structures are mapped heuristically as individual tokens in sequences, and numerical properties like indices are either predicted using an external decoder such as Pointer NetworksVinyalset al.(2015a)or cast to additional tokens in the vocabulary. While these methods are found to be effective, we hypothesize that S2S models can learn complex structures without adapting such patches. To challenge the limit of S2S modeling, BARTLewiset al.(2020)is finetuned on four tasks without extra decoders: part-of-speech tagging (POS), named entity recognition (NER), constituency parsing (CON), and dependency parsing (DEP). Three novel linearization schemas are introduced for each task: label sequence (LS), label with text (LT), and prompt (PT).LStoPTfeature an increasing number of lexicons and a decreasing number of labels,which are not in the vocabulary (Section3). Every schema is equipped with a constrained decoding algorithm searching over valid sequences (Section4). Our experiments on three popular datasets depict that S2S models can learn these linguistic structures without external resources such as index tokens or Pointer Networks. Our best models perform on par with or better than the other state-of-the-art models for all four tasks (Section5). Finally, a detailed analysis is provided to compare the distinctive natures of our proposed schemas (Section6).111All our resources including source codes are publicly available:https://github.com/emorynlp/seq2seq-corenlp
====================
	 Adaptive whitening with fast gain modulation and slow synaptic plasticity
==========
Abstract:  Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to match structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale whitening objective that factorizes the inverse whitening matrix into basis vectors, which correspond to synaptic weights, and a diagonal matrix, which corresponds to neuronal gains. We test our model on synthetic and natural datasets and find that the synapses learn optimal configurations over long timescales that enable adaptive whitening on short timescales using gain modulation.
Fluency score: 4
Fluency issues: [The text is generally fluent, but the heavy use of technical jargon might hinder fluency for general audiences]

Clarity score: 3
Clarity issues: [Use of terms such as "statistical whitening," "synaptic plasticity," "gain modulation," and "inverse whitening matrix" without explanations lowers clarity for a non-specialist reader]

Grammaticality score: 5
Grammaticality issues: [No grammatical issues detected]

Readability score: 3
Readability issues: [The text is dense with specialized vocabulary, making it less readable for a wider audience]

Coherence score: 4
Coherence issues: [The specialized language might affect coherence for readers not familiar with the subject matter, despite the logical flow of information]
==========
Introduction:  Individual neurons in early sensory areas rapidly adapt to changing sensory statistics by normalizing the variance of their responses[11;21;43]. At the population level, neurons also adapt by reducing correlations between their responses[42;10]. These adjustments enable the neurons to maximize the information that they transmit by utilizing their entire dynamic range and reducing redundancies in their representations[4;8;33;7]. A natural normative interpretation of these transformations isadaptive whitening, a context-dependent linear transformation of the sensory inputs yielding responses that have unit variance and are uncorrelated. Decorrelation of the neural responses requires coordination between neurons and the neural mechanisms underlying such coordination are not known. Since neurons communicate via synaptic connections, it is perhaps unsurprising that most existing mechanistic models of adaptive whitening decorrelate neural responses by modifying the strength of these connections[60;30;45;46;14;37;59]. However, long-term synaptic plasticity is generally associated with long-term learning and memory[38], and thus may not be a suitable biological substrate for adaptive whitening (though short-term synaptic plasticity has been reported[63]). On the other hand, there is extensive neuroscience literature on rapid and reversible gain modulation[1;48;50;13;51;47;22;61]. Motivated by this,Duonget al.[18]proposed a mechanistic model of adaptive whitening in a neural circuit withfixedsynaptic connections that adapts exclusively by modifying the gains of interneurons that mediate communication between the primary neurons. They demonstrate that an appropriate choice of the fixed synaptic weights can both accelerate adaptation and significantly reduce the number of interneurons that the circuit requires. However, it remains unclear how the circuitlearnssuch an optimal synaptic configuration, which would seem to require synaptic plasticity. In this study, we combine the learning and adaptation of synapses and gains, respectively, in a unified mechanistic neural circuit model that adaptively whitens its inputs over multiple timescales (Fig.1). Our main contributions are as follows: We introduce a novel adaptive whitening objective in which the (inverse) whitening matrix is factorized into a synaptic weight matrix that is optimized across contexts and a diagonal (gain) matrix that is optimized within each statistical context. With this objective, we derive a multi-timescale online algorithm for adaptive whitening that can be implemented in a neural circuit comprised of primary neurons and an auxiliary population of interneurons with slow synaptic plasticity and fast gain modulation (Fig.1). We test our algorithm on synthetic and natural datasets, and demonstrate that the synapses learn optimal configurations over long timescales that enable the circuit to adaptively whiten its responses on short timescales exclusively using gain modulation. Beyond the biological setting, multi-timescale learning and adaptation may also prove important in machine learning systems. For example,Mohanet al.[41]introduced “gain-tuning”, in which the gains of channels in a deep denoising neural network (with pre-trained synaptic weights) are adjusted to improve performance on samples with out-of-distribution noise corruption or signal properties. The normative multi-timescale framework developed here offers a new approach to continual learning and test-time adaptation problems such as this.
====================
	 NMS Threshold matters for Ego4D Moment Queries -- 2nd place solution to the Ego4D Moment Queries Challenge 2023
==========
Abstract:  This report describes our submission to the Ego4D Moment Queries Challenge 2023. Our submission extends ActionFormer, a latest method for temporal action localization. Our extension combines an improved ground-truth assignment strategy during training and a refined version of SoftNMS at inference time. Our solution is ranked 2nd on the public leaderboard with 26.62% average mAP and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge. Our code is available at https://github.com/happyharrycn/actionformer_release.
Fluency score: 5
Fluency issues: 

Clarity score: 4
Clarity issues: [The term "ActionFormer" might be unclear to those unfamiliar with the specific technology or challenge; "Recall@1x at tIoU=0.5" may not be clear for non-specialist audiences.]

Grammaticality score: 5
Grammaticality issues: 

Readability score: 4
Readability issues: [The use of abbreviations and technical terms like "SoftNMS", "mAP", and "Recall@1x at tIoU=0.5" might affect the readability for a general audience.]

Coherence score: 5
Coherence issues:
==========
Introduction:  The Ego4D Moment Queries (MQ) task aims to localize all moments of actions in time and recognize their categories within an untrimmed egocentric video. We adopt a two-stage approach for this task, where clip-level features are first extracted from raw video frames using a pre-trained feature network, followed by a temporal localization model that predicts the onset and offset of action instances as well as their categories. Our submission last year explored the combination of a latest localization model (ActionFormer[12]) and a strong set of video features[9]. This work seeks to improve the localization model . A limitation of ActionFormer[12]lies in its label assignment at training time; annotated action instances are assigned to candidate moments based oncenter sampling, a heuristic that designates positive labels to moments proximal to the center of an action instance. Recent literature in object detection, however, shows that such static assignment strategy is insufficient for complex spatial configuration of objects. Inspired by this insight, we propose to adapt SimOTA[5], a dynamic label assignment strategy, for temporal action localization. SimOTA assigns ground-truth action instances to the candidate moments on the fly by solving an optimal transport problem. Further, we refine SoftNMS to account for densely overlapping actions in Ego4D. Equipped with these modifications, our solution extends our prior work and is ranked 2ndon the public leaderboard. Specifically, our solution attains 26.62% average mAP and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge. We hope our work will shed light on future development in temporal action localization and egocentric vision.
====================
	 Textbooks Are All You Need II: phi-1.5 technical report
==========
Abstract:  We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics.
Fluency score: [4]
Fluency issues: ["--" is used inconsistently which might slightly disrupt reading flow]

Clarity score: [4]
Clarity issues: [Technical jargon and model names may confuse non-expert readers]

Grammaticality score: [5]
Grammaticality issues: [No grammatical issues detected]

Readability score: [4]
Readability issues: [Complex sentence structures and specialized terminology might challenge some readers]

Coherence score: [4]
Coherence issues: [The argument moves quickly between different aspects which could confuse readers not familiar with the context]
==========
Introduction:  Over the past few years, Large Language Models (LLMs) have transformed the field of Natural Language Processing. More broadly, they hold the promise of a paradigm shift for human-computer interaction. These advancements have far-reaching economic implications, as well as the potential to redefine our conceptual frameworks of artificial intelligence and perhaps even cognition itself. Moreover, the latest generation of models such as GPT-4[Ope23]have demonstrated remarkable improvements over their predecessors, offering capabilities previously thought to be unattainable in the short term; see for example[BCE{}^{+}23]for an in-depth comparison between GPT-4 and its predecessor GPT-3.5. The improvement from one generation of LLMs to the next seems at the moment to primarily stem fromscale, with the most powerful models nearing trillions of parameters and trillion of tokens for training data (for example, PaLM[CND{}^{+}22]has 540 billion parameters and was trained on 780 billion tokens). A natural question arises: Is this large scale indispensable for achieving high levels of capability? Far from being merely an academic question, answering this holds implications across several dimensions. Economically, the cost of training, deploying, and maintaining such large models can be substantial. Scientifically, understanding whether similar capabilities can be achieved at a smaller scale could provide insights into the architectures and development of intelligent systems. From a responsible AI standpoint, the energy consumption of large-scale models is becoming an increasing concern, as is the question of how controllable or governable these large models can be. Finally, the ability to train compact models with cutting-edge capabilities would democratize advanced AI, enabling a broader range of individuals and organizations to study and deploy them, instead of being an exclusive domain of a few with vast computational resources. In this work we continue the investigation into the fundamental question of “how small can a LLM be to achieve certain capabilities”. The prior work[EL23]considered this question for the task of “speaking fluent English”, while the subsequent work[GZA{}^{+}23]considered the more challenging task of coding simple functions in Python. Here we focus on the more elusive concept ofcommon sense reasoning, a notoriously challenging task for AI[SBBC21]. Our results are summarized in Figure1. In a nutshell we buildphi-1.5, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves common sense reasoning benchmark results comparable to models ten times its size that were trained on datasets more than ten times larger. Moreover, our dataset consists almost exclusively of synthetically generated data (closely following the approach from[GZA{}^{+}23], see next section for more details), which has important implications for the potential to control for the notoriously challenging issue of toxic and biased content generation with LLMs[BGMMS21]. Additionally, we discuss the performance of a relatedfiltered web dataenhanced version ofphi-1.5, which we callphi-1.5-web​. We open-source our rawphi-1.5model (without instruction fine-tuning or any other stage of alignment) to empower the research community in its work on some of the most urgent questions around LLMs: in-context learning, mechanistic interpretability, and mitigation strategies for hallucinations, toxic content generation, and biased outputs. Indeed,phi-1.5is the first LLM at the one billion parameters scale to exhibit most of the relevant traits of larger LLMs for research on these topics. We hope thatphi-1.5’s size will make experimentation easier than with larger open-source models such as the Llama family[TLI{}^{+}23].
====================
	 A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition
==========
Abstract:  Human Activity Recognition (HAR) has become one of the leading research topics of the last decade. As sensing technologies have matured and their economic costs have declined, a host of novel applications, e.g., in healthcare, industry, sports, and daily life activities have become popular. The design of HAR systems requires different time-consuming processing steps, such as data collection, annotation, and model training and optimization. In particular, data annotation represents the most labor-intensive and cumbersome step in HAR, since it requires extensive and detailed manual work from human annotators. Therefore, different methodologies concerning the automation of the annotation procedure in HAR have been proposed. The annotation problem occurs in different notions and scenarios, which all require individual solutions. In this paper, we provide the first systematic review on data annotation techniques for HAR. By grouping existing approaches into classes and providing a taxonomy, our goal is to support the decision on which techniques can be beneficially used in a given scenario.
Fluency score: 5
Fluency issues: None

Clarity score: 4
Clarity issues: The phrase "The annotation problem occurs in different notions and scenarios" might be slightly confusing.

Grammaticality score: 5
Grammaticality issues: None

Readability score: 4
Readability issues: Dense terminology and compound sentences may slightly impede readability for non-expert readers.

Coherence score: 5
Coherence issues: None
==========
Introduction:  In the last decade, we have witnessed the spread and adoption of sensors, wearables, the Internet of Things (IoT), the Internet of Medical Things (IoMT), and edge computing technologies(Li et al.,2015). Sensors can detect and measure physical properties such as temperature, pressure, light, and motion. They are becoming ubiquitous in various industries, including automotive, aerospace, and consumer electronics. Moreover, their miniaturization has led to their integration into wearables, such as fitness trackers, smartwatches, clothes, and dedicated devices. Wearables are frequently used to track various aspects of a person’s health and activity. Recent developments even involve integrating medical sensors for remote patient monitoring, digital therapeutics, and real-time intervention into wearables(Seneviratne et al.,2017; Dunn et al.,2018; Cheng et al.,2021). On the other side, the IoT is formed by networks of interconnected devices, vehicles, and buildings that communicate with each other and exchange data. It has been adopted across various industries, including home automation, agriculture, and manufacturing. In addition, IoT devices can be remotely monitored and controlled, improving efficiency and productivity(Li et al.,2015). Instead, IoMT refers to using IoT devices in medical applications, enabling the healthcare providers’ capacity to monitor patients remotely, collect data for analysis, improve patient outcomes, and reduce healthcare costs(Vishnu et al.,2020). Finally, edge computing refers to processing data at or near the source rather than sending it to a central/remote server for processing. This technology has become increasingly important as the amount of data generated by IoT and IoMT devices grows. Edge computing enables faster processing times and reduces the latency and amount of data that needs to be transmitted over the network(Baker and Xiang,2023). The adoption and spread of these technologies have revolutionized various industries and enabled new applications and capabilities. With such systems now being ubiquitous, they serve as a common infrastructure for recognizing human activity, as described next. Human Activity Recognition (HAR):In such a context, HAR is a central research field that finds applications in various areas, including healthcare, sports, industry, and smart homes. HAR refers to the ability to identify and classify human activities using sensors, wearables, or other devices that capture data about the person’s movements and actions. With regard to healthcare, HAR can be used to monitor a patients’ status and detect abnormalities or changes in their behavior that may indicate a deterioration of health or the onset of a medical condition. For example, HAR can be used to detect falls of elderly patients or to monitor the movements of patients with Parkinson’s disease or other motor disorders(Demrozi et al.,2020). Moreover, HAR also has applications in sports and fitness to monitor the athletes’ performance and technique, helping them to improve their training and prevent injuries. HAR can also be used in activity tracking devices, such as fitness trackers, to provide users with insights into their daily activity levels and help them to achieve their fitness goals. In addition, HAR automates various tasks in smart homes based on the occupant’s activities. For example, lights can be turned on or off automatically based on the person’s movements, or the thermostat can be adjusted based on the person’s activity level(Demrozi et al.,2020,2021). HAR is related to various technologies, including sensors, wearables, IoT, IoMT, edge computing, machine learning (ML), Deep Learning (DL), and Artificial Intelligence (AI). Sensors and wearables are used to capture data about the person’s movements and actions, which is then used to identify and classify human activities in HAR applications. IoT and IoMT systems are used to collect data from sensors and wearables, which can be transmitted over the network for processing and analysis. Edge computing can process this data at or near the source, reducing latency and enabling real-time processing of HAR data(Baker and Xiang,2023; Vishnu et al.,2020). In HAR systems, the data collected from such devices is analyzed to classify a user’s activity. While, in principle, this analysis can be done based on heuristics (e.g., a feature exceeds certain thresholds, etc.), ML- and DL-based HAR techniques have become the most popular solution. Using them, also more complex analyses can be carried out, allowing for reliable recognition of activities even in data in which the properties or patterns that represent a certain activity or behavior are not obvious. ML- and Dl-based HAR methods can also integrate other data sources, such as environmental data, to provide more comprehensive insights into human behavior and activity(Demrozi et al.,2020). As the technology continues to improve and becomes widely available, we expect to see further advancements and new applications for ML-based HAR(Baker and Xiang,2023). When generating HAR model, a set of sensor data is recorded first. This data is then labeled with the activities under consideration. This step is calledannotation. Next, a machine-learning model is trained, which can then be used to classify unlabeled data. In the following, we describe the individual steps(Demrozi et al.,2020; Gupta et al.,2022)that are involved in creating a HAR system in more detail. An overview is shown in Figure1. Definition of Target Activities: Definition and analyzation of the real-world characteristics of the target activities to be recognized. For example, this can be their duration, distribution, similarity with other activities, etc. Device Setup: Identification and study of requirements and determination of the devices to be used in the data collection phase, based on the target human activities. Data Collection: In this phase, data is collected from sensors, wearables, or other devices that capture information about the person’s movements and actions. Data Annotation: The process of assigning labels to the human activities being performed. Labels are crucial in supervised learning as they provide the ground truth or correct answers that guide the learning process. By associating input data with corresponding labels, the model can learn to make accurate predictions and generalize its knowledge to unseen examples. Data Preprocessing: The collected data is then preprocessed to remove noise, irrelevant information are filtered out, and the data is prepared for analysis. As a part of this, the following analysis is carried out: Feature extraction: The preprocessed data is analyzed to extract relevant features that can be used to classify human activities. These features may include movement patterns, body position, or other characteristics. Feature selection: Once the features have been extracted, a subset of features may be selected for use in the classification model. This helps to reduce the dimensionality (e.g., the number of features) of the data and improve the accuracy of the model. Model generation and testing: A HAR (i.e., ML or DL) model is developed to classify human activities based on the selected features in this phase. The model may be trained using a labeled dataset or unsupervised learning techniques. After the model has been generated, the following steps are carried out before the model is ready to be used: Model evaluation: The developed model is then evaluated using a test dataset to assess its accuracy and performance. This phase helps to identify any issues or areas for improvement in the model. Deployment: Finally, the developed model is deployed to a real-world environment, where it is used to classify human activities. Data Annotation in HAR:The most labor-intensive step in creating a HAR system is data annotation, which involves creating a labeled dataset for training the ML/DL models. Manual labeling, in which human annotators manually label each recorded sample with the corresponding activity, is a common approach in data annotation. Although time-consuming and resource-intensive, it can produce high-quality labels that are accurate and consistent. Nevertheless, several factors can pose challenges in the manual data annotation process for HAR systems.Firstly, subjectivity can lead to inconsistencies and errors in labeling as the interpretation of the activity being performed can vary among annotators. This can ultimately affect the accuracy of the ML/DL model.Secondly, the data annotation process can be time-consuming, particularly when labeling large amounts of data, which can cause delays in the development of the HAR system and increase project costs.Thirdly, the economic cost can be a limiting factor since hiring human annotators or utilizing crowdsourcing platforms for data labeling can become expensive, mainly when the studied activities are complex.Fourthly, the variability of human activities can also pose a challenge in the annotation process. Since different individuals can perform activities differently, creating accurate and consistent labels for the data can be challenging.Lastly, label noise may exist in annotated data, resulting in errors in the labeling process. Label noise can occur due to human error, subjectivity, or inconsistencies in the annotation process, which ultimately reduces the performance of the HAR system’s ML/DL model. Careful consideration of these limitations and appropriate methods can help mitigate these challenges and improve the accuracy and performance of the final HAR system. Alternatively, automated methods, such as rule-based systems or unsupervised learning algorithms, can be employed for data annotation. These approaches are more efficient and scalable but may be less precise or necessitate additional manual validation. The quality of the annotated data is pivotal to the efficacy of the HAR system. Inaccurate or inconsistent labeling can cause poor ML/DL model performance, leading to the misclassification of human activities(Diete et al.,2017; Adaimi and Thomaz,2019). There are several (partial) possible solutions to the limitations of the annotation process in HAR(Diete et al.,2017; Adaimi and Thomaz,2019). Some of these solutions include: Standardization: Standardizing the annotation process can help to reduce subjectivity and increase consistency in the labeling process. This can be achieved by defining clear guidelines and procedures for annotators to follow and providing training and feedback to ensure the quality of the annotations. Automation: Automated methods, such as unsupervised learning algorithms or rule-based systems, can be used to annotate data. These methods can be faster and more scalable than manual labeling and reduce the annotation process’s cost. Active learning: Active learning techniques can reduce the labeled data needed for training an ML or DL model. This involves selecting the most informative data samples for annotation, which can help reduce the labeling process’s time and cost. Crowdsourcing: Crowdsourcing platforms can be used to engage many annotators to label the data. This can be a cost-effective solution, as well as provide a diverse range of perspectives on the activity being performed. Quality control: Quality control measures can be implemented to ensure the accuracy and consistency of the labeled data. This can include using multiple annotators to label the same data samples and comparing their annotations, as well as conducting regular checks on the quality of the annotations. While these solutions can enhance the accuracy and performance of the final HAR system, they do not completely eliminate the cost and time needed for the annotation process. Systematic Review Objectives:This paper aims to systematically review existing methodologies for automating data annotation in HAR. The objective is to identify the strengths and limitations of different techniques and provide insights into the current research and ongoing trends in this area. Specifically, the paper explores different approaches and algorithms used in automatic data annotation techniques. This does not only help in developing novel techniques in the future, but also supports the choice of an appropriate labeling technique for a given application. This review considers 2401 publications on automating data annotation in HAR. To the best of our knowledge, no systematic review has been published prior to this paper. The absence of such a review aggravates overseeing the different technologies used in this area, makes it difficult to follow recent trends, and leaves unclear which technical solution is most beneficial for realizing a given scenario. We in this paper close this gap by providing the first systematic review on this field of research. Paper organization:The rest of the paper is organized as follows. Section2delves into the background of HAR, presenting a comprehensive overview of the field, including its applications and challenges. Following that, Section3discusses the selection criteria for annotation methods in HAR, examining the key factors that we consider when choosing appropriate techniques. Section4presents an in-depth analysis and discussion of various annotation methods employed in HAR, exploring their strengths, limitations, and effectiveness in accurately identifying and classifying human activities Finally, Sections5and6conclude the paper by summarizing the key findings and contributions of the study, emphasizing the significance of automatic annotation methods in advancing HAR research and suggesting potential avenues for future exploration in this area.
====================
	 Elementwise Language Representation
==========
Abstract:  We propose a new technique for computational language representation called elementwise embedding, in which a material (semantic unit) is abstracted into a horizontal concatenation of lower-dimensional element (character) embeddings. While elements are always characters, materials are arbitrary levels of semantic units so it generalizes to any type of tokenization. To focus only on the important letters, the $n^{th}$ spellings of each semantic unit are aligned in $n^{th}$ attention heads, then concatenated back into original forms creating unique embedding representations; they are jointly projected thereby determining own contextual importance. Technically, this framework is achieved by passing a sequence of materials, each consists of $v$ elements, to a transformer having $h=v$ attention heads. As a pure embedding technique, elementwise embedding replaces the $w$-dimensional embedding table of a transformer model with $256$ $c$-dimensional elements (each corresponding to one of UTF-8 bytes) where $c=w/v$. Using this novel approach, we show that the standard transformer architecture can be reused for all levels of language representations and be able to process much longer sequences at the same time-complexity without "any" architectural modification and additional overhead. BERT trained with elementwise embedding outperforms its subword equivalence (original implementation) in multilabel patent document classification exhibiting superior robustness to domain-specificity and data imbalance, despite using $0.005\%$ of embedding parameters. Experiments demonstrate the generalizability of the proposed method by successfully transferring these enhancements to differently architected transformers CANINE and ALBERT.
Fluency score: 4
Fluency issues: ["$n^{th}$ spellings", "$n^{th}$ attention heads", "$w$-dimensional", "UTF-8 bytes"]

Clarity score: 2
Clarity issues: Complex sentence structures, jargon-heavy (e.g., "semantic unit", "horizontal concatenation", "element embeddings", "architectural modification").

Grammaticality score: 5
Grammaticality issues: None

Readability score: 2
Readability issues: Dense technical language and numerous acronyms

Coherence score: 3
Coherence issues: Use of undefined terms, acronyms without explanation (e.g. BERT, CANINE, ALBERT), and assumption of reader's familiarity with the subject.
==========
Introduction:  We understand texts from various levels of semantics but current language representation strategies leverage tokenization which relies on a certain level of semantics exclusively, fully ignoring the hierarchical structures of natural languages. Text is encoded to a sequence of integers then projected into fixed-size latent embeddings. These types of expressions result in a recursive trade-off between different levels of language representations: (sub)word-level models indirectly recover characters(Itzhak and Levy,2021)but it is not always sufficient for spelling-sensitive tasks, character-level models need much longer sequences to reach comparable performance to word-level models thus amplifying the computational complexity of self-attention. Some recently proposed studies(Clarket al.,2022; Godeyet al.,2022; Tayet al.,2021)attempt to solve this by downsampling long character sequences into an acceptable length, however, they share the same limitation as pure character-level models because their valid downsampling rates are constrained to relatively small values mainly due to the smoothing and overhead issues. Instead, we propose elementwise embedding, a language representation technique for addressing this trade-off in which a set of lower-dimensional character embeddings calledelementsare horizontally concatenated into a single latent embedding calledmaterialthat mimics a semantic unit such as a word, phrase, sentence and etc. Using this method, models with higher-dimensional hidden representations create each semantic unit (i.e., a material) by concatenating a greater numbers of characters (i.e., elements), which implies that larger models can process longer sequences than smaller ones at the same computational complexity. This means that the acceptable sequence length scales with the size of a transformer model, but the complexity is fixed as that of its attention. Assuming that a character-level GPT-3 [processing 2048 12,288-dimensional token embeddings with 96 attention heads;Brownet al.(2020)] is trained with elementwise embedding, it aligns a sequence of2,048\times 96=296,608characters which is 96x longer at the sameO(N\sqrt{N})_{N=2048}complexity. The proposed methodology follows the two-step framework of"reshape, then focus". First, the given text is encoded as a sequence ofuvUTF-8 bytes and projected into a(uv,c)embedding matrix in which each row is ac-dimensional element; it’s"reshaped"into a(u,w)embedding matrix in which each row is aw-dimensional material (e.g., a word), wherec=w/v. As a result, one material consists ofvelements so that we can alignuvelements at theO(u^{2})complexity using multihead self-attention(Vaswaniet al.,2017)withvattention heads. Eachi^{th}column of this(u,v)material matrix is a sequence of thei^{th}elements of allumaterials, soi^{th}attention head alignsi^{th}elements. This operation is most straightforward when a material is avletters word:i^{th}spellings of alluwords are aligned ini^{th}attention head, then concatenated back creating unique embedding representations wherei\in[1,v]. Each attendedi^{th}spelling is referred asfocusbecause it is quite similar to that we often read text inferring the meanings of words by"focusing"on a few important letters. The contextual importance of each word is determined jointly via linear transformation. Theoretically, this can be understood as lowering the entropy of character sequences concentrating distributed probabilities into several important spellings. Technically, it is just to pass a(u,w)word embedding matrix in which each row is a horizontal concatenation ofvc-dimensional character embeddings as input to a transformer model withw-dimensional hidden layers. It’s identical to aligning words using character-level semantics and vice versa. In practical implementation, focus is performed by multihead attention of the parent (any transformer model) by setting the number of attention heads toh=v, so applying elementwise embedding is simply to replace the embedding table of parent model with a set of 256c-dimensional character embeddings (each mapping to one of UTF-8 bytes; elements) and a following tensor reshaping operation. Neither structural modification of neural networks nor additional operations such as up/downsampling that entail unnecessary engineering efforts and overheads are required. Fig1offers an intuitive visualization of elementwise embedding.
====================
	 Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications
==========
Abstract:  This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.
Fluency score: 4
Fluency issues: [The overall structure is fairly fluent, but certain parts might challenge non-specialist readers, like "semantic features," "intra-link errors," and "joint source-channel coding algorithm."]

Clarity score: 3
Clarity issues: [Phrases such as "semantic-forward (SF)," "reduces forwarding payload," and "iteratively exchange the extrinsic information" may not be clear to those unfamiliar with the subject.]

Grammaticality score: 5
Grammaticality issues: []

Readability score: 3
Readability issues: [The text is dense with technical terms and concepts that may be difficult for a layperson to understand quickly.]

Coherence score: 4
Coherence issues: [The sentence structure and flow are coherent, but the high density of technical language might slightly impair coherence for readers without a background in the subject.]
==========
Introduction:  Cooperative communications are acknowledged schemes to improve the transmission quality. One of the most important categories of cooperative communications is in the form of relaying. Although relaying requires extra energy and time slot, it is an effective solution when the path-loss of the direct link is very large. On the other hand, with the research trend towards the sixth-generation (6G) wireless networks[13], various transmission technologies have been invented, among which semantic communications[16]are considered to have a great potential in media transmissions. The semantic encoder extracts the semantic features for transmissions[17], while the semantic decoder works in a similar way to the generativeartificial intelligence (AI)[4]. Inspired by the principle of semantic communications, this letter proposes a novel framework,semantic-forward (SF)relaying, for cooperative communications. There have been already diverse relaying schemes in the literature[8]. One simple relaying scheme isamplify-and-forward (AF), in which the relay directly amplifies the signals received from the source and then forwards to the destination. In 1979, Cover and El Gamal[2]established the fundamental theorems of relaying systems, and proposed thedecode-and-forward (DF)andcompress-and-forward (CF)schemes. In the DF scheme, the relay decodes the received signals at the first step, and then the recovered information sequence is forwarded or discarded, respectively, depending on the recovery is error-free or not. In the CF scheme, the relay quantizes and compresses its received signals into the relay information to be transmitted to the destination. Beyond DF,lossy-forward (LF)[10]was proposed to overcome the drawback of DF, where the communication resources are completely wasted once errors occur in the relay information. In the LF scheme, the relay always forwards the relay information to the destination regardless of whether or not intra-link error is detected at the relay. At the destination, a joint decoder recovers the source information with the help of the relayed information, based on the principle of correlated sources transmission. Nevertheless, the previous relaying schemes are designed for general types of information, which do not exploit the features of information to improve the information efficiency. By adopting the semantic communications, the system adaptively exploits diverse types of information. Hence, we aims at designing a relaying systems where the relay forwards the semantic information to the destination, i.e., SF relaying, so that the destination can utilize the semantic information to help recovering the source information. The terminology of SF has been used for the first time in[11], up to the authors’ maximum knowledge, where the source transmits semantic information to the relay, and the relay translates and forwards the processed semantic information to the destination. However, there is no direct source-destination link in[11]. Different from the relay-assisted semantic communications in[11], this work proposes a semantic-assisted relaying system. In our proposed SF relaying system, the relay reconstructs the information received from the source at the first step. Then, in spite of whether or not the reconstruction error-free, the relay extracts the semantic information and sends it to the destination. The semantic coding achieves robustness of the relaying system against errors, and hence can reduce the payload in therelay-destination (R-D)link. At the destination, a joint decoder performs iterative decoding utilizing the Turbo principle[1]that exchanges the extrinsic information between the lossy information of thesource-destination (S-D)link and the semantic information of theR-Dlink. In this way, the SF relaying can help for the lossless recovery of the original information at the destinations, even in bad channel conditions. With the pre-trained semantic encoder/decoder, the SF relaying can reduce the payload of the R-D link in practical systems. The contributions of this letter are summarized as follows: We propose a novel relaying framework, i.e., SF relaying, which adopts semantic communications at the relay and the destination to reduce the payload of theR-Dlink. We design a joint source-channel coding algorithm for SF relaying systems, where the destination can losslessly recover the source information with the assistance of the semantic information received from the relay. We conduct a series of simulations with image transmissions to evaluate the performance of SF relaying. The simulation results demonstrate that SF relaying systems can exploit the semantic information to reduce theEuclidean distance (ED)and improve the image quality. Notation.Capital lettersX,Y,V,Udenote the random variables for constructing information sequences.Mrepresents the codeword satisfying the link rate constraintR.
====================
	 Learning Spatially-Continuous Fiber Orientation Functions
==========
Abstract:  Our understanding of the human connectome is fundamentally limited by the resolution of diffusion MR images. Reconstructing a connectome's constituent neural pathways with tractography requires following a continuous field of fiber directions. Typically, this field is found with simple trilinear interpolation in low-resolution, noisy diffusion MRIs. However, trilinear interpolation struggles following fine-scale changes in low-quality data. Recent deep learning methods in super-resolving diffusion MRIs have focused on upsampling to a fixed spatial grid, but this does not satisfy tractography's need for a continuous field. In this work, we propose FENRI, a novel method that learns spatially-continuous fiber orientation density functions from low-resolution diffusion-weighted images. To quantify FENRI's capabilities in tractography, we also introduce an expanded simulated dataset built for evaluating deep-learning tractography models. We demonstrate that FENRI accurately predicts high-resolution fiber orientations from realistic low-quality data, and that FENRI-based tractography offers improved streamline reconstruction over the current use of trilinear interpolation.
Fluency score: 4
Fluency issues: The overall fluency is good, but the extensive use of technical jargon and specialized terms could impede fluency for a lay reader.

Clarity score: 3
Clarity issues: The technical jargon and specialized terms ("diffusion MR images," "tractography," "fiber orientation density functions") may make the text unclear to readers unfamiliar with the field.

Grammaticality score: 5
Grammaticality issues: No issues detected.

Readability score: 3
Readability issues: The specialized terminology and complex sentence structures make the text less accessible to a general audience.

Coherence score: 4
Coherence issues: The text maintains topic coherence but assumes a level of background knowledge from the reader that may not be present.
==========
Introduction:  Mapping the human connectome relies upon a continuous and accurate representation of the underlying brain tissue. This is needed for tracing streamlines, resolving crossing fibers, and deciding when to terminate a tract. Often, tractography algorithms rely on simple trilinear interpolation to “fill out” a continuous field from discretely-sampled diffusion magnetic resonance images (dMRIs). If this interpolation could be improved, then tractography algorithms could produce more detailed and accurate human white matter (WM) fiber tracts. In this work, we propose FENRI (Fiber orientations fromExplicitNeuralRepresentatIons), a novel deep learning-based super-resolution model for estimating fODFs continuously in space. We demonstrate FENRI’s capabilities through the following experiments: 1) a quantitative evaluation of fODF reconstruction in Human Connectome Project (HCP) data, 2) a qualitative evaluation of tractography in HCP data, and 3) a quantitative measure of tractography performance on a new, expanded simulation dataset. As an image upsampler, FENRI outperforms more generic single-image super-resolution (SISR) methods on a variety of test metrics. We also show how, as a tractography enhancement, FENRI’s explicit representation sampling provides a powerful improvement over standard tractography methods. Background.Reconstructing streamlines from diffusion-weighted images (DWIs) requires a model of neuron fiber directionality. One popular model is the general fODF represented by coefficients in the spherical harmonic (SH) orthonormal basis, estimated by constrained spherical deconvolution (CSD)[7]. Several deep learning models have recently been proposed to super-sample diffusion representations. For example, Qin et. al., 2021 used convolutional neural networks (CNNs), an efficient sub-pixel CNN (ESPCN) layer, and high-resolution T1w volumes to predict high-resolution diffusion model parameters[10,13]. However, these previous works were limited to upsampling by aninteger upscaling factor, e.g.2\times, which is not ideal for estimating continuous fields. The recently proposed implicit neural representation (INR) method, which learns continuous-valued representations in some Euclidean space, is one solution to this challenge[14]. INRs are most commonly applied to 3D rendering, but INR-like models have been used in SISR. For example, the Local Implicit Image Function where a low-resolution input image is encoded into a feature space and sampled continuously for upsampling[3]. To our knowledge, the only proposed model that utilizes INRs for super-resolving dMRIs is given in[4], which focused on uncertainty in continuous predictions rather than tractography. We place FENRI alongside these INR models, but note that FENRI does not model animplicitfunction, but anexplicitfunction of SH coefficients.
====================
	 Explainable Boosting Machines with Sparsity -- Maintaining Explainability in High-Dimensional Settings
==========
Abstract:  Compared to "black-box" models, like random forests and deep neural networks, explainable boosting machines (EBMs) are considered "glass-box" models that can be competitively accurate while also maintaining a higher degree of transparency and explainability. However, EBMs become readily less transparent and harder to interpret in high-dimensional settings with many predictor variables; they also become more difficult to use in production due to increases in scoring time. We propose a simple solution based on the least absolute shrinkage and selection operator (LASSO) that can help introduce sparsity by reweighting the individual model terms and removing the less relevant ones, thereby allowing these models to maintain their transparency and relatively fast scoring times in higher-dimensional settings. In short, post-processing a fitted EBM with many (i.e., possibly hundreds or thousands) of terms using the LASSO can help reduce the model's complexity and drastically improve scoring time. We illustrate the basic idea using two real-world examples with code.
Fluency score: 4
Fluency issues: Generally fluent but some long technical sentences could challenge non-expert readers.

Clarity score: 3
Clarity issues: Overuse of domain-specific jargon and acronyms can confuse readers unfamiliar with the topic. 

Grammaticality score: 5
Grammaticality issues: None.

Readability score: 3
Readability issues: The text is dense with technical terms and may not be readily accessible to a lay audience. 

Coherence score: 4
Coherence issues: The text introduces ideas logically but heavy use of technical language may affect overall coherence for some readers.
==========
Introduction:  Explainable boosting machines (Nori et al. 2019), or EBMs for short, are a modern class ofgeneralized additive models(GAMs) that can offer both competitive accuracy and explicit transparency and explainability, which are often considered to be two opposing goals of a machine learning (ML) model. For example, full-complexity models, like random forests (Breiman 2001), tend to be highly competitive in terms of accuracy, but are readily less transparent and explainable (due in part to the high-order interaction effects often captured by such black-box models). Essentially, with EBMs, it’s possible to “have your cake and eat it too.” In short, EBM models have the general form where gis alink functionthat allows the model to handle various response types (e.g., thelogitlink for logistic regression or the identify function for ordinary regression with continuous outcomes); \theta_{0}is a constant intercept (or bias term); f_{i}is theterm contribution(orshape function) for predictorx_{i}(i.e., it captures the main effect ofx_{i}); f_{ij}is the term contribution for the pair of predictorsx_{i}andx_{j}(i.e., it captures the joint effect, or pairwise interaction effect ofx_{i}andx_{j}). Similar togeneralized additive models plus interactions(Lou et al. 2013), or GA2Ms for short, the pairwise interaction terms are determined automatically using the FAST algorithm described in Lou et al. (2013). In short, FAST is a novel, computationally efficient method for ranking all possible pairs of feature candidates for inclusion into the model (by default, the top 10 pairwise interactions are used). The primary difference between EBMs and GA2Ms is in how the shape functions are estimated. In particular, EBMs usecyclic gradient boosting(Nori et al. 2019; Wick, Kerzel, and Feindt 2020) to estimate the shape functions for each feature (and selected pairs of interactions) in a round-robin fashion using a low learning rate to help ensure that the order in which the feature effects are estimated does not matter. Estimating each feature one-at-a-time in a round-robin fashion also helps mitigate potential issues withcollinearitybetween predictors (Nori et al. 2019; Wick, Kerzel, and Feindt 2020). However, in contrast to the more common gradient boosting machine (J. H. Friedman 2001, 2002), or GBM for short, which can ignore irrelevant inputs, EBMs include at least one term in the model for each feature: one main effect (f_{i}) for each predictor, and a term for each selected pairwise interaction effect. This is due to the cyclic nature of the underlying boosting framework. For example, an EBM applied to a training set withp=300features will result in a model with at least 300 terms. While EBMs are consideredglass-boxmodels, an EBM with, say, hundreds or thousands of terms, starts to become much less transparent and explainable. Moreover, the larger the fitted model (i.e., the more terms there are), then the more time it will take the EBM to make predictions, making larger models less fit for deployment and production. To this end, we propose in Section2a way to introduce sparsity into a fitted EBM by rescaling the individual terms (i.e., thef_{i}andf_{ij}) via regression coefficients estimated from a method called the LASSO (Tibshirani 1996). Consequently, by nature of theL_{1}regularization enforced by the LASSO, many of these coefficients can be estimated to be zero, resulting in a reduced EBM with far fewer terms (and hopefully, comparable accuracy)!
====================
	 Approximate inference of marginals using the IBIA framework
==========
Abstract:  Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity trade-off is controlled using user-defined clique size bounds. Results for several benchmark sets from recent UAI competitions show that our method gives either better or comparable accuracy than existing variational and sampling based methods, with smaller runtimes.
Fluency score: 4
Fluency issues: [No specific issues, but slightly complex sentence structures could slightly impede fluency for non-specialized readers]

Clarity score: 3
Clarity issues: ["IBIA paradigm", "SLCTF", "CTFs in the SLCTF" are not explained and may confuse readers]

Grammaticality score: 5
Grammaticality issues: [No issues detected]

Readability score: 3
Readability issues: [Complex terminology and lengthy sentences]

Coherence score: 4
Coherence issues: [Use of acronyms without explanation (IBIA, SLCTF, CTFs) could affect coherence for some readers]
==========
Introduction:  Discrete probabilistic graphical models (PGM) including Bayesian networks (BN) and Markov networks (MN) are used for probabilistic inference in a wide variety of applications. An important task in probabilistic reasoning is the computation of posterior marginals of all the variables in the network. Exact inference is known to be #P-complete[Roth,1996], thus necessitating approximations. Approximate techniques can be broadly classified as sampling based and variational methods. Sampling based methods include Markov chain Monte Carlo based techniques like Gibbs sampling[Gelfand,2000, Kellyet al.,2019]and importance sampling based methods[Gogate and Dechter,2011, Friedman and Van den Broeck,2018, Kasket al.,2020, Broka,2018, Louet al.,2019,2017b,2017a, Marinescuet al.,2019,2018]. An advantage of these methods is that accuracy can be improved with time without increasing the required memory. However, in many benchmarks the improvement becomes slow with time.Moreover, many of the recent sampling/search based techniquesKasket al.[2020], Broka [2018], Louet al.[2019,2017b,2017a], Marinescuet al.[2019,2018]have been evaluated either for approximate inference of partition function (PR) or for finding the marginal maximum a posteriori assignment (MMAP). Currently, there are no published results for posterior marginals (MAR) using these methods, and the publicly available implementations do not support the MAR task.Alternatively, variational techniques can be used. These include loopy belief propagation (LBP)[Frey and MacKay,1998]region-graph based techniques like generalized belief propagation (GBP)[Yedidiaet al.,2000]and its variants[Heskeset al.,2003, Mooij and Kappen,2007, Linet al.,2020], mini-bucket based schemes like iterative join graph propagation (IJGP)[Mateescuet al.,2010]and weighted mini-bucket elimination (WMB)[Liu and Ihler,2011]and methods that simplify the graph structure like edge deletion belief propagation (EDBP) and the related relax-compensate-recover (RCR) techniques[Choiet al.,2005, Choi and Darwiche,2006,2010]. While the accuracy-complexity trade-off can be achieved using a single user-defined clique size bound in mini-bucket based methods, it is non-trivial in many of the other region graph based methods. Most of these techniques use iterative message passing to solve an optimization problem, for which convergence is not guaranteed and even if possible, can be slow to achieve.Non-iterative methods like Deep Bucket Elimination (DBE)[Razeghiet al.,2021]and NeuroBE[Agarwalet al.,2022]are extensions of bucket elimination that approximate messages using neural networks. However, training these networks takes several hours. Moreover, the publicly available implementations of these methods do not support the MAR task. The recently proposedincremental build-infer-approximate(IBIA) framework[Bathla and Vasudevan,2023]uses a different approach. It converts the PGM into a sequence of calibrated clique tree forests (SCTF) with clique sizes bounded to a user-defined value.Bathla and Vasudevan [2023]show that the normalization constant (NC) of clique beliefs in the last CTF in the sequence isa good approximationof the partition function of the overall distribution. This framework has two main advantages. Firstly, since it is based on clique trees and not loopy graphs, the belief propagation step is non-iterative. Therefore, it is fast and has no issues related to convergence. Secondly, it provides an easy control of the accuracy complexity trade-off using two user-defined parametersand hence can be used in anytime manner.However, the framework inBathla and Vasudevan [2023]cannot be used to infer marginals. This is because only the clique beliefs in the last CTF account for all factors in the PGM. Beliefs in all other CTFs account for a subset of factors and thus, cannot be used for inference of marginals. Contributions of this work:In this paper, we propose a method for marginal inference that uses the IBIA framework. We show that the approximation algorithm used in this framework preserves the within-clique beliefs. Based on this property, we modify the data structure generated by IBIA to add links between adjacent CTFs. We refer to the modified data structure as asequence of linked clique tree forests(SLCTF). We propose a heuristic belief update algorithmthat back-propagates beliefs from the last CTF to the previous CTFs via the links and re-calibrates each CTF so that the updated beliefs account for all factors in the PGM. We also propose a greedy heuristic for the choice of links used for belief update.Results for several UAI benchmark sets show that our method givesan accuracy that is better than or comparableto the existing variational and sampling based methods, with competitive runtimes. For the special case of BNs, we show that if the incremental build step in IBIA is performed in the topological order of variables then (a) the estimated partition function is guaranteed to be one if no evidence variables are present (b) the prior marginals of all variables are consistent across all CTFs in the sequence and (c) once all the evidence variables have been added to the SLCTF, the posterior marginals of variables in subsequent CTFs are consistent. Our results show that using the topological ordering for BNs leads to better estimates of partition function, prior marginals and posterior marginals in most benchmarks.
====================
	 The Distortion of Binomial Voting Defies Expectation
==========
Abstract:  In computational social choice, the distortion of a voting rule quantifies the degree to which the rule overcomes limited preference information to select a socially desirable outcome. This concept has been investigated extensively, but only through a worst-case lens. Instead, we study the expected distortion of voting rules with respect to an underlying distribution over voter utilities. Our main contribution is the design and analysis of a novel and intuitive rule, binomial voting, which provides strong distribution-independent guarantees for both expected distortion and expected welfare.
Fluency score: 4
Fluency issues: N/A

Clarity score: 3
Clarity issues: "limited preference information" and "underlying distribution over voter utilities"

Grammaticality score: 5
Grammaticality issues: N/A

Readability score: 3
Readability issues: Complex sentence structures, specialized terminology

Coherence score: 4
Coherence issues: N/A
==========
Introduction:  In an election, voters report their preferences by casting ballots. Under the ubiquitous plurality rule, each voter names a single alternative, whereas other rules — such as the badly namedranked-choice voting,111Also known as “instant-runoff voting” or “alternative vote.”whose adoption is rapidly expanding in the United States — require voters to rank the alternatives. However, even these ostensibly expressive ordinal ballots (whereby voters rank the alternatives) fail to capture voters’intensityof preference under truthful reporting. If voters could report utility functions that are comparable to each other, then we would want to select socially desirable alternatives with respect to these utilities, but it is typically impractical to expect voters to compute and report such utilities. This creates a tension between the limited information available to the voting rule (through the report of ordinal ballots only) and its goal (good outcomes with respect to latent cardinal utilities). A significant body of work in computational social choice aims to understand and alleviate this tension(Anshelevichet al.,2021). It revolves around the notion ofdistortion, defined as the worst-case ratio between the utilitariansocial welfare(sum of utilities) of the voting rule’s outcome and that of the welfare-maximizing alternative. The worst case is taken over rankings, which serve as input to the voting rule, and over utilities that are consistent with these rankings. As is often the case with worst-case analysis, however, the classic notion of distortion is arguably too conservative. In particular, nontrivial guarantees require restrictive assumptions (see Section1.2), and so this type of analysis may not help identify appealing voting rules. With this difficulty in mind, we focus onexpected distortion. Its definition includes the same ratio as before, and we are still interested in the worst case over rankings. However, we now take the conditional expectation over utilities consistent with the rankings, given an i.i.d. distribution over the utilities. Of course, it might not be realistic for such a distribution to be known a priori,222For this reason, we avoid calling this distribution a Bayesianprior.and thus we search for voting rules that aredistribution independent333This term is inspired by the literature on prior-independent mechanisms within mechanism design.: rules that choose the outcome in a way that does not depend on this distribution, even though their guarantees are stated in terms of this distribution. Overall, our goal is todesign distribution-independent voting rules that provide appealing expected distortion guarantees.We uncover novel, potentially practical, voting rules with these properties. We start by considering the important case of two alternatives (e.g., US presidential elections or yes/no decisions) in Section3. We show that, for any underlying distribution, themajorityrule optimizes both expected distortion and expected social welfare. This result suggests that maximizing expected social welfare may be a good approach for optimizing expected distortion. Indeed, our main result in Section4is that, under mild conditions on the underlying distribution and for a sufficiently large number of votersoralternatives, the expected-welfare-maximizing rule optimizes expected distortion almost perfectly. The expected-welfare-maximizing rule, however, is tailored to the underlying distribution and relies on intimate knowledge thereof. Our aim is therefore to approximately optimize expected welfare via a distribution-independent rule. In Section5, we design and analyze such a rule, which belongs to the family of (positional)scoring rules. Under this rule, which we callbinomial voting, each voter awards\sum_{\ell=k}^{m}\binom{m}{\ell}points to the alternative ranked in thekth position, wheremis the number of alternatives, and the alternative awarded the largest number of points overall is selected; note that this rule is distribution independent. Our main result of Section5is that for any underlying distribution supported on[0,1]with (largest) median\nu, binomial voting provides a multiplicative\frac{\nu}{2}-approximation to the optimal expected welfare. Combining this result with that of Section4, it follows that binomial voting gives almost the same\frac{\nu}{2}guarantee for expected distortion, when the number of voters or alternatives is sufficiently large. It is worth noting that binomial voting isnotan outlandish rule designed purely to achieve low expected distortion. On the contrary: as a positional voting rule, it inherits the desirable properties of this family. In fact, positional scoring rules are characterized by a number of natural axioms(Young,1975). Furthermore, we are aware of very few positional scoring rules that have received attention in their own right, as it is typically difficult to justify any specific choice of scores; in this sense, the expected distortion framework can be seen as a way of pinpointing particularly useful parameters. In summary, we identify binomial voting as an unusually attractive rule when viewed through the lens of expected distortion. The literature on (worst-case) distortion(Anshelevichet al.,2021)can generally be partitioned into two threads. In the first thread(Procaccia and Rosenschein,2006; Boutilieret al.,2015; Caragianniset al.,2017; Mandalet al.,2019; Ebadianet al.,2022), it is assumed that voters have normalized utilities, that is, for each voter, the sum of utilities is one. Even with this restrictive assumption, deterministic voting rules cannot give nontrivial distortion bounds(Caragianniset al.,2017), and the best possible distortion for randomized rules is\Theta(\nicefrac{{1}}{{\sqrt{m}}})(Ebadianet al.,2022). In the second thread(Anshelevich and Postl,2017; Grosset al.,2017; Anshelevichet al.,2018; Gkatzeliset al.,2020; Kizilkaya and Kempe,2022), known asmetric distortion, it is assumed that utilities (rather, costs) are induced by an underlying metric space. While some well-known voting rules have constant distortion in this setting(Anshelevichet al.,2018), the metric assumption is arguably difficult to justify in most domains of interest. By contrast, we make no assumptions on utilities. Our work is most closely related to that ofBoutilieret al.(2015). While their most substantial results pertain to worst-case distortion, one of their results deals with a distributional setting that can be seen as the starting point for our work. They show that the voting rule that maximizes expected social welfare is a scoring rule whose scores depend on the underlying distribution; we will revisit and build on this result. However, they do not study expected distortion, nor do they explore (in this context) voting rules that are agnostic to the distribution. Previous papers that analyze expected distortion include those ofChenget al.(2017,2018). However, their papers are fundamentally different. For one, they study metric distortion. More importantly, they focus on one intuitive but very specific distribution, where the positions of alternatives in the underlying metric space are drawn uniformly at random from the voter positions. By contrast, we study general (i.i.d.) distributions over utilities and design distribution-independent voting rules. The work ofGhodsiet al.(2019)is more distantly related: they also analyze expected distortion in the metric setting, assuming that voters abstain with some probability. By replacing worst-case analysis with expectation, our work introduces a Bayesian view canonical to economic theory into the literature on distortion. The Wilson doctrine(Wilson,1987)advocates for the use of mechanisms that require as little prior information as possible. Distribution-independent (sometimes called prior-independent) mechanisms are, through this lens, the most desirable ones as they require no prior information at all, and have been studied within economics(see, e.g., McAfee,1992; Segal,2003)as well as within computer science(see, e.g., Hartline and Roughgarden,2009; Devanuret al.,2011; Babaioffet al.,2018). Our work can also be seen as belonging to a recent push within computer science on “beyond worst-case” analysis of algorithms(Roughgarden,2021).
====================
