id	title	abstract	introduction	abstract_smart	introduction_smart
2303.08774v4	GPT-4 Technical Report	We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.		"SMART: No

Evidence_specific: Yes; ""development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs.""
Evidence_measurable: Yes; ""passing a simulated bar exam with a score around the top 10% of test takers.""
Evidence_achievable: No; there is no indication in the text regarding the achievability of the development.
Evidence_relevant: No; relevance is not applicable since the ""goal"" aspect of SMART is ignored as per the instructions.
Evidence_time-bound: No; there is no mention of a time frame or deadline within the given text."	
2307.09288v2	Llama 2: Open Foundation and Fine-Tuned Chat Models	In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.	Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public. The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM(scao2022bloom), LLaMa-1(Touvron2023LLaMAOA), and Falcon(penedo2023refinedweb)) that match the performance of closed pretrained competitors like GPT-3(gpt3)and Chinchilla(hoffmann2022training), but none of these models are suitable substitutes for closed “product” LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research. In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,Llama 2andLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,Llama 2-Chatmodels generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures2and3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development ofLlama 2andLlama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. We are releasing the following models to the general public for research and commercial use111https://ai.meta.com/resources/models-and-libraries/llama/: Llama 2, an updated version ofLlama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention(gqa2023). We are releasing variants ofLlama 2with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.222We are delaying the release of the 34B model due to a lack of time to sufficiently red team. Llama 2-Chat, a fine-tuned version ofLlama 2that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs,Llama 2is a new technology that carries potential risks with use(bender2021dangers;weidinger2021ethical;solaiman2023evaluating). Testing conducted to date has been in English and has not — and could not — cover all scenarios. Therefore, before deploying any applications ofLlama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guide333https://ai.meta.com/llamaand code examples444https://github.com/facebookresearch/llamato facilitate the safe deployment ofLlama 2andLlama 2-Chat. More details of our responsible release strategy can be found in Section5.3. The remainder of this paper describes our pretraining methodology (Section2), fine-tuning methodology (Section3), approach to model safety (Section4), key observations and insights (Section5), relevant related work (Section6), and conclusions (Section7).	"SMART: No
Evidence_specific: Yes; [The evidence text: ""develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs)... optimized for dialogue use cases""]
Evidence_measurable: Yes; [The evidence text: ""ranging in scale from 7 billion to 70 billion parameters"", ""outperform open-source chat models on most benchmarks we tested""]
Evidence_achievable: No; [Lacks evidence text showing how the goal is achievable]
Evidence_relevant: Yes; [The evidence text: ""fine-tuning and safety improvements of Llama 2-Chat... contribute to the responsible development of LLMs""]
Evidence_time-bound: No; [Lacks evidence text indicating a time frame]"	"SMART: No

Evidence_specific: No; the text offers general insights into LLMs but does not set a specific goal or target.

Evidence_measurable: No; although the text mentions benchmarks and human evaluations, it does not provide a clear metric that allows for direct measurement.

Evidence_achievable: No; there is no evidence of a concrete action plan or steps that ensure the goals (if any) are actually achievable.

Evidence_relevant: No; there is no mention of relevance to a larger goal or strategy.

Evidence_time-bound: No; the text does not provide a timeline or a deadline for achieving the goals or objectives."
2302.13971v1	LLaMA: Open and Efficient Foundation Language Models	We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.	Large Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examplesBrownet al.(2020). These few-shot properties first appeared when scaling models to a sufficient sizeKaplanet al.(2020), resulting in a line of work that focuses on further scaling these modelsChowdheryet al.(2022); Raeet al.(2021). These efforts are based on the assumption that more parameters will lead to better performance. However, recent work fromHoffmannet al.(2022)shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. The objective of the scaling laws fromHoffmannet al.(2022)is to determine how to best scale the dataset and model sizes for a particulartrainingcompute budget. However, this objective disregards theinferencebudget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, althoughHoffmannet al.(2022)recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of language models that achieve the best possible performance at various inference budgets, by training on more tokens than what is typically used. The resulting models, calledLLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most benchmarks, despite being 10\timessmaller. We believe that this model will help democratize the access and study of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter model is also competitive with the best large language models such as Chinchilla or PaLM-540B. Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g. “Books – 2TB” or “Social media conversations”). There exist some exceptions, notably OPT(Zhanget al.,2022), GPT-NeoX(Blacket al.,2022), BLOOM(Scaoet al.,2022)and GLM(Zenget al.,2022), but none that are competitive with PaLM-62B or Chinchilla. In the rest of this paper, we present an overview of the modifications we made to the transformer architectureVaswaniet al.(2017), as well as our training method. We then report the performance of our models and compare with others LLMs on a set of standard benchmarks. Finally, we expose some of the biases and toxicity encoded in our models, using some of the most recent benchmarks from the responsible AI community.	"SMART: No

Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" - ""We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.""

Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" - ""LLaMA-13B outperforms GPT-3 (175B) on most benchmarks""

Evidence_achievable: No; there is no indication of the achievability of their initiative since it is already completed.

Evidence_relevant: No; the text doesn't address relevance as the SMART goal aspect has been disregarded for the purpose of this analysis.

Evidence_time-bound: No; the text lacks any mention of time or deadlines."	"SMART: no
- The text does not contain a clear goal statement, rather it provides an overview of research findings and intentions.

Evidence_specific: yes
- The text specifically discusses the scaling of large language models and the focus on training to achieve the best performance within various inference budgets.

Evidence_measurable: yes
- The text provides measurable components, such as references to model sizes (7B to 65B parameters) and the amount of data trained on (up to 1T tokens).

Evidence_achievable: no
- There is no evidence provided within the text regarding an action plan or steps describing how the objective will be achieved.

Evidence_relevant: yes
- The text is indeed relevant to the context of Large Language Models (LLMs) and the challenges surrounding the scaling and inference performance.

Evidence_time-bound: no
- The text does not provide any timescales or deadlines related to the objectives mentioned."
2306.05685v4	Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena	Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.	There has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised instruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new instruction following and conversational abilities[31,2,30,8,52,48,14]. Once aligned with humans, these chat models are strongly preferred by human users over the original, unaligned models on which they are built. However, the heightened user preference does not always correspond to improved scores on traditional LLM benchmarks – benchmarks like MMLU[19]and HELM[24]cannot effectively tell the difference between these aligned models and the base models. This phenomenon suggests that there is a fundamental discrepancy between user perceptions of the usefulness of chatbots and the criteria adopted by conventional benchmarks. We argue that this discrepancy primarily arises due to existing evaluation that only measures LLMs’ core capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions), without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to accurately adhere to instructions in multi-turn dialogues. As a demonstration, we show conversation histories with two models on an MMLU question inFigure1. The two models are LLaMA-13B[39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned model from LLaMA-13B on high-quality conversations (the training details are inAppendixE). Despite the base LLaMA models showing competitive performance on conventional benchmarks (Table9), its answers to open-ended questions are often not preferred by humans. This misalignment of conventional benchmarks underscores the core problem driving this paper:the need for a robust and scalable automated method to evaluate LLM alignment with human preferences. To study this, we introduce two benchmarks with human ratings as the primary evaluation metric: MT-bench and Chatbot Arena. MT-bench is a series of open-ended questions that evaluate a chatbot’s multi-turn conversational and instruction-following ability – two critical elements for human preference. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities, such as reasoning and math. In addition, we develop Chatbot Arena, a crowdsourced platform featuring anonymous battles between chatbots in real-world scenarios – Users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences. While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow and costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans. Because these models are often trained with RLHF, they already exhibit strong human alignment. We call this approach“LLM-as-a-judge”. This approach has been tried in our earlier blog post[8]and other concurrent or follow-up work[5,29,14,12,52,18,33,40,7,43]. However, there has not been a systematic study of this approach. In this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of human evaluation. We examine several potential limitations of the LLM-as-a-judge approach including position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We show that some of the biases are minor or can be mitigated. Once addressed, our results from 3K controlled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match human evaluations at an agreement rate exceeding 80%, achieving the same level of human-human agreement (§4.2, Table4). Consequently, this suggests LLM-as-a-judge is a scalable method to swiftly evaluate human preference, serving as a promising alternative to traditional human evaluations. This paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human preference datasets with high-quality questions and diverse user interactions from MT-bench and Chatbot Arena. In addition, we argue for the adoption of a hybrid evaluation framework for future LLM benchmarks: by combining the existing capability-based benchmarks and the new preference-based benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core capabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study. Category 1st Turn Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions. 2nd Turn Rewrite your previous response. Start every sentence with the letter A. 1st Turn Given thatf(x)=4x^{3}-9x-14, find the value off(2). 2nd Turn Findxsuch thatf(x)=0. 1st Turn Provide insights into the correlation between economic indicators such as GDP, inflation, and unemployment rates. Explain how fiscal and monetary policies … 2nd Turn Now, explain them again like I’m five.	"SMART: No

Evidence_specific: Yes; the text specifies the challenge addressed, the solution explored, and introduces two benchmarks (MT-bench and Chatbot Arena).
Evidence_measurable: Yes; it mentions achieving over 80% agreement and references 3K expert votes and 30K conversations.
Evidence_achievable: No; the text discusses what has been done but does not clearly establish if the goals set were deemed achievable beforehand.
Evidence_relevant: No; the relevance criterion is not directly addressed in terms of writing style.
Evidence_time-bound: No; there is no mention of a specific timeframe or deadline associated with the objectives."	"SMART: No
Evidence_specific: No
Evidence_measurable: No
Evidence_achievable: No
Evidence_relevant: No
Evidence_time-bound: No

Explanation: The text appears to be an abstract or section from an academic paper discussing LLM-based chatbots, their evaluation, the introduction of new benchmarks (MT-bench and Chatbot Arena), and automated evaluation methods like LLM-as-a-judge. As such, the text is informative and presents insights into a particular area of study but does not have explicit SMART criteria goals or objectives that are meant to be analyzed according to the SMART framework. 

The text does not provide a clear, specific goal; there are no numerical or measurable targets; it does not mention whether the discussed methods are achievable or realistic within the context; it does not reflect upon the relevance toward a particular end goal; and there is no time frame specified for any objectives. The SMART criteria typically apply to goal setting, not to the descriptive or expository style found in this text."
2303.12712v5	Sparks of Artificial General Intelligence: Early experiments with GPT-4	Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.		"SMART: No
Evidence_specific: Yes; ""GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more.""
Evidence_measurable: Yes; ""GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT.""
Evidence_achievable: No; The text does not mention anything about the achievability of the development or research processes.
Evidence_relevant: No; The text does not discuss relevance directly. It focuses on the capabilities of GPT-4 without tying them to any specific, overarching goals or outcomes.
Evidence_time-bound: No; The text does not provide specific timelines or deadlines for any goals or milestones."	
2312.11805v1	Gemini: A Family of Highly Capable Multimodal Models	This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.		"SMART: No
Evidence_specific: Yes; the evidence text is ""The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases.""
Evidence_measurable: Yes; the evidence text is ""our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks"" and ""being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU.""
Evidence_achievable: No; there's no evidence in the text provided regarding the achievability of the initiative.
Evidence_relevant: Yes; the evidence text is ""We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases.""
Evidence_time-bound: No; there's no evidence of a timeline or deadline mentioned in the text provided."	
2301.12597v3	BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models	The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.	Vision-language pre-training (VLP) research has witnessed a rapid advancement in the past few years, where pre-trained models with increasingly larger scale have been developed to continuously push the state-of-the-art on various downstream tasks(Radfordet al.,2021; Liet al.,2021,2022; Wanget al.,2022a; Alayracet al.,2022; Wanget al.,2022b). However, most state-of-the-art vision-language models incur a high computation cost during pre-training, due to end-to-end training using large-scale models and datasets. Vision-language research sits at the intersection between vision and language, therefore it is naturally expected that vision-language models can harvest from the readily-available unimodal models from the vision and natural language communities. In this paper, we propose agenericandcompute-efficientVLP method by bootstrapping from off-the-shelf pre-trained vision models and language models. Pre-trained vision models offer high-quality visual representation. Pre-trained language models, in particularlarge language models(LLMs), offer strong language generation and zero-shot transfer abilities. To reduce computation cost and counteract the issue of catastrophic forgetting, the unimodal pre-trained models remain frozen during the pre-training. In order to leverage pre-trained unimodal models for VLP, it is key to facilitate cross-modal alignment. However, since LLMs have not seen images during their unimodal pre-training, freezing them makes vision-language alignment in particular challenging. In this regard, existing methods (e.g. Frozen(Tsimpoukelliet al.,2021), Flamingo(Alayracet al.,2022)) resort to an image-to-text generation loss, which we show is insufficient to bridge the modality gap. To achieve effective vision-language alignment with frozen unimodal models, we propose a Querying Transformer (Q-Former) pre-trained with a new two-stage pre-training strategy. As shown in Figure1, Q-Former is a lightweight transformer which employs a set of learnable query vectors to extract visual features from the frozen image encoder. It acts as an information bottleneck between the frozen image encoder and the frozen LLM, where it feeds the most useful visual feature for the LLM to output the desired text. In the first pre-training stage, we perform vision-language representation learning which enforces the Q-Former to learn visual representation most relevant to the text. In the second pre-training stage, we perform vision-to-language generative learning by connecting the output of the Q-Former to a frozen LLM, and trains the Q-Former such that its output visual representation can be interpreted by the LLM. We name our VLP framework as BLIP-2: Bootstrapping Language-Image Pre-training with frozen unimodal models. The key advantages of BLIP-2 include: [leftmargin=*] BLIP-2 effectively leverages both frozen pre-trained image models and language models. We bridge the modality gap using a Q-Former pre-trained in two-stages: representation learning stage and generative learning stage. BLIP-2 achieves state-of-the-art performance on various vision-language tasks including visual question answering, image captioning, and image-text retrieval. Powered by LLMs (e.g. OPT(Zhanget al.,2022), FlanT5(Chunget al.,2022)), BLIP-2 can be prompted to perform zero-shot image-to-text generation that follows natural language instructions, which enables emerging capabilities such as visual knowledge reasoning, visual conversation, etc. (see Figure4for examples). Due to the use of frozen unimodal models and a lightweight Q-Former, BLIP-2 is more compute-efficient than exisiting state-of-the-arts. For example, BLIP-2 outperforms Flamingo(Alayracet al.,2022)by 8.7% on zero-shot VQAv2, while using 54\timesfewer trainable parameters. Furthermore, our results show that BLIP-2 is a generic method that can harvest more advanced unimodal models for better VLP performance.	"SMART: No

Evidence_specific: Yes; The evidence text in the given text for criterion ""specific"": Detailed explanation of the BLIP-2 strategy and specific vision-language tasks (zero-shot VQAv2) and capabilities (zero-shot image-to-text generation).

Evidence_measurable: Yes; The evidence text in the given text for criterion ""measurable"": Comparison to Flamingo80B with a quantifiable improvement of 8.7% and mentioning of 54x fewer trainable parameters.

Evidence_achievable: No; There is no evidence regarding the achievability of the BLIP-2 approach itself in the given text, only the outcomes are described.

Evidence_relevant: Yes; The evidence text in the given text for criterion ""relevant"": The strategy is described as addressing the cost issue of pre-training and aims for efficiency, suggesting relevance to contemporary challenges in the field.

Evidence_time-bound: No; There is no evidence in the given text regarding a time-bound component for achieving the outcomes reported."	"SMART: No
Evidence_specific: Yes; ""proposing a generic and compute-efficient VLP method,"" ""reduce computation cost,"" ""achieve effective vision-language alignment.""
Evidence_measurable: Yes; ""BLIP-2 outperforms Flamingo by 8.7% on zero-shot VQAv2,"" ""using 54× fewer trainable parameters.""
Evidence_achievable: Yes; leveraging ""frozen pre-trained image models and language models"" and ""enabled by using a lightweight Q-Former.""
Evidence_relevant: Yes; the text is relevant to the VLP research field ensuring relevance to the goals within that context.
Evidence_time-bound: No; there are no specific deadlines or time frames mentioned regarding the proposed advancements or achievements."
2304.02643v1	Segment Anything	We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.		"SMART: No
Evidence_specific: Yes; the evidence text: ""Segment Anything (SA) project: a new task, model, and dataset for image segmentation.""
Evidence_measurable: Yes; the evidence text: ""with over 1 billion masks on 11M licensed and privacy respecting images.""
Evidence_achievable: No; there is no mention of the resources or plans needed to achieve the goal.
Evidence_relevant: No; the relevance of the project to broader goals is not discussed in the given text.
Evidence_time-bound: No; there is no information regarding a timeline or deadlines."	
2312.00752v1	Mamba: Linear-Time Sequence Modeling with Selective State Spaces	Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.		"SMART: No
Evidence_specific: No; The text does not outline specific criteria or components for evaluation.
Evidence_measurable: No; It does not provide quantifiable targets or metrics for measurement except stating vague comparative performance.
Evidence_assignable: No; The text does not mention a specific person or team responsible for any tasks or initiatives.
Evidence_relevant: No; There is no stated goal or objective to determine relevance.
Evidence_time-bound: No; There are no timeframes or deadlines mentioned."	
2305.14314v1	QLoRA: Efficient Finetuning of Quantized LLMs	We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.	Finetuning large language models (LLMs) is a highly effective way to improve their performance,[40,62,43,61,59,37]and to add desirable or remove undesirable behaviors[43,2,4]. However, finetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B parameter model[57]requires more than 780 GB of GPU memory. While recent quantization methods can reduce the memory footprint of LLMs[13,15,18,66], such techniques only work for inference and break down during training[65]. We demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any performance degradation. Our method,QLoRA, uses a novel high-precision technique to quantize a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights[28] that are tuned by backpropagating gradients through the quantized weights. QLoRAreduces the average memory requirements of finetuning a 65B parameter model from>780GB of GPU memory to<48GB without degrading the runtime or predictive performance compared to a 16-bit fully finetuned baseline. This marks a significant shift in accessibility of LLM finetuning: now the largest publicly available models to date finetunable on a single GPU. UsingQLoRA, we train theGuanacofamily of models, with the second best model reaching 97.8% of the performance level of ChatGPT on the Vicuna[10]benchmark, while being trainable in less than 12 hours on a single consumer GPU; using a single professional GPU over 24 hours we achieve 99.3% with our largest model, essentially closing the gap to ChatGPT on the Vicuna benchmark. When deployed, our smallestGuanacomodel (7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than 20 percentage points on the Vicuna benchmark (Table6). QLoRAintroduces multiple innovations designed to reduce memory use without sacrificing performance: (1)4-bit NormalFloat, an information theoretically optimal quantization data type for normally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats. (2)Double Quantization, a method that quantizes the quantization constants, saving an average of about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3)Paged Optimizers, using NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when processing a mini-batch with a long sequence length. We combine these contributions into a better tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of the accuracy tradeoffs seen in prior work. QLoRA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot performance on model scales that would be impossible using regular finetuning due to memory overhead. Therefore, we train more than 1,000 models across several instruction tuning datasets, model architectures, and sizes between 80M to 65B parameters. In addition to showing thatQLoRArecovers 16-bit performance (§4) and training a state-of-the-art chatbot,Guanaco, (§5), we also analyze trends in the trained models. First, we find that data quality is far more important than dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2, subsampled) on chatbot performance, even when both are meant to support instruction following generalization. Second, we show that strong Massive Multitask Language Understanding (MMLU) benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice versa—in other words, dataset suitability matters more than size for a given task. Furthermore, we also provide a extensive analysis of chatbot performance that uses both human raters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete against each other in matches to produce the best response for a given prompt. The winner of a match is judged by either GPT-4 or human annotators. The tournament results are aggregated into Elo scores[16,17]which determine the ranking of chatbot performance. We find that GPT-4 and human evaluations largely agree on the rank of model performance in the tournaments, but we also find there are instances of strong disagreement. As such, we highlight that model-based evaluation while providing a cheap alternative to human-annotation also has its uncertainties. We augment our chatbot benchmark results with a qualitative analysis ofGuanacomodels. Our analysis highlights success and failure cases that were not captured by the quantitative benchmarks. We release all model generations with human and GPT-4 annotations to facilitate further study. We open-source our codebase and CUDA kernels and integrate our methods into the Hugging Face transformers stack[64], making them easily accessible to all. We release a collection of adapters for 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32 different open sourced, finetuned models.	"SMART: No

Evidence_specific: Yes; ""reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU"" and ""4-bit quantized pretrained language model into Low Rank Adapters (LoRA)""

Evidence_measurable: Yes; ""preserving full 16-bit finetuning task performance,"" ""reaching 99.3% of the performance level of ChatGPT,"" and ""24 hours of finetuning""

Evidence_achievable: No; The text does not explicitly address the feasibility or the resources available to achieve the innovations mentioned.

Evidence_relevant: Yes; The text is focused on the efficiency and effectiveness of QLoRA in finetuning large models, which is relevant to the field of machine learning and AI.

Evidence_time-bound: Yes; ""only requiring 24 hours of finetuning on a single GPU"" indicates a time-related aspect."	"SMART: No
The text is technical, reporting research on finetuning language models, but it does not explicitly conform to the SMART principle which is used for setting clear and reachable goals. Although it does describe specific technologies and methods, it does not explicitly set targets adhering to all aspects of SMART.

Evidence_specific: Yes; the text provides specific information about the method (QLoRA), the type of models (LLMs), and what the method achieves (e.g., ""finetune a quantized 4-bit model without any performance degradation"").

Evidence_measurable: Yes; the text provides measurable outcomes, such as memory requirements (""reduces the average memory requirements [...] to <48GB""), performance levels (""second best model reaching 97.8% [...] 99.3% with our largest model""), and quantitative research results.

Evidence_achievable: No; the text does not explicitly discuss the feasibility of the technology or methods in terms of achievability, although the context implies that the achievements are within reach given their successful implementation.

Evidence_relevant: No; the relevance criterion does not apply directly here, as the text is about research findings rather than goal setting.

Evidence_time-bound: No; the text mentions time in the context of how long the training takes (""trainable in less than 12 hours""), but it does not set a time-bound goal."
2308.12950v3	Code Llama: Open Foundation Models for Code	We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.	"Large language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency in natural language that allows them to be commanded and prompted to perform a variety of tasks(OpenAI,2023; Touvronet al.,2023b). By utilizing large, in-domain datasets, their efficacy can be greatly improved for applications that require a combination of both natural and domain-specific language and understanding of specialized terminology. By training on domain-specific datasets, they have proved effective more broadly on applications that require advanced natural language understanding. A prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation(for a survey, see Xu and Zhu,2022, also seeSection5). In this work, we presentCode Llama, a family of LLMs for code generation and infilling derived fromLlama 2(Touvronet al.,2023b)and released under the same custom permissive license. We provide inference code for both completion and infilling models in the accompanying repository.11footnotemark:1Our approach is based on gradually specializing and increasing the capabilities ofLlama 2models by applying a cascade of training and fine-tuning steps (Figure2):00footnotetext:\dagger: Core contributors.\ast: Meta AI, CERMICS École des Ponts ParisTech.\diamond: Meta AI & Hebrew University of Jerusalem [leftmargin=*,noitemsep,topsep=0pt] Code-training from foundation models.While most LLMs for code generation such as AlphaCode(Liet al.,2022), InCoder(Friedet al.,2023)or StarCoder(Liet al.,2023)are trained on code only, Codex(Chenet al.,2021)was fine-tuned from a general language model. We also start from a foundation model(Llama 2, Touvronet al.,2023b)pretrained on general-purpose text and code data. Our comparison (Section3.4.1) shows that initializing our model withLlama 2outperforms the same architecture trained on code only for a given budget. Infilling.Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does not provide the capability to fill a missing portion of text while taking the full surrounding context into account. Our code-training for 7B, 13B and 70BCode Llamamodels features a multitask objective(Friedet al.,2023)consisting of both autoregressive and causal infilling prediction, enabling applications such as real-time completion in source code editors or docstring generation. Long input contexts.Unlocking repository-level reasoning for completion or synthesis – as opposed to function-level or file-level – requires prompting the model with much longer context than the 4,096 tokens supported byLlama 2. We propose an additional fine-tuning stage that extends the maximum context length from 4,096 tokens to 100,000 tokens by modifying the parameters of the RoPE positional embeddings(Suet al.,2021)used inLlama 2. Our experiments showCode Llamaoperating on very large contexts with a moderate impact on performances on standard coding benchmarks (Section3.3). Instruction fine-tuning.For end-users, the utility of LLMs is significantly improved by instruction fine-tuning(Ouyanget al.,2022; Weiet al.,2022; OpenAI,2023; Touvronet al.,2023b), which also helps preventing unsafe, toxic or biased generations.Code Llama - Instructvariants are further fine-tuned on a mix of proprietary instruction data for improved safety and helpfulness, and a new machine-generatedself-instructdataset created by promptingLlama 2for coding problems andCode Llamato generate associated unit tests and solutions. Our results show thatCode Llama - Instructsignificantly improves performance on various truthfulness, toxicity and bias benchmarks at moderate cost in terms of code generation performance (Section4). Different combinations of these approaches lead to a family of code-specializedLlama 2models with three main variants that we release in four sizes (7B, 13B, 34B and 70B parameters): [leftmargin=*,noitemsep,topsep=0pt] Code Llama: a foundational model for code generation tasks, Code Llama - Python: specialized for Python, Code Llama - Instruct: fine-tuned with human instructions and self-instruct code synthesis data. Prompt:In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the last monthResponse:You can use the ‘find‘ command in Bash to list all text files in the current directory (excluding subdirectories) that have been modified in the last month. The command would be:{minted}bash find . -maxdepth 1 -type f -mtime -30 -name ""*.txt"" Here’s a breakdown of the command: * ‘.‘ specifies the current directory.* ‘-maxdepth 1‘ tells ‘find‘ to only search the current directory and not any subdirectories.* ‘-type f‘ tells ‘find‘ to only search for files (not directories or other types of files).* ‘-mtime -30‘ tells ‘find‘ to only search for files that have been modified in the last 30 days.* ‘-name ""*.txt""‘ tells ‘find‘ to only search for files with the ‘.txt‘ extension. Note that the ‘-mtime‘ option uses the number of days since the file was last modified, so ‘-mtime -30‘ means ""modified in the last 30 days"". If you want to search for files modified in the last month, you can use the ‘-mtime -30‘ option. An example of usingCode Llama - Instructis given inFigure1. It show-cases that the model interprets natural language to determine suitable options for a command-line program and provides an explanation of the solution. We provide further qualitative examples inAppendixL. We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval(Chenet al.,2021), MBPP(Austinet al.,2021), and APPS(Hendryckset al.,2021), as well as a multilingual version of HumanEval(MultiPL-E, Cassanoet al.,2023), where our best models establish a new state of the art amongst open-source LLMs. The technical details of our training and fine-tuning procedures are provided inSection2, followed by in-depth experiments and ablation studies, details of the safety/helpfulness evaluations and a discussion of related work."	"SMART: No
Evidence_specific: Yes; the evidence text: ""Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks.""
Evidence_measurable: Yes; the evidence text: ""with 7B, 13B, 34B and 70B parameters each,"" and ""with scores of up to 67% and 65% on HumanEval and MBPP, respectively.""
Evidence_achievable: No; the text does not mention any goals or targets that indicate achievability.
Evidence_relevant: No; the text does not specify relevance to broader goals or targets.
Evidence_time-bound: No; the text does not mention any deadlines or time frames."	"SMART: No
Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is the detailed description of the training and fine-tuning steps, along with the specific benchmarks the models are tested against.
Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" includes the model sizes (7B, 13B, 34B, and 70B parameters) and specific performance measures on benchmarks and safety/helpfulness evaluations.
Evidence_achievable: No; there is no clear success or progress criteria defined in the given text that indicate the achievability of the goals.
Evidence_relevant: Yes; the evidence text in the given text for criterion ""relevant"" includes the application of the models to tasks like code generation, natural language understanding, and the generation of documentation, which are significant and relevant in the context of LLMs.
Evidence_time-bound: No; the given text does not specify any timeframes or deadlines associated with the objectives."
2302.05543v3	Adding Conditional Control to Text-to-Image Diffusion Models	"We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with ""zero convolutions"" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."		"SMART: No
Evidence_specific: Yes; the evidence text is ""to add spatial conditioning controls to large, pretrained text-to-image diffusion models""
Evidence_measurable: Yes; the evidence text is ""We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc,"" and ""robust with small (<50k) and large (>1m) datasets""
Evidence_attainable: No; the evidence text in the given text for attainability is not provided
Evidence_relevant: No; the evidence text in the given text for relevancy is not provided
Evidence_time-bound: No; the evidence text in the given text for time-bound criteria is not provided"	
2305.10403v3	PaLM 2 Technical Report	We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.   When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.	Language modeling has long been an important research area sinceshannon_lmestimated the information in language with next word prediction. Modeling began withn-gram based approaches(kneser_ney)but rapidly advanced with LSTMs(Hochreiter;graves2014generating). Later work showed that language modelling also led to language understanding(NIPS2015_dai_le). With increased scale and the Transformer architecture(transformer_paper), large language models (LLMs) have shown strong performance in language understanding and generation capabilities over the last few years, leading to breakthrough performance in reasoning, math, science, and language tasks(howard-ruder-2018-universal;gpt3;du_glam_2021;chowdhery2022palm;rae2021scaling;lewkowycz2022solving;tay2023ul;openai2023gpt4). Key factors in these advances have been scaling up model size(gpt3;rae2021scaling)and the amount of data(hoffmann2022training). To date, most LLMs follow a standard recipe of mostly monolingual corpora with a language modeling objective. We introduce PaLM 2, the successor to PaLM(chowdhery2022palm), a language model unifying modeling advances, data improvements, and scaling insights. PaLM 2 incorporates the following diverse set of research advances: [noitemsep,topsep=0pt] Compute-optimal scaling: Recently, compute-optimal scaling(hoffmann2022training)showed that data size is at least as important as model size. We validate this study for larger amounts of compute and similarly find that data and model size should be scaled roughly 1:1 to achieve the best performance for a given amount of training compute (as opposed to past trends, which scaled the model 3\timesfaster than the dataset). Improved dataset mixtures: Previous large pre-trained language models typically used a dataset dominated by English text (e.g.,\sim78% of non-code inchowdhery2022palm). We designed a more multilingual and diverse pre-training mixture, which extends across hundreds of languages and domains (e.g., programming languages, mathematics, and parallel multilingual documents). We show that larger models can handle more disparate non-English datasets without causing a drop in English language understanding performance, and apply deduplication to reduce memorization(lee2021deduplicating) Architectural and objective improvements: Our model architecture is based on the Transformer. Past LLMs have almost exclusively used a single causal or masked language modeling objective. Given the strong results of UL2(tay2023ul), we use a tuned mixture of different pre-training objectives in this model to train the model to understand different aspects of language. The largest model in the PaLM 2 family, PaLM 2-L, is significantly smaller than the largest PaLM model but uses more training compute. Our evaluation results show that PaLM 2 models significantly outperform PaLM on a variety of tasks, including natural language generation, translation, and reasoning. These results suggest that model scaling is not the only way to improve performance. Instead, performance can be unlocked by meticulous data selection and efficient architecture/objectives. Moreover, a smaller but higher quality model significantly improves inference efficiency, reduces serving cost, and enables the model’s downstream application for more applications and users. PaLM 2 demonstrates significant multilingual language, code generation and reasoning abilities, which we illustrate in Figures2and3. More examples can be found in AppendixB.222Note that not all capabilities of PaLM 2 are currently exposed via PaLM 2 APIs.PaLM 2 performs significantly better than PaLM on real-world advanced language proficiency exams and passes exams in all evaluated languages (see Figure1). For some exams, this is a level of language proficiency sufficient to teach that language. In this report, generated samples and measured metrics are from the model itself without any external augmentations such as Google Search or Translate. PaLM 2 includes control tokens to enable inference-time control over toxicity, modifying only a fraction of pre-training as compared to prior work(korbak2023pretraining). Special ‘canary’ token sequences were injected into PaLM 2 pre-training data to enable improved measures of memorization across languages(carlini2019secret;carlini2021extracting). We find that PaLM 2 has lower average rates of verbatim memorization than PaLM, and for tail languages we observe that memorization rates increase above English only when data is repeated several times across documents. We show that PaLM 2 has improved multilingual toxicity classification capabilities, and evaluate potential harms and biases across a range of potential downstream uses. We also include an analysis of the representation of people in pre-training data. These sections help downstream developers assess potential harms in their specific application contexts(shelby2023identifying), so that they can prioritize additional procedural and technical safeguards earlier in development. The rest of this report focuses on describing the considerations that went into designing PaLM 2 and evaluating its capabilities.	"SMART: No
Evidence_specific: Yes; ""PaLM 2 is a Transformer-based model trained using a mixture of objectives.""
Evidence_measurable: Yes; ""PaLM 2 has significantly improved quality on downstream tasks across different model sizes...""
Evidence_achievable: No; There is no specific evidence provided on achievability.
Evidence_relevant: No; The text does not explicitly address relevance in terms of SMART criteria.
Evidence_time-bound: No; There are no clear timelines or deadlines mentioned in the text."	"SMART: No

Evidence_specific: Yes; the text specifies aspects such as scaling insights, multilingual datasets, architectural improvements, and the model's strong performance in tasks.
Evidence_measurable: Yes; measurement is implied through phrases like ""improved dataset mixtures,"" ""significantly outperform,"" and lower average rates of verbatim memorization.
Evidence_attainable: No; the text is descriptive and does not set goals to determine attainability.
Evidence_relevant: No; the text does not establish relevance to specific overarching goals or objectives.
Evidence_time-bound: No; there are no clear timeframes or deadlines mentioned in the text."
2305.18290v2	Direct Preference Optimization: Your Language Model is Secretly a Reward Model	While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.	Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities(Chowdhery et al.,2022; Brown et al.,2020b; Touvron et al.,2023; Bubeck et al.,2023). However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant tounderstandcommon programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to beawareof a common misconception believed by 50% of people, but we certainly do not want the model to claim this misconception to be true in 50% of queries about it! In other words, selecting the model’sdesired responses and behaviorfrom its very wideknowledge and abilitiesis crucial to building AI systems that are safe, performant, and controllable(Ouyang et al.,2022). While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline. At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF;(Christiano et al.,2017; Bai et al.,2022b)). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs. In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We proposeDirect Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model;Bradley and Terry (1952)) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data. Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.	"SMART: No

Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is ""fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward"" and ""fine-tune LMs to align with human preferences"" which are specific objectives.

Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" is ""Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods."" which mentions that the success of DPO can be measured by comparing it to existing methods.

Evidence_attainable: No; the text does not indicate how the attainability of the results will be ensured.

Evidence_relevant: No; since we are ignoring the ""goal"" aspect of SMART, we cannot assess ""relevant"" in this context. However, from the text's context, relevance can be inferred to the field of language model control, but this is not explicitly mentioned.

Evidence_time-bound: No; the text does not specify any time frame within which the goals should be achieved or the experiments were conducted."	"SMART: No
Evidence_specific: Yes; evidence text: ""Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint)"" and ""using language models with up to 6B parameters.""
Evidence_measurable: No; the text lacks clear, quantifiable measures by which to evaluate progress or success.
Evidence_assignable: No; there's no mention of who is responsible for implementing DPO.
Evidence_realistic: No; the text doesn't explicitly state how realistic or attainable the method is within given constraints.
Evidence_time-bound: No; there is no mention of a timeframe or deadline for the optimization process."
2304.08485v2	Visual Instruction Tuning	Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.	Humans interact with the world through many channels such as vision and language, as each individual channel has a unique advantage in representing and communicating certain concepts, and thus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language instructions, aligned with human intent to complete various real-world tasks in the wildaskell2021general;li2022elevater;li2023multimodal. To this end, the community has witnessed an emergent interest in developing language-augmented foundation vision modelsli2022elevater;gan2022vision, with strong capabilities in open-world visual understanding such as classification(radford2021learning;openclip;yuan2021florence;yang2022unicl;pham2021combined), detectionli2022grounded;zhong2022regionclip;liu2023grounding, segmentation(li2022language;zou2022generalized;zhang2023simple)and captioning(wang2022git;li2023blip), as well as visual generation and editing(DALLE2;LDM;PARTI;MAKEASCENE;Imagen;li2023gligen). We refer readers to theComputer Vision in the Wildreading list for a more up-to-date literature compilation(cvinw). In this line of work, each task is solved independently by one single large vision model, with the task instruction implicitly considered in the model design. Further, language is only utilized to describe the image content. While this allows language to play an important role in mapping visual signals to language semantics—a common channel for human communication, it leads to models that usually have a fixed interface with limited interactivity and adaptability to the user’s instructions. Large language models (LLM), on the other hand, have shown that language can play a wider role: a universal interface for a general-purpose assistant, where various task instructions can be explicitly represented in language and guide the end-to-end trained neural assistant to switch to the task of interest to solve it. For example, the recent success of ChatGPT(chatgpt)and GPT-4(gpt4)have demonstrated the power of aligned LLMs in following human instructions, and have stimulated tremendous interest in developing open-source LLMs. Among them, LLaMA(touvron2023llama)is an open-source LLM that matches the performance of GPT-3. Alpaca(alpaca), Vicuna(vicuna), GPT-4-LLM(peng2023instruction)utilize various machine-generated high-quality instruction-following samples to improve the LLM’s alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly, this line of work istext-only. In this paper, we presentvisual instruction-tuning, the first attempt to extend instruction-tuning to the language-image multimodal space, to pave the way towards building a general-purpose visual assistant. In particular, our paper makes the following contributions: [leftmargin=7.5mm] Multimodal instruction-following data. One key challenge is the lack of vision-language instruction-following data. We present a data reformation perspective and pipeline to convert image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4. Large multimodal models. We develop a large multimodal model (LMM), by connecting the open-set visual encoder of CLIP(radford2021learning)with the language decoder Vicunavicuna, and fine-tuning end-to-end on our generated instructional vision-language data. Our empirical study validates the effectiveness of using generated data for LMM instruction-tuning, and suggests practical tips for building a general-purpose instruction-following visual agent. When ensembled with GPT-4, our approach achieves SoTA on the Science QAlu2022learnmultimodal reasoning dataset. Multimodal instruction-following benchmark. We present LLaVA-Bench with two challenging benchmarks, with a diverse selection of paired images, instructions and detailed annotations. Open-source. We release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.	"SMART: No

Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"": ""use language-only GPT-4 to generate multimodal language-image instruction-following data""

Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"": ""yields a 85.1% relative score compared with GPT-4"", ""accuracy of 92.53%""

Evidence_assignable: No; there is no evidence text in the given text for criterion ""assignable""

Evidence_realistic: No; there is no evidence text in the given text for criterion ""realistic""

Evidence_time-related: No; there is no evidence text in the given text for criterion ""time-related"""	"SMART: No
Evidence_specific: No
Evidence_measurable: No
Evidence_attainable: No
Evidence_relevant: No
Evidence_time-bound: No

This text does not adhere to the SMART principles in terms of writing style, as it lacks clear, specific objectives, quantifiable metrics, attainability details, relevance settings, and timelines. It is more of a general exposition on the development and aspirations in AI research rather than a goal setting or achievement outline."
2303.03378v1	PaLM-E: An Embodied Multimodal Language Model	Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.		"SMART: No

Evidence_specific: Yes; The text is specific in describing the aim to ""directly incorporate real-world continuous sensor modalities into language models"" and the tasks such as ""sequential robotic manipulation planning, visual question answering, and captioning.""
Evidence_measurable: Yes; The text implies measurability by stating that the model's performance can be evaluated on tasks and mentions ""state-of-the-art performance on OK-VQA.""
Evidence_achievable: No; Achievability in terms of writing style cannot be assessed as it typically relates to the practical viability of a goal rather than literary expression.
Evidence_relevant: No; Relevance in writing style cannot be determined from the text as it is not focused on a specific purpose or audience within the context of SMART principles.
Evidence_time-bound: No; The text does not mention any specific timeframe or deadline for the project or its evaluations."	
2309.10305v2	Baichuan 2: Open Large-scale Language Models	Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.	The field of large language models has witnessed promising and remarkable progress in recent years. The size of language models has grown from millions of parameters, such as ELMoPeterset al.(2018), GPT-1Radfordet al.(2018), to billions or even trillions of parameters such as GPT-3Brownet al.(2020), PaLMChowdheryet al.(2022); Anilet al.(2023)and Switch TransformersFeduset al.(2022). This increase in scale has led to significant improvements in the capabilities of language models, enabling more human-like fluency and the ability to perform a diverse range of natural language tasks. With the introduction of ChatGPTOpenAI (2022)from OpenAI, the power of these models to generate human-like text has captured widespread public attention. ChatGPT demonstrates strong language proficiency across a variety of domains, from conversing casually to explaining complex concepts. This breakthrough highlights the potential for large language models to automate tasks involving natural language generation and comprehension. While there have been exciting breakthroughs and applications of LLMs, most leading LLMs like GPT-4OpenAI (2023), PaLM-2Anilet al.(2023), and ClaudeClaude (2023)remain closed-sourced. Developers and researchers have limited access to the full model parameters, making it difficult for the community to deeply study or fine-tune these systems. More openness and transparency around LLMs could accelerate research and responsible development within this rapidly advancing field. LLaMATouvronet al.(2023b), a series of large language models developed by Meta containing up to 65 billion parameters, has significantly benefited the LLM research community by being fully open-sourced. The open nature of LLaMA, along with other open-source LLMs such as OPTZhanget al.(2022), BloomScaoet al.(2022), MPTMosaicML (2023)and FalconPenedoet al.(2023), enables researchers to freely access the models for examination, experimentation, and further development. This transparency and access distinguishes LLaMA from other proprietary LLMs. By providing full access, the open-source LLMs have accelerated research and advances in the field, leading to new models like AlpacaTaoriet al.(2023), VicunaChianget al.(2023), and othersWanget al.(2022); Zhuet al.(2023); Anandet al.(2023). However, most open-source large language models have focused primarily on English. For instance, the main data source for LLaMA is Common Crawl111https://commoncrawl.org/, which comprises 67% of LLaMA’s pre-training data but is filtered to English content only. Other open source LLMs such as MPTMosaicML (2023)and FalconPenedoet al.(2023)are also focused on English and have limited capabilities in other languages. This hinders the development and application of LLMs in specific languages, such as Chinese. In this technical report, we introduce Baichuan 2, a series of large-scale multilingual language models. Baichuan 2 has two separate models, Baichuan 2-7B with 7 billion parameters and Baichuan 2-13B with 13 billion parameters. Both models were trained on 2.6 trillion tokens, which to our knowledge is the largest to date, more than double that of Baichuan 1Baichuan (2023b,a). With such a massive amount of training data, Baichuan 2 achieves significant improvements over Baichuan 1. On general benchmarks like MMLUHendryckset al.(2021a), CMMLULiet al.(2023), and C-EvalHuanget al.(2023), Baichuan 2-7B achieves nearly 30% higher performance compared to Baichuan 1-7B. Specifically, Baichuan 2 is optimized to improve performance on math and code problems. On the GSM8KCobbeet al.(2021)and HumanEvalChenet al.(2021)evaluations, Baichuan 2 nearly doubles the results of the Baichuan 1. In addition, Baichuan 2 also demonstrates strong performance on medical and legal domain tasks. On benchmarks such as MedQAJinet al.(2021)and JEC-QAZhonget al.(2020), Baichuan 2 outperforms other open-source models, making it a suitable foundation model for domain-specific optimization. Additionally, we also released two chat models, Baichuan 2-7B-Chat and Baichuan 2-13B-Chat, optimized to follow human instructions. These models excel at dialogue and context understanding. We will elaborate on our approaches to improve the safety of Baichuan 2. By open-sourcing these models, we hope to enable the community to further improve the safety of large language models, facilitating more research on responsible LLMs development. Furthermore, in spirit of research collaboration and continuous improvement, we are also releasing the checkpoints of Baichuan 2 at various stages of training from 200 billion tokens up to the full 2.6 trillion tokens. We found that even for the 7 billion parameter model, performance continued to improve after training on more than 2.6 trillion tokens. By sharing these intermediary results, we hope to provide the community with greater insight into the training dynamics of Baichuan 2. Understanding these dynamics is key to unraveling the inner working mechanism of large language modelsBidermanet al.(2023b); Tirumalaet al.(2022). We believe the release of these checkpoints will pave the way for further advances in this rapidly developing field. In this technical report, we will also share some of the trials, errors, and lessons learned through training Baichuan 2. In the following sections, we will present detailed modifications made to the vanilla Transformer architecture and our training methodology. We will then describe our fine-tuning methods to align the foundation model with human preferences. Finally, we will benchmark the performance of our models against other LLMs on a set of standard tests. Throughout the report, we aim to provide transparency into our process, including unsuccessful experiments, to advance collective knowledge in developing LLMs. Baichuan 2’s foundation models and chat models are available for both research and commercial use athttps://github.com/baichuan-inc/Baichuan2	"SMART: No
Evidence_specific: Yes; ""Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens""
Evidence_measurable: Yes; ""matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval""
Evidence_achievable: No; The text does not mention any resources or plans to ensure the achievability of the presented data or objectives.
Evidence_relevant: No; The text does not explicitly state how the presented information is relevant to specific goals or the context in which the relevance would be gauged.
Evidence_time-bound: No; The text does not provide any specific timelines or deadlines for releasing the pre-training model checkpoints or achieving the stated performance."	"SMART: No
Evidence_specific: Yes; ""Baichuan 2 has two separate models, Baichuan 2-7B with 7 billion parameters and Baichuan 2-13B with 13 billion parameters.""
Evidence_measurable: Yes; ""...Baichuan 2-7B achieves nearly 30% higher performance compared to Baichuan 1-7B.""
Evidence_assignable: No; There is no clear assignation of tasks or responsibilities.
Evidence_realistic: No; The text does not explicitly mention realism, though real results are referenced.
Evidence_time-bound: No; There are no clear deadlines or time frames mentioned for the goals being discussed."
2306.01116v1	The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only	Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.	Progress in natural language processing is increasingly driven by sheer compute scale alone(sevilla2022compute): as more compute is expended to train large language models (LLM), they gain and exhibit powerful emergent capabilities(brown2020language;weiemergent). To best benefit from scaling, recent scaling laws dictate that both model size and dataset size should jointly be increased(hoffmann2022training). This is at variance with earlier findings, which had argued that scaling should focus on model size first and foremost, with minimal data scaling(kaplan2020scaling). This joint scaling paradigm raises significant challenges: although plentiful, text data is not infinite, especially so when considerations on data quality and licensing are taken into account–leading some researchers to argue scaling may soon be bottlenecked by data availability(villalobos2022will). Concretely, optimally training a GPT-3 sized model (175B parameters) would require no less than 3,500 billion tokens of text according tohoffmann2022training. This is twice as much as the largest pretraining datasets ever demonstrated(hoffmann2022training;touvron2023llama), and ten times more than the largest publicly available English datasets such as OSCAR(OrtizSuarezSagotRomary2019), C4(2020t5), or The Pile(gao2020pile). Massively scaling-up pretraining data is made even more challenging by the fact LLMs are commonly trained using a mixture of web crawls and so-called “high-quality” data(brown2020language;gao2020pile). Typical high-quality corpora include curated sources of books, technical documents, human-selected web pages, or social media conversations. The increased diversity and quality brought forth by these curated corpora is believed to be a key component of performant models(scao2022language). Unfortunately, curation is labour intensive: typically, each source requires specialized processing, while yielding a limited amount of data. Furthermore, licensed sources raise legal challenges. Nevertheless, most pretraining data is still sourced from massive web crawls which can be scaled up to trillions of tokens with limited human intervention. However, the quality of this data has traditionally been seen as (much) inferior to that of the manually curated data sources. Even finely processed sources of web data, such as C4(2020t5)or OSCAR(OrtizSuarezSagotRomary2019), are regarded as inferior to curated corpora for LLMs(gopher;scao2022language), producing less performant models. To sustain the ever-increasing data needs of larger and larger LLMs, and to streamline data pipelines and reduce the need for human-intensive curation, we propose to explore how web data can be better processed to significantly improve its quality, resulting in models as capable, if not more capable, than models trained on curated corpora. We make the following contributions: We introduceRefinedWeb, a high-quality five trillion tokens web-only English pretraining dataset; We demonstrate thatweb data alone can result in models outperforming both public and private curated corpora, as captured by zero-shot benchmarks, challenging current views about data quality; We publicly release a 600B tokens extract of RefinedWeb, and 1/7B parameters LLMs trained on it, to serve as a new baseline high-quality web dataset for the natural language processing community.	"SMART: No

Evidence_specific: No
- There is no clear, specific statement regarding the characteristics or features of the curation process, nor is there a specific outcome being described, only a broad statement about previous beliefs and a general assertion about model performance.

Evidence_measurable: Yes
- The text includes measurable elements, ""five trillion tokens,"" ""600 billion tokens,"" and ""1.3/7.5B parameters,"" which refer to quantifiable aspects of the dataset and model sizes.

Evidence_achievable: No
- The text does not provide any context that would suggest whether the process of training on the dataset is achievable, as it does not discuss resources, limitations, or capabilities required for such a task.

Evidence_relevant: Yes
- The relevance is indicated by discussing the performance of models trained on web data and comparing it to models trained on ""The Pile,"" which appears to be a baseline or standard in the field.

Evidence_time-bound: No
- There is no indication of a timeframe or deadline for obtaining data, training models, or releasing datasets, which would be necessary to fulfill the ""time-bound"" aspect of the SMART principle."	"SMART: No
Evidence_specific: Yes; ""RefinedWeb, a high-quality five trillion tokens web-only English pretraining dataset"" is a specific contribution.
Evidence_measurable: Yes; ""600B tokens extract"" and ""1/7B parameters LLMs"" provide measurable aspects.
Evidence_achievable: No; There's no specific evidence regarding the achievability of the goals.
Evidence_relevant: Yes; The goals are relevant to ""natural language processing community.""
Evidence_time-bound: No; There are no explicit time frames or deadlines mentioned."
2305.06500v2	InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning	Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.	A longstanding aspiration of Artificial Intelligence (AI) research is to build a single model that can solve arbitrary tasks specified by the user. In natural language processing (NLP), instruction tuning[46,6]proves to be a promising approach toward that goal. By finetuning a large language model (LLM) on a wide range of tasks described by natural language instructions, instruction tuning enables the model to follow arbitrary instructions. Recently, instruction-tuned LLMs have also been leveraged for vision-language tasks. For example, BLIP-2[19]effectively adapts frozen instruction-tuned LLMs to understand visual inputs and exhibits preliminary capabilities to follow instructions in image-to-text generation. Compared to NLP tasks, vision-language tasks are more diverse in nature due to the additional visual inputs from various domains. This poses a greater challenge to a unified model that is supposed to generalize to diverse vision-language tasks, many unseen during training. Most previous work can be grouped into two approaches. The first approach, multitask learning[5,26], formulates various vision-language tasks into the same input-output format. However, we empirically find multitask learning without instructions (Table4) does not generalize well to unseen datasets and tasks. The second approach[19,2]extends a pre-trained LLM with additional visual components, and trains the visual components with image caption data. Nevertheless, such data are too limited to allow broad generalization to vision-language tasks that require more than visual descriptions. To address the aforementioned challenges, this paper presents InstructBLIP, a vision-language instruction tuning framework that enables general-purpose models to solve a wide range of visual-language tasks through a unified natural language interface. InstructBLIP uses a diverse set of instruction data to train a multimodal LLM. Specifically, we initialize training with a pre-trained BLIP-2 model consisting of an image encoder, an LLM, and a Query Transformer (Q-Former) to bridge the two. During instruction tuning, we finetune the Q-Former while keeping the image encoder and LLM frozen. Our paper makes the following key contributions: [leftmargin=2em] We perform a comprehensive and systematic study on vision-language instruction tuning. We transform 26 datasets into the instruction tuning format and group them into 11 task categories. We use 13 held-in datasets for instruction tuning and 13 held-out datasets for zero-shot evaluation. Moreover, we withhold four entire task categories for zero-shot evaluation at the task level. Exhaustive quantitative and qualitative results demonstrate the effectiveness of InstructBLIP on vision-language zero-shot generalization. We propose instruction-aware visual feature extraction, a novel mechanism that enables flexible and informative feature extraction according to the given instructions. Specifically, the textual instruction is given not only to the frozen LLM, but also to the Q-Former, so that it can extract instruction-aware visual features from the frozen image encoder. Also, we propose a balanced sampling strategy to synchronize learning progress across datasets. We evaluate and open-source a suite of InstructBLIP models using two families of LLMs: 1) FlanT5[6], an encoder-decoder LLM finetuned from T5[33]; 2) Vicuna[42], a decoder-only LLM finetuned from LLaMA[40]. The InstructBLIP models achieve state-of-the-art zero-shot performance on a wide range of vision-language tasks. Furthermore, InstructBLIP models lead to state-of-the-art finetuning performance when used as the model initialization on individual downstream tasks.	"SMART: No

Evidence_specific: Yes; ""conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models.""

Evidence_measurable: No; The text mentions qualitative demonstrations and state-of-the-art performance comparisons, but specific, quantifiable metrics for evaluation within the writing are not present.

Evidence_achievable: No; Achievability in terms of the writing style is not directly addressed in the text.

Evidence_relevant: Yes; ""

Evidence_time-bound: No; There are no specific time frames or deadlines mentioned in the writing style."	"SMART: No
Evidence_specific: Yes; the evidence text is ""build a single model that can solve arbitrary tasks specified by the user"" and describes what InstructBLIP aims to achieve with specifics.
Evidence_measurable: No; the text does not provide any quantifiable criteria for measuring the achievement outside of the specific context of the paper’s contributions (like ""state-of-the-art zero-shot performance"").
Evidence_achievable: No; the text does not mention any resources, skills, or capabilities specifically acquired to make achieving the aim probable or feasible.
Evidence_relevant: No; without the context of a broader goal or strategy, it cannot be determined from the text alone if the aim is relevant.
Evidence_time-bound: No; the text does not specify a time frame within which the goal should be achieved."
2309.07864v3	The Rise and Potential of Large Language Model Based Agents: A Survey	For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.		"SMART: No
Evidence_specific: No; the text is an overview of a topic without a defined goal.
Evidence_measurable: No; there are no specific metrics or indicators of measuring progress or success mentioned.
Evidence_attainable: No; no sense of whether the concepts are feasible is conveyed as there is no goal-oriented context.
Evidence_relevant: No; relevance is not assessed due to the lack of a goal or objective.
Evidence_time-bound: No; there are no deadlines or timeframes specified in the text."	
2307.15043v2	Universal and Transferable Adversarial Attacks on Aligned Language Models	"Because ""out-of-the-box"" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called ""jailbreaks"" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks."	Large language models (LLMs) are typically trained on massive text corpora scraped from the internet, which are known to contain a substantial amount of objectionable content. Owing to this, recent LLM developers have taken to “aligning” such models via various finetuning mechanisms111“Alignment” can generically refer to many efforts to make AI systems better aligned with human values. Here we use it in the narrow sense adopted by the LLM community, that of ensuring that these models do not generate harmful content, although we believe our results will likely apply to other alignment objectives.; there are different methods employed for this task(Ouyang et al.,2022; Bai et al.,2022b; Korbak et al.,2023; Glaese et al.,2022), but the overall goal of these approaches is to attempt ensure that these LLMs do not generate harmful or objectionable responses to user queries. And at least on the surface, these attempts seem to succeed: public chatbots will not generate certain obviously-inappropriate content when asked directly. In a largely separate line of work, there has also been a great deal of effort invested into identifying (and ideally preventing)adversarial attackson machine learning models(Szegedy et al.,2014; Biggio et al.,2013; Papernot et al.,2016b; Carlini and Wagner,2017b). Most commonly raised in computer vision domains (though with some applications to other modalities, including text), it is well-established that adding small perturbations to the input of a machine learning model can drastically change its output. To a certain extent, similar approaches are already known to work against LLMs: there exist a number of published “jailbreaks”: carefully engineered prompts that result in aligned LLMs generating clearly objectionable content(Wei et al.,2023). Unlike traditional adversarial examples, however, these jailbreaks are typically crafted through human ingenuity—carefully setting up scenarios that intuitively lead the models astray—rather than automated methods, and thus they require substantial manual effort. Indeed, although there has been some work on automatic prompt-tuning for adversarial attacks on LLMs(Shin et al.,2020; Wen et al.,2023; Jones et al.,2023), this has traditionally proven to be a challenging task, with some papers explicitly mentioning that they had been unable to generate reliable attacks through automatic search methods(Carlini et al.,2023). This owes largely to the fact that, unlike image models, LLMs operate ondiscretetoken inputs, which both substantially limits the effective input dimensionality, and seems to induce a computationally difficult search. In this paper, however, we propose a new class of adversarial attacks that can in fact induce aligned language models to produce virtually any objectionable content. Specifically, given a (potentially harmful) user query, our attack appends an adversarialsuffixto the query that attempts to induce negative behavior. that is, the user’s original query is left intact, but we add additional tokens to attack the model. To choose these adversarial suffix tokens, our attack consists of three key elements; these elements have indeed existed in very similar forms in the literature, but we find that it is their careful combination that leads to reliably successful attacks in practice. Initial affirmative responses.As identified in past work(Wei et al.,2023; Carlini et al.,2023), one way to induce objectionable behavior in language models is to force the model to give (just a few tokens of) an affirmative response to a harmful query. As such, our attack targets the model to begin its response with “Sure, here is (content of query)” in response to a number of prompts eliciting undesirable behavior. Similar to past work, we find that just targeting thestartof the response in this manner switches the model into a kind of “mode” where it then produces the objectionable content immediately after in its response. Combined greedy and gradient-based discrete optimization.Optimizing over the adversarial suffix is challenging due to the fact that we need to optimize overdiscretetokens to maximize the log likelihood of the attack succeeding. To accomplish this, we leverage gradients at the token level to identify asetof promising single-token replacements, evaluate the loss of some number of candidates in this set, and select the best of the evaluated substitutions. The method is, in fact, similar to the AutoPrompt(Shin et al.,2020)approach, but with the (we find, practically quite important) difference that we search overallpossible tokens to replace at each step, rather than just a single one. Robust multi-prompt and multi-model attacks.Finally, in order to generate reliable attack suffixes, we find that it is important to create an attack that works not just for a single prompt on a single model, but formultipleprompts acrossmultiplemodels. In other words, we use our greedy gradient-based method to search for asinglesuffix string that was able to induce negative behavior across multiple different user prompts, and across three different models (in our case, Vicuna-7B and 13bZheng et al. (2023)and Guanoco-7BDettmers et al. (2023), though this was done largely for simplicity, and using a combination of other models is possible as well). Putting these three elements together, we find that we can reliably create adversarial suffixes that circumvent the alignment of a target language model. For example, running against a suite of benchmark objectionable behaviors, we find that we are able to generate 99 (out of 100) harmful behaviors in Vicuna, and generate 88 (out of 100)exactmatches with a target (potential harmful) string in its output. Furthermore, we find that the prompts achieve up to 84% success rates at attacking GPT-3.5 and GPT-4, and 66% for PaLM-2; success rates for Claude are substantially lower (2.1%), but notably the attacks stillcaninduce behavior that is otherwise never generated. Illustrative examples are shown in Figure1. Futhermore, our results highlight the importance of our specific optimizer: previous optimizers, specifically PEZ(Wen et al.,2023)(a gradient-based approach) and GBDA(Guo et al.,2021)(an approach using Gumbel-softmax reparameterization) are not able to achieve any exact output matches, whereas AutoPrompt(Shin et al.,2020)only achieves a 25% success rate, compared to our attack success rate of 88%. Overall, this work substantially pushes forward the state of the art in demonstrated adversarial attacks against such LLMs. It thus also raises an important question: if adversarial attacks against aligned language models follow a similar pattern to those against vision systems, what does this mean for the overall agenda of this approach to alignment? Specifically, in modern computer vision systems, adversarial attacks are still an omnipresent phenomenon. State-of-the-art methods to prevent adversarial examples(Madry et al.,2018; Cohen et al.,2019; Leino et al.,2021)are almost never used in practice, owing to 1) their computational inefficiency, 2) the fact that they lead to substantial performance drops (in terms of “clean” model performance) in the resulting systems, and 3) the fact that they can only prevent attacks usually against a narrowly-defined attack model. It remains to be seen how this “arms race” between adversarial attacks and defenses plays out in the LLM space, but historical precedent suggests that we should consider rigorous wholesale alternatives to current attempts, which aim at posthoc “repair” of underlying models that are already capable of generating harmful content. Prior to publishing this work, we shared preliminary results with OpenAI, Google, Meta, and Anthropic. We discuss ethical considerations and the broader impacts of this work further in Section6.	"SMART: No
Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is ""our approach finds a suffix that, when attached to a wide range of queries for an LLM...""
Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" is ""aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer).""
Evidence_achievable: No; there is no clear statement regarding the achievability of the procedure.
Evidence_relevant: Yes; the evidence text in the given text for criterion ""relevant"" is the entire text which focuses on addressing and understanding a relevant concern in the field of language model alignment.
Evidence_time-bound: No; there is no mention of a specific timeframe or deadline for achieving the outcomes described in the text."	"SMART: No

Evidence_specific: Yes; ""create an attack that works not just for a single prompt on a single model, but for multiple prompts across multiple models""
Evidence_measurable: Yes; ""generate 99 (out of 100) harmful behaviors... and generate 88 (out of 100) exact matches... success rates for Claude are substantially lower (2.1%)""
Evidence_Achievable: Not applicable to the text, since it is descriptive in nature, not goal-setting.
Evidence_Relevant: Not applicable to the text, as it does not discuss relevance in the context of goal alignment.
Evidence_time-bound: No; there is no specific timeframe or deadline mentioned for any objectives or outcomes."
2305.10601v2	Tree of Thoughts: Deliberate Problem Solving with Large Language Models	Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.	Originally designed to generate text, scaled-up versions of language models (LMs) such as GPTRadford et al. (2018,2019); Brown et al. (2020); OpenAI (2023)and PaLMChowdhery et al. (2022)have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what problems would challenge the current paradigm, and what should be alternative mechanisms? The literature on human cognition provides some clues to answer these questions. Research on “dual process” models suggests that people have two modes in which they engage with decisions – a fast, automatic, unconscious mode (“System 1”) and a slow, deliberate, conscious mode (“System 2”)Sloman (1996); Stanovich (1999); Kahneman et al. (2002); Kahneman (2011). These two modes have previously been connected to a variety of mathematical models used in machine learning. For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative “model free” learning or more deliberative “model based” planningDaw et al. (2005). The simple associative token-level choices of LMs are also reminiscent of “System 1”, and thus might benefit from augmentation by a more deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions. To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950sNewell et al. (1959,1972). Newell and colleagues characterized problem solvingNewell et al. (1959)as search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. As Figure1illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where eachthoughtis a coherent language sequence that serves as an intermediate step toward problem solving (Table1). Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures2,4,6). This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking. Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4OpenAI (2023): Game of 24, Creative Writing, and Crosswords (Table1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search. We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems. We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs.	"SMART: No
Evidence_specific: No
Evidence_measurable: Yes; the evidence text is ""in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%""
Evidence_achievable: No
Evidence_relevant: No
Evidence_time-bound: No"	"SMART: No
Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" – ""We propose the Tree of Thoughts (ToT) framework for general problem solving with language models.""
Evidence_measurable: No; there is no clear metric or standard mentioned to measure progress or success.
Evidence_achievable: No; it's not clear how realistic or achievable the proposal is within any specific constraints.
Evidence_relevant: Yes; the evidence text in the given text for criterion ""relevant"" – ""Empirically, we propose three new problems that challenge existing LM inference methods...""
Evidence_time-bound: No; there is no mention of a timeline or deadline for achieving the goals discussed."
2401.04088v1	Mixtral of Experts	We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.		"SMART: No
Evidence_specific: Yes; ""Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts).""
Evidence_measurable: Yes; ""For every token, at each layer, a router network selects two experts...each token has access to 47B parameters, but only uses 13B active parameters during inference.""
Evidence_achievable: No; there is no specific evidence about the goal's achievability.
Evidence_relevant: Yes; ""Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks.""
Evidence_time-bound: No; there is no specific evidence about the time frame for the goals or objectives."	
2310.06825v1	Mistral 7B	We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.		"SMART: No

Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is ""a 7-billion-parameter language model engineered for superior performance and efficiency.""

Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" is ""outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation.""

Evidence_assignable: No; there is no evidence of an ""assignable"" criteria, i.e., specifying who will be doing what.

Evidence_realistic: No; the text does not provide information on whether the goals mentioned are realistic.

Evidence_time-bound: No; the text does not mention any deadlines or time frames."	
2309.06180v1	Efficient Memory Management for Large Language Model Serving with PagedAttention	High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm	The emergence of large language models (LLMs) like GPT(OpenAI,2023b; Brownet al.,2020)and PaLM(Chowdheryet al.,2022)have enabled new applications such as programming assistants(Github,2022; Chenet al.,2021)and universal chatbots(OpenAI,2022; Google,2023)that are starting to profoundly impact our work and daily routines. Many cloud companies(OpenAI,2020; Services,2023)are racing to provide these applications as hosted services. However, running these applications is very expensive, requiring a large number of hardware accelerators such as GPUs. According to recent estimates, processing an LLM request can be 10\timesmore expensive than a traditional keyword query(Reuters,2023). Given these high costs, increasing the throughput—and hence reducing the cost per request—ofLLM servingsystems is becoming more important. At the core of LLMs lies an autoregressive Transformer model(Vaswaniet al.,2017). This model generates words (tokens),one at a time, based on the input (prompt) and the previous sequence of the output’s tokens it has generated so far. For each request, this expensive process is repeated until the model outputs a termination token. This sequential generation process makes the workloadmemory-bound, underutilizing the computation power of GPUs and limiting the serving throughput. Improving the throughput is possible by batching multiple requests together. However, to process many requests in a batch, the memory space for each request should be efficiently managed. For example, Fig.1(left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM. Approximately 65% of the memory is allocated for the model weights, which remain static during serving. Close to 30% of the memory is used to store the dynamic states of the requests. For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to asKV cache(Popeet al.,2022), which represent the context from earlier tokens to generate new output tokens in sequence. The remaining small percentage of memory is used for other data, including activations – the ephemeral tensors created when evaluating the LLM. Since the model weights are constant and the activations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size. When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig.1(right). In this paper, we observe that existing LLM serving systems(Yuet al.,2022; NVIDIA,2023a)fall short of managing the KV cache memory efficiently. This is mainly because they store the KV cache of a request in contiguous memory space, as most deep learning frameworks(Paszkeet al.,2019; Olstonet al.,2017)require tensors to be stored in contiguous memory. However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique characteristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori. These characteristics make the existing systems’ approach significantly inefficient in two ways: First, the existing systems(Yuet al.,2022; NVIDIA,2023a)suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, theypre-allocatea contiguous chunk of memory with the request’s maximum length (e.g., 2048 tokens). This can result in severe internal fragmentation, since the request’s actual length can be much shorter than its maximum length (e.g., Fig.11). Moreover, even if the actual length is known a priori, the pre-allocation is still inefficient: As the entire chunk is reserved during the request’s lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused. Besides, external memory fragmentation can also be significant, since the pre-allocated size can be different for each request. Indeed, our profiling results in Fig.2show that only 20.4% - 38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportunities for memory sharing. LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request. In these scenarios, the request consists of multiple sequences that can partially share their KV cache. However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. To address the above limitations, we proposePagedAttention, an attention algorithm inspired by the operating system’s (OS) solution to memory fragmentation and sharing:virtual memory with paging. PagedAttention divides the request’s KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens. In PagedAttention, the blocks for the KV cache are not necessarily stored in contiguous space. Therefore, we can manage the KV cache in a more flexible way as in OS’s virtual memory: one can think of blocks as pages, tokens as bytes, and requests as processes. This design alleviates internal fragmentation by using relatively small blocks and allocating them on demand. Moreover, it eliminates external fragmentation as all blocks have the same size. Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. In this work, we buildvLLM, a high-throughput distributed LLM serving engine on top of PagedAttention that achieves near-zero waste in KV cache memory. vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention. vLLM supports popular LLMs such as GPT(Brownet al.,2020), OPT(Zhanget al.,2022), and LLaMA(Touvronet al.,2023)with varying sizes, including the ones exceeding the memory capacity of a single GPU. Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by 2-4\timescompared to the state-of-the-art systems(Yuet al.,2022; NVIDIA,2023a), without affecting the model accuracy at all. The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms (§4.3). In summary, we make the following contributions: [noitemsep,topsep=0pt,leftmargin=*] We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance. We propose PagedAttention, an attention algorithm that operates on KV cache stored in non-contiguous paged memory, which is inspired by the virtual memory and paging in OS. We design and implement vLLM, a distributed LLM serving engine built on top of PagedAttention. We evaluate vLLM on various scenarios and demonstrate that it substantially outperforms the previous state-of-the-art solutions such as FasterTransformer(NVIDIA,2023a)and Orca(Yuet al.,2022).	"SMART: No
Evidence_specific: Yes; the text specifies goals such as ""near-zero waste in KV cache memory"" and ""flexible sharing of KV cache within and across requests to reduce memory usage.""
Evidence_measurable: Yes; the text provides measurable outcomes such as ""improves the throughput of popular LLMs by 2-4$\times$"" and specifies conditions such as ""with the same level of latency.""
Evidence_achievable: No; the text does not provide information on how the goals are achieved, only that they can be achieved.
Evidence_relevant: No; the text does not explicitly state why the goals are important, though it implies relevance to improving the serving of LLMs.
Evidence_time-bound: No; the text does not mention any time frames or deadlines for achieving the stated outcomes."	"SMART: No
Evidence_specific: Yes; The text specifies Large Language Models (LLMs), hardware accelerators such as GPUs, and the concept of KV cache memory management.
Evidence_measurable: Yes; Performance improvements like ""LLM serving throughput by 2-4 times"" are mentioned, as well as memory usage percentages like ""close to 30% of the memory is used to store the dynamic states.""
Evidence_attainable: No; There are no statements related to the feasibility or attainability of the results discussed.
Evidence_relevant: Yes; The text is relevant to the field of computing, discussing the real-world problem of the high cost and efficiency of running LLM applications.
Evidence_time-bound: No; There is no mention of a timeline or deadline associated with the goals or the research presented."
2307.15818v1	RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control	We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).	High-capacity models pretrained on broad web-scale datasets provide an effective and powerful platform for a wide range of downstream tasks: large language models can enable not only fluent text generation(Brohan et al.,2022; OpenAI,2023; Anil et al.,2023)but emergent problem-solving(Cobbe et al.,2021; Lewkowycz et al.,2022; Polu et al.,2022)and creative generation of prose(Brown et al.,2020; OpenAI,2023)and code(Chen et al.,2021), while vision-language models enable open-vocabulary visual recognition(Radford et al.,2021; Minderer et al.,2022; Kirillov et al.,2023)and can even make complex inferences about object-agent interactions in images(Alayrac et al.,2022; Hao et al.,2022; Wang et al.,2022; Chen et al.,2023b,a; Driess et al.,2023; Huang et al.,2023). Such semantic reasoning, problem solving, and visual interpretation capabilities would be tremendously useful for generalist robots that must perform a variety of tasks in real-world environments. However, it is unclear how robots should acquire such capabilities. While a brute force approach might entail collecting millions of robotic interaction trials, the most capable language and vision-language models are trained on billions of tokens and images from the web(Alayrac et al.,2022; Chen et al.,2023b,a; Huang et al.,2023)– an amount unlikely to be matched with robot data in the near future. On the other hand, directly applying such models to robotic tasks is also difficult: such models reason about semantics, labels, and textual prompts, whereas robots require grounded low-level actions, such as Cartesian end-effector commands. While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics(Ahn et al.,2022; Driess et al.,2023; Vemprala et al.,2023), such methods generally address only the “higher level” aspects of robotic planning, essentially taking the role of a state machine that interprets commands and parses them into individual primitives (such as picking and placing objects), which are then executed by separate low-level controllers that themselves do not benefit from the rich semantic knowledge of Internet-scale models during training. Therefore, in this paper we ask: can large pretrained vision-language models be integrated directly into low-level robotic control to boost generalization and enable emergent semantic reasoning? To this end, we explore an approach that is both simple and surprisingly effective: we directly train vision-language models designed for open-vocabulary visual question answering and visual dialogue to output low-level robot actions, along with solving other Internet-scale vision-language tasks. Although such models are typically trained to produce natural language tokens, we can train them on robotic trajectories bytokenizing the actions into text tokensand creating “multimodal sentences”(Driess et al.,2023)that “respond” to robotic instructions paired with camera observations by producing corresponding actions. In this way, vision-language models can be directly trained to act as instruction following robotic policies. This simple approach is in contrast with prior alternatives for incorporating VLMs into robot policies(Shridhar et al.,2022a)or designing new vision-language-action architectures from scratch(Reed et al.,2022): instead, pre-existing vision-language models, with already-amortized significant compute investment, are trained without any new parameters to output text-encoded actions. We refer to this category of models as vision-language-action (VLA) models. We instantiate VLA models by building on the protocol proposed for RT-1(Brohan et al.,2022), using a similar dataset, but expanding the model to use a large vision-language backbone. Hence we refer to our model as RT-2 (Robotics Transformer 2). We provide an overview in Figure1. We observe that robotic policies derived from such vision-language models exhibit a range of remarkable capabilities, combining the physical motions learned from the robot data with the ability to interpret images and text learned from web data into a single model. Besides the expected benefit of dramatically improving generalization to novel objects and semantically varied instructions, we observe a number of emergent capabilities. While the model’s physical skills are still limited to the distribution of skills seen in the robot data, the model acquires the ability to deploy those skills in new ways by interpreting images and language commands using knowledge gleaned from the web. Some example highlights are shown in Figure2. The model is able to re-purpose pick and place skills learned from robot data to place objects near semantically indicated locations, such as specific numbers or icons, despite those cues not being present in the robot data. The model can also interpret relations between objects to determine which object to pick and where to place it, despite no such relations being provided in the robot demonstrations. Furthermore, if we augment the command with chain of thought prompting, the model is able to make even more complex semantic inferences, such as figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink). Our main contribution is RT-2, a family of models derived from fine-tuning large vision-language models trained on web-scale data to directly act as generalizable and semantically aware robotic policies. Our experiments investigate models with up to 55B parameters trained on Internet data and instruction-annotated robotic trajectories from previous work(Brohan et al.,2022). Over the course of 6k robotic evaluations, we show that RT-2 enable significant improvements to generalization over objects, scenes, and instructions, and exhibit a breadth of emergent capabilities inherited from web-scale vision-language pretraining.	"SMART: No (Not all the criteria are met or evidenced in the text.)
Evidence_specific: Yes; the text specifically talks about incorporating vision-language models into robotic control, enabling a single end-to-end trained model, and co-fine-tuning on both robotic trajectory data and Internet-scale vision-language tasks.
Evidence_measurable: Yes; the text mentions '6k evaluation trials' as a means to measure the effectiveness of the approach.
Evidence_achievable: No; the text does not provide clear evidence of how the goal is achievable or any steps taken to ensure it is realistic.
Evidence_relevant: No; the text outlines the relevance to the field, but does not clearly align with specific wider objectives that would illustrate relevance in the SMART context.
Evidence_time-bound: No; the text does not mention a timeframe or deadline for achieving the stated objectives."	"SMART: No

Evidence_specific: No; the text speaks generally about the capabilities and applications of large language models and does not outline a specific feature or quality of the approach.

Evidence_measurable: No; there is no quantifiable measure or target outlined in the text.

Evidence_achievable: No; the text does not talk about the practical steps or possibility of achieving the outcomes described.

Evidence_relevant: No; there is no discussion about the relevance or importance that directly ties back to a specific goal or objective.

Evidence_time-bound: No; the text lacks any reference to a timeline or deadline for achieving the outcomes discussed."
2304.10592v2	MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models	The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.	In recent years, large language models (LLMs) have experienced rapid advancements(Ouyanget al.,2022; OpenAI,2022; Brownet al.,2020; Scaoet al.,2022a; Touvronet al.,2023; Chowdheryet al.,2022; Hoffmannet al.,2022). With exceptional language understanding capabilities, these models can perform a variety of intricate linguistic tasks in a zero-shot manner. Notably, GPT-4, a large-scale multimodal model, has been recently introduced and demonstrated several impressive capabilities of vision-language understanding and generation(OpenAI,2023). For example, GPT-4 can produce detailed and accurate image descriptions, explain unusual visual phenomena, and even construct websites based on handwritten text instructions. Although GPT-4 has exhibited remarkable vision language capabilities, the methods behind its exceptional abilities are still a mystery(OpenAI,2023). We believe that these impressive skills may stem from the utilization of a more advanced large language model (LLM). LLMs have demonstrated various emergent abilities, as evidenced in GPT-3’s few-shot prompting setup(Brownet al.,2020)and the findings of Weiet al. (2022)(Weiet al.,2022). Such emergent properties are hard to find in smaller-scale models. It is conjectured that these emergent abilities are also applicable to multi-modal models, which could be the foundation of GPT-4’s impressive visual description capabilities. To substantiate our hypothesis, we present a novel vision-language model named MiniGPT-4. It utilizes an advanced large language model (LLM), Vicuna(Chianget al.,2023), which is built upon LLaMA(Touvronet al.,2023)and reported to achieve 90% of ChatGPT’s quality as per GPT-4’s evaluation, as the language decoder. In terms of visual perception, we employ the same pretrained vision components of BLIP-2(Liet al.,2023)that consists of a ViT-G/14 from EVA-CLIP(Fanget al.,2022)and a Q-Former network. MiniGPT-4 adds a single projection layer to align the encoded visual features with the Vicuna language model and freezes all the other vision and language components. MiniGPT-4 is initially trained for 20k steps using a batch size of 256 on 4 A100 GPUs, leveraging a combined image captioning dataset that includes images from LAION(Schuhmannet al.,2021), Conceptual Captions(Changpinyoet al.,2021; Sharmaet al.,2018), and SBU(Ordonezet al.,2011)to align visual features with the Vicuna language model. Nevertheless, merely aligning visual features with the language model (LLM) is inadequate to ensure robust visual conversation capabilities, resembling that of a chatbot. The presence of underlying noise in raw image-text pairs can lead to subpar language outputs. Therefore, we collect another 3,500 detailed image description pairs to further fine-tune the model with a designed conversational template in order to improve the naturalness of the generated language and its usability. In our experiments, we discovered that MiniGPT-4 possesses numerous capabilities similar to those demonstrated by GPT-4. For instance, MiniGPT-4 can generate intricate image descriptions, create websites based on handwritten text instructions, and explain unusual visual phenomena. Furthermore, our findings revealed that MiniGPT-4 also has a variety of other intriguing abilities not showcased in the GPT-4 demonstrations. For example, MiniGPT-4 can directly generate detailed cooking recipes from food photos, write stories or poems inspired by images, write advertisements for products in images, identify problems shown in photos and provide corresponding solutions, and retrieve rich facts about people, movies, or art directly from images, among other capabilities. These abilities are absent in previous vision-language models like Kosmos-1(Huanget al.,2023)and BLIP-2(Liet al.,2023)that use less powerful language models. This further validates that integrating visual features with an advanced language model is one of the keys to enhancing vision-language models. We present a summary of our key findings: Our research reveals with compelling evidence that by aligning visual features with advanced large language models like Vicuna, MiniGPT-4 can achieve advanced vision-language capabilities comparable to those exhibited in the GPT-4 demonstrations. Our findings suggest that training merely one projection layer can effectively align a pretrained vision encoder with the large language model. Our MiniGPT-4 only requires training approximately 10 hours on 4 A100 GPUs. We discovered that simply aligning visual features with large language models using short image caption pairs is not sufficient for developing a well-performing model and leads to unnatural language generation. Further finetuning with a small but detailed image description pairs can address this limitation and significantly improves its usability.	"SMART: no
Evidence_specific: no; the text does not outline any clear, specific goal but rather provides a descriptive statement on the capabilities and features of GPT-4 and MiniGPT-4.
Evidence_measurable: no; there is no quantifiable measure defined in the given text.
Evidence_achievable: no; the text does not define any goal, so achievability is not addressed.
Evidence_relevant: no; there is no goal provided to assess the relevance of the activities.
Evidence_time-bound: no; there are no deadlines or time frames mentioned in the text."	"SMART: No

Evidence_specific:
Yes; the evidence text in the given text for criterion ""specific"":
- Examples of tasks performed by GPT-4: ""produce detailed and accurate image descriptions,"" ""explain unusual visual phenomena,"" and ""construct websites based on handwritten text instructions.""
- Details about MiniGPT-4: its architecture and components such as using the ""Vicuna language model"" and ""pretrained vision components of BLIP-2.""

Evidence_measurable:
Yes; the evidence text in the given text for criterion ""measurable"":
- Reference to the training regimen for MiniGPT-4: ""initially trained for 20k steps using a batch size of 256 on 4 A100 GPUs.""
- Mention of evaluation measure: ""reported to achieve 90% of ChatGPT’s quality as per GPT-4’s evaluation.""

Evidence_achievable:
No clear evidence of ""achievable""; while the text describes the model and its capabilities, it does not lay out a clear vision or steps that show how the described achievements will build on or improve current capabilities in a real-world sense.

Evidence_relevant:
No clear evidence of ""relevant""; the relevance to a specific goal, context, or problem that the model addresses isn't explicitly stated (Note: ignoring the ""goal"" aspect as per the user's instructions may conflict with assessing relevance).

Evidence_time-bound:
No; the evidence text in the given text for criterion ""time-bound"":
- While it's mentioned that the training requires ""approximately 10 hours on 4 A100 GPUs,"" this refers only to the time to train MiniGPT-4, not a deadline or a timeframe associated with achieving objectives or implementing the model in practice."
2303.18223v13	A Survey of Large Language Models	Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.		"SMART: no
Evidence_specific: no
Evidence_measurable: no
Evidence_achievable: no
Evidence_relevant: no
Evidence_time-bound: no

Explanation: The text provides an overview of the development and impact of large language models (LLMs) in AI research. It does not contain goals but rather a descriptive survey of advancements and aspects of LLMs. There are no clear, articulated objectives nor statements that align with SMART criteria in the context of goal setting. Thus, the text is not framed in the style of a SMART objective."	
2307.03172v3	Lost in the Middle: How Language Models Use Long Contexts	While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.	Language models have become an important and flexible building block in a variety of user-facing language technologies, including conversational interfaces, search and summarization, and collaborative writing(Shusteret al.,2022; Thoppilanet al.,2022; Leeet al.,2022,inter alia). These models perform downstream tasks primarily via prompting: all relevant task specification and data to process is formatted as a textual input context, and the model returns a generated text completion. These input contexts can contain thousands of tokens, especially when language models are used to process long documents (e.g., legal or scientific documents, conversation histories, etc.) or when language models are augmented with external information (e.g., relevant documents from a search engine, database query results, etc;Petroniet al.,2020; Ramet al.,2023; Shiet al.,2023; Mallenet al.,2023; Schicket al.,2023,inter alia). Handling these use-cases requires language models to successfully operate over long sequences. Existing language models are generally implemented with Transformers(Vaswaniet al.,2017), which require memory and compute that increases quadratically in sequence length. As a result, Transformer language models were often trained with relatively small context windows (between 512-2048 tokens). Recent improvements in hardware (e.g., faster GPUs with more memory) and algorithms(Daiet al.,2019; Daoet al.,2022; Poliet al.,2023; Rubin and Berant,2023,inter alia)have resulted in language models with larger context windows (e.g., 4096, 32K, and even 100K tokens), but it remains unclear how these extended-context language models make use of their input contexts when performing downstream tasks. We empirically investigate this question via controlled experiments with a variety of state-of-the-art open (MPT-30B-Instruct, LongChat-13B (16K)) and closed (OpenAI’s GPT-3.5-Turbo and Anthropic’s Claude-1.3) language models in settings that require accessing and using information within an input context. In particular, our experiments make controlled changes to the input context size and the position of the relevant information within the input context and study their effects on language model performance. If language models can robustly use information within long input contexts, then their performance should beminimally affectedby the position of the relevant information in the input context. We first experiment with multi-document question answering, which requires models to reason over provided documents to find relevant information and use it to answer a given question; this task mimics the retrieval-augmented generation setup underlying many commercial generative search and question answering applications (e.g., Bing Chat). In this setting, we control (i) the input context length by changing the number of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to place the relevant document at the beginning, middle or end of the context. We find that changing the position of relevant information in the input context can substantially affect model performance, indicating that current language models do not robustly access and use information in long input contexts. Furthermore, we observe a distinctive U-shaped performance curve (Figure1); language model performance is highest when relevant information occurs at the very beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and use information in the middle of their input context (§2.3). For example, when relevant information is placed in the middle of its input context, GPT-3.5-Turbo’s performance on the multi-document question task is lower than its performance when predictingwithout any documents(i.e., the closed-book setting; 56.1%). Furthermore, we find that models often have identical performance to their extended-context counterparts, indicating that extended-context models are not necessarily better at using their input context (§2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can language models evenretrievefrom their input contexts? We study this question with a synthetic key-value retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching tokens from the input context. In this task, models are given a collection of JSON-formatted key-value pairs and must return the value associated with a specific key. Similar to the multi-document QA task, the key-value retrieval task admits controlled changes to the input context length (adding more key-value pairs) and the position of relevant information. Although some models perform the synthetic key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. To better understand why language models struggle to robustly access and use information in their input contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), query-aware contextualization, and instruction fine-tuning (§4). We find that: Encoder-decoder models are relatively robust to changes in the position of relevant information within their input context, but only when evaluated on sequences within its training-time sequence length. When evaluated on sequences longer than those seen during training, we observe a U-shaped performance curve (§4.1). Query-aware contextualization (placing the query beforeandafter the documents or key-value pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA (§4.2). Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of relevant information in the input context. Our results indicate that prompting language models with longer input contexts is a trade-off—providing the language model with more information may help it perform the downstream task, but it also increases the amount of content that the model must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader models on open-domain question answering (§5). In contrast to our controlled multi-document QA task, where the context always contains exactlyonedocument that answers the question, none or many of the topkdocuments may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from NaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents—using 50 documents instead of 20 retrieved documents only marginally improves performance (\sim1.5% for GPT-3.5-Turbo and\sim1% for claude-1.3). Our analysis provides a better understanding of how language models use their input context and introduces new evaluation protocols for future long-context models; to claim that a language model can robustly use information within long input contexts, it is necessary to show that its performance is minimally affected by the position of the relevant information in the input context (e.g., minimal difference in best- and worst-case performance). To facilitate further work on understanding and improving how language models use their input context, we release our code and evaluation data.111nelsonliu.me/papers/lost-in-the-middle	"SMART: No
Evidence_specific: Yes; the evidence text - ""analyzing the performance of language models on two tasks: multi-document question answering and key-value retrieval""
Evidence_measurable: Yes; the evidence text - ""performance can degrade significantly when changing the position of relevant information"" and ""performance is often highest when relevant information occurs at the beginning or end""
Evidence_achievable: No
Evidence_relevant: No
Evidence_time-bound: No"	"SMART: No
Evidence_specific: No
Evidence_measurable: No
Evidence_achievable: No
Evidence_relevant: No
Evidence_time-bound: No

The text provided is a summary of a research study on language models' performance in processing long input contexts. It does not set goals or objectives but rather describes the research that was done, which makes it not compliant with the SMART criteria, which pertain to goal setting."
2307.01952v1	SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis	We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models		"SMART: No
Evidence_specific: Yes; ""a latent diffusion model for text-to-image synthesis""
Evidence_measurable: No; there is no quantitative measure or specific metric provided.
Evidence_achievable: No; the text does not mention any evidence that the goals are attainable or realistic.
Evidence_relevant: No; the text lacks specific information on relevance to broader goals or why SDXL is important.
Evidence_time-bound: No; there is no mention of a timeline or deadline."	
2309.01219v2	Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models	While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.	Large language models (LLMs), particularly characterized by their substantial number of parameters, have arisen as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence(zhao2023survey). With proper alignment techniques, such as supervised finetuning(SFT;zhang2023instruction)and reinforcement learning from human feedback(RLHF;ouyang2022training;fernandes2023bridging), recent LLMs(chatgpt;touvron2023llama2;openai2023gpt4,inter alia)have exhibited strong capabilities in solving various downstream tasks. Nonetheless, as exemplified in Figure1, LLMs, despite their remarkable success, occasionally produce outputs that, while seemingly plausible, deviate from user input(adlakha2023evaluating), previously generated context(liu-etal-2022-token), or factual knowledge(min2023factscore;muhlgay2023generating;li2023halueval)—this phenomenon is commonly referred to as hallucination, which significantly undermines the reliability of LLMs in real-world scenarios(kaddour2023challenges). For instance, LLMs can potentially fabricate erroneous medical diagnoses or treatment plans that lead to tangible real-life risks(umapathi2023med). While hallucination in conventional natural language generation (NLG) settings has been widely studied(nlg_hallucination_survey), understanding and addressing the hallucination problem within the realm of LLMs encounters unique challenges introduced by [leftmargin=*,topsep=2pt] Massive training data: in contrast to carefully curating data for a specific task, LLM pre-training uses trillions of tokens obtained from the web, making it difficult to eliminate fabricated, outdated or biased information; Versatility of LLMs: general-purpose LLMs are expected to excel in cross-task, cross-lingual, and cross-domain settings, posing challenges for comprehensive evaluation and mitigation of hallucination. Imperceptibility of errors: as a byproduct of their strong abilities, LLMs may generate false information that initially seems highly plausible, making it challenging for models or even humans to detect hallucination. In addition, the RLHF process(ouyang2022training), the vague knowledge boundary(ren2023boundary)and the black-box property of LLMs(sun2022black)also complicate the detection, explanation, and mitigation of hallucination in LLMs. There has been a notable upsurge in cutting-edge research dedicated to addressing the aforementioned challenges, which strongly motivates us to compile this survey. We organize this paper as follows, as also depicted in Figure2. We first introduce the background of LLMs and offer our definition of hallucination in LLMs (§2). Next, we introduce relevant benchmarks and metrics (§3). Subsequently, we discuss potential sources of LLM hallucinations (§4), and provide an in-depth review of recent work towards addressing the problem (§5). Finally, we present forward-looking perspectives (§6). We will consistently update the related open-source materials, which can be accessed athttps://github.com/HillZhang1999/llm-hallucination-survey.	"SMART: No

Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"": ""survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs.""

Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"": ""present taxonomies of the LLM hallucination phenomena and evaluation benchmarks""

Evidence_achievable: No; There is no clear evidence in the text suggesting attainability or realistic approaches.

Evidence_relevant: Yes; the evidence text in the given text for criterion ""relevant"": ""This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios.""

Evidence_time-bound: No; There is no specific timeframe or deadline mentioned in the text."	"SMART: No
Evidence_specific: Yes; ""we will compile this survey"", ""we organize this paper as follows""
Evidence_measurable: No; no quantifiable metrics or indicators of progress
Evidence_achievable: No; no mention of resources, capabilities, or plan regarding achievability
Evidence_relevant: No; does not establish relevance towards broader goals or context
Evidence_time-bound: No; no specific timelines or deadlines are provided"
2304.14178v1	mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality	Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.	Large language models (LLMs) such as GPT-3(Brownet al.,2020), BLOOM(Scaoet al.,2022), LLaMA(Touvronet al.,2023)have experienced rapid development to make general artificial intelligence possible, which demonstrates impressive zero-shot abilities on various linguistic applications. However, except GPT-4(OpenAI,2023), current general LLMs cannot support different modalities of input and develop impressive multimodal abilities. Although GPT-4(OpenAI,2023)has exhibited remarkable multimodal abilities, the methods behind its extraordinary abilities remain a mystery. Recently, researchers have been extending LLMs to understand visual inputs in two different paradigms: systematic collaboration and end-to-end trained models. However, systematic collaboration approaches, including Visual ChatGPT(Wuet al.,2023), MM-REACT(Yanget al.,2023), and HuggingGPT(Shenet al.,2023), are designed to facilitate the coordination of various vision models or tools to express visual information with text descriptions. However, these approaches may not be able to comprehend specific multimodal instructions due to their lack of alignment with different modalities. Additionally, these approaches may encounter challenges related to inference efficiency and cost. End-to-end models, such as BLIP-2(Liet al.,2023), LLaVA(Liuet al.,2023), and MiniGPT-4(Zhuet al.,2023a)aim to use unified models to support different modalities. However, these models have some limitations as they take frozen visual models, which may lead to inadequate alignment due to the limited number of parameters. Moreover, they cannot unlock various abilities due to missing unimodal and multimodal instruction. In this paper, we present mPLUG-Owl with an innovative modularized training paradigm for large multi-modal language models that can support multiple modalities concurrently, drawing inspiration from the concept of modularization(Xuet al.,2023b; Liet al.,2022; Xuet al.,2021; Yeet al.,2022). Our method harnesses the power of pre-trained LLM, visual knowledge module, and connected visual abstractor module to achieve effective alignment between images and text, and utilizes a two-stage training scheme to stimulate impressive unimodal and multimodal abilities. Our approach even enhances the strong generation abilities of LLM by modality collaboration between modalities. In the first step, we align the image and text to acquire comprehensive visual knowledge using text-image pairs, which is accomplished by training the visual knowledge module and abstractor module with the frozen LLM module. Subsequently, we fine-tune mPLUG-Owl with language-only and multi-modal instructions to unlock a range of unimodal and multimodal abilities. We freeze the visual knowledge module and train low-rank adaption (LoRA)(Huet al.,2022)on LLM and visual abstractor module jointly. This approach allows for the effective integration of textual and visual information, facilitating the development of versatile and robust cognitive abilities. Our experiments on a carefully-built visually related instruction evaluation set OwlEval shows that mPLUG-Owl outperforms existing models such as MiniGPT-4(Zhuet al.,2023a)and LLaVA(Liuet al.,2023). We separately verifies mPLUG-Owl’s remarkable abilities in instruction understanding, visual understanding, knowledge transfer, and multi-turn dialogue. Abundant ablation study is performed to show the effectiveness of our training paradigm. Furthermore, we find some unexpected emerging ability such as multi-image correlation, multilingual conversation and scene text understanding. Our main contributions can be highlighted as follows: We propose mPLUG-Owl, a novel training paradigm for large language models through modularization. We carefully construct an instruction evaluation set, dubbedOwlEval, to assess the capabilities of different models in the context of visual-related tasks. Experimental results demonstrate that mPLUG-Owl excels in multi-modal instruction understanding and multi-turn dialogue, surpassing the performance of existing models.	"SMART: No

Evidence_specific: Yes; the evidence text ""introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module""

Evidence_measurable: No; there is no clear quantifiable measure stated in the given text for criterion ""measurable""

Evidence_attainable: No; there's no clear reference to the feasibility or attainability of the project or outcomes

Evidence_relevant: No; the text does not specify the relevance of the goals or objectives in a broader context

Evidence_time-bound: No; there is no mention of a specific timeline or deadline for the project or objectives to be achieved"	"SMART: No

Evidence_specific: Yes; Evidence: ""we present mPLUG-Owl with an innovative modularized training paradigm for large multi-modal language models that can support multiple modalities concurrently""

Evidence_measurable: Yes; Evidence: ""Our experiments on a carefully-built visually related instruction evaluation set OwlEval shows that mPLUG-Owl outperforms existing models""

Evidence_achievable: No; No specific evidence of resource availability, individual or organizational constraints.

Evidence_relevant: No; No specific reference to broader goals or reasons why achieving the described outcome is important in relation to an overarching goal.

Evidence_time-bound: No; The text does not provide a specific timeline for achieving the described outcomes."
2308.04079v1	3D Gaussian Splatting for Real-Time Radiance Field Rendering	Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.	Meshes and points are the most common 3D scene representations because they are explicit and are a good fit for fast GPU/CUDA-based rasterization. In contrast, recent Neural Radiance Field (NeRF) methods build on continuous scene representations, typically optimizing a Multi-Layer Perceptron (MLP) using volumetric ray-marching for novel-view synthesis of captured scenes. Similarly, the most efficient radiance field solutions to date build on continuous representations by interpolating values stored in, e.g., voxel(Fridovich-Keil and Yu et al.,2022)or hash(Müller et al.,2022)grids or points(Xu et al.,2022). While the continuous nature of these methods helps optimization, the stochastic sampling required for rendering is costly and can result in noise. We introduce a new approach that combines the best of both worlds: our 3D Gaussian representation allows optimization with state-of-the-art (SOTA) visual quality and competitive training times, while our tile-based splatting solution ensures real-time rendering at SOTA quality for 1080p resolution on several previously published datasets(Knapitsch et al.,2017; Hedman et al.,2018; Barron et al.,2022)(see Fig.1). Our goal is to allow real-time rendering for scenes captured with multiple photos, and create the representations with optimization times as fast as the most efficient previous methods for typical real scenes. Recent methods achieve fast training(Fridovich-Keil and Yu et al.,2022; Müller et al.,2022), but struggle to achieve the visual quality obtained by the current SOTA NeRF methods, i.e., Mip-NeRF360(Barron et al.,2022), which requires up to 48 hours of training time. The fast – but lower-quality – radiance field methods can achieve interactive rendering times depending on the scene (10-15 frames per second), but fall short of real-time rendering at high resolution. Our solution builds on three main components. We first introduce3D Gaussiansas a flexible and expressive scene representation. We start with the same input as previous NeRF-like methods, i.e., cameras calibrated with Structure-from-Motion (SfM)(Snavely et al.,2006)and initialize the set of 3D Gaussians with the sparse point cloud produced for free as part of the SfM process. In contrast to most point-based solutions that require Multi-View Stereo (MVS) data(Aliev et al.,2020; Kopanas et al.,2021; Rückert et al.,2022), we achieve high-quality results with only SfM points as input. Note that for the NeRF-synthetic dataset, our method achieves high quality even with random initialization. We show that 3D Gaussians are an excellent choice, since they are a differentiable volumetric representation, but they can also be rasterized very efficiently by projecting them to 2D, and applying standard\alpha-blending, using an equivalent image formation model as NeRF. The second component of our method is optimization of the properties of the 3D Gaussians – 3D position, opacity\alpha, anisotropic covariance, and spherical harmonic (SH) coefficients – interleaved with adaptive density control steps, where we add and occasionally remove 3D Gaussians during optimization. The optimization procedure produces a reasonably compact, unstructured, and precise representation of the scene (1-5 million Gaussians for all scenes tested). The third and final element of our method is our real-time rendering solution that uses fast GPU sorting algorithms and is inspired by tile-based rasterization, following recent work(Lassner and Zollhofer,2021). However, thanks to our 3D Gaussian representation, we can perform anisotropic splatting that respects visibility ordering – thanks to sorting and\alpha-blending – and enable a fast and accurate backward pass by tracking the traversal of as many sorted splats as required. To summarize, we provide the following contributions: The introduction of anisotropic 3D Gaussians as a high-quality, unstructured representation of radiance fields. An optimization method of 3D Gaussian properties, interleaved with adaptive density control that creates high-quality representations for captured scenes. A fast, differentiable rendering approach for the GPU, which is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis. Our results on previously published datasets show that we can optimize our 3D Gaussians from multi-view captures and achieve equal or better quality than the best quality previous implicit radiance field approaches. We also can achieve training speeds and quality similar to the fastest methods and importantly provide the firstreal-time renderingwith high quality for novel-view synthesis.	"SMART: No
Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" - ""We introduce three key elements..."", ""unbounded and complete scenes"", ""real-time (>= 30 fps) novel-view synthesis at 1080p resolution""
Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" - "">= 30 fps"", ""1080p resolution"", ""competitive training times""
Evidence_achievable: Partial; no clear metrics or baselines are provided for what defines ""competitive training times"" or ""state-of-the-art visual quality"", although it is implied to be possible within the text.
Evidence_relevant: Yes; the evidence text in the given text for criterion ""relevant"" - This assumes relevance to the field's goals and challenges, regarding ""novel-view synthesis of scenes"" and ""high visual quality"".
Evidence_time-bound: No; no specific time frame or deadline is mentioned regarding when these elements will be introduced or by when the results will be achieved."	"SMART: No

Evidence_specific: No
There are no specifics regarding quantifiable attributes within the excerpt that address the SMART criterion of being specific.

Evidence_measurable: No
The excerpt mentions ""competitive training times,"" ""real-time rendering,"" and ""high quality,"" but it does not provide specific measurements to gauge success or progress.

Evidence_achievable: No
The text discusses the ambitious integration of solutions without specifying the feasibility or concrete steps to achieve these solutions.

Evidence_relevant: Yes
The evidence text in the given text for criterion ""relevant"": “...our goal is to allow real-time rendering for scenes captured with multiple photos, and create the representations with optimization times as fast as the most efficient previous methods for typical real scenes...” suggests that the work is relevant to the field of rendering and visualization.

Evidence_time-bound: No
While the text mentions ""competitive training times"" and ""real-time rendering,"" there is no specific timeframe or deadline provided that makes the goal time-bound."
2310.16944v1	Zephyr: Direct Distillation of LM Alignment	We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.	Smaller, open large language models (LLMs) have greatly increased in ability in recent years, from early GPT-2-like models(Wang2021-ti)to accurate and compact models(Touvron2023-af;penedo2023refinedweb;Jiang2023-qi)that are trained on significantly more tokens than the “compute-optimal” amount suggested by the Chincilla scaling laws(devries2023chinchilla_analysis). In addition, researchers have shown that these models can be further trained through distilled supervised fine-tuning (dSFT) based on proprietary models to increase their accuracy(Taori2023-ga). In this approach, the output of a more capable teacher model is used as supervised data for the student model. Distillation has proven to be an effective tool for improving open models on a range of different tasks(Chiang2023-rz); however, it does not reach the performance of the teacher models(Gudibande2023-vd). Users have noted that these models are not “intent aligned”, i.e. they do not behave in a manner that aligns with human users’ preferences. This property often leads to outputs that do not provide correct responses to queries. Intention alignment has been difficult to quantify, but recent work has led to the development of benchmarks like MT-Bench(Zheng2023-yf)and AlpacaEval(Li2023-zl)that specifically target this behavior. These benchmarks yield scores that correlate closely with human ratings of model outputs and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation. This motivates careful collection of human feedback for alignment, often at enormous cost at scale, such as inLlama2-Chat(Touvron2023-af). In this work, we consider the problem of aligning a small open LLM entirely through distillation. The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective(Rafailov2023-mv). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling compared to using other approaches like proximal preference optimization (PPO)(Schulman2017-mu). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB). To validate this approach, we constructZephyr-7B, an aligned version of Mistral-7B(Jiang2023-qi). We first use dSFT, based on the UltraChat(Ding2023-cc)dataset. Next we use the AI feedback data collected in the UltraFeedback dataset(Cui2023-cb). Finally, we apply dDPO based on this feedback data. Experiments show that this 7B parameter model can achieve performance comparable to 70B-parameter chat models aligned with human feedback. Results show improvements both in terms of standard academic benchmarks as well as benchmarks that take into account conversational capabilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available athttps://github.com/huggingface/alignment-handbook. We note an important caveat for these results. We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice(bai2022training). As distillation only works with the output of publicly available models this is technically more challenging to do because of added challenges in curating that type of synthetic data, and is an important subject for future work.	"SMART: No
Evidence_specific: Yes; ""produce a smaller language model that is aligned to user intent"" and ""applying distilled supervised fine-tuning (dSFT)"" detail specific actions and outcomes.
Evidence_measurable: No; While the text mentions ""significantly improved task accuracy"" and ""significantly improved intent alignment,"" it lacks specific quantitative measures or benchmarks.
Evidence_achievable: Yes; The text implies achievability by stating ""requires only a few hours of training without any additional sampling during fine-tuning.""
Evidence_relevant: No; The text doesn't discuss the relevance of the goal directly, but relevance is inferred through the context of improving language models.
Evidence_time-bound: No; There's no mention of a specific timeframe for the goals aside from the few hours of training required."	"SMART: no
The text does not fully adhere to the SMART principle when considering goals or objectives, as it's more of an explanatory passage discussing recent advancements and strategies in aligning open large language models.

Evidence_specific: yes
Evidence for ""specific"": The text specifies the research focus on ""aligning a small open LLM entirely through distillation"" and details the strategy (dDPO) as well as the tools used, such as the AI Feedback from an ensemble of teacher models.

Evidence_measurable: yes
Evidence for ""measurable"": The text mentions benchmarks like MT-Bench and AlpacaEval and discusses measuring model performance in comparison to human feedback and larger models, suggesting measurable elements.

Evidence_achievable: no
The text presents a research study's outcomes rather than a goal or objective that needs to be achieved; hence it does not directly reflect achievability.

Evidence_relevant: no
The text is relevant to the field of AI research but does not approach relevance from a goal-achievement perspective, which is the typical context for the SMART framework.

Evidence_time-bound: no
The text lacks any temporal goals or constraints, which would be indicative of the ""time-bound"" quality of the SMART criterion."
2306.14824v3	Kosmos-2: Grounding Multimodal Large Language Models to the World	We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.	Multimodal Large Language Models (MLLMs)[9,2,12,8,24]have successfully played a role as a general-purpose interface across a wide range of tasks, such as language, vision, and vision-language tasks. MLLMs can perceive general modalities, including texts, images, and audio, and generate responses using free-form texts under zero-shot and few-shot settings. In this work, we unlock the grounding capability for multimodal large language models. Grounding capability can provide a more convenient and efficient human-AI interaction for vision-language tasks. It enables the user to point to the object or region in the image directly rather than input detailed text descriptions to refer to it, the model can understand that image region with its spatial locations. Grounding capability also enables the model to respond with visual answers (i.e., bounding boxes), which can support more vision-language tasks such as referring expression comprehension. Visual answers are more accurate and resolve the coreference ambiguity compared with text-only responses. In addition, grounding capability can link noun phrases and referring expressions in the generated free-form text response to the image regions, providing more accurate, informational, and comprehensive answers. We introduceKosmos-2, a multimodal large language model with grounding capability built uponKosmos-1.Kosmos-2is a Transformer-based causal language model and is trained using the next-word prediction task. In order to unlock the grounding capability, we construct a web-scale dataset of grounded image-text pairs, and combine it with the multimodal corpora inKosmos-1to train the model. The grounded image-text pairs are built upon a subset of image-text pairs from LAION-2B[26]and COYO-700M[4]. We construct a pipeline to extract and link the text spans (i.e., noun phrases and referring expressions) in the caption to the spatial locations (e.g., bounding boxes) of its corresponding objects or regions in the image. We convert the spatial coordinates of the bounding boxes to a sequence of location tokens, which is then appended after its respective text spans. The data format serves as a “hyperlink” to connect the objects or regions of the image to the caption. Experimental results demonstrate thatKosmos-2not only achieves competitive performance on language and vision-language tasks evaluated inKosmos-1, but also achieves impressive performance on grounding tasks (phrase grounding and referring expression comprehension) and referring tasks (referring expression generation). As shown in Figure2, integrating the grounding capability enablesKosmos-2to be used for more downstream tasks, such as grounded image captioning, and grounded visual question answering.	"SMART: No
Evidence_specific: No; the text explains what Kosmos-2 is and its capabilities but doesn't provide specific evidence that fulfills the SMART ""specific"" criterion.
Evidence_measurable: Yes; ""We evaluate Kosmos-2 on a wide range of tasks"" shows that measurements will be taken, although the specific metrics are not mentioned.
Evidence_achievable: No; there is no evidence discussing the feasibility or achievability of the described capabilities.
Evidence_relevant: No; there is no evidence of relevance in the context of the SMART criteria.
Evidence_time-bound: No; there is no mention of a timeframe or deadline for the evaluation or development of Kosmos-2."	"SMART: No

Evidence_specific: Yes; the evidence text is ""Grounding capability can provide a more convenient and efficient human-AI interaction for vision-language tasks.""

Evidence_measurable: No; there is no specific criteria for measuring performance or progress mentioned.

Evidence_achievable: No; the text does not discuss the achievability of the tasks or methods.

Evidence_relevant: Yes; the evidence text is ""The grounded image-text pairs are built upon a subset of image-text pairs from LAION-2B[26]and COYO-700M[4].""

Evidence_time-bound: No; there is no reference to a specific timeline or deadline."
2308.16512v2	MVDream: Multi-view Diffusion for 3D Generation	We introduce MVDream, a multi-view diffusion model that is able to generate consistent multi-view images from a given text prompt. Learning from both 2D and 3D data, a multi-view diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that such a multi-view prior can serve as a generalizable 3D prior that is agnostic to 3D representations. It can be applied to 3D generation via Score Distillation Sampling, significantly enhancing the consistency and stability of existing 2D-lifting methods. It can also learn new concepts from a few 2D examples, akin to DreamBooth, but for 3D generation.	3D content creation is an important step in the pipeline of modern game and media industry, yet it is a labor-intensive task that requires well-trained designers to work for hours or days to create a single 3D asset. A system that can generate 3D content in an easy way for non-professional users is thus of great value. Existing 3D object generation methods can be categorized into three types: (1) template-based generation pipeline, (2) 3D generative models, and (3) 2D-lifting methods. Due to limited accessible 3D models and large data complexity, both template-based generators and 3D generative models struggle to generalize effectively to arbitrary object generation. Their generated content is often confined to common real-world objects with relatively simple topology and texture. Yet in industry, popular 3D assets usually come as a mixture of complicated, artistic, and sometimes non-realistic structures and styles(43). Recently, 2D-lifting methods have shown that pre-trained 2D generation models can be potentially applied to 3D generation. The typical representations are Dreamfusion(Pooleet al.,2023)and Magic3D(Linet al.,2023a)systems, which utilize 2D diffusion models as supervision for the optimization of a 3D representation via score distillation sampling (SDS). Trained on large-scale 2D image datasets, these 2D models are able to generate unseen and counterfactual scenes whose details can be specified through a text input, making them great tools for creating artistic assets. Nevertheless, in 2D-lifting techniques, challenges arise due to the lack of comprehensive multi-view knowledge or 3D-awareness during score distillation. These challenges encompass: (1) The multi-face Janus issue: The system frequently regenerates content described by the text prompt. (2) Content drift across different views. Examples can be seen in Fig. (1). The multi-face issue can stem from various factors. For instance, certain objects, like blades, may be nearly invisible from some angles. Meanwhile, vital parts of a character or animal might be hidden or self-occluded from specific viewpoints. While humans assess these objects from multiple angles, a 2D diffusion model cannot, leading it to produce redundant and inconsistent content. In spite of all the weaknesses of 2D-lifting methods, we believe that large-scale 2D data is crucial to generalizable 3D generation. Therefore, we propose multi-view diffusion models, which can be used as a multi-view 3D prior agnostic to 3D representations. The proposed model simultaneously generates a set of multi-view images that are consistent with each other. It can leverage pre-trained 2D diffusion models for transfer learning to inherit the generalizability. Then, by jointly training the model on multi-view images (from 3D assets) and 2D image-text pairs, we find that it can achieve both good consistency and generalizability. When applied to 3D generation through score distillation, our multi-view supervision proves significantly more stable than that of single-view 2D diffusion models. And we can still create unseen, counterfactual 3D contents as from pure 2D diffusion models. Inspired by DreamBooth(Ruizet al.,2023), we can also employ our multi-view diffusion model to assimilate identity information from a collection of 2D images and it demonstrates robust multi-view consistency after fine-tuning. Overall, our model, namelyMVDream, successfully generates 3D Nerf models without the multi-view consistency issue. It either surpasses or matches the diversity seen in other state-of-the-art methods.	"SMART: No

Evidence_specific: No; the text does not establish a specific target or detail about the model's performance or specific improvements over existing methods.
Evidence_measurable: Yes; the text implies measurability by suggesting ""enhancing the consistency and stability"" and ""learn new concepts from a few 2D examples,"" which can be quantitatively assessed.
Evidence_achievable: No; the text does not mention any steps, resources, or plans that would make achieving the described capabilities possible.
Evidence_relevant: No; there's no evidence in the text pointing out the relevance or importance of the model in a broader context.
Evidence_time-bound: No; the text provides no information about timeframes or deadlines for achieving the mentioned model capabilities."	"SMART: No
Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is ""generate 3D content in an easy way for non-professional users,"" ""challenges arise due to the lack of comprehensive multi-view knowledge or 3D-awareness during score distillation,"" and ""our multi-view diffusion model to assimilate identity information from a collection of 2D images.""
Evidence_measurable: No; the text does not mention how the success of the method can be quantitatively measured.
Evidence_achievable: No; there's no statement in the text on the feasibility of the proposed method regarding available resources and capabilities.
Evidence_relevant: Yes; the evidence text in the given text for criterion ""relevant"" is ""3D content creation is an important step in the pipeline of modern game and media industry"" and ""we believe that large-scale 2D data is crucial to generalizable 3D generation"".
Evidence_time-bound: No; the text does not specify a time frame or deadline for the goals or progress."
2305.16291v2	Voyager: An Open-Ended Embodied Agent with Large Language Models	We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.	Building generally capable embodied agents that continuously explore, plan, and develop new skills in open-ended worlds is a grand challenge for the AI communitykolve2017ai2thor;savva2019habitat;zhu2020robosuite;xia2019igibson0.5; Shenet al.(2020). Classical approaches employ reinforcement learning (RL)Koberet al.(2013); Arulkumaranet al.(2017)and imitation learningBakeret al.(2022); Teamet al.(2021); Vinyalset al.(2019)that operate on primitive actions, which could be challenging for systematic explorationecoffet2019goexplore;huizinga2022evolving;wang2020enhanced;kanitscheider2021multitask;dennis2020paired, interpretabilityLianget al.(2022); Sunet al.(2020); Zhaoet al.(2021), and generalizationjiang2022vima; Shridharet al.(2021); Fanet al.(2021). Recent advances in large language model (LLM) based agents harness the world knowledge encapsulated in pre-trained LLMs to generate consistent action plans or executable policiesLianget al.(2022); Singhet al.(2022);jiang2022vima. They are applied to embodied tasks like games and roboticsFanet al.(2022); Zenget al.(2022); Ahnet al.(2022); Huanget al.(2022b,a), as well as NLP tasks without embodimentautogpt (2023); Yaoet al.(2022); Shinnet al.(2023). However, these agents are not lifelong learners that can progressively acquire, update, accumulate, and transfer knowledge over extended time spansParisiet al.(2019); Wanget al.(2023a). Let us consider Minecraft as an example. Unlike most other games studied in AImnih2013playing;openai2019dota; Vinyalset al.(2019), Minecraft does not impose a predefined end goal or a fixed storyline but rather provides a unique playground with endless possibilitiesFanet al.(2022). Minecraft requires players to explore vast, procedurally generated 3D terrains and unlock a tech tree using gathered resources. Human players typically start by learning the basics, such as mining wood and cooking food, before advancing to more complex tasks like combating monsters and crafting diamond tools. We argue that an effective lifelong learning agent should have similar capabilities as human players: (1)propose suitable tasksbased on its current skill level and world state, e.g., learn to harvest sand and cactus before iron if it finds itself in a desert rather than a forest; (2)refine skillsbased on environmental feedback andcommit mastered skills to memoryfor future reuse in similar situations (e.g. fighting zombies is similar to fighting spiders); (3)continually explore the worldand seek out new tasks in a self-driven manner. Towards these goals, we introduceVoyager, the firstLLM-powered embodied lifelong learning agentto drive exploration, master a wide range of skills, and make new discoveries continually without human intervention in Minecraft.Voyageris made possible through three key modules (Fig.2): 1) anautomatic curriculumthat maximizes exploration; 2) askill libraryfor storing and retrieving complex behaviors; and 3) a newiterative prompting mechanismthat generates executable code for embodied control. We opt to use code as the action space instead of low-level motor commands because programs can naturally represent temporally extended and compositional actionsLianget al.(2022); Singhet al.(2022), which are essential for many long-horizon tasks in Minecraft.Voyagerinteracts with a blackbox LLM (GPT-4OpenAI (2023)) through prompting and in-context learningwei2022emergent; Brownet al.(2020); Raffelet al.(2020). Our approach bypasses the need for model parameter access and explicit gradient-based training or finetuning. More specifically,Voyagerattempts to solve progressively harder tasks proposed by theautomatic curriculum, which takes into account the exploration progress and the agent’s state. The curriculum is generated by GPT-4 based on the overarching goal of “discovering as many diverse things as possible”. This approach can be perceived as an in-context form ofnovelty searchEysenbachet al.(2019); Contiet al.(2018).Voyagerincrementally builds askill libraryby storing the action programs that help solve a task successfully. Each program is indexed by the embedding of its description, which can be retrieved in similar situations in the future. Complex skills can be synthesized bycomposingsimpler programs, which compoundsVoyager’s capabilities rapidly over time and alleviates catastrophic forgetting in other continual learning methodsParisiet al.(2019); Wanget al.(2023a). However, LLMs struggle to produce the correct action code consistently in one shotChenet al.(2021a). To address this challenge, we propose aniterative prompting mechanismthat: (1) executes the generated program to obtain observations from the Minecraft simulation (such as inventory listing and nearby creatures) and error trace from the code interpreter (if any); (2) incorporates the feedback into GPT-4’s prompt for another round of code refinement; and (3) repeats the process until a self-verification module confirms the task completion, at which point we commit the program to the skill library (e.g.,craftStoneShovel()andcombatZombieWithSword()) and query the automatic curriculum for the next milestone (Fig.2). Empirically,Voyagerdemonstrates strongin-context lifelong learningcapabilities. It can construct an ever-growing skill library of action programs that are reusable, interpretable, and generalizable to novel tasks. We evaluateVoyagersystematically against other LLM-based agent techniques (e.g., ReActYaoet al.(2022), ReflexionShinnet al.(2023), AutoGPTautogpt (2023)) in MineDojoFanet al.(2022), an open-source Minecraft AI framework.Voyageroutperforms prior SOTA by obtaining3.3\timesmore unique items, unlocking key tech tree milestones up to15.3\timesfaster, and traversing2.3\timeslonger distances. We further demonstrate thatVoyageris able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other methods struggle to generalize.	"SMART: No

Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is ""Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention.""

Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" is ""obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA.""

Evidence_achievable: No; the text does not provide evidence about the feasibility or methods for achieving the results.

Evidence_relevant: No; the text does not specify how the agent's tasks align with a particular objective or the relevance of its capabilities directly.

Evidence_time-bound: No; there is no mention of a specific time frame for the goals or milestones to be achieved."	"SMART: No
Evidence_specific: No; there is no specific evidence as the text is outlining a general approach and framework without detail on a specific metric or goal.
Evidence_measurable: Yes; evidence is the evaluation measures like ""3.3× more unique items"", ""15.3× faster"", ""2.3× longer distances"".
Evidence_achievable: No; the text does not provide information about the feasibility or the resources and strategies that ensure the goals are achievable.
Evidence_relevant: Yes; the text outlines the relevance of the agent's capabilities like having skills similar to human players and outperforming SOTA (state of the art).
Evidence_time-bound: No; there are no specific time frames or deadlines mentioned in relation to achieving objectives."
2310.15008v3	Wonder3D: Single Image to 3D using Cross-Domain Diffusion	In this work, we introduce Wonder3D, a novel method for efficiently generating high-fidelity textured meshes from single-view images.Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of image-to-3D tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure consistency, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a geometry-aware normal fusion algorithm that extracts high-quality surfaces from the multi-view 2D representations. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and reasonably good efficiency compared to prior works.		"SMART: No
Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is ""generating high-fidelity textured meshes from single-view images"" and ""cross-domain diffusion model that generates multi-view normal maps and corresponding color images.""
Evidence_measurable: No; there is no quantifiable measurement specified.
Evidence_achievable: Yes; evidence text in the given text for criterion ""achievable"" is ""generating high-fidelity textured meshes,"" ""employ a multi-view cross-domain attention mechanism,"" and ""introduce a geometry-aware normal fusion algorithm.""
Evidence_relevant: Yes; the evidence text in the given text for criterion ""relevant"" is the overall aim to ""improve the quality, consistency, and efficiency of image-to-3D tasks.""
Evidence_time-bound: No; there is no time frame or deadline mentioned."	
2309.00267v2	RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback	Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences. However, gathering high-quality human preference labels can be a time-consuming and expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al., offers a promising alternative that leverages a powerful off-the-shelf LLM to generate preferences in lieu of human annotators. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, RLAIF achieves comparable or superior performance to RLHF, as rated by human evaluators. Furthermore, RLAIF demonstrates the ability to outperform a supervised fine-tuned baseline even when the LLM preference labeler is the same size as the policy. In another experiment, directly prompting the LLM for reward scores achieves superior performance to the canonical RLAIF setup, where LLM preference labels are first distilled into a reward model. Finally, we conduct extensive studies on techniques for generating aligned AI preferences. Our results suggest that RLAIF can achieve human-level performance, offering a potential solution to the scalability limitations of RLHF.	Reinforcement Learning from Human Feedback (RLHF) is an effective technique for aligning language models to human preferences(Stiennonet al.,2020; Ouyanget al.,2022). It is cited as one of the key drivers of success in modern conversational language models, such as ChatGPT(Liuet al.,2023)and Bard(Manyika,2023). Training language models with reinforcement learning (RL) enables optimization on complex, sequence-level objectives that are not easily differentiable and therefore ill-suited for traditional supervised fine-tuning (SFT). One obstacle for employing RLHF at scale is its dependence on high-quality human preference labels. This raises the question of whether artificially generated labels can be a viable substitute. Generating labels with large language models (LLMs) is one promising approach, as LLMs have shown a high degree of alignment with human judgment(Gilardiet al.,2023; Dinget al.,2023).Baiet al.(2022b)was the first effort to explore Reinforcement Learning from AI Feedback (RLAIF)111This is distinct from “Constitutional AI”, which improves upon a supervised learning model through iteratively asking an LLM to generate better responses according to a a set of written value statements. Both were introduced inBaiet al.(2022b)and are sometimes conflated., where RL was conducted using a reward model trained on LLM preferences.Baiet al.(2022b)showed that utilizing a hybrid of human and AI preferences, in conjunction with their “Constitutional AI” self-revision technique, outperforms supervised fine-tuning for training a conversational assistant. However, it did not directly compare the efficacy of human vs. AI feedback, leaving the question of whether RLAIF can be a suitable alternative to RLHF unanswered. In this work, we study the impact of RLAIF and RLHF (see Figure2) on three text generation tasks: summarization, helpful dialogue generation, and harmless dialogue generation. Our experiments show that RLAIF and RLHF are preferred by humans over the SFT baseline 71% and 73% of the time for summarization and 63% and 64% of the time for helpful dialogue generation, respectively, where the differences between RLAIF and RLHF win rates are not statistically significant. We also conduct a head-to-head comparison of RLAIF against RLHF and find that both policies are equally preferred222The win rate for one policy vs. the other is not statistically significantly different from 50%. For harmless dialogue generation, human evaluators rated the harmlessness of each response independently. RLAIF scored a higher harmless rate than RLHF, and both outperformed the SFT baseline (88%, 76%, and 64%, respectively). These results suggest that RLAIF is a viable alternative to RLHF that does not depend on human annotation, while offering appealing scaling properties. Additionally, we investigate two related questions. First, we explore whether RLAIF can improve upon a SFT policy when the LLM labeler has the same number of parameters as policy. Even in this scenario, RLAIF significantly improves over the SFT baseline. Second, we conduct an experiment where the off-the-shelf LLM is directly prompted for reward scores during RL, bypassing the step of distilling LLM preference labels into a reward model. This method achieves an even higher win rate over SFT than the canonical distillation method. Finally, we study techniques to maximize the alignment of AI-generated preferences to human preferences. We find that soliciting chain-of-thought reasoning(Weiet al.,2022)consistently improves alignment, while using a detailed preamble and few-shot prompting(Brownet al.,2020)are only beneficial for certain tasks. We also conduct scaling experiments to examine the trade-off between the size of the LLM labeler and alignment with human preferences. The main contributions of this work are as follows: [noitemsep] We demonstrate that RLAIF achieves comparable or superior performance to RLHF on the tasks of summarization, helpful dialogue generation, and harmless dialogue generation. We show that RLAIF can improve upon a SFT policy even when the LLM labeler is the same size as the policy. We find that directly prompting the LLM for reward scores during RL can outperform the canonical setup where a reward model is trained on LLM preferences. We compare various techniques for generating AI labels and identify optimal settings for RLAIF practitioners.	"SMART: No
Evidence_specific: Yes; the evidence text in the given text for criterion ""specific"" is ""across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation.""
Evidence_measurable: Yes; the evidence text in the given text for criterion ""measurable"" is ""RLAIF achieves comparable or superior performance to RLHF, as rated by human evaluators.""
Evidence_achievable: No; there is no explicit evidence suggesting that the goals are achievable.
Evidence_relevant: No; there is no evidence discussing the relevance or importance of the objectives.
Evidence_time-bound: No; there is no indication of a time frame or deadline."	"SMART: No
Evidence_specific: Yes; Specific information is mentioned regarding the focus of the study, tasks like summarization, helpful dialogue generation, and harmless dialogue generation.
Evidence_measurable: Yes; The use of percentages and comparisons provides measurable outcomes, such as ""RLAIF and RLHF are preferred by humans over the SFT baseline 71% and 73% of the time for summarization.""
Evidence_achievable: No; The text does not mention the 'achievable' aspect clearly in terms of writing style.
Evidence_relevant: No; The relevance aspect isn't clear without context because the text is focused on the content of the study rather than goal setting.
Evidence_time-bound: No; There are no time constraints or deadlines mentioned in the provided text."
2304.12244v2	WizardLM: Empowering Large Language Models to Follow Complex Instructions	Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM	Large-scale language models (LLMs) have become the go-to approach for numerous natural language processing (NLP) tasks(Brownet al.,2020; Ouyanget al.,2022; OpenAI,2023; Touvronet al.,2023). LLMs are trained on large volumes of text data to predict the subsequent tokens, enabling them to generate coherent and fluent text in response to various inputs. However, these models often struggle to follow instructions or goals specified by users, which limits their usefulness and applicability in real-world scenarios. The NLP community has recently witnessed many endeavors to train LLMs to follow instructions better and be more helpful(Zhaoet al.,2023; Heet al.,2023; Guoet al.,2023; Liet al.,2023). Initial attempts(Aribandiet al.,2022; Weiet al.,2021; Xuet al.,2022; Sanhet al.,2022; Chunget al.,2022)to train instruction-following language models are based on a collection of various NLP tasks, with a small amount of hand-written instructions accompanying each task. These closed-domain instructions suffer from two main drawbacks: first, all the samples in an NLP dataset share only a few common instructions, severely limiting their diversity; second, the instructions usually only ask for one task, such as translation or summarization. But in real life, human instructions often have multiple and varied task demands. By using open-domain instruction data generated by real human users, OpenAI’s LLMs (e.g., InstructGPT(Ouyanget al.,2022)and ChatGPT111https://chat.openai.com/) have achieved great success. These open-domain instructions can fully unleash the unlimited potential of LLMs(Luoet al.,2023; Maet al.,2023; Huet al.,2023; Zhuet al.,2023)and enable them to perform more complex and diverse tasks. However, using humans to create open-domain instruction datasets like OpenAI did will encounter the following challenges. The whole annotating process is extremely expensive and time-consuming(Kopfet al.,2023; Chenet al.,2023; Sunet al.,2023; Yuanet al.,2023). On the other hand, the difficulty level distribution of human-created instructions is skewed towards being easy or moderate, with fewer difficult ones (according to the difficulty statistics of ShareGPT(Chianget al.,2023)from Figure6(a)). Possible reasons for this are that the proportion of experts among annotators is low and creating complex instructions demands a lot of mental effort. Human annotators are prone to fatigue and cannot sustain high-intensity work to produce a sufficient proportion of high-difficulty instructions(Zhanget al.,2023; Xiaoet al.,2023; Manakulet al.,2023; Zhonget al.,2023). Based on these issues, developing an automatic method that can mass-produce open-domain instructions (especially the more difficult ones) at a relatively low cost becomes the key to further advancing instruction-tuned language models(Baoet al.,2023; Liuet al.,2023; Bianet al.,2023; Cabanneset al.,2023). In this work, we introduceEvol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs. Figure1shows the running examples ofEvol-Instruct. Starting from a simple initial instruction “1+1=?”, our method randomly selects In-depth Evolving (blue direction line) or In-breadth Evolving (red direction line) to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes five types of operations: add constraints, deepening, concretizing, increase reasoning steps, and complicate input. The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction. These six operations are implemented by prompting an LLM with specific prompts. Since the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction eliminator to filter the failed instructions, which is called Elimination Evolving. We repeat this evolutionary process for several rounds to obtain enough instruction data containing various complexities. We validate ourEvol-Instructby fine-tuning open-source LLaMA(Touvronet al.,2023)with our evolved instructions and evaluating its performance similar to existing SOTA works (e.g., Alpaca(Taoriet al.,2023)and Vicuna(Chianget al.,2023)) on instruction finetuning. The instruction datasets we compare with are the data used by Alpaca (generated using self-instruct(Wanget al.,2022a)) and the 70k ShareGPT (shared by real users) used by Vicuna. To prove that the instruction dataset from our method is superior to human-created instruction datasets, we select Alpaca’s training data (generated from only 175 human-created seed instructions) as the initial dataset. We execute four epochs of evolution using OpenAI ChatGPT API222gpt-3.5-turbo fromhttps://oai.azure.com/portaland finally obtain 250k instructions. To ensure a fair comparison with Vicuna’s 70k real user data, we sampled an equal amount from the full 250k data and trained the LLaMA 7B model. We name our modelWizardLM. Due to the low proportion of difficult instructions in the previous instruction-following test dataset, we manually created a new difficulty-balanced test dataset, namedEvol-Instructtestset. We hire annotators and leverage GPT-4 to evaluate Alpaca, Vicuna, ChatGPT, andWizardLMonEvol-Instructtestset and Vicuna’s testset. Our main findings are as follows: Instructions fromEvol-Instructare superior to the ones from human-created ShareGPT.When we use the same amount ofEvol-Instructdata (i.e., 70k) as Vicuna to fine-tune LLaMA 7B, our modelWizardLMsignificantly outperforms Vicuna, with the win rate of12.4\%and3.8\%higher than Vicuna onEvol-Instructtestset and Vicuna’s testset respectively on human evaluation. In addition,WizardLMalso achieves better response quality than Alpaca and Vicuna on the automatic evaluation of GPT-4. Labelers preferWizardLMoutputs over outputs from ChatGPT under complex test instructions.OnEvol-Instructtestset,WizardLMperforms worse than ChatGPT, with a win rate12.8\%lower than ChatGPT (28.0\%vs.40.8\%). However, in the high-difficulty section ofEvol-Instructtest set (difficulty level\geq 8), ourWizardLMeven outperforms ChatGPT, with a win rate7.9\%larger than ChatGPT (42.9\%vs.35.0\%), that is human annotators even prefer the output of our model than ChatGPT on those hard questions. This indicates thatEvol-Instructcan significantly improve the ability of LLMs to handle complex instructions.	"SMART: No
Evidence_specific: Yes; the specifics about using ""Evol-Instruct to rewrite them step by step into more complex instructions"" and fine-tuning ""LLaMA.""
Evidence_measurable: Yes; humans evaluate on ""a complexity-balanced test bed and Vicuna's testset"" and WizardLM's performance is quantified, ""90% capacity of ChatGPT on 17 out of 29 skills.""
Evidence_achievable: No; the achievability of the results is implied, but there's no explicit mention of the resources or capabilities to achieve it.
Evidence_relevant: No; since the ""goal"" aspect of SMART isn’t considered, relevance isn’t directly addressed.
Evidence_time-bound: No; there is no mention of a timeline or deadline for the objectives."	"SMART: No
Evidence_specific: Yes; the text specifically outlines the aim of using open-domain instructions to improve LLMs and discusses a novel method called Evol-Instruct.
Evidence_measurable: No; the text does not specify measurable criteria within the writing itself (focuses on the research rather than setting measurable goals).
Evidence_achievable: No; the text discusses research outcomes rather than setting out achievable targets.
Evidence_relevant: No; the text does not explicitly express relevance in terms of a goal-setting framework.
Evidence_time-bound: No; the text does not set any time-specific targets."
