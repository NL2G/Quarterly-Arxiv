id	title	abstract	introduction
2310.10294v1	Key-phrase boosted unsupervised summary generation for FinTech organization	"With the recent advances in social media, the use of NLP techniques in social media data analysis has become an emerging research direction. Business organizations can particularly benefit from such an analysis of social media discourse, providing an external perspective on consumer behavior. Some of the NLP applications such as intent detection, sentiment classification, text summarization can help FinTech organizations to utilize the social media language data to find useful external insights and can be further utilized for downstream NLP tasks. Particularly, a summary which highlights the intents and sentiments of the users can be very useful for these organizations to get an external perspective. This external perspective can help organizations to better manage their products, offers, promotional campaigns, etc. However, certain challenges, such as a lack of labeled domain-specific datasets impede further exploration of these tasks in the FinTech domain. To overcome these challenges, we design an unsupervised phrase-based summary generation from social media data, using 'Action-Object' pairs (intent phrases). We evaluated the proposed method with other key-phrase based summary generation methods in the direction of contextual information of various Reddit discussion threads, available in the different summaries. We introduce certain ""Context Metrics"" such as the number of Unique words, Action-Object pairs, and Noun chunks to evaluate the contextual information retrieved from the source text in these phrase-based summaries. We demonstrate that our methods significantly outperform the baseline on these metrics, thus providing a qualitative and quantitative measure of their efficacy. Proposed framework has been leveraged as a web utility portal hosted within Amex."	Post: Bank Rewards Checking Account. Comments: I hope they add the ability to make it ajoint account with a spouse.<sep>Nice to see they’re attempting tocompetewith neweronline banks.<sep>When they firstopenedup theirHYSAthey eventually had asignup bonus. I wonder if they’ll do something like that at some point<sep>The main difference between this and the HYSA is that you’ll be able to makes charges andget rewardsfor purchases<sep>If you link your new Checking account to your credit card there’s no way you wouldget MR pointson that transaction, right?<sep>Can youuse Zellethrough the account? signup bonus;compete online banks;opened HYSA;get MR points;use Zelle With the advent of social media, the analysis of data available on these platforms has the potential to provide a gold mine of actionable insights to business organizations. With the ubiquity of social media, the amount of available data has skyrocketed, and social media reflects popular discourse and discussion in a larger societal context. Thus, building a social media knowledge ecosystem in addition to existing data analysis platforms can help an organization to better understand its consumers’ preferences and reactions to the services they offer. These insights help businesses form an understanding of the “external perspective”, directly from the source - the consumers. This additional analysis can have long-term positive impacts and reveal insights that may not be surfaced through traditional channels like customer service call transcripts. Summarizing the discussions available on these social media platforms can give a precise external perspective to an organization about their products, offers, competitors etc, while helping them to make decision for their future products and offers. Hence, in this study, we focus our attention on the problem of phrase based unsupervised summary generation from the Reddit discussions data, with the goal of developing a broader understanding of social media discussions related to financial organization’s products and services. In the last decade, NLP techniques have been successfully applied to social media data in the FinTech context to improve business operations and understand customer opinions. They have been applied to problems like sentiment analysis, stock value prediction, document summarization, intent detection, and so on. A facet that has received particular attention is sentiment analysis from social media data and its utility in predicting trends in the stock market and share prices(Day and Lee,2016; Aji et al.,2019; Karalevicius et al.,2018; Esichaikul and Phumdontree,2018; Sangsavate et al.,2019). Lack of publicly available data and the cost of generating gold standard labels has been a challenge for multiple NLP tasks such as summarization, intent detection, which has lead to the requirement of more unsupervised or semi supervised approaches. Some of the work in this direction are,(Abdaljalil and Bouamor,2021; La Quatra and Cagliero,2020)for summarizing financial reports and FinTech data,(Weld et al.,2022)for task of identifying the user’s intention. Despite availability of some of the labelled datasets for summarization, intent detection, its difficult to solve domain specific problems with them. Most studies on intent detection focus on the spoken language understanding (SLU) datasets - ATIS(Hemphill et al.,1990), and SNIPS(Coucke et al.,2018), which were originally designed for mobile voice assistants, and hence do not generalize well. Secondly, the current formulation of intent detection frames it as a multi-class classification problem. Though pretrained models perform well(Hardalov et al.,2020), the intent class labels designed for ATIS and SNIPS do not apply to other domains and are too restrictive for real-world data. Finally, this supervised definition of intent detection means crowd-sourcing is the only viable option for creating domain-specific social media datasets. Crowd-sourcing comes with its own set of problems, as it is often expensive and presents problems with annotator bias(Eickhoff,2018). Unsupervised methods in intent detection have been explored in the past(Popov et al.,2019; Dopierre et al.,2021; Siddique et al.,2021), but they do not pertain to FinTech data. Traditionally employed in SLU, intent detection has great potential in this domain. Not only can it summarize and simplify lengthy social media discussions, but these salient features can also form representative features for various downstream NLP tasks. In this regard, we propose a completely unsupervised method for phrase-based summary generation, specifically for FinTech social media data. We build a database of posts and comments (threads) from the social news platform Reddit and extract key intent phrases to summarize lengthy threads. None of our data is annotated with summary text or keywords, and we propose a flexible unsupervised approach. In this work the summary of the Reddit discussions is represented by intent keywords in the data. For intent phrase extraction, we follow a two-stage pipeline by first identifying different categories of ’Action-Object’ pairs (tokens matching pre-defined rules), followed by a scoring function to rank them. Additionally for comparison purpose, we perform aspect-based sentiment analysis and identify the key phrases that are associated with the most positive or negative comments. Figure1, demonstrates an overview of our pipeline by identifying the most salient intent phrases from a Reddit thread. These intent phrases are further used as a as vector representations to cluster the post corpus and generate cluster-level summaries to gather insights from large corpus of customer feedback on Reddit. We compared our method by generating summary using an existing keyphrase extraction method, ‘Yake’(Campos et al.,2018)and aspect-based sentiment phrases. For evaluation, we additionally introduce ‘context metrics’ to evaluate the contextual information in the respective cluster summaries. The main contributions of this paper are as follows. (1) We propose a new unsupervised method for phrase-based summary generation using intent key phrases. (2) We study publicly available social media data (pertaining to FinTech), (without crowdsourcing annotation). Finally, (3) we suggest new metrics to evaluate the contextual information retrieved in the clusters and find that our method outperforms existing keyphrase based summary generation methods. Thus, our goal in this paper is to demonstrate that intent based summary can generate meaningful insights in domain-specific financial text without supervision or labeled datasets. In Section2, data collection and description of the end-to-end summary generation pipeline is described. Section3experimental setup, qualitative and qualitative results are demonstrated. Section4conclude the paper and briefly discuss the limitations of the study and future directions.
2312.10490v1	Spatial Deep Learning for Site-Specific Movement Optimization of Aerial Base Stations	Unmanned aerial vehicles (UAVs) can be utilized as aerial base stations (ABSs) to provide wireless connectivity for ground users (GUs) in various emergency scenarios. However, it is a NP-hard problem with exponential complexity in $M$ and $N$, in order to maximize the coverage rate of $M$ GUs by jointly placing $N$ ABSs with limited coverage range. The problem is further complicated when the coverage range becomes irregular due to site-specific blockages (e.g., buildings) on the air-ground channel, and/or when the GUs are moving. To address the above challenges, we study a multi-ABS movement optimization problem to maximize the average coverage rate of mobile GUs in a site-specific environment. The Spatial Deep Learning with Multi-dimensional Archive of Phenotypic Elites (SDL-ME) algorithm is proposed to tackle this challenging problem by 1) partitioning the complicated ABS movement problem into ABS placement sub-problems each spanning finite time horizon; 2) using an encoder-decoder deep neural network (DNN) as the emulator to capture the spatial correlation of ABSs/GUs and thereby reducing the cost of interaction with the actual environment; 3) employing the emulator to speed up a quality-diversity search for the optimal placement solution; and 4) proposing a planning-exploration-serving scheme for multi-ABS movement coordination. Numerical results demonstrate that the proposed approach significantly outperforms the benchmark Deep Reinforcement Learning (DRL)-based method and other two baselines in terms of average coverage rate, training time and/or sample efficiency. Moreover, with one-time training, our proposed method can be applied in scenarios where the number of ABSs/GUs dynamically changes on site and/or with different/varying GU speeds, which is thus more robust and flexible compared with conventional DRL-based methods.	With their high mobility and reducing cost, unmanned aerial vehicles (UAVs) have attracted increasing interests in military and civilian domains in recent years. In particular, integrating UAVs into wireless communication networks as aerial base stations (ABSs) to assist terrestrial communication infrastructure in various emergency scenarios such as battlefields, disaster scenes and hotspot events, has been regarded as an important and promising technology[36]. One of the key problems in UAV-aided communication systems is to find applicable placement ofNABSs with limited coverage range in order to achieve maximum coverage ofM(static) ground users (GUs)[21,13,24], which is known to be aNP-hard problemwith exponential complexity inMandN[21]. A tutorial in[31]discusses the ABS placement problem and the most commonly used schemes in scenarios with free space (FS) or non-FS propagation in recent literature. In particular, some algorithms including the spiral algorithm[21], K-means algorithm[13], circle packing theory[24], and user-majority based adaptive UAV deployment[34], are proposed to solve the type of problems with dominant line-of-sight (LoS) or probabilistic LoS/non-LoS (NLoS) channel model[7], under which each ABS has auniformcoverage range. However, due tosite-specific blockages(e.g., buildings), the above channel models might fail to capture the fine-grained structure of LoS or NLoS propagation at specific ABS and GU locations[8,28]. For example, with a slight change of its position, an ABS might transit from LoS to NLoS propagation to the GU due to building edges. This critically affects the ABS-GU channel and further complicates the problem. Some efforts have been made using deep learning (DL) to learnsite-specific channelinformation[15,11], and/or using radio map[8]to construct/utilize spatial channel distribution[20,38,37,29]. In[15], the authors propose to use an end-to-end neural network to learn a site-specific probing codebook in order to predict the optimal narrow beam for beam alignment. The authors in[11]propose a DL-based method for the optimal scheduling of interfering links in a dense wireless network with full frequency reuse. The proposed methods in[15,11]are able to reduce the time/computational overhead of channel estimation and/or schedule links efficiently based on geographic locations of the devices, which yet are not directly applicable to the ABS placement/movement problems. In the context of UAV communications, radio map has been utilized to represent site-specific spatial distribution of average received power radiated from given transmitting source(s), e.g., the fixed ground base stations for cellular-connected UAV[20,38,37], or the ABSs[29]to provide ground coverage. For ABS placement, the authors in[29]leverage on a given spatial loss field (SLF) function to construct the radio map with low complexity, whereas how to obtain/store site-specific SLF for any given ABS-GU location pair with high sample efficiency is yet to be addressed. Other authors in[9]partially circumvent this challenge by developing adaptive UAV positioning strategy with on-site LoS condition measurements for a given pair of UAV-relay and GU, whereas multi-ABS/multi-GU scenarios are yet to be considered. The ABS placement problem can be further complicated due toGU mobility, which brings additional complexity and the practical requirement of finding desired solutions within limited time. In this regard, machine learning methods including DL and reinforcement learning (RL)/deep RL (DRL) have been developed/applied to solve complicated problems of UAV joint optimization considering multiple factors such as UAV trajectory design, user association, resource allocation and power consumption[18,10,32](see the recent survey[30]for more references). In particular, RL/DRL methods have recently been applied to tackle the multi-UAV movement optimization problems[16,17,19,33,39,22]. In terms of ABS coverage and energy consumption trade-off, a DRL-based approach is proposed in[16]to achieve energy-efficient and fair communication coverage. A decentralized DRL-based framework is further proposed in[17]to provide energy-efficient long-term communication coverage. The authors in[19]propose a genetic algorithm-based K-means algorithm to partition GUs into clusters, and further apply the Q-learning algorithm in each cluster for ABS movement. The authors in[33]and[39]both formulate the ABS movement problem as a constrained markov decision process, and propose the dueling Deep Q-network (DQN) and/or constrained DQN algorithms to maximize the downlink capacity with full coverage constraint. However, the above works typically assume uniform coverage range in a generic environment, whereas thesite-specific LoS/NLoS propagationscenario is yet to be considered. Forsite-specific multi-ABS movementoptimization, the authors in[22]propose a single-agent Deep Deterministic Policy Gradient (DDPG) based approach to maximize the average sum-rate of all GUs via UAV dynamic movement and communication co-design, whereas the tested network is relatively small (e.g., withM=10andN=2). On the other hand, our early work in[28]proposes a Double DQN with Prioritized Experience Replay (PER-DDQN) to address the site-specific ABS placement problem with a moderate network size (e.g.,M=80andN=10). However, a straightforward extension to ABS movement optimization encounters further difficulties. In particular, the action space of RL/DRL methods growscombinatoriallywithNand the number of steps to explore. Moreover, a new neural network (NN) model in DRL would often need to be re-trained in order to cater fornetwork changes(e.g., ABSs/GUs turning on/off, varying GU speeds, etc.), which calls for more timely adaptation and more flexible design. To circumvent the above difficulties, we shift our mindset and attempt a different approach other than DRL. First, the ABS movement problem is partitioned into a time series of ABS placement sub-problems, each of which aims for maximum GU coverage under given GU locations, and thus amounts to apattern matching/search problem. Second, a state-of-the-art search algorithm called Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)[23]is adopted with tailored modifications to solve each ABS placement sub-problem. Third, an environment emulator is built to predict the site-specific coverage status of all GUs in the actual environment and assist fast evaluation of ABS placement solutions. Our main contributions are summarized as follows: Spatial Deep Learning (SDL)111We reuse the term SDL as in[11]to refer to the general method of learning spatial characteristics by DL, although different problem setup and DL architecture are considered here.for Coverage Prediction: An encoder-decoder type of deep-NN (DNN) with careful incorporation of domain knowledge is proposed. First, we use threegrid-mapsto quantize and represent the location patterns of ABSs, GUs, and covered GUs (CGUs), respectively, with the first two as input and the last one as output of the DNN. These grid-maps endow the DNN with 1) input/outputdimension invariancewith the number of ABSs/GUs; and 2) input/outputpermutation invariancewith ABS/GU indexes. These invariance properties significantly reduce the learning burden of DNN and also render more flexibility and robustness to scenarios where the number of ABSs/GUs dynamically changes on site. Second, tailored design techniques are proposed includingbinary mask processingandelement-wise binary classification, which effectively boost the training efficiency. MAP-Elites as Quality-Diversity Search Engine: Based on the trained DNN emulator, we further propose theSDL-based MAP-Elites (SDL-ME)algorithm. First, based on MAP-Elites, the search over the original variable space (ofO(N)dimensions) is effectively reduced to that over a low-dimensional (e.g., two in our proposed design) feature space, which flexibly tradeoffs betweencomplexity reductionandsolution diversity, thus encouraging more efficient search for better-quality solutions. Second, the SDL-based emulator captures the site-specific ABS-GU coverage states and enablesvirtual explorationof a much larger part of the search space compared with direct on-site trials and errors, thus leading to potentially better solutions. Top-kMechanism and Planning-Exploration-Serving (PES) Scheme: The SDL-ME based planning helps sift outktop performing candidate solutions (in terms of emulator-predicted coverage rate), which are then explored and validated on site to further elect the best performing solution (in terms of actual coverage rate) for ABS placement to serve GUs in the current time period. Such a top-kmechanism and PES scheme seamlessly amalgamate theemulator-based planningandon-site exploration/serving, thus significantly reducing the cost of extensive on-site search. Moreover, the measurements for the top-kcandidate solutions effectively compensate for the quantization/prediction errors due to model approximation. Numerical results demonstrate that the proposed approach significantly outperforms the benchmark DRL-based method and other two baselines in terms of average coverage rate, training time and/or sample efficiency. Moreover, with one-time training, our proposed method can be applied indynamic scenarioswhere the number of ABSs/GUs changes on site and/or with different/varying GU speeds, which is more robust and flexible compared with conventional DRL methods.
2401.09356v1	Swing: Short-cutting Rings for Higher Bandwidth Allreduce	The allreduce collective operation accounts for a significant fraction of the runtime of workloads running on distributed systems. One factor determining its performance is the distance between communicating nodes, especially on networks like torus, where a higher distance implies multiple messages being forwarded on the same link, thus reducing the allreduce bandwidth. Torus networks are widely used on systems optimized for machine learning workloads (e.g., Google TPUs and Amazon Trainium devices), as well as on some of the Top500 supercomputers. To improve allreduce performance on torus networks we introduce Swing, a new algorithm that keeps a low distance between communicating nodes by swinging between torus directions. Our analysis and experimental evaluation show that Swing outperforms by up to 3x existing allreduce algorithms for vectors ranging from 32B to 128MiB, on different types of torus and torus-like topologies, regardless of their shape and size.	Allreduce is a collective operation used to aggregate vectors among a set of nodes and to distribute back to them the aggregated result. Among others, allreduce is widely used to perform distributed gradient aggregation during the training of deep learning models[10]. Several studies have shown that it can account for up to 40% of the total training time[44,28,39]and between 19% and 30% of the total core hours in MPI jobs running on production supercomputers[14]. Researchers proposed several allreduce algorithms[43,6,25], and the most performing one depends on a combination of vector size, number of nodes, and physical topology[42,23]. Those algorithms perform a predefined number of steps and, at each step, each node sends and receives data to and from some predetermined nodes. Different trade-offs exist between the number of steps to perform (more critical for allreduce on small vectors) and the total number of bytes it transmits (more relevant for larger allreduce). However, a third factor that must be considered when designing a new collective algorithm is the physical distance between communicating nodes[43,33,38,42]. This is particularly relevant on networks that do not provide full bisection bandwidth such as torus, since the higher the distance, the higher the number of flows sharing the same links. Torus networks are widely used, both on systems optimized for running machine learning (ML) workloads (e.g., Google deploys TPUs v4 on a 3D torus-like network[27], and Amazon deploys Trainium devices on a 2D torus[7]), and on some of the top supercomputers[2,11](e.g., Fugaku uses a 6D torus[4]). We show the importance of the distance between communicating nodes in the allreduce through an example. In Fig.1, we show a16node 1D torus (we only show a subset of the nodes since the communications are symmetric). We assume minimal (i.e., shortest path) routing and we show the communications performed by the bandwidth-optimal recursive doubling algorithm[43](also known asRabenseifneralgorithm[35], which we describe more in detail in Sec.2.3.3), and by theSwingalgorithm (that we propose in this work). Both algorithms perform the same number of steps (we show only the first three for simplicity). We denote withnthe number of bytes of the allreduce vector. In the first step, in both algorithms, each nodersendsn/2bytes to nodeq=r\text{ XOR }1(and receivesn/2bytes from it). In the second step, however, in the recursive doubling, each nodersendsn/4bytes to nodeq=r\text{ XOR }2(two hops distant), whereas in the Swing algorithm, each node still sendsn/4bytes of data, but with the other neighbor (one hop distant). Although both algorithms transmit the same number of bytes, two different messages cross the same link in the recursive doubling. For example, two messages cross the link between nodes1and2and that between nodes5and6. As a consequence, in the worst case all nodestransmit data at most at half the bandwidthof the link between1and2, thus slowing down the entire allreduce operation. Instead, in the Swing algorithm, each node can still transmit at full bandwidth because, in this example, in the second step each link is crossed at most by one message per direction. Something similar also happens in the third step. Indeed, when using Swing at most two messages cross each link instead of the four messages crossing the link between nodes3and4in recursive doubling. It is thus clear how even if two different algorithms transmit the same number of bytes and perform the same number of steps, they might be practically characterized by different performance, depending on the network characteristics and the distance between communicating nodes. In this example, we have shown an extreme case using a 1D torus. However, similar effects can happenon any topology that does not provide full bisection bandwidth. Although some algorithms (i.e., ring[33,43]and bucket[9,38,25]) avoid this problem by having each node communicate with its neighbors only, they perform more steps (linear in the number of nodes) and are thus not well-suited for small- and medium-sized vectors. Those are the sizes that, however, are practically used in most machine learning[30]and HPC[14]workloads. Indeed, larger allreduce are split into smaller ones to overlap better computation and communication, especially more when using 3D parallelism in machine learning training[10]. For these reasons, this work makes the following contributions: [leftmargin=*] We design a new allreduce algorithm calledSwing, which performs a logarithmic number of steps and transmits the minimal number of bytes while reducing the distance between communicating nodes compared to other known algorithms designed for small- and medium-sized vectors (Sec.3and Sec.4). We evaluate Swing on different torus and torus-like topologies (e.g., HammingMesh[22]and HyperX[3,18]), by comparing it with the best state-of-the-art algorithms (Sec.5). Our evaluation shows thatSwing outperforms the other existing algorithms for allreduce on vectors ranging from 32B to 128MiB on different torus-like topologies, and regardless of their shape and size. We show that Swing outperforms the best-known algorithm up to 2.2x on square torus with4096to16384nodes and up to 3x on rectangular tori and HyperX with4096nodes.
2304.04336v3	Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search	Achieving tight bounding boxes of a shape while guaranteeing complete boundness is an essential task for efficient geometric operations and unsupervised semantic part detection. But previous methods fail to achieve both full coverage and tightness. Neural-network-based methods are not suitable for these goals due to the non-differentiability of the objective, while classic iterative search methods suffer from their sensitivity to the initialization. We propose a novel framework for finding a set of tight bounding boxes of a 3D shape via over-segmentation and iterative merging and refinement. Our result shows that utilizing effective search methods with appropriate objectives is the key to producing bounding boxes with both properties. We employ an existing pre-segmentation to split the shape and obtain over-segmentation. Then, we apply hierarchical merging with our novel tightness-aware merging and stopping criteria. To overcome the sensitivity to the initialization, we also define actions to refine the bounding box parameters in an Markov Decision Process (MDP) setup with a soft reward function promoting a wider exploration. Lastly, we further improve the refinement step with Monte Carlo Tree Search (MCTS) based multi-action space exploration. By thoughtful evaluation on diverse 3D shapes, we demonstrate full coverage, tightness, and an adequate number of bounding boxes of our method without requiring any training data or supervision. It thus can be applied to various downstream tasks in computer vision and graphics.	Approximating complex 3D shapes using primitives offers several capabilities, including shape structure analysis, shape abstraction, and efficient geometric computations. To achieve these, many recent self-supervised learning approaches[tulsiani17abstract,paschalidou19sq,sun19abstract,yang21cubseg,paschalidou21np,chen20bspnet,niu22rimnet]have successfully addressed the problem while exploring different types of primitives. Despite the recent advances, in this work, we pay attention to some desired yet underinvestigated properties of the bounding primitives of a shape and propose a novel approach aiming to achieve them. The properties are 1)full coverage— guaranteeing the boundness of the entire shape by the primitives, 2)tightness, and 3)adequate number of primitives. Attaining these three properties is particularly crucial for the downstream applications requiringefficient geometric computations, such as intersection tests[collision_survey,orientedbbox], robust transmissions[robust_transmission], ray tracing[ellipsoids_intersection], or proximity computations[oriented]. The primitives that just approximate but do not fully cover the shape can result in imprecise computation in such tasks. Also, loosely bounding primitives and too many primitives increase the computation time. Moreover, the bounding primitives satisfying these properties typically provide a better abstraction of the shape aligned with thehuman perceptionof the shape decomposition. Although neural-network-based methods have demonstrated their powerful generalizability and expressivity, they typically fail to achieve these three properties mainly due to thenon-differentiablenature of the objectives. The full coverage and tightness can be computed with the volumetric intersection or difference operations, which are not differentiable. Finding the proper number of parts is also a discrete problem that cannot be easily solved via backpropagation. As tight bounding primitives have been essential in various applications, there also has been a line of work[robust_transmission,simari2005extraction,kalaiah2005statistical]addressing the problem before the deep learning era. One notable example is the work by Lu et al.[2007bvc]that proposes to find the tight bounding boxes by iteratively exchanging point-to-primitive assignments, starting from an initialization. The major drawback of such a method is its dependency and sensitivity to theinitialization, resulting in a suboptimal output when it starts from a poor initialization. To address the problem of achieving the three properties that introduce challenges ofnon-differentiabilityfor neural networks andinitialization sensitivityfor iterative search methods, we propose a framework that performsSplitting,Merging,AndRefinementTechniques, and is thus dubbedSMART. We first find that a simple post-processing applied to a pre-segmentation, can provide appropriateover-segmentationof a 3D shape. Hence, we performhierarchical mergingto find the adequate number of parts. We introducetightness-awaremerging and stopping criteria that enable selection of the optimal number of parts the best number of parts given the trade-off between tightness and parsimony in the decomposition. While merging over the over-segments already produces promising results, the results are yet dependent on the quality of the over-segmentation (third column of Fig.1-a). To overcome the dependency to the initialization, we present the nextrefinementstep that adjusts the bounding box parameters following a sequence of predefined actions. We design a Markov Decision Process (MDP) setup with asoftreward function that allows the bounding boxes to break the full coverage in themiddleof the process. This is the key to having more flexibility in traversing various cases and obtaining better results at the end (fourth column of Fig.1-a). Lastly, we extend the refinement step to see not only a single step of actions but multiple steps in a small sequence. We utilizeMCTS[mcts]and introduce acceleration techniques (in the supplementary) to speed up the additional refinement (last column of Fig.1-a). In our experiments with ShapeNet[shapenet2015], we demonstrate that our method guaranteeing full coverage provides better tightness and reconstruction compared with the baseline methods while approximating the shapes into similar numbers of cuboids. We additionally show that the decomposition of a shape based on our bounding boxes is better aligned with the semantic parts than other methods. Furthermore, we provide the result ofSMARTon Objaverse[objaverse]and OmniObject3D[wu2023omniobject3d]to show its applicability to real data and various categories of 3D shapes. To summarize, We present a novel framework for finding a set of tight bounding boxes of a 3D shape by optimizing volumetric objectives with iterative search methods. We first propose a hierarchical merging method exploiting over-segmentation from a pre-segmentation and tightness-aware merging and stopping criteria. We also introduce a bounding box refinement process with an effective soft reward function that allows wider exploration in the action space. Finally, we present an MCTS-based efficient exploration of multi-action sequences with acceleration techniques.
2305.12799v1	Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration	Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. In parallel, the problem of data scarcity has brought a growing interest in employing AIGC technology for high-quality data expansion. However, this paradigm requires well-designed prompt engineering that cost-less data expansion and labeling remain under-explored. Inspired by LLM's powerful capability in task guidance, we propose a new paradigm of annotated data expansion named as ChatGenImage. The core idea behind it is to leverage the complementary strengths of diverse models to establish a highly effective and user-friendly pipeline for interactive data augmentation. In this work, we extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and make the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks. Finally, we present fascinating results obtained from our ChatGenImage framework and demonstrate the powerful potential of our synthetic data for systematic vision adaptation. Our codes are available at https://github.com/Yuqifan1117/Labal-Anything-Pipeline.	"In the past decade, deep learning techniques have demonstrated promising performance across diverse tasks, owing to the availability of large-scale annotated data[12,19,10]. However, it is time-consuming and expensive to manually collect a large-scale annotated dataset containing every possible domain for robust training. Besides, the problem of cross-domain and long-tail distributions within existing datasets have a detrimental effect on the performance and robustness of vision models, thereby impeding their generalization ability to novel categories or unseen domains. This promotes us to explore a less labor-intensive way to harvest labeled data containing multiple domains in one step for robust vision tasks. One effective strategy to improve generalization and robustness is to enlarge the scale of training data by intricate augmentations[14]. There are several GAN-based models[7,17]generating images for vision tasks, but their applicability remains constrained by their narrow focus on specific settings or small scales. Recently, AIGC models[27,28,29]have emerged as promising candidates for generating high-quality synthetic data, with the ability to address the limitations of the existing dataset. There are several early attempts at exploring synthetic data from generative models for data augmentation[13,23,3,37]. Albeit promising, early works usually produce simple scenarios or object-centric images only by global constraints (i.e., “airplane"" or “a white airplane hovering over a beach and a city"".), which limits downstream models’ perception of intricate scenes and fine-grained attributes. Additionally, these methods concentrate on generating images under typical scenarios (e.g., daylight, field), while neglecting less common but predictable circumstances (e.g., snow, forest, night). This limitation may impede the ability of deep learning models to generalize when deployed in real-world environments that exhibit unseen test distributions. In this paper, we present a novel approach named ChatGenImage that facilitates more controllabel data augmentation. ChatGenImage harnesses the collaborative power of the LLM and AIGC models, enabling iterative communication between them in a cost-effective and controllable manner. This automatically iterative process facilitates the generation of high-quality synthetic images depicting complex scenes and diverse domains, along with fine-grained annotations. Our fundamental intuition is that large language models have remarkable capabilities to perform new tasks in a zero-shot manner when presented with well-crafted instruction prompts[34,35,36,11]. We discover that these LLMs like ChatGPT possess the capability to autonomously navigate image editing processes. By strategically designing appropriate prompts, LLMs can leverage the inherent knowledge within the system and effectively guide the AIGC models to produce highly controllable and intricate images. While ChatGPT contains diverse world knowledge for simulating the human brain’s efficient processing, it is non-trival to elicit this knowledge from it for data augmentation with automatic labeling because ChatGPT is a pure language model that lacks the ability to visually perceive any information. We explore this issue in the context of generative data augmentation, showing that language can act as a bridge connecting LLMs and AIGC models, producing elaborate images for downstream tasks by globally controllable prompts and iteratively local editing instructions. To this end, we demonstrate three key findings. First, we find that the LLM such asChatGPTcontains a wealth of conceptual knowledge and can imagine vivid descriptions even with only one label word (e.g. Adogplaying in a lush green park, with a frisbee in its mouth. The dog should havea shiny coat of fur.)[33,6]. We further obverse that the existing AIGC models can only generate simple image with few objects and backgrounds, which are not diverse for domain generalization[20]. Thus, we establish the iterative pipeline to repair missing details and refine generated images with the help of label foundation toolkits and local editing prompts. Finally, we demonstrate our method flow to produce large amounts of high-quality synthetic data with fine-grained labels in a scalable manner for data augmentation in data scarcity scenarios."
2306.07553v1	DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback	Traffic Signal Control (TSC) aims to reduce the average travel time of vehicles in a road network, which in turn enhances fuel utilization efficiency, air quality, and road safety, benefiting society as a whole. Due to the complexity of long-horizon control and coordination, most prior TSC methods leverage deep reinforcement learning (RL) to search for a control policy and have witnessed great success. However, TSC still faces two significant challenges. 1) The travel time of a vehicle is delayed feedback on the effectiveness of TSC policy at each traffic intersection since it is obtained after the vehicle has left the road network. Although several heuristic reward functions have been proposed as substitutes for travel time, they are usually biased and not leading the policy to improve in the correct direction. 2) The traffic condition of each intersection is influenced by the non-local intersections since vehicles traverse multiple intersections over time. Therefore, the TSC agent is required to leverage both the local observation and the non-local traffic conditions to predict the long-horizontal traffic conditions of each intersection comprehensively. To address these challenges, we propose DenseLight, a novel RL-based TSC method that employs an unbiased reward function to provide dense feedback on policy effectiveness and a non-local enhanced TSC agent to better predict future traffic conditions for more precise traffic control. Extensive experiments and ablation studies demonstrate that DenseLight can consistently outperform advanced baselines on various road networks with diverse traffic flows. The code is available at https://github.com/junfanlin/DenseLight.	Reducing traffic congestion is an essential task for efficient modern urban systems. As the number of vehicles in the cities increases year by year, backward traffic coordination not only does harm to the driving experience but also aggravates air contamination with more harmful fuel emissionsZhang and Batterman (2013). Alleviating traffic congestion by efficient traffic signal control (TSC)Mirchandani and Head (2001)is one of the most practical and economical approachesTaylor (2002). Specifically, TSC aims at coordinating the traffic lights of a road network to regulate the traffic flows to minimize the average travel time of vehicles. However, the traffic data collected from a road network is usually massive yet incomprehensibleLiuet al.(2020); Wanget al.(2023). Therefore, the widely-adopted TSC strategies either fix signal routineRoesset al.(2004)or adapt the traffic signal plans in real-time according to traffic flow patternsCoolset al.(2013). To automatically mine useful information from the massive traffic data, more and more studies leverage the powerful representation capability of deep neural networksLeCunet al.(2015)to learn TSC agentsVan der Pol and Oliehoek (2016); Zhanget al.(2021); Zhenget al.(2019); Oroojlooyet al.(2020); Weiet al.(2018,2019a); D’Almeidaet al.(2021)through the advanced reinforcement learning (RL) methodsMnihet al.(2015); Schulmanet al.(2017); Lianget al.(2021). A proper reward function is required for applying RL to the TSC problem and improving the policy. Since the travel time is only feasible after a vehicle leaves the road network and fails to provide instant/dense feedback to the TSC policy in time during its journey, many previous worksVaraiya (2013); Weiet al.(2019a,b); Xuet al.(2021)draw the traditional characteristics of traffic intersections for the reward design, such as traffic pressure and queue length at the intersection. However, most of these heuristic rewards may be biased from the ultimate goal of TSC, i.e., the average travel time minimization. The introduced biases could lead the RL methods to adjust the TSC policy in an incorrect direction. Apart from the reward design, it is also critical to endow the RL agents with the capability of precisely predicting the future dynamics of the environment to make well-founded decisionsVan Hasseltet al.(2016); Fujimotoet al.(2018); Yanget al.(2016); Louet al.(2020). However, it is non-trivial for an RL agent to capture the future traffic dynamics of the intersections in the context of TSCChenet al.(2022). The future arriving vehicles of one intersection may be running in adistantintersection at present. To this end, thelocalobservations of either the intersection itself (i.e., a snapshot of vehicles at the intersection)Varaiya (2013); Weiet al.(2019a); D’Almeidaet al.(2021)or neighboring intersectionsWeiet al.(2019b); Xuet al.(2021)might be insufficient to predict the long-horizontal traffic dynamics of the intersection. Additionally, the location and traffic flow trend of an intersection also play a role in predicting the traffic dynamics of the intersection. For example, downtown intersections tend to be more crowded than suburban intersections. For example, a large number of vehicles entering an empty intersection may indicate the beginning of the rush hour. Therefore, the non-local observations and the location of the intersections, and historical traffic conditions are all important for a TSC agent to estimate future traffic situations more accurately. To address the issues discussed above, we propose a novel RL-based TSC method namedDenseLight, which improves traffic light coordination by exploiting dense information from both an unbiased reward and non-local intersection information fusion. Specifically, to provide dense and unbiased feedback for the policy improvement, we propose an equivalent substitute for travel time, i.e., the gap between the ideal traveling distance (i.e., the ideal distance a vehicle could have traveled at the full speed during its journey) and the actual traveling distance during the whole journey, namely Ideal-Factual Distance Gap (IFDG). Since the length of the factual journey of a vehicle is fixed according to its traveling lanes of the road network, minimizing IFDG is equivalent to minimizing travel time. Most importantly, IFDG can be calculated at each intersection and at each timestep. Therefore, IFDG can also provide instant feedback on the effectiveness of the control policy at each intersection. Besides an unbiased and dense reward, DenseLight also features aNon-local enhanced Traffic Signal Control(NL-TSC) agent to benefit TSC from the spatial-temporal augmented observation and the non-local fusion network architecture. Specifically, the NL-TSC agent supplements the original observation of an intersection with its location information and the previous observation so that each intersection can customize its own signal plans w.r.t. historical and spatial information. As for facilitating the TSC agents with a better awareness of the future traffic dynamics affected by other intersections, a non-local branch is proposed to enhance the local features of each intersection with the features of the non-local intersections. By learning to communicate the non-local information across non-local intersections, the NL-TSC agent can better predict the long horizontal traffic condition of each intersection and can thus make better coordination at present. Overall, our contributions are three-fold:1)we propose a novel RL-based TSC method, i.e., DenseLight, which is optimized by an unbiased and dense reward termed IFDG;2)to better model the future accumulated IFDG of each intersection, we develop the NL-TSC agent, effectively gathering spatial-temporal features of each intersection and propagating the non-local intersection information to improve the multi-agent RL policy;3)comprehensive experiments conducted on different real-world road networks and various traffic flows show that DenseLight can consistently outperform traditional and advanced RL-based baselines.
2302.10650v1	Predicting Privacy Preferences for Smart Devices as Norms	Smart devices, such as smart speakers, are becoming ubiquitous, and users expect these devices to act in accordance with their preferences. In particular, since these devices gather and manage personal data, users expect them to adhere to their privacy preferences. However, the current approach of gathering these preferences consists in asking the users directly, which usually triggers automatic responses failing to capture their true preferences. In response, in this paper we present a collaborative filtering approach to predict user preferences as norms. These preference predictions can be readily adopted or can serve to assist users in determining their own preferences. Using a dataset of privacy preferences of smart assistant users, we test the accuracy of our predictions.	Artificial intelligence (AI) technologies are making their way into our daily lives and into our homes. We have grown accustomed to using our devices to call friends, set reminders, or check the weather.However, for these technologies to be adopted and trusted by users, they must act as users expect, and this problem is especially apparent in the area of privacy preferences. Studies show that users are deeply concerned about how their data is being collected onlinemadden2014perceptions. Interestingly, while they expect AI to act as they desire, they are unwilling to spend time setting their preferences. For example, despite users’ concerns about privacy, studies show that they ignore or blindly accept cookie bannerskretschmer2021cookieand privacy policies in social networksObar2018lie. Furthermore, in social networks, a large proportion of users do not change default privacy settingsKrishnamurthy2009leakage. This can be explained as a result of privacy fatigueChoi2018Fatigue, the sensation of loss of control and futility over protecting one’s privacy. This leads to privacy cynicism, when users do not adopt a privacy protecting behaviour even if they are concerned about their privacyHoffmann2016Cynicism. Thus, the current approach of directly asking the user when a preference is unknown but needed fails to capture the user’s true preferences. Additionally, continual questioning prevents users from achieving their objectives with the device. In response, this paper advocates for an approach that can understand user preferences with less user involvement, in turn bringing more importance to user interactions whenever such preferences are needed. A particular platform in which capturing privacy preferences is challenging and yet essential is that of smart speakers and other smart personal assistants. These devices have benefited from widespread early adoption, and it is estimated that 500 million units were installed in the last quarter of 2021Strategy2021smartspeakerinstalled. Nonetheless, the early adoption of these technologies means that they still have several vulnerabilities that pose a threat to the security and privacy of their usersEduSPASurvey. Indeed, there have already been cases reported in which smart assistants have not functioned as expected; for example, a smart speaker recorded and sent a private conversation without the user’s consentGuardianAlexa. These situations hinder user trust in the technology and can ultimately lead users to limit the functionalities of the devices used, or even to adopting coping mechanismsAbdi2019More. This paper describes an alternative approach that addresses the issues outlined above. The critical observation underpinning our approach is that smart devices are just one part of a larger ecosystem (e.g. seeEduSPASurveyfor a description of the ecosystem of smart speakers), and they interact and share data with agents like services, apps, and other devices. For example, a smart watch might send a voice recording to a smart speaker, or might share the wearer’s heart rate with a health app. In this respect, we can understand this ecosystem as a multi-agent system in which the use of norms can help to regulate these interactions, implementing privacy preferences. Norms can effectively summarise complex privacy preferences into simple sets of regulations, as shown by Abdi et al.NouraPrivacyNorms2021, who gathered over 800 privacy preferences on data transmissions, yet produced just 17 norms. Moreover, although here we assume no knowledge of the domain, if such knowledge is available there exist techniques to generalise norms (seeMoralesAAMAS2013;MoralesAAMAS2014for an example) or find and resolve inconsistencies among themVasconcelosKN09. Furthermore, norms are also used by people, and are naturally understood by them, representing a good base upon which to construct explanations. This can be used not only to generate explanations for a user if something unexpected happens, but also to tailor interactions with a user to validate predicted norms. Norms are regarded as expected patterns of behaviourWooldridgeIntrodMAS, causing agents (each component in the smart device ecosystem) to coordinate better and function more efficiently. As an example with smart devices, imagine a service knows in advance the privacy norms of a user with regard to each component of the ecosystem. If this service needs to interact with other components, it can use the user’s norms to adapt its behaviour to avoid violating norms or to avoid performing unregulated transmissions of information, which might require consent. As informally outlined inSerramiaPRIMA22Collaborative, we canexploit the large user bases of smart devices to use knowledge of previously specified privacy preferences to infer new preferences or to assist users in specifying their preferences. In particular, we aim to exploit similarities between users to make privacy preference predictions using collaborative filteringSu2009Collaborative.Effectively, we see the smart device ecosystem as a multi-layered multi-agent system. The lower level represents the multi-agent system associated with a single user (that is, the user’s device, and the other devices, skills, and services that can be accessed from it). The higher level is that of the multi-agent system composed of all the users. Our approach is centred on the norm creation stage in the lower level multi-agent systems related to each user. Therefore, each device user has its own associated set of norms, and all agents within its lower level multi-agent system, be they devices, skills, or other services, are informed and affected by the norms whenever they want access to the user’s personal data.While many researchers have studied different approaches to constructing norm systems, like norm synthesisMoralesAAMAS2013;morales2015compactor norm emergenceshoham1997emergence;SavarimuthuAAMAS07;sugawara2011emergence, we are not aware of any similar approach like the collaborative filtering presented here. In taking this approach, we make the following contributions. Formalisation of the problem of predicting norms to ensure that computational behaviour aligns with user preferences. This is divided into two subproblems, namely preference approximation (predicting unknown user preferences) and norm inference (inferring norms from predicted preferences). Formalisation of preference prediction functions. We provide a specific example of this type of function based on the preferences of similar users. Inference of norms from the predicted preferences, and specification of different methods to do so based on the confidence of the prediction or other variables. The paper is structured as follows: Section2formalises the core problems we aim to address in the paper. In Section3we detail the process of predicting preferences. We then use these predictions to infer norms in Section4. Section5is dedicated to validate our findings. In Section6we discuss related work. Finally, in Section7we discuss conclusions and future work.
2305.00140v2	Space reduction techniques for the $3$-wise Kemeny problem	Kemeny's rule is one of the most studied and well-known voting schemes with various important applications in computational social choice and biology. Recently, Kemeny's rule was generalized via a set-wise approach by Gilbert et. al. This paradigm presents interesting advantages in comparison with Kemeny's rule since not only pairwise comparisons but also the discordance between the winners of subsets of three alternatives are also taken into account in the definition of the $3$-wise Kendall-tau distance between two rankings. In spite of the NP-hardness of the 3-wise Kemeny problem which consists of computing the set of $3$-wise consensus rankings, namely rankings whose total $3$-wise Kendall-tau distance to a given voting profile is minimized, we establish in this paper several generalizations of the Major Order Theorems, as obtained by Milosz and Hamel for Kemeny's rule, for the $3$-wise Kemeny voting schemes to achieve a substantial search space reduction by efficiently determining in polynomial time the relative orders of pairs of alternatives. Essentially, our theorems quantify precisely the nontrivial property that if the preference for an alternative over another one in an election is strong enough, not only in the head-to-head competition but even when taking into account one or two more alternatives, then the relative order of these two alternatives in all $3$-wise consensus rankings must be as expected. As an application, we also obtain an improvement of the Major Order Theorems for Kememy's rule. Moreover, we show that the well-known $3/4$-majority rule of Betzler et al. for Kemeny's rule is only valid in general for elections with no more than $5$ alternatives with respect to the $3$-wise Kemeny scheme. Several simulations and tests of our algorithms on real-world and uniform data are provided.	In this article, anelectionis a finite collectionC=\{c_{1},\dots,c_{n}\}of alternatives together with avoting profileconsisting of a finite number of (not necessarily distinct) votes. Arankingor avoteis simply a complete and strict total ordering\pi\colon c_{\pi(1)}>c_{\pi(2)}>\dots>c_{\pi(n)}which we identify with a permutation of\{1,2,\dots,n\}also denoted by\pi. Here,x>ymeans that the alternativexis ranked before the alternativey. The space of all rankings, for fixedn, can be equipped with several natural distances, for example, the Kendall-tau distance which counts the number of order disagreements between pairs of elements in two permutations, namely, the bubble-sort distance between two permutations, or more generally thek-wise Kendall-tau distance[14](see Equation (2.1)). The important Kemeny problem (cf.[15],[16],[26]) consists of finding the set ofk-wise medians of a given election, i.e., permutations whose total distance to the voting profile is minimized with respect to thek-wise Kendall-tau distance. In other words, a median is a ranking that agrees the most with the voting profile. By taking into consideration not only pairwise comparisons but also the discordance between the winners of subsets of three alternatives, the3-wise Kemeny voting scheme seems to be more resistant to coalitional manipulation than the classical2-wise Kemeny rule: it is much more difficult for an alternative to win an election or even to simply win another specific alternative in an election under the3-wise Kemeny voting scheme. In fact, most of the best-known space reduction results for Kemeny’s rule fail in the3-wise setting (see[21, Table 1]), including the powerful Major Order Theorems discovered in[19](see Example5.1) and the Condorcet criterion. It was shown in[21]that even the2/3majority in every duel is not enough to guarantee that an alternative will win an election according to the3-wise Kemeny voting scheme. This phenomenon is in stark contrast to the Condorcet criterion where a Condorcet winner, namely an alternative which is preferred by more voters than any others, must be the unique winner of the election. Nevertheless, we know that an alternative obtaining a3/4majority in every duel must be the unique winner in the3-wise Kemeny voting scheme[21]. In many situations, the3-wise Kemeny rule is also more suitable than Kemeny’s rule since it puts more weight on alternatives which are more frequently ranked in top positions in the votes. Indeed, Kemeny’s rule puts equal weight on the head-to-head competition of two alternativesx,ywhen in a particular vote,xis the winner followed byyand when in another vote,xandyoccupy the last two position in that order. However, it is reasonable to assume that typical voters only pay attention to a shortlist of their favorite alternatives and put a rather random order for the rest of the alternatives. Such undesirable behavior creates noises that can alter the Kemeny median ranking while the problem can be solved effectively using the3-wise Kemeny voting scheme in specific instances (see Example5.1and AppendixB). The above limitation of the Kemeny rule leads to the notion of weighted Kendall tau distances introduced by Kumar and Vassilvitskii[17]as well as the notion of set-wise Kemeny distance of Gilbert et. al.[14]. Motivated by the above potential and interesting features of the3-wise Kemeny rule as well as the NP-hardness of the various Kemeny problems (see[arrow],[12],[14]), our main goal is to formulate new quantitative results concerning the majority rules in3-wise Kemeny voting schemes associated with the3-wise Kendall-tau distance introduced recently in[14], which provide much more refined space reductions to the Kemeny problem in comparison to existing techniques in the literature. Our first result (cf. Theorem3.1) shows that the fundamental3/4-majority rule of Betzler et al.[6]for the classical Kemeny rule is also applicable for all elections with no more than5alternatives with respect to the3-wise Kemeny scheme. However, the3/4-majority rule fails in general as soon as there are at least6alternatives. Note that without restriction on the number of alternatives, the5/6-majority rule obtained in[21]serves as the3-wise counterpart of the3/4-majority rule. The second and central result of the paper is the Major Order Theorem for the3-wise voting scheme, denoted 3MOT (Theorem5.4), and its improved version, denoted Iterated 3MOT (Theorem6.2), which, to the limit of our knowledge, are the most efficient space reduction techniques for the3-wise Kemeny rule in the literature. In essence, our Major Order Theorems show that if the preference for an alternativexover another alternativeyin an election is strong enough, as measured quantitatively not only in the head-to-head competition but also when taking into account the interactions withoneortwomore other alternatives, thenxmust be ranked beforeyinall3-wise medians of the election. The corresponding algorithms not only function efficiently in polynomial timeO(n^{4}m), wherenis the number of alternatives andmis size of the voting profile, but also drastically reduce the search space of3-wise medians. To get an idea on the efficiency and interests of our results, let0\leq p<1be the proportion of pairs of alternatives solved by 3MOT or Iterated 3MOT out of the total ofn(n-1)/2pairs. Then the original search space consisting of alln!possible rankings would be reduced by a factor (reduction rate) at least equal to (cf.[linear-extension-2018, Lemma 2, Lemma 4], see also Table2) On real-world data, especially for political elections and competitions where there exists a high similarity between the votes, our algorithms prove to be particularly useful aspusually ranges from0.6to0.9after only a few iterations of Iterated 3MOT (see Table2and also AppendixC.1for some concrete examples). The performance is also very encouraging even on the hard case of uniformly generated data where, for example whenm=15, the3-wise Major Order Theorems can determine the relative rankings of approximately47\%pairs of alternatives on average whenn=10and approximately31\%whenn=15. Our results not only extend and improve the important2-wise Major Order Theorem of[19](see Section2.4and Theorem7.1) but also provide a unified approach and technique which should pave the way for the research of more refined algorithms and quantitative properties ofk-wise Kemeny voting schemes fork\geq 2. It is worth comparing our method to the space reduction method based on a3-wise majority digraph introduced in[14, Theorem 3]whose vertices are the alternatives. While we can obtain, under some assumptions on the3-wise digraph of an election, a set of rankings which containssome3-wise median using[14, Theorem 3], our3-wise Major Order Theorems provide, for all pairs of alternatives(x,y), easy-to-compute and mild sufficient conditions so thatx>yinall3-wise medians. In fact, by relaxing the conditions in our Major Order Theorems, we can determine more relative orderings of a pair of alternatives insome3-wise median (see Theorem8.1). Another major difference is that[14, Theorem 3]only considers the strength of the preference forxoveryin the presence ofoneother alternativez\neq x,ywhile the3-wise Major Order Theorems quantify the preference forxoverynot only in the head-to-head competition but even when taking into accountoneortwomore alternatives, which should provide a more refined space reduction method (see Example6.4). In any case, the constraints on all3-wise medians found by our 3-wise Major Order Theorems should greatly accelerate other complementary space reduction methods and vice versa, nontrivial constraints obtained by other methods can serve as the inputs to boost our algorithms, especially Iterated 3MOT. Table1below summarizes our results and some well-known space reduction criteria for the classical and3-wise Kemeny voting schemes. 3-wise Kemeny rule Extended Always theorem (Pareto efficiency, unanimity) Phung & Hamel[21, Theorem 3] Phung & Hamel[21](TheoremC.4) 3/4-majority rule (Section2.3) Betzler et al.[6] Valid only for elections of5alternatives or less, Theorem3.1 Extendeds-majority rule Phung & Hamel[21, Section 4] Valid ifs\geq 5/6[21, Section 8] Major Order Theorems Milosz & Hamel[19](Section2.4) Improved Iterated MOT (Theorem7.1) Not valid, see Example5.1 3-wise Major Order Theorems Theorems5.4,6.2,8.1. To illustrate the utility of our obtained algorithms, we performed several simulations and tests on real-world and uniform data. Table3and Table4in AppendixCrecord the approximate average proportion of pairs with relative order solved by the3-wise Extended Always Theorem obtained in[21](see TheoremC.4) and the 3-wise Major Order Theorems over 100 000 uniformly generated instances. Several concrete examples with real-world data (elections with a dozen and up to 250 alternatives) taken from PREFLIB[18]are described in AppendixC.1(see also Table2). A direct implementation shows that for voting profiles withn,m\leq 40, the list of all pairs of alternatives with relative order in all3-wise medians determined by 3MOT, resp. Iterated 3MOT, can be obtained in approximately less than 2.5 seconds, resp. 1 minute, with an M1 MacBook Air 16GB RAM. More optimized implementation can definitely improve the running time of the algorithms. Finally, we explain how to apply our results and the set-wise Kemeny rule to deal with incomplete votes in Section9and propose the usage of the proportion of pairs whose relative order are determined by the3-wise Major Order Theorems as a meaningful measure of the consensus level of the electorates (see AppendixA).
2311.10786v1	A Systems-Theoretical Formalization of Closed Systems	There is a lack of formalism for some key foundational concepts in systems engineering. One of the most recently acknowledged deficits is the inadequacy of systems engineering practices for engineering intelligent systems. In our previous works, we proposed that closed systems precepts could be used to accomplish a required paradigm shift for the systems engineering of intelligent systems. However, to enable such a shift, formal foundations for closed systems precepts that expand the theory of systems engineering are needed. The concept of closure is a critical concept in the formalism underlying closed systems precepts. In this paper, we provide formal, systems- and information-theoretic definitions of closure to identify and distinguish different types of closed systems. Then, we assert a mathematical framework to evaluate the subjective formation of the boundaries and constraints of such systems. Finally, we argue that engineering an intelligent system can benefit from appropriate closed and open systems paradigms on multiple levels of abstraction of the system. In the main, this framework will provide the necessary fundamentals to aid in systems engineering of intelligent systems.	There has long been a call for a theory of Systems Engineering (SE) within the SE community with the aim of establishing SE as a standalone engineering field capable of addressing modern engineering problems[hazelrigg2022toward][kasser2011unifying]. However, there is an existing gap in concrete formalism and distinction for some fundamental concepts within the field that has led to ambiguity in some SE practices[salado2021systems]. While such formalism might not have been necessary in the past, the emergence of new kinds of complex systems such as Artificial Intelligence (AI)-enabled systems has challenged traditional SE practices[llinas2021motivations,tolk2011towards,mcdermott2021artificial,llinas2021systems,shadab2021shifting,smith2017cognitive]. In our previous work, we identified potential gaps in the current SE foundations to address the unique nature of AI-enabled systems[shadab2022closed]. We argued that intelligence is a relational property that can be characterized and engineered as a relation between the system and its context with both learning and intelligence properties embodied in the context regardless of the nature of the relations between them[shadab2022closed]. In this situation, intelligence is no longer relegated to a component or the physical boundary of the system. Therefore, we posited that owing to this high coupling between AI-enabled systems and their environment, utilizing the concept of closure in SE is a potential path forward to build general engineered intelligence[shadab2022closed]. We proposed that closed SE practices could be employed to realize the closed notion of this relational property between intelligent systems and their context. We further examined the possibility of employing closed systems precepts in an engineering framework in our later paper[cody2022core], concluding a lack of concrete definitions and formalism in the theory and practice of SE presents a barrier to applying closed system precept in engineering applications. Currently, most of the theoretical foundations in both systems theory and the theory of SE are bounded to the open systems precepts (i.e., inputs-outputs relations)[dag2000introduction]. Although the concept of closed systems is being utilized in limited applications in SE, there is little to no theoretical basis for these practices, making SE activities based on closed system precepts prone to interpretation and over-abstraction. As we have identified at least one domain that can benefit from closed systems precepts (AI-enabled systems), the need for clear definitions and formalism becomes increasingly important in the field of SE. This paper revisits the concept of closure in SE, aiming to formalize, define, and evaluate this concept as the first step towards employing closed systems precepts for intelligent systems. As mentioned earlier, closure has been vaguely applied in SE with limited underlying formal foundations[hutchison2018framework,di2018closed]. Various types of closure have been introduced in systems theory literature, including functional closure, organizational closure, operational closure, and informational closure, among others[zeleny1981autopoiesis,varela1974autopoiesis,bednarz1988autopoiesis,bertschinger2006information,bertschinger2008autonomy,mora2012closure]. As a starting point, closure can be understood as a property of a system that makes the system closed, and a closed system is defined as one that does not exchange energy, information, or matter through its boundaries[bertalanffy1968bertalanffy]. (This concept will be revisited in detail later in the paper.) However, there is little to no formal framework to describe the relationships and differences between each type of closure, and many of the closure types lack formal systems-theoretic definitions that distinguish them from the other types of closure; in fact, on many occasions, these terms are used interchangeably, which can cause confusion in the application of each type of closure[goodenough2002concept][di2018closed]. In this paper, we develop formal systems-theoretic definitions for two types of closure, functional and informational closure, in systems and compare them in terms of system’s characteristics and the relations between systems and their environment. Among all types of closure, functional and informational closure were selected as we believe they were more relevant to the systems engineering practices. We utilize a mathematical definition of functional dependency, information theory, and the systems-theoretic foundations for open and closed systems to produce formalism for functional and informational closure. Then, we determine the conditions and constraints to meaningfully use each of the two types of closure in systems. Our aim is to elaborate on the relations between these types of closure to determine their applications at different levels of abstractions for systems. Throughout this paper, we will use the termsclosureandclosednessinterchangeably.
2306.03409v1	Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems	We study the multi-objective minimum weight base problem, an abstraction of classical NP-hard combinatorial problems such as the multi-objective minimum spanning tree problem. We prove some important properties of the convex hull of the non-dominated front, such as its approximation quality and an upper bound on the number of extreme points. Using these properties, we give the first run-time analysis of the MOEA/D algorithm for this problem, an evolutionary algorithm that effectively optimizes by decomposing the objectives into single-objective components. We show that the MOEA/D, given an appropriate decomposition setting, finds all extreme points within expected fixed-parameter polynomial time in the oracle model, the parameter being the number of objectives. Experiments are conducted on random bi-objective minimum spanning tree instances, and the results agree with our theoretical findings. Furthermore, compared with a previously studied evolutionary algorithm for the problem GSEMO, MOEA/D finds all extreme points much faster across all instances.	Evolutionary algorithms have been widely used to tackle multi-objective optimization problems in many areas such as robotics, pattern recognition, data mining, bioinformatics, scheduling and planning, and neural network training[Zhou2011]. Their population-based search operators make them a natural choice for simultaneously handling several possibly conflicting objectives. Many generic evolutionary multi-objective frameworks have been developed to supply basic implementations for any problem, and to provide templates that can be fine-tuned for specific applications (we refer to[Wang2023]for an overview of common approaches). Such features, along with their strong empirical performances in challenging applications, have led them to becoming one of the most attractive topics to researchers and practitioners alike. Among evolutionary multi-objective algorithms (EMOs), arguably the most exemplary are dominance-based approaches such as GSEMO and NSGA variants, with the former often being considered a baseline. Another popular technique for multi-objective optimization is to decompose the multiple objectives into a single-objective subproblem. The MOEA/D algorithm is a state-of-the-art application of this technique in evolutionary computation[Trivedi2016,Xu2020]. Despite the prevalence of EMOs on practical applications, rigorous analyses of their runtime behavior on meaningful problems are scarce. Nevertheless, these kinds of analyses are critical for (1) providing performance guarantees and guidelines to practitioners who use and develop these techniques in the field, and (2) promoting the explainability of heuristic search and optimization techniques by clarifying their working principles through a careful mathematical analysis. Run-time analyses on the performance of evolutionary algorithms have been provided for simple algorithms such as GSEMO in both artificial benchmark problems[Bian2018,Doerr2021]and others such as bi-objective minimum spanning tree[Neumann20071,Roostapour2020]and constrained submodular optimization[NIPS2015_b4d168b4,NIPS2017_d7a84628,Do2020,Qian2020]. In recent years, theoretical analyses of state of the art approaches such as NSGA-II and MOEA/D have been conducted[Huang2019,Huang2021,Huang20211,Zheng2022,Doerr2023,cerf2023proven]. Most of these run-time results are on artificial benchmark problems, and the one for NSGA-II on bi-objective minimum spanning tree proves promising. In this paper, we present for the first time rigorous results on MOEA/D for a classical multi-objective optimization problem, namely the multi-objective minimum weight base problem. This problem, falling under the matroid optimization category, is a significant generalization of the previously studied bi-objective minimum spanning tree problem. In this work, we focus on approximating the non-dominated front, as determining whether the front is reached is EXPSPACE. In particular, we show that MOEA/D obtains a factor2-approximation for two objectives in expected polynomial time. Previous analyses for the special case of graphic matroid (i.e. spanning forests) were only able to show a pseudo-polynomial run-time for GSEMO to obtain this approximation[Neumann20071]. We further extend the analyses by deriving a fixed-parameter polynomial expected run-time in instances withk>2objectives to reach ak-approximation. Instrumental to our analyses is a deeper understanding of the problem, and as such, we formally examine certain properties of the multi-objective minimum weight base problem. We first prove a tight approximation guarantee from computing the convex hull of the non-dominated front, extending the known guarantee for two objectives[Neumann20071]. With this in mind, we explore insight regarding this convex hull, including its vertex complexity and the structural relation among solutions whose weights constitute said convex hull. In addition, we briefly formulate an efficient deterministic approach to enumerate extreme points. These findings may be of interest in areas beyond runtime analysis.
2401.07890v2	A Strategy for Implementing description Temporal Dynamic Algorithms in Dynamic Knowledge Graphs by SPIN	Planning and reasoning about actions and processes, in addition to reasoning about propositions, are important issues in recent logical and computer science studies. The widespread use of actions in everyday life such as IoT, semantic web services, etc., and the limitations and issues in the action formalisms are two factors that lead us to study how actions are represented.   Since 2007, there have been some ideas to integrate Description Logic (DL) and action formalisms for representing both static and dynamic knowledge. Meanwhile, time is an important factor in dynamic situations, and actions change states over time. In this study, on the one hand, we examined related logical structures such as extensions of description logics (DLs), temporal formalisms, and action formalisms. On the other hand, we analyzed possible tools for designing and developing the Knowledge and Action Base (KAB).   For representation and reasoning about actions, we embedded actions into DLs (such as Dynamic-ALC and its extensions). We propose a terminable algorithm for action projection, planning, checking the satisfiability, consistency, realizability, and executability, and also querying from KAB. Actions in this framework were modeled with SPIN and added to state space. This framework has also been implemented as a plugin for the Prot\'eg\'e ontology editor.   During the last two decades, various algorithms have been presented, but due to the high computational complexity, we face many problems in implementing dynamic ontologies. In addition, an algorithm to detect the inconsistency of actions' effects was not explicitly stated. In the proposed strategy, the interactions of actions with other parts of modeled knowledge, and a method to check consistency between the effects of actions are presented. With this framework, the ramification problem can be well handled in future works.	Many of the knowledge modeling and representation in our everyday lives are in dynamic knowledge structures. For example, in a web service architecture that contains a requester, a broker, and a provider, we are faced with step-by-step actions, and we can illustrate and represent the web service compositions in different case studies based on semantic webs[83]. There are many ways that dynamic-temporal knowledge can be described, modeled, reasoned about, and implemented[79]. In a related work section, we will briefly review works about action formalism in formal logics, computer science, and artificial intelligence. Change and time formalism has a very long history from Aristotle, Diodorus, and early Islamic logicians such as Avicenna, up to modern logicians. Change formalism, mostly studies in the field of “temporal logics”. In temporal logic, using one of the quantified or modal approaches, we formalize time in either point-based or interval-based time. The hybridization of action and change formalism, especially in dynamic knowledge base projects, is done in different methods in many of the formal logic and AI fields. Some projects, introduce dynamic logics and explicitly formalize the notion of actions and states, whereas the notion of time and change are implicit in them[13]. Other works, introduce temporal logic and formalize the notion of time and change, while the notion of action and dynamic behavior of the knowledge base is implicit in them. In this paper, we introduce an individual-based algorithm for description dynamic logics that implicitly contains the time notion. Unlike ontology versioning, we don’t track the ontology changes manually, but the actions automatically cause a new version of the knowledge base to appear. Therefore, we do not only have the ontology versions. The ”changes”, semantically, with the help of actions within the ontology itself, cause communication between two different states. Ontology versioning ignores the parts of an ontology that are constant and haven’t changed. In this paper, the entire new knowledge base produced by changes in actions will be examined by projection, and all realizability, executability, and planning services will be provided using TBox and ABox reasoning.
2305.05402v2	Consistent Text Categorization using Data Augmentation in e-Commerce	The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.   To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. We use a semi-supervised approach for data augmentation and presents two different methods for utilizing unlabeled samples. One method relies directly on existing catalogs, while the other uses a generative model. We compare the pros and cons of each approach and present our experimental results.	In the last two decades, widespread use of e-commerce platforms such as Amazon and eBay has contributed to a substantial growth in online retail. Such platforms rely on both explicit and implicit product features in order to deliver a satisfying user experience. There, the inferred product category is typically a crucial signal for many application such as browsing, search and recommendation. We focus on improving an existing product categorization model, we refer to as ’the categorizer’, that is employed by our company for fast categorization of billions of items on a daily basis. It classifies e-commerce items, such as products or deals, based on a predefined hierarchy of categories, namely GPT (Google Product Taxonomy). Given a product title, the categorizer assigns the most appropriate label in the taxonomy. The model itself is highly scalable and effective, so it is well-suited for settings with large and rapidly growing item catalogs. In our company, the categorizer is used as a standalone component in various e-commerce related services, such as recommendation, search, and ad ranking. A recent examination of the categorizer’s output revealed inconsistencies in the labeling of similar items. It was evident that in some cases small variations in product titles, such as those relating to colors or measurements, significantly affect the categorizer’s output. This inconsistency negatively impacts search and recommendation algorithms that rely on the inferred category, leading to a poor user experience. The concept of consistency in NLP tasks has been studied in various research works, including robustness to paraphrasing(Elazaret al.,2021)and robustness to adversarial attacks(Jinet al.,2020; Wanget al.,2020). Other works relate consistency issues with the misuse of spurious features during the learning phase(Arjovskyet al.,2019; Veitchet al.,2021; Wanget al.,2021). When examining the performance of the categorizer in terms of accuracy alone, the inconsistency issue may be overlooked. But, since many recommendation pipelines depend on the output of the product categorizer, an inconsistent model can have severe implications on the user experience. In most cases, the differences include returning the parent category or a sibling category, rather than a completely different category path. To tackle this inconsistency problem, we use differentdata augmentationtechniques and enrich the training data with item versioning, leading to a more consistent model. Data augmentation for improving various NLP tasks has been widely studied and surveyed(Shortenet al.,2021), and particularly in the context of consistency(Xieet al.,2020). Generating such data, both manually(Kaushiket al.,2019)and automatically(Rizoset al.,2019; Bariet al.,2020; Kumaret al.,2020), has shown to contribute to the robustness of learnt models in different settings. We chose to use data augmentation, without changing the current architecture of the already-in-use product categorizer for two main reasons. First, for scalability reasons, any change in the architecture might degrade the model’s ability to infer the categories of billions of items per day. Second, maintaining the current model architecture expedites the productization process and requires only minimal engineering effort. This work defines a new framework,Consistent Semi-Supervised Learning (Consistent-SSL), for consistent text categorization in the context of e-commerce (Section2). We use an unlabeled clustered dataset as a source of legit item versioning. The dataset is derived from product catalogs, and includes clusters of different versions of items. We present two different methods to utilize this unlabeled clustered data: a self-training method and a generative approach (Section3). We describe the datasets and the experimental framework we use for the evaluation of the proposed methods (Section4). Finally, we detail results, showing an improvement in the consistency rate of 4-10% above the baseline model, and discuss the advantages and weaknesses of each method (Section5).
2401.10506v1	FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis	Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Extensive experimental results on BULL demonstrate that FinSQL achieves the state-of-the-art Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer.	Text-to-SQL aims to transform natural language questions into executable SQL queries, which enables low-code operations for relational databases. It can facilitate the data access procedure for non-professional database users who are not familiar with SQL and has gained much attention in various areas, especially in financial analysis. While financial professionals (e.g., investment advisors) need to query relevant databases frequently, they are usually not well-skilled in SQL programming. Therefore, Text-to-SQL is significantly important for financial analysis and has gained much attention. However, there is no Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases used in financial analysis. To address these issues, we construct a practical Text-to-SQL dataset for financial analysis based on the intelligent investment assistant product of Hundsun Technologies Inc., which facilitates more than 50 financial institutions (including Alipay, China Merchants Bank, and so on) and serves millions of personal users. This dataset, dubbed BULL, contains three databases corresponding to fund, stock, and macro economy respectively. Besides, in this dataset, there are 4,966 natural language question-SQL pairs annotated by financial professionals, data scientists, and software engineers from Hundsun Technologies Inc. Furthermore, BULL has both English and Chinese versions. Compared with the widely used Text-to-SQL benchmark datasets (e.g., Spider(Yuet al.,2018b)and BIRD(Liet al.,2023c)), BULL has much more tables for each database and much more columns for each table, illustrating as in Table1. Furthermore, table and column names in BULL are often expressed with abbreviations or vague representations. These characters require financial Text-to-SQL models to support large input context length and have strong context understanding ability. Fortunately, Large Language Models (LLMs)-based Text-to-SQL can satisfy these requirements, and several LLMs-based Text-to-SQL methods have been proposed recently. However, existing state-of-the-art LLMs-based Text-to-SQL methods typically depend on OpenAI’s APIs, such as GPT-3.5-turbo or GPT-4, which are expensive and have risks of information leakage. Therefore, these methods cannot be used in the financial applications where the information privacy is critically important. To avoid information leakage, a feasible way is to adopt open-source LLMs (e.g., LLaMA(Touvronet al.,2023)and Baichuan(Yanget al.,2023)) and train them in private domains. However, it faces three challenges: (1) Schema linking dilemma and data scarcity. It is difficult to establish connections between question and schema items for financial databases usually have a large number of columns and tables. Furthermore, due to the labeling cost, the number and diversity of labeled question-SQL pairs are limited. These issues obstruct the construction of concise and diversified prompts, which hinders the model’s performance; (2) Resource-consuming fine-tuning and cross-database generalization difficulties. Fine-tuning LLMs on downstream tasks demands several days of computation across multiple GPUs. The substantial cost associated with model updates and iterations poses a considerable challenge. Additionally, transferring the model to a new database incurs significant costs, which impedes the cross-database transfer. (3) Inconsistent output. Due to the inherent randomness and the decoder strategy of sampling, LLMs often generate inconsistent outputs, leading to syntactically incorrect and invalid SQL queries. To tackle these challenges, this paper proposes a model-agnostic LLMs-based Finacial Text-to-SQL model training and inference framework, dubbed FinSQL. It can be used to develop Text-to-SQL models based on any open-source LLMs. Figure1demonstrates the overall overview of FinSQL, which consists of three key components: prompt construction, parameter-efficient fine-tuning, and output calibration, corresponding to the above challenges correspondingly. Specifically, prompt construction consists of a parallel schema linking method and a hybrid data augmentation method, which help to construct more concise and diverse prompts and enhance the model’s performance from the input side. The parameter-efficient fine-tuning component adopts Low-Rank Adaptation (LoRA) to fine-tune a very small percentage of parameters (¡1%) to obtain weight plugins for different business scenarios and manages these plugins through a plugin hub. Based on this plugin hub, the database-specific Text-to-SQL models can achieve efficiently few-shot cross-database transfer. In output calibration, SQL post-processing is performed to enhance the correctness of the generated SQL. These three components contribute to FinSQL’s superior performance. Our contributions can be summarized as follows: We propose BULL, a practical benchmark dataset for financial Text-to-SQL. We propose FinSQL, a model-agnostic LLMs-based Text-to-SQL framework for financial analysis. Extensive experimental results on BULL demonstrate that FinSQL is model-agnostic and able to achieve the state-of-the-art performance; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer.
2304.12404v1	Semantic Tokenizer for Enhanced Natural Language Processing	Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.	NLP models have two primary components—a deep neural network and a vocabulary of embeddings. Recent improvements in NLP model performance have focused on improving deep networks and increasing model sizes. Interestingly, little attention has been paid to optimizing vocabularies. An analysis of recent models[9], plotted in Figure1,111Credit for figure to Huggingface’s DistilBERT: https://research.aimultiple.com/gpt/shows that model sizes (i.e., number of parameters) have increased by 15,000% over the last few years (excluding 175B parameter GPT-3[1]or 1T parameters of GPT4). The increase in model size significantly increases training costs. Recent publications show that a single training run for GPT-3 could cost $12M.222https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/Even BERT-Large[3]training costs reach tens of thousands of dollars. Furthermore, increased model size place additional computational burden during model execution. These costs can have a detrimental effect on NLP innovation. At the same time, the size of vocabularies has increased only about 100% and the size of embedding vectors has increased about 200%. Hence, over the same period, the fraction of NLP parameters representing the vocabulary has shrunk from 21% in BERT-base[3]to 0.3% in GPT-3[1]. An average person uses 42,000 root words and hundreds of thousands of wordforms[2]. Technical terminology and jargon add tens of thousands of additional words to the vocabulary. Even some of the largest vocabularies currently in use, DeBERTa[4], have only 128,000 tokens. Hence, NLP vocabularies need to include subwords that can be combined to form multiple words[11]. Words that can not be represented using a single token are segmented into an initial subword followed by as many intermediate subwords as required. NLP models use a tokenizer to convert strings of characters into a sequence of lexical tokens. Tokenizers also construct the vocabulary of lexical tokens. We present a novel tokenizer that improves NLP performance by improving subword formation and embedding quality throughsemantic tokenization.
2301.06690v1	Audio2Gestures: Generating Diverse Gestures from Audio	People may perform diverse gestures affected by various mental and physical factors when speaking the same sentences. This inherent one-to-many relationship makes co-speech gesture generation from audio particularly challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, easily resulting in plain/boring motions during inference. So we propose to explicitly model the one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code is expected to be responsible for the motion component that is more correlated to the audio while the motion-specific code is expected to capture diverse motion information that is more independent of the audio. However, splitting the latent code into two parts poses extra training difficulties. Several crucial training losses/strategies, including relaxed motion loss, bicycle constraint, and diversity loss, are designed to better train the VAE.   Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than previous state-of-the-art methods, quantitatively and qualitatively. Besides, our formulation is compatible with discrete cosine transformation (DCT) modeling and other popular backbones (\textit{i.e.} RNN, Transformer). As for motion losses and quantitative motion evaluation, we find structured losses/metrics (\textit{e.g.} STFT) that consider temporal and/or spatial context complement the most commonly used point-wise losses (\textit{e.g.} PCK), resulting in better motion dynamics and more nuanced motion details. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline.	Generating vivid human-like co-speech gestures is of great importance for producing attractive avatars that people are willing to interact with. There has been a surging demand for generating realistic human motions for given audio clips recently. However, this problem is very challenging because of the complicated one-to-many relationship between audio and motion. For example, a speaker may perform different gestures under different conditions (e.g.happy/peaceful mood, standing/sitting state, or different environments) when speaking the same words due to different mental and physical states. Existing algorithms developed for audio to body dynamics have some obvious limitations. For example,[speech2gesture]adapts a fully convolutional neural network to co-speech gesture synthesis tasks. Nevertheless, their model tends to predict averaged motion and thus generates motions lacking diversity. This is due to the underlying one-to-one mapping assumption of their model, which ignores that the relationship between speech and co-speech gesture is one-to-many in nature. Under such an overly simplified assumption, the model has no choice but to learn the averaged motion when several motions match almost the same audio clips in order to minimize the error. The above evidence inspires us to study whether or not explicitly modeling this multimodality improves the overall motion quality. To enhance the regression capability, we introduce an extra motion-specific latent code. With this varyingfulllatent code, which contains the same shared code and varying motion-specific code, the decoder can regress different motion targets well for the same audio, achieving one-to-many mapping results. Under this formulation, the shared code extracted from audio input serves as part of the control signal. The motion-specific code further modulates the audio-controlled motion, enabling multimodal motion generation. Although this formulation is straightforward, it is not trivial to make it work as expected. Firstly, there exists an easy degenerated solution since the motion decoder could utilize only the motion-specific code to reconstruct the motion. Secondly, we need to generate the motion-specific code since we do not have access to the target motion during inference. Our solution to the aforementioned problems is providingrandom noiseto the motion-specific code so that the decoder has to utilize the deterministic information contained in the shared code to reconstruct the target. But under this circumstance, it is unsuitable for forcing the motion decoder to reconstruct the exact original target motion anymore. So arelaxed motion lossis proposed to apply to the motions generated with random motion-specific code. Specifically, it only penalizes the joints deviating from their targets larger than a threshold. This loss encourages the motion-specific code to tune the final motion while respecting the shared code’s control. A preliminary version of this work was presented in[audio2gestures]. In this paper, we extend it from the following aspects: (1) We thoroughly investigate structured and perceptual metrics (i.e.STFT, SSIM, LPIPS, and FID) as training losses in our framework. We find them complementary to the previous point-wise losses since they additionally consider local temporal and/or spatial data structures. Especially with STFT, our network can consistently yield higher-quality motions. (2) We improve the user controllability by switching the latent space into discrete cosine transform (DCT) space, where different DCT components can control different dynamics of the generated motions. And we find the audio-motion shared code is more related to motion speed and rhythm while motion-specific code is more related to small range variations. (3) We conduct extensive ablation studies on different network backbones and hyper-parameters of motion losses and present more detailed discussions and analysis on the observations. The overall contributions are summarized as follows: We present a co-speech gesture generation model whose latent space is split into audio-motion shared code and motion-specific code, to better model the training data pairs and generate diverse motions. We propose a new relaxed motion loss, accompanied by other training losses/strategies, to better avoid degeneration of the proposed network, enabling multimodal motion generation given the same audio input. The effectiveness of the proposed method has been verified on 3D and 2D gesture generation tasks by comparing it with several state-of-the-art methods. And this split formulation is compatible with DCT space modeling and other backbones (i.e.GRU, Transformer), and robust to hyper-parameter choice in the relaxed motion loss. In complement to the most commonly used point-wise metrics/losses, we analyze the generated motion by taking local spatial and/or temporal structure into consideration and introduce them as training losses to further improve the motion quality. As a byproduct, the proposed method is suitable for motion synthesis from annotations since it can well respect the predefined actions on the timeline by simply using their corresponding motion-specific code.
2301.11233v2	BiBench: Benchmarking and Analyzing Network Binarization	Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support. The results and analysis also lead to a promising paradigm for accurate and efficient binarization. We believe that BiBench will contribute to the broader adoption of binarization and serve as a foundation for future research. The code for our BiBench is released https://github.com/htqin/BiBench .	The rising of deep learning leads to the persistent contradiction between larger models and the limitations of deployment resources. Compression technologies have been widely studied to address this issue, including quantization(Gonget al.,2014; Wuet al.,2016; Vanhouckeet al.,2011; Guptaet al.,2015), pruning(Hanet al.,2015,2016; Heet al.,2017), distillation(Hintonet al.,2015; Xuet al.,2018; Chenet al.,2018; Yimet al.,2017; Zagoruyko and Komodakis,2017), lightweight architecture design(Howardet al.,2017; Sandleret al.,2018; Zhanget al.,2018b; Maet al.,2018), and low-rank decomposition(Dentonet al.,2014; Lebedevet al.,2015; Jaderberget al.,2014; Lebedev and Lempitsky,2016). These technologies are essential for the practical application of deep learning. As a compression approach that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology(Rusciet al.,2020; Choukrounet al.,2019; Qinet al.,2022a; Shanget al.,2022b; Zhanget al.,2022b; Bethgeet al.,2020,2019; Martinezet al.,2019; Helwegenet al.,2019). The binarized models take little storage and memory and accelerate the inference by efficient bitwise operations. Compared to other compression technologies like pruning and architecture design, network binarization has potent topological generics, as it only applies to parameters. As a result, it is widely studied in academic research as a standalone compression technique rather than just a 1-bit specialization of quantization(Gonget al.,2019; Gholamiet al.,2021). Some state-of-the-art (SOTA) binarization algorithms have even achieved full-precision performance with binarized models on large-scale tasks(Denget al.,2009; Liuet al.,2020). However, existing network binarization is still far from practical, and two worrisome trends appear in current research: Trend-1: Accuracy comparison scope is limited.In recent research, several image classification tasks (such as CIFAR-10 and ImageNet) have become standard options for comparing accuracy among different binarization algorithms. While this helps to clearly and fairly compare accuracy performance, it causes most binarization algorithms to be only engineered for image inputs (2D visual modality), and their insights and conclusions are rarely verified in other modalities and tasks. The use of monotonic tasks also hinders a comprehensive evaluation from an architectural perspective. Furthermore, data noise, such as corruption(Hendrycks and Dietterich,2018), is a common problem on low-cost edge devices and is widely studied in compression, but this situation is hardly considered existing binarization algorithms. Trend-2: Efficiency analysis remains theoretical.Network binarization is widely recognized for its significant storage and computation savings, with theoretical savings of up to 32\timesand 64\timesfor convolutions, respectively(Rastegariet al.,2016; Baiet al.,2021). However, these efficiency claims lack experimental evidence due to the lack of hardware library support for deploying binarized models on real-world hardware. Additionally, the training efficiency of binarization algorithms is often ignored in current research, leading to negative phenomena during the training of binary networks, such as increased demand for computation resources and time consumption, sensitivity to hyperparameters, and the need for detailed optimization tuning. This paper presentsBiBench, a networkbinarizationbenchmark designed to evaluate binarization algorithms comprehensively in terms of accuracy and efficiency (Table1). Using BiBench, we select 8 representative binarization algorithms that are extensively influential and function at the operator level (details and selection criteria are in AppendixA.1) and benchmark algorithms on 9 deep learning datasets, 13 neural architectures, 2 deployment libraries, 14 hardware chips, and various hyperparameter settings. We invested approximately 4 GPU years of computation time in creating BiBench, intending to promote a comprehensive evaluation of network binarization from both accuracy and efficiency perspectives. We also provide in-depth analysis of the benchmark results, uncovering insights and offering suggestions for designing practical binarization algorithms. We emphasize that our BiBench includes the following significant contributions: (1)Thefirstsystematic benchmark enables a new view to quantitatively evaluate binarization algorithms at the operator level.BiBench is the first effort to facilitate systematic and comprehensive comparisons between binarized algorithms. It provides a brand new perspective to decouple the binarized operators from the neural architectures for quantitative evaluations at the generic operator level. (2)Revealing a practical binarized operator design paradigm.BiBench reveals a practical paradigm of binarized operator designing. Based on the systemic and quantitative evaluation, superior techniques for more satisfactory binarization operators can emerge, which is essential for pushing binarization algorithms to be accurate and efficient. A more detailed discussion is in AppendixA.5.
2304.13255v1	SHIELD: Thwarting Code Authorship Attribution	Authorship attribution has become increasingly accurate, posing a serious privacy risk for programmers who wish to remain anonymous. In this paper, we introduce SHIELD to examine the robustness of different code authorship attribution approaches against adversarial code examples. We define four attacks on attribution techniques, which include targeted and non-targeted attacks, and realize them using adversarial code perturbation. We experiment with a dataset of 200 programmers from the Google Code Jam competition to validate our methods targeting six state-of-the-art authorship attribution methods that adopt a variety of techniques for extracting authorship traits from source-code, including RNN, CNN, and code stylometry. Our experiments demonstrate the vulnerability of current authorship attribution methods against adversarial attacks. For the non-targeted attack, our experiments demonstrate the vulnerability of current authorship attribution methods against the attack with an attack success rate exceeds 98.5\% accompanied by a degradation of the identification confidence that exceeds 13\%. For the targeted attacks, we show the possibility of impersonating a programmer using targeted-adversarial perturbations with a success rate ranging from 66\% to 88\% for different authorship attribution techniques under several adversarial scenarios.	Code authorship attribution is the process of recognizing programmers of a given software, and there has been several works on robust and scalable attribution[4,13,23,8,6]. These methods have shown that programmers can be accurately identified by their coding style, making this problem an easy task thanks to the rapid development of code analysis and machine learning techniques. Accurate attribution benefits software forensics and security, especially for identifying malicious code programmers, detecting plagiarism, and settling authorship disputes. However, the process also poses privacy risks for programmers who prefer to stay anonymous. Recent code authorship attribution techniques heavily use machine learning models. While effective, those techniques are prone to manipulations that force the identification models to generate specific desired outputs, e.g., misclassification. One line of work for general machine learning algorithms utilized small perturbations to the input domain, resulting in adversarial examples (AEs) that are similar to the original ones, making it hard to distinguish them and posing a significant threat to machine learning models[24]. Examining AEs in the context of authorship attributions is an under-explored topic. This work introduces SHIELD for generating AEs at the source code level to prevent attribution while preserving code functionality. Such AEs will fool a classifier into misidentifying programmers and lead to targeted attacks, e.g., imitation or mimicking. Investigating such capabilities by examining how prone authorship attribution is to practical AEs allows a finer understanding of the state-of-the-art methods and helps address their shortcomings, especially with the increasing reliance on them for identifying coding style of programmers for security applications[7,13,8]. As a byproduct, our attacks can serve as a building block for maintaining the privacy of programmers in the presence of attribution techniques. SHIELD examines both the non-targeted and targeted attacks. In the non-targeted attacks, known asconfidence reductionormisclassificationattacks, SHIELD manipulates the input source code so that the identification model outputs any other author, i.e.,authorship dodging[28,26], where SHIELD can use this strategy to conceal the code author identity. In the targeted attack, SHIELD manipulates the input so that the identification model outputs a specifictargetauthor, i.e.,authorship imitationorevasion, depending on the adversarial capability and objective. We apply those scenarios to various authorship attribution methods with SHIELD for an in-depth analysis of each method. Although possibly extendable to binaries[14], SHIELD targets source-code authorship attribution for its prevalence. We note that source code-level attacks are challenging, since the generated AEs should be syntactically correct, should preserve the code functionality, and should not be easily detected. Although those attacks can be conducted using code transformation[21,26,19], a process similar to author obfuscation whereby the authorship traits are hidden in the transformed code, code transformation techniques require analyzing and changing the code to the target features across various categories, including layout, lexical, syntactic, control-flow, and data-flow features. However, a code perturbation approach provides an effective alternative for targeted and non-targeted attacks, by generating AEs to meet a specific goal without targeting the features of different categories. Conventionally, adversarial perturbations are applied directly to the input source and not to the feature space, e.g., perturbations in an image, not in the features extracted from that image. However, the code authorship attribution techniques are typically based on the authorship traits extracted from code, assuming a closed system where the feature extraction is not manipulated and requires perturbations at the code level. Therefore, attribution attacks should be designed explicitly using perturbations applied directly to the code. SHIELD injects carefully chosen code samples into the target source code and then obfuscates it using an off-the-shelf obfuscator. Unlike the prior work, where obfuscation is used to conceal the identity of the code author, we use obfuscation to make it difficult for the adversary to recognize or remove the injected code parts statically. Our threat model is more restrictive than without adopting obfuscation because code attribution must also be capable of identifying obfuscated source codes. However, it is well known that the attribution models work even in the obfuscated domain[4,13]. We note that while our AEs are generated at the code level, the eventual effect of the injection will be perturbations in the feature space. For convenience, we use the term perturbation to refer to injection. Contributions.Our key contributions are as follows. (1) We introduce SHIELD, a simple yet effective approach for generating AEs on code authorship attribution. The proposed approach does not change the original code but adds carefully-crafted code blocks (i.e., perturbations) to alter the authorship attributes of the code, leading to authorship dodging and imitation. (2) We provide a comprehensive evaluation of our technique against six state-of-the-art authorship attribution methods: DL-CAIS[4], WE-C-CNN, WE-S-CNN, TF-IDF-C-CNN, and TF-IDF-S-CNN[7], and Code Stylometry[13]. Our evaluation features a large-scale analysis of code authorship robustness under various adversarial scenarios using a dataset of 200 programmers. Our approach achieved a misidentification rate exceeding 98.5% for non-targeted attacks while significantly reducing the confidence of the model output. We also demonstrate imitation attacks at a rate of more than 66% for all targeted systems when using sufficient perturbations and at 88% when the adversary has access to samples of the targeted author. Organization.In SectionII, we provide a brief overview of authorship attribution workflow alongside specific details of the six targeted systems. In SectionIII, we introduce SHIELD and the attack strategy adopted in this work. In SectionIV, we define the non-targeted attacks and present the associated experimental results, followed by the targeted attacks and associated results in SectionV. In SectionVI, we discuss our findings, including the limitations and shortcomings of our approach. In SectionVII, we review the related work. We conclude our work in SectionVIII.
2312.06002v1	Large Language Models on Lexical Semantic Change Detection: An Evaluation	Lexical Semantic Change Detection stands out as one of the few areas where Large Language Models (LLMs) have not been extensively involved. Traditional methods like PPMI, and SGNS remain prevalent in research, alongside newer BERT-based approaches. Despite the comprehensive coverage of various natural language processing domains by LLMs, there is a notable scarcity of literature concerning their application in this specific realm. In this work, we seek to bridge this gap by introducing LLMs into the domain of Lexical Semantic Change Detection. Our work presents novel prompting solutions and a comprehensive evaluation that spans all three generations of language models, contributing to the exploration of LLMs in this research area.	Languages are evolving. A perpetual flux of changes occurs, driven by an array of factors encompassing cultural, social, technological, and often, undiscovered influences. In this ever-shifting linguistic landscape, words shed unused senses while concurrently acquiring new meanings. Languages engage in a reciprocal exchange, borrowing senses from one another, and simultaneously exerting influence. This intricate web of linguistic evolution necessitates an automatic approach to comprehend and assess the fluidity of languages. Automation becomes the key to navigating and interpreting the dynamic currents of linguistic transformation. However, the development of advanced computational methods for Diachronic Lexical Semantic Change (LSC) has been a blank slate for researchers. Since the 2010s, traditional embedding methods like PPMI, SVD, and SGNS have shown significant statistical correlation with human annotators and produced promising results in detecting shifts in word meaning(kulkarni2014statistically;hamilton2018diachronic;schlechtweg_wind_2019). As a result, previous works often lean towards using existing tools to uncover new meaning shifts rather than exploring novel algorithms to enhance them. Additionally, frequency-based algorithms typically depend on large corpora(tahmasebi2019survey). Their performance on relatively low-resource datasets remains a challenge, and an efficient solution for this has yet to be discovered. Since its introduction byvaswani2017attention, models based on the Transformer architecture have become the latest trend. Contextualized word embeddings generated by BERT(devlin2019bert)have provided a solid foundation for various downstream language tasks. Moreover, recently, Large Language Models (LLMs) have showcased remarkable capabilities in logical thinking and solving language tasks based on instructions(bubeck2023sparks;zhao2023survey;openai_gpt-4_2023;roziere_code_2023). This has inspired researchers to embrace LLMs for a modern approach to a series of lexical semantic tasks and explore their ability to understand natural language meanings. In this study111Access the GitHub repository through thislink., we conducted a series of tasks to assess the suitability of LLMs for LSC detection forTempoWiC(loureiro_tempowic_2022), a low-resource annotated tweet dataset. Our key findings are outlined as follows: We reassess the performance oftraditional methods(i.e., PPMI, SGNS, SVD) in addressing diachronic LSC on a low-resource dataset. We introduce a simple yet innovative generative approach for diachronic LSC detection. This method achieves promising results without requiring fine-tuning on pre-trained models. We conduct comprehensive evaluations for LLMs, BERT-based methods, and traditional methods at both the corpus level and instance level, offering insights into their respective capabilities in diachronic LSC detection.
2303.16580v3	Generalized Relation Modeling for Transformer Tracking	Compared with previous two-stream trackers, the recent one-stream tracking pipeline, which allows earlier interaction between the template and search region, has achieved a remarkable performance gain. However, existing one-stream trackers always let the template interact with all parts inside the search region throughout all the encoder layers. This could potentially lead to target-background confusion when the extracted feature representations are not sufficiently discriminative. To alleviate this issue, we propose a generalized relation modeling method based on adaptive token division. The proposed method is a generalized formulation of attention-based relation modeling for Transformer tracking, which inherits the merits of both previous two-stream and one-stream pipelines whilst enabling more flexible relation modeling by selecting appropriate search tokens to interact with template tokens. An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the token division module. Extensive experiments show that our method is superior to the two-stream and one-stream pipelines and achieves state-of-the-art performance on six challenging benchmarks with a real-time running speed.	Given the target bounding box in the initial frame of a video, visual tracking[javed2022visual]aims to localize the target in successive frames. Over the past few years, two-stream trackers[bertinetto2016fully,li2018high,li2019siamrpn++,zhang2020ocean], which extract features of the template and search region separately and then model cross-relations of the template and search region in a sequential fashion, have emerged as a dominant tracking paradigm and made a significant progress. Following this two-stream pipeline, several Transformer-based trackers[wang2021transformer,chen2021transformer,gao2022aiatrack]utilize parallel self-attention blocks to enhance the extracted features by modeling global self-relations within each image as illustrated in Fig.1(a). Recently, leveraging the flexibility of the attention mechanism, the one-stream pipeline[cui2022mixformer,ye2022joint,chen2022backbone]is proposed to jointly extract features and model relations, achieving promising performance. By conducting self-attention among all concatenated tokens, both cross-relation modeling and self-relation modeling can be performed simultaneously as illustrated in Fig.1(b). It is demonstrated in[xie2022correlation,cui2022mixformer,ye2022joint,chen2022backbone]that letting the search region interact with the template as early as possible is beneficial to target-specific feature generation. However, there is no evidence suggesting that all parts inside the search region should always be forced to interact with the template. Actually, due to the cropping strategy[bertinetto2016fully], there is a large proportion of background inside the search region, where distractors with similar appearance to the target may exist. This would lead to undesired cross-relations between the template and search region as the highly discriminative representations have not been extracted in some early layers. Although the attention mechanism can inherently weaken improper cross-relations, applying global cross-relation modeling to all layers may still be more or less disruptive. On the one hand, for the search tokens outside the target region, if undesired cross-relations are modeled between the template and distractors, the aggregated features of the distractors may contain the target features from the template, which could cause confusion for precisely identifying the actual target in the search region. On the other hand, for the template tokens, their quality could also be degraded by undesired cross-relations during the iterative update since certain features from the background or even distractors could be aggregated into these tokens. These situations could weaken the target-background discrimination capability of the one-stream pipeline. Intuitively, only a portion of search tokens,e.g. tokens belonging to the target, are suitable for cross-relation modeling when the feature representations are not perfect for target-background discrimination. In some cases, the two-stream relation modeling pipeline could even be better if the feature representations of both the template and search region are imperfect to model cross-relations. The potential limitations of the one-stream pipeline motivates us to ponder: is it really optimal for the template to interact with all parts inside the search region through all encoder layers in the one-stream pipeline? In this paper, we answer this question by proposing GRM, a generalized relation modeling method that can adaptively select the appropriate search tokens to interact with the template. To be specific, we classify the template and search tokens as three categories. The template tokens form one category while the search tokens are divided into another two categories. Instead of modeling relations within all the tokens as the one-stream pipeline, we restrict the interaction among the three token categories. Only the search tokens that are suitable for cross-relation modeling will interact with the template tokens, whilst the interaction between the remaining search tokens and the template tokens is blocked. With proper divisions, the two-stream pipeline and one-stream pipeline become two degenerated forms of our relation modeling method as discussed in Sec.3.2. Consequently, our method is a generalized formulation of attention-based relation modeling for Transformer tracking, which embraces the advantages of both previous pipelines while being more flexible. The search token division is performed by a lightweight prediction module, which can adaptively determine which search tokens are suitable for cross-relation modeling based on the input tokens. To accomplish this objective, there are two obstacles to overcome. First, the separate relation modeling for different token categories makes it hard for parallel computation. Second, the discrete token categorization is non-differentiable, thus impeding the end-to-end learning of the token division module. To facilitate parallel computation, we adopt an attention masking strategy to unify the individual attention operations into a single one. Additionally, we introduce the Gumbel-softmax technique[jang2016categorical]to make the discrete token categorization differentiable. Consequently, the search token division module can be implicitly optimized in an end-to-end manner, which promotes its adaptability to deal with different situations. In summary, our main contributions are three-fold: We present a generalized formulation of relation modeling for Transformer trackers, which divides the input tokens into three categories and enables more flexible interaction between the template and search region. To realize the generalized relation modeling, we devise a token division module to adaptively classify the input tokens. An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the proposed module. We conduct extensive experiments and analyses to validate the efficacy of our method. The proposed GRM exhibits outstanding results on six challenging visual tracking benchmarks.
2307.02284v1	Absorbing Phase Transitions in Artificial Deep Neural Networks	Theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics.	The 21st century has witnessed the tremendous success of deep learning applications. Properly trained deep neural networks have successfully demonstrated performance comparable with, or even superior to, that of human experts in various tasks, a few remarkable examples being the game of GoSilver2016, image synthesisRombach2022, and natural language processingStahlberg2020. Boosted by an exciting discovery of the so-called neural network scaling lawsHenighan2020;Kaplan2020, the frenetic pace in improving neural network performance is likely to persist, and hence it may be safe to say that deep learning technologies will constitute indispensable building blocks of the next-generation human society. Despite the fact that practically deep learning models can achieve such impressive performances, theoretically their behaviors are not yet fully understood. Deep neural networks are usually heavily over-parametrized, with the number of parameters in state-of-the-art neural networks growing exponentially over timeXu2018. From an energetic viewpoint, the state-of-the-art deep learning models consume a lot of energy, as the number of parameters is correlated to the amount of energy needed to perform an inference. In contrast, human brains seem to be good at learning and generalizing in an energetically efficient manner, even though strictly speaking they are generally not at doing arithmetic operations. This suggests that placing and comparing artificial neural networks in a broader context of biological neural networks on an equal footing, at least from a functional perspective, is promising for developing their understanding. The notion ofcriticalityis the key to linking biological and artificial neural networks. Systems at a particular condition (e.g. at the critical point of second-order phase transitions) exhibit anomalous behavior, referred to ascritical phenomena. They are universal in the sense that microscopically diverse systems can be described by a single mathematical model as long as the essential properties remain unchanged. The critical phenomena of particular interest in neuroscience are those ofabsorbing phase transitionsHenkel2008;Hinrichsen2000: transitions to a state from which a system cannot escape (hereafter referred to as “an absorbing state”). Besides the obvious analogy with brains without any neuronal activity (i.e. death), absorbing phase transitions are considered to be one of the essential ingredients for self-organized criticalityDickman1998, by which the systems can be automatically tuned to the critical point. Recent theoretical and experimental studies support the view that the brains may operate near the critical point (albeit in a slightly nuanced manner), and the universal scaling law in the critical phenomena of absorbing phase transition has been attracting considerable interest among the community; interested readers are referred to, for example, the recent review by Girardi-SchappoGirardi-Schappo2021. As a matter of fact, the deep learning research community is also familiar (albeit implicitly) with the notion of criticality. In theoretical studies on deep neural networks, the concept ofthe edge of chaoshas played a considerable role. While the discovery of chaos in random neural networks dates back to (at least) as early as the late 1980sSompolinsky1988, the concept has attracted recent interest among the community when Pooleet al. theoretically demonstrated that infinitely-wide deep neural networks also exhibit the order-to-chaos transitionPoole2016. Remarkably, at the onset of chaos,trainabledepth of the networks is suggested to divergeSchoenholz2017, which is reminiscent of the divergence of the correlation length at the critical point of second-order phase transitions at equilibrium. Furthermore, recent work has successfully applied the renormalization group method to classify the order-to-chaos transitions in the fully-connected feedforward neural networks for various activation functions into a small number of universality classesRoberts2022. Nevertheless, we argue that the notion of criticality has not been fully exploited in studies of artificial deep neural networks. As also discussed by Hesse and GrossHesse2014, bottom-up approaches (in which one derives macroscopic properties from microscopic theories) and top-down ones (in which one starts from phenomenological observations or some heuristics to deduce macroscopic properties) are complementary to each other for studying complicated systems. Numerous works, including those cited in the previous paragraph, have successfully adopted one of the bottom-up approaches for a specific architecture and/or an activation function. However, the situation with regard to the top-down approaches is less satisfactory. Since the universality of the critical phenomena enables the classification of the systems into a reduced number of universality classes based on their fundamental properties, taking full advantage of it would lead us to intuitive and yet powerful understanding of the behavior of deep neural networks across different architectures. Given all these observations, the purpose of the present work is to demonstrate that the notion of absorbing phase transition is a promising tool for theoretical understanding of the deep neural networks. First, we establish an analogy between the aforementioned order-to-chaos transition and an absorbing phase transition by studying the linear stability of the ordered state. In the framework of the mean-field theory of signal propagation in deep neural networksPoole2016, the critical point is characterized by loss of linear stability of the fixed point corresponding to the ordered phase. We extend the analysis to the networks of finite width, and we directly see that the transition to chaos in artificial deep neural networks is an emergent property of the networks which requires the participation of sufficiently many neurons (and thus more appropriately seen as a phase transition, rather than a mere bifurcation in dynamical systems). Next, we show that the order-to-chaos transitions in initialized artificial deep neural networks exhibit the universal scaling laws of absorbing phase transition. Actually it is fairly straightforward to find the scaling exponents associated with the transition in the framework of the mean-field theory (or equivalently in the infinitely-wide networks) for the fully-connected feedforward neural networksSchoenholz2017, but it is not clear how we can extend the analysis into the networks of finite width or a different architecture. Our empirical study reveals that the idea of the universal scaling can still be successfully applied to such cases. We also provide an intuitive way to understand the resulting universality class for each architecture, based on a phenomenological theory. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenological argument could lead us to semi-quantitative description of the signal propagation dynamics in the finite networks. To summarize, we believe that the this work places the order-to-chaos transition in the initialized artificial deep neural networks in the broader context of absorbing phase transitions, and serves as the first step toward the systematic comparison between natural/biological and artificial neural networks.
2311.08524v1	Cross-dataset domain adaptation for the classification COVID-19 using chest computed tomography images	Detecting COVID-19 patients using Computed Tomography (CT) images of the lungs is an active area of research. Datasets of CT images from COVID-19 patients are becoming available. Deep learning (DL) solutions and in particular Convolutional Neural Networks (CNN) have achieved impressive results for the classification of COVID-19 CT images, but only when the training and testing take place within the same dataset. Work on the cross-dataset problem is still limited and the achieved results are low. Our work tackles the cross-dataset problem through a Domain Adaptation (DA) technique with deep learning. Our proposed solution, COVID19-DANet, is based on pre-trained CNN backbone for feature extraction. For this task, we select the pre-trained Efficientnet-B3 CNN because it has achieved impressive classification accuracy in previous work. The backbone CNN is followed by a prototypical layer which is a concept borrowed from prototypical networks in few-shot learning (FSL). It computes a cosine distance between given samples and the class prototypes and then converts them to class probabilities using the Softmax function. To train the COVID19-DANet model, we propose a combined loss function that is composed of the standard cross-entropy loss for class discrimination and another entropy loss computed over the unlabelled target set only. This so-called unlabelled target entropy loss is minimized and maximized in an alternative fashion, to reach the two objectives of class discrimination and domain invariance. COVID19-DANet is tested under four cross-dataset scenarios using the SARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging results compared to recent work in the literature.	On December 31, 2019, the World Health Organization (WHO) reported unknown cases of respiratory diseases that have spread in Wuhan, China[29]. The disease is identified as a new virus part of the Coronavirus family that can cause illnesses ranging from the common cold to more serious respiratory diseases. The new virus was later known as COVID-19. On January 30, 2020, due to the spread of this disease in China and many other parts of the world, it was classified as a public health emergency by the WHO[62]. COVID-19 is a respiratory illness with symptoms similar to typical influenza. The transcription-polymerase chain reaction (RT-PCR) laboratory test is used as a reference tool for diagnosing COVID-19. In addition, X-rays and chest Computed Tomography (CT) scans are considered to be new information technology (IT) tools for COVID-19 diagnostics. What distinguishes the IT approach is its interpretability, which can help in fast decisions taken by doctors regarding COVID-19. CT is a painless and non-surgical imaging method characterized by speed and high accuracy. CT uses advanced X-ray technology to help detect many diseases and obtain detailed images of bones, internal tissues, and organs, where CT images give more details than traditional X-rays. Body parts absorb X-rays in unequal ways which allow the doctor to distinguish body parts and any changes due to disease[44]. Figure1shows examples of CT images from COVID-19 patients. Ai et al.[2]report on 1014 patients who received both PCR and CT scans in Wuhan, China, during the epidemic. They found that 90% of confirmed diagnostics of PCR had clear signs of COVID19 in chest CT scans appearing in the form of bilateral opacity. In another study, chest CT revealed bilateral opacity in the lung in 40 out of 41 patients (98%) with COVID-19 in Wuhan[54]. In addition, they show a high rate of appearance of ground glass opacity and uniformity with the round shape or occasional peripheral pulmonary distribution[54]. The probability of error in PCR tests, their limitations, and the length of time of their results, especially in areas affected by epidemic[63], as well as the low accuracy and sensitivity of X-rays in COVID-19 diagnosis, all made CT scans of the chest a very promising diagnostic tool for COVID19[8]. Recently, advances in computer vision and Machine Learning (ML) have led to the emergence of a novel generation of techniques in computer-aided disease diagnosis (CAD)[41]. In particular, Deep Learning (DL) in medical imaging has achieved outstanding performance in disease diagnosis and follow-up. DL has proven its important role and efficiency in medical image processing including classification, detection and segmentation tasks[5]. In this Chapter, we propose a DL method for the diagnosis of COVID-19 disease using chest CT scans. In particular, we focus on the problem of removing the need for labeling large amounts of data to train DL models for COVID-19 detection. In machine learning, deep CNN models provide excellent results with large amounts of labeled data. However, this is neither reasonable nor practical. Ideally, our ultimate goal is to design a model that can provide good classification results for new datasets without the need for a large labeling effort. One solution is cross-dataset learning, where we transfer knowledge from one labeled dataset to another unlabeled dataset. This is also known as domain adaptation (DA) in which we develop learning models that can intelligently adapt from one source domain (dataset) to another target domain. To our knowledge, this is the first work that addresses the problem of DA in COVID-19 detection. First, recall that the basic assumption of many machine learning algorithms is that training (source) and test (target) data come from the same distribution. However, in DA, the training and testing data come from different datasets taken under different circumstances which disproves the validity of this assumption. The distribution of data between source (training) and target (test) domains may change, causing low classification accuracy on the target data. The difference in distribution between different domains is still a very relevant problem among medical image datasets due to different image acquisition machines and circumstances. As a result, there is increased interest in DA within the field of medical images to solve this problem and improve classification performance. But in the area of COVID-19 detection, the DA research is still very limited[52,22]. In this objective, our proposed method is based on a new family of powerful CNN models called EfficientNet and on DA techniques to transfer knowledge from one domain to another. Our main contributions can be summarized in the following points: We present a DA method, called COVID19-DANet that can adapt a DL model from a source COVID-19 dataset to a target dataset. It uses the unlabeled samples from the target dataset to reduce the data distribution shift between the source and target datasets. The proposed DA method uses EfficientNet-B3 CNN as a feature extractor and a classification layer inspired by prototypical networks from the few-shot learning area. Inspired by the semi-supervised learning methods proposed in the machine learning literature, COVID19-DANet uses the entropy of the output probabilities over the unlabelled target set as a loss function for reducing the distribution shift between domains. The remainder of the chapter is organized as follows. In Section2, we review some related work using CT images for COVID-19 detection. We also survey some approaches based on DA learning, especially its field of application and typical setup. In Section3, we present the proposed DL-based DA model for COVID19 CT classification. Next, we present our experimental results in section4in terms of the most relevant assessment metrics. Finally, we outline our concluding remarks and future work suggestions in Section5.
2304.02724v1	Exploring the Utility of Self-Supervised Pretraining Strategies for the Detection of Absent Lung Sliding in M-Mode Lung Ultrasound	Self-supervised pretraining has been observed to improve performance in supervised learning tasks in medical imaging. This study investigates the utility of self-supervised pretraining prior to conducting supervised fine-tuning for the downstream task of lung sliding classification in M-mode lung ultrasound images. We propose a novel pairwise relationship that couples M-mode images constructed from the same B-mode image and investigate the utility of data augmentation procedure specific to M-mode lung ultrasound. The results indicate that self-supervised pretraining yields better performance than full supervision, most notably for feature extractors not initialized with ImageNet-pretrained weights. Moreover, we observe that including a vast volume of unlabelled data results in improved performance on external validation datasets, underscoring the value of self-supervision for improving generalizability in automatic ultrasound interpretation. To the authors' best knowledge, this study is the first to characterize the influence of self-supervised pretraining for M-mode ultrasound.	Pneumothorax (PTX) is a potentially life-threatening acute condition in which air occupies the space between the pleura of the lungs, resulting in collapse of the lung. Rapid identification of PTX is crucial in emergency, critical, and acute care settings to expedite medical intervention. Point-of-care lung ultrasound (LUS) is a quick, inexpensive, portable, imaging examination that does not expose patients to radiation. Despite its low prevalence compared to chest radiographs, LUS has been shown to exhibit superior diagnostic performance for the diagnosis of PTX[Nagarsheth2011,Alrajhi2012]. The lung sliding artefact, caused by the normal motion of the pleura, has been described as a means to rule out PTX[Lichtenstein1995]. Notably, the presence of lung sliding excludes a diagnosis of PTX within the purview of the ultrasound probe[Lichtenstein1995]. Conversely, PTX is likely present when lung sliding is absent. Previous studies have demonstrated that deep convolutional neural networks (CNN) can be trained to distinguish between the presence and absence of lung sliding on motion mode (M-mode) ultrasound images[Javsvcur2021,VanBerlo2022]. Prior studies were limited by the amount of labelled data available for training and evaluation. Furthermore, previous studies initialized their networks using weights pretrained on the ImageNet dataset[Deng2009]. Despite the fact that M-mode images are profoundly distinct from the natural images present in ImageNet, it is common for medical imaging studies to leverage ImageNet-pretrained weights. They are publicly available for several common architectures and are able to extract low-level features present in medical images. Unfortunately, there are no publicly available equivalents for M-mode images, let alone LUS. Self-supervised learning (SSL) is a representation learning strategy applicable in the absence of labelled data. CNNs pretrained using SSL have exhibited superior performance and label efficiency compared to fully supervised counterparts[Chen2020,Grill2020,Zbontar2021,Bardes2022]. Broadly, SSL pretraining consists of training a deep neural network to solve apretext task, whose solution can be computed from unlabelled examples. The pretrained weights may be fine-tuned to solve adownstream taskfor which labels are present. This study explores the impact of self-supervised pretraining for the downstream task of detecting absent lung sliding in M-mode LUS, varying the choice of SSL method, weight initialization, data augmentation strategy, and inclusion of unlabelled data. Crucially, we demonstrate that incorporating large volumes of unlabelled M-mode images during the pretraining phase improves the performance of a fine-tuned classifier on external datasets. More specifically, our major contributions are as follows: A pairwise relationship for contrastive and non-contrastive learning that is specific to M-mode images A data augmentation pipeline specific to M-mode images in the context of pretraining A comprehensive investigation of factors that influence the utility of SSL pretraining for the downstream task of absent lung sliding detection, such as label efficiency, ImageNet initialization, and data augmentation Evidence that the inclusion of unlabelled data results in improved generalization to external datasets for absent lung sliding detection Fig.1summarizes our methods. To the best of our knowledge, no study has investigated the efficacy of SSL for M-mode ultrasound tasks.
2308.10112v1	PDL: Regularizing Multiple Instance Learning with Progressive Dropout Layers	Multiple instance learning (MIL) was a weakly supervised learning approach that sought to assign binary class labels to collections of instances known as bags. However, due to their weak supervision nature, the MIL methods were susceptible to overfitting and required assistance in developing comprehensive representations of target instances. While regularization typically effectively combated overfitting, its integration with the MIL model has been frequently overlooked in prior studies. Meanwhile, current regularization methods for MIL have shown limitations in their capacity to uncover a diverse array of representations. In this study, we delve into the realm of regularization within the MIL model, presenting a novel approach in the form of a Progressive Dropout Layer (PDL). We aim to not only address overfitting but also empower the MIL model in uncovering intricate and impactful feature representations. The proposed method was orthogonal to existing MIL methods and could be easily integrated into them to boost performance. Our extensive evaluation across a range of MIL benchmark datasets demonstrated that the incorporation of the PDL into multiple MIL methods not only elevated their classification performance but also augmented their potential for weakly-supervised feature localizations.	[2]\lipsum[3]
2310.05241v1	SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval	Video moment retrieval aims to localize moments in video corresponding to a given language query. To avoid the expensive cost of annotating the temporal moments, weakly-supervised VMR (wsVMR) systems have been studied. For such systems, generating a number of proposals as moment candidates and then selecting the most appropriate proposal has been a popular approach. These proposals are assumed to contain many distinguishable scenes in a video as candidates. However, existing proposals of wsVMR systems do not respect the varying numbers of scenes in each video, where the proposals are heuristically determined irrespective of the video. We argue that the retrieval system should be able to counter the complexities caused by varying numbers of scenes in each video. To this end, we present a novel concept of a retrieval system referred to as Scene Complexity Aware Network (SCANet), which measures the `scene complexity' of multiple scenes in each video and generates adaptive proposals responding to variable complexities of scenes in each video. Experimental results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR) achieve state-of-the-art performances and demonstrate the effectiveness of incorporating the scene complexity.	Video search has the core building block of recently growing video streaming services (\egYouTube, Netflix). To enhance the capability of video search, video moment retrieval (VMR) aims to localize the start and end time of the moment pertinent to a given language query in an untrimmed video. The success of the VMR provides us with accurate video contextual information in less time and effort. Until recently, these remarkable search performances have been dependent on the size and quality of labeled training datasets. However, these datasets cost a labor-intensive annotating process (\ieAnnotators should find the start-end time of moments corresponding to query descriptions), and sometimes the annotated moments are ambiguous. To cope with this problem, many weakly-supervised VMR (wsVMR) methods[mithun2019weakly,duan2018weakly,zhang2020counterfactual,zheng2022weakly2]have been proposed by only utilizing the video-query pairs, which are less laborious to annotate. To perform the weak supervision using video-query pairs, if one query is paired (\ieannotated) with multiple videos, we can identify the common scene among these videos and determine the alignment between the query and the scene. To implement this, all videos are divided into multiple segments, and the retrieval system maximizes the similarity scores between each query and paired segments while suppressing the scores between the query and unpaired segments in other videos. During the inference, the system selects a segment with the highest score as a moment prediction for a given query. For the wsVMR systems to accurately classify the best segment in a video, numerous video-language joint representation learning methods[lin2020weakly,song2020weakly,zheng2022weakly2,wang2021visual]have been proposed. Recently, researchers also have another focus on a study of how to generate video segments to capture many scenes in a video[ma2020vlanet,zheng2022weakly2]. These segments are referred to as ‘candidate moment proposals’, which is crucial, as they directly affect the retrieval performances by regulating the proposal quantities. Unfortunately, as supervision is not available in generating proposals, wsVMR systems[zheng2022weakly1,zheng2022weakly2]use a fixed number of proposals for all input videos under heuristic optimization of a specific dataset, which is not reasonable to deal with varying numbers of scenes in a video. While some methods[ma2020vlanet,huang2021cross]consider varying numbers of proposals for each video, they still rely on spurious correlations, such as generating proposals proportionally to the video length or using sliding window. Therefore, the current proposal generation method could not accurately respond to the diverse number of scenes in each video. We refer to this situation as a ‘scene-proposal mismatch’. For instance, in Figure1(a), the systems produce an unnecessarily large number of proposals by referring to the long length of the video, but the video only contains a single scene (\iescene of sitting still in a chair throughout the video), which should be handled by small amounts of proposals. They also show scene-proposal mismatch by producing a small number of proposals for the video containing many scenes, such that those scarce proposals seem not to work correctly. Our experimental evidence in Figure1(b) validates the current wsVMR systems’ incorrectness due to the scene-proposal mismatch. We plot performances (\iemean Intersection over Union (mIoU) scores) over predictions along the number of scenes in videos and the number of proposals generated, which shows irregularities in the scores. The scores are low for videos with many scenes but few proposals and also low for videos with few scenes but many proposals. To estimate the number of scenes in a video, as shown in Figure1(c), we counted the number of paired queries for each video as a discrete approximation of the scene. Here, we found that some queries describing the same scene led to redundancy in the counting. Thus, we remove the redundancy of those queries via calculating their IoUs111We remove redundancy by scenes with IoU>0.5.between temporal boundary annotations222Temporal annotations are used only for identifying the proposal-scene mismatch problem and they are not involved in the wsVMR task. Our study further showed that the scene-proposal mismatch affects about 41% of videos in VMR benchmarks (\ieCharades-STA[gao2017tall], ActivityNet-Caption[krishna2017dense]). Intrigued by the scene-proposal mismatch, this paper proposes a wsVMR system referred to as Scene Complexity Aware Network (SCANet), which allows the system to mitigate the scene-proposal mismatch problem and generate proposals adaptive to the complexity of the scenes contained in the video. For a given input video, SCANet first defines the scene complexity with a scalar, meaning how difficult for the system to find (\ieretrieve) a specific scene among multiple distinguishable scenes in the video, which can be effective prior knowledge of video by complementing weak supervision of VMR. On top of the scene complexity, SCANet adaptively generates proposals and enhances their representations. Therefore, SCANet incorporates (1) Complexity-Adaptive Proposal Generation (CPG) that generates adaptive proposals by leveraging the quantities of proposals under consideration of the complexity and (2) Complexity-Adaptive Proposal Enhancement (CPE) that enhances the proposals’ representations corresponding to the scene complexity. Furthermore, motivated by recent success[zhang2020counterfactual,zheng2022weakly2]of contrastive learning for wsVMR system, we introduce technical contributions to mine hard negatives in the input video and further video corpus together under our designed framework. Our extensive experiments show the effectiveness of the proposed SCANet, and qualitative results validate enhanced interpretability.
2309.08167v1	Differentiable Resolution Compression and Alignment for Efficient Video Classification and Retrieval	Optimizing video inference efficiency has become increasingly important with the growing demand for video analysis in various fields. Some existing methods achieve high efficiency by explicit discard of spatial or temporal information, which poses challenges in fast-changing and fine-grained scenarios. To address these issues, we propose an efficient video representation network with Differentiable Resolution Compression and Alignment mechanism, which compresses non-essential information in the early stage of the network to reduce computational costs while maintaining consistent temporal correlations. Specifically, we leverage a Differentiable Context-aware Compression Module to encode the saliency and non-saliency frame features, refining and updating the features into a high-low resolution video sequence. To process the new sequence, we introduce a new Resolution-Align Transformer Layer to capture global temporal correlations among frame features with different resolutions, while reducing spatial computation costs quadratically by utilizing fewer spatial tokens in low-resolution non-saliency frames. The entire network can be end-to-end optimized via the integration of the differentiable compression module. Experimental results show that our method achieves the best trade-off between efficiency and performance on near-duplicate video retrieval and competitive results on dynamic video classification compared to state-of-the-art methods. Code:https://github.com/dun-research/DRCA	Video representation learning is a crucial research topic due to its numerous applications, such as recommendation system, video search, etc. Due to the high redundancy across video frames, a judicious and effective strategy to expedite video inference involves mitigating redundant computations. Some existingskipping-basedmethods[7,28,3,26,23]propose to skip non-saliency frames to save computation costs via various temporal saliency sampling strategies. These methods are based on the assumption that the most saliency frame/regions contribute the most to the video representation. However, due to the inevitable loss of spatial information and inaccurate saliency measures, these methods encounter challenges in some fast-changing and fine-grained scenarios. For example, discriminating between different dancing types is concealed within the gradual transitions of each frame. Besides, for tasks that rely heavily on detailed information, such as video retrieval, discarding non-saliency frames may result in losing essential clues for accurate matching. In contrast, somemulti-network-basedmethods[19,27,25]have been proposed to retain non-saliency frames while using multiple networks with different computational costs to mitigate the overall expenses. Although fusion layers are designed to integrate multiple streams of information, extracting global temporal information may be inadequate in representing fine-grained information to achieve precise contextual understanding. Besides, several of the above algorithms select different numbers of saliency frames based on the video content, making batched inference complicated and challenging. To address these issues, in this paper, we cast efficient video learning as a Differentiable Resolution Compression and Alignment(DRCA)task, which consists of two main components to accomplish resolution compression and alignment. We instantiate the two components asDCCM(Differentiable Context-aware Compression Module) andRAT-Layer(Resolution-Align Transformer Layer). Specifically, to reduce video tokens and minimize information loss, unlike previous methods[27,28,25]that use an additional network to measure frame saliency separately, ourDCCMincorporates the saliency assessment and frame compression directly into the network, enabling them to be not only context-aware but also learnable. Concretely, the DCCM can be end-to-end optimized through differentiable ranking, which solves an optimal saliency sorting sequence based on the predicted scores. By reducing the resolution of non-salient frames and decreasing the number of spatial tokens, significant reductions in spatial computational costs can be achieved, as the computational complexity of the Transformer increases quadratically for the number of tokens. Meanwhile, this also poses a challenge for existing networks like 3D convolutional neural networks[6,17]and Transformers[1,20,21], as they lack the capability to directly extract features in this multi-resolution context. Therefore, we propose a simple but effectiveRAT Layerto process the frame tokens with different resolutions. The RAT Layer can extract strong spatial-temporal correlations layer-by-layer, which is crucial for video tasks that require reasoning, such as retrieving the same incident, where some video frames captured by different photographers are visually similar but not identical. Notably, this novel approach enables efficient and accurate video representation learning while preserving the ability of batched inference with fixed token numbers. This is achieved by fully end-to-end optimizing integrated with a differentiable compression module. As a result, our method achieves the best trade-off between efficiency and performance compared to state-of-the-art (SOTA) methods in near-duplicated video retrieval(NDVR). Additionally, it demonstrates competitive performance in dynamic video classification compared to other SOTA methods. Our contributions are summarized as follows:(1) a novelDifferentiable Resolution Compression and Alignmentnetwork for efficient video learning(2) a novelDCCMdesigned to compress the non-essential information with differentiable ranking(3) a simple but effectRAT Layerto extract spatial-temporal correlations in multi-resolution compressed video sequence(4) achieving the best trade-off between efficiency and performance on NDVR task and competitive result on dynamic video classification task against SOTA.
2305.02360v1	Fashionpedia-Ads: Do Your Favorite Advertisements Reveal Your Fashion Taste?	Consumers are exposed to advertisements across many different domains on the internet, such as fashion, beauty, car, food, and others. On the other hand, fashion represents second highest e-commerce shopping category. Does consumer digital record behavior on various fashion ad images reveal their fashion taste? Does ads from other domains infer their fashion taste as well? In this paper, we study the correlation between advertisements and fashion taste. Towards this goal, we introduce a new dataset, Fashionpedia-Ads, which asks subjects to provide their preferences on both ad (fashion, beauty, car, and dessert) and fashion product (social network and e-commerce style) images. Furthermore, we exhaustively collect and annotate the emotional, visual and textual information on the ad images from multi-perspectives (abstractive level, physical level, captions, and brands). We open-source Fashionpedia-Ads to enable future studies and encourage more approaches to interpretability research between advertisements and fashion taste.	It is understandable that there could be some correlation between ads and products for a same domain. For example, a user likes the style of a neckline in a fashion ads and might also like a fashion product that has similar style (Fig.1). However, is there any correlation between ads and products from different domains? Specifically, can we interpret a consumer’s product preference from her website browsing logs of various advertising domains? In the context of fashion online shopping, however, to our knowledge, no study has investigated the correlation between various ads domain and fashion taste on the consumer level, as shown in Fig.2. In this paper, we introduce a new user taste understanding dataset, Fashionpedia-Ads, which asks subjects to provide their preference on both ad images of various domain (fashion, beauty, car, food) and fashion product images. Furthermore, unlike fashion product images, ads images usually contains complicated and multiple perspectives of information (emotional, visually, textually…) that cause a consumer like them. For example, for a same ad image (Fig.1), a consumer might like it because of the neckline of the dress. However, another consumer might like this ad image because the emotional feeling created in this ad image. To fully understand the multi-correlation (both visual and textual) between ads and fashion product images liked by subjects, we exhaustively annotated both ads and fashion images from different perspectives: 1) abstractive level; 2) physical attributes with associated segmentations (localized attributes); 3) caption, and 4) brands on the ads. The aim of this work is to enable future studies and encourage more exploration to interpretability research between advertisements and fashion taste. The contributions of this work are: 1) we introduce Fashionpedia-Ads, consisting of three datasets (Ads, Social network style and E-commerce style fashion products). We bridge the connection among them through the subjects’ preference (like or dislike) on these images and the annotation from multi-perspectives (e.g. abstract & physical attributes). 2) we formalize a new task that not only requires models to predict whether a subject like or dislike a fashion product image based on given ad images of various domains, but also provide a rationale explaination why it makes this prediction from multi-perspectives.
2308.04735v1	Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations	Physics-informed neural networks have been widely applied to partial differential equations with great success because the physics-informed loss essentially requires no observations or discretization. However, it is difficult to optimize model parameters, and these parameters must be trained for each distinct initial condition. To overcome these challenges in second-order reaction-diffusion type equations, a possible way is to use five-point stencil convolutional neural networks (FCNNs). FCNNs are trained using two consecutive snapshots, where the time step corresponds to the step size of the given snapshots. Thus, the time evolution of FCNNs depends on the time step, and the time step must satisfy its CFL condition to avoid blow-up solutions. In this work, we propose deep FCNNs that have large receptive fields to predict time evolutions with a time step larger than the threshold of the CFL condition. To evaluate our models, we consider the heat, Fisher's, and Allen-Cahn equations with diverse initial conditions. We demonstrate that deep FCNNs retain certain accuracies, in contrast to FDMs that blow up.	Natural and chemical phenomena, as well as some problems in the real world, can be described through mathematical expressions, in particular partial differential equations (PDEs). For example, fluid flow, chemical reaction-diffusion, phase separation, image analysis, image segmentation, cell division, the spread of infectious diseases, etc., can be mathematically expressed. Therefore, to find solutions to PDEs or mathematically analyze the characteristics of phenomena and changes in energy over time that these equations represent, numerical studies aimed at approximating PDE solutions are continuously being conducted such as finite difference method (FDM)RLV2007;YZ2009;EBKP2011;JKDJSYYC2017;YLJK2017, finite element method (FEM)NFJXYY2011;AD2011;CJ2012;NKSMAB2022, finite volume method (FVM)QXXXNF2012;EFA2015;SM2015;PZZL2022, and so on. Moreover, machine learning/deep learning models have been developed to solve approximately PDE solutions. The application of physics-informed neural networks (PINNs)pinnhas led to great success in solving partial differential equations because the physics-informed loss does not require any observations or discretization. Also, PINNs achieve acceptable accuracy for diverse simulationspinn2;pinn3;pinn4;pinn5;pinn6. However, optimizing model parameters remains a challenge, and PINNs should be trained separately by each initial condition. To address these problems, a possible way is to use data-driven models that can learn numerical schemes using snapshots and predict solutions at further time steps. Using a supervised learning approach, convolutional neural networkscnnhave been widely applied to solve partial differential equationsphygeo;convpde;poicnnbecause the mechanism of the convolution operator is similar to numerical methods that utilize neighboring points to obtain values at the next time step. However, the prediction of data-driven machine learning models and numerical methods is strongly affected by the time step of given snapshots, so appropriate time steps should be chosen. Here, we focus on thereceptive fieldreceptivefield;yolov4which refers to the size of input nodes that affect a single output node. Modern convolutional neural networksvgg;resnet;effnethave been designed to acquire large receptive fields for good feature extraction. In other words, the large receptive field increases the capacity of the indirect connectivity between an input and its output so that plenty of the input nodes are involved in the output extraction. In FDMs, simulation errors and time steps are also influenced by the receptive field size related to the order of approximations to derivatives (e.g., 5-point stencil vs. 9-point stencil). The stability condition of 2D heat equation (\phi(x,y)_{t}=\phi_{xx}+\phi_{yy}) ish^{2}/4, whereh=\frac{1}{\Delta x}=\frac{1}{\Delta y}. \phi(x,y,t_{n})=e^{iqx}e^{iry}then, whereGfis a growth factor defined as The worst case isq\Delta x=r\Delta y=\pi, then Therefore, the stability condition is Sinceh=\frac{1}{\Delta x}=\frac{1}{\Delta y}, we obtain ∎ Basically, the time step size\Delta tcan be decided by Theorem1, which provides an analysis of the stability range of the explicit scheme for the two-dimensional heat equation. The stability analysis determines the range of suitable time steps that ensures the numerical solution remains stable. In other words, if\Delta tdoes not satisfy the Eq. (1), a blowup could occur. In this paper, the main idea of our approach is to utilize a receptive field that permits a time step larger than the threshold of the CFL condition. Therefore, we propose a deep CNN architecture to increase the receptive field size. The contents of this paper is as follows: In Section2, we explain our proposed deep five-point stencil convolutional neural networks (deep FCNNs) and algorithms. In Section3, we perform numerical simulations for various initial conditions. In Section4, we summarize the paper and discuss a possible research direction.
2307.05694v1	A Survey on Figure Classification Techniques in Scientific Documents	Figures visually represent an essential piece of information and provide an effective means to communicate scientific facts. Recently there have been many efforts toward extracting data directly from figures, specifically from tables, diagrams, and plots, using different Artificial Intelligence and Machine Learning techniques. This is because removing information from figures could lead to deeper insights into the concepts highlighted in the scientific documents. In this survey paper, we systematically categorize figures into five classes - tables, photos, diagrams, maps, and plots, and subsequently present a critical review of the existing methodologies and data sets that address the problem of figure classification. Finally, we identify the current research gaps and provide possible directions for further research on figure classification.	Classification of images finds tremendous applications in various fields such as automobile, healthcare, agriculture, surveillance, and document analysis[7,29,23,19]. In scientific documents, different graphical visualizations such as tables, photos, diagrams, maps, and plots convey specific facts that are more effective than simple text. This factual information improves comprehension. Hence, extracting underlying information represented by figures is an important task. In general, it is referred to as figure mining. Figure mining includes enhancing the figure design, outlining the data represented by figures, detecting plagiarized documents, etc. The figure mining pipeline consists of (i) figure extraction from academic documents, (ii) classification of figures, and (iii) data extraction from each figure type. This paper aims to survey figure classification techniques and their related datasets comprehensively. To address the problem of figure classification, it is crucial to detect and extract the figures from the respective documents using document segmentation techniques, as illustrated in Fig-1. Generally, a document image may be segmented into text and non-text components. The non-text details are then further processed to classify them into an appropriate category. Much research has been done on the textual processing of documents. But as far as figures are concerned, there need to be more state-of-the-art methods that classify the scientific figures in their appropriate category. Chart image classification has recently interested many research groups[12]. This paper aims to highlight the work on chart image classification and include results that include other figure types. The techniques used for classification can be divided into handcrafted-based methods and deep learning-based methods. The hand-crafted methods manually extract features using traditional feature extraction techniques, then classify the figures using machine learning models. On the other hand, deep learning techniques automatically learn features and classify the figures. Various approaches employed in these two categories are discussed in detail in the upcoming sections. This follows a discussion on several data sets reported in the related literature. The rest of the paper is organized as follows. Section 2 provides information on the existing literature on the figure classification problem, and a summary of significant contributions is shown in TableI. Section 3 includes a discussion of datasets used in recent works, and details of a few publicly available datasets are summarised in Table-III. Section 4 provides pointers for future research work and many interesting problems that still need to be addressed in figure classification.
2310.10666v1	Extracting Physical Causality from Measurements to Detect and Localize False Data Injection Attacks	False Data Injection Attack (FDIA) has become a growing concern in modern cyber-physical power systems. Most existing FDIA detection techniques project the raw measurement data into a high-dimensional latent space to separate normal and attacked samples. These approaches focus more on the statistical correlations of data values and are therefore susceptible to data distribution drifts induced by changes in system operating points or changes in FDIA types and strengths, especially for FDIA localization tasks. Causal inference, on the other hand, extracts the causality behind the coordinated fluctuations of different measurements. The causality patterns are determined by fundamental physical laws such as Ohm's Law and Kirchhoff's Law. They are sensitive to the violation of physical laws caused by FDIA, but tend to remain stable with the drift of system operating points. Leveraging this advantage, this paper proposes a joint FDIA detection and localization framework based on causal inference and the Graph Attention Network (GAT) to identify the attacked system nodes. The proposed framework consists of two levels. The lower level uses the X-learner algorithm to estimate the causality strength between measurements and generate Measurement Causality Graphs (MCGs). The upper level then applies a GAT to identify the anomaly patterns in the MCGs. Since the extracted causality patterns are intrinsically related to the measurements, it is easier for the upper level to figure out the attacked nodes than the existing FDIA localization approaches. The performance of the proposed framework is evaluated on the IEEE 39-bus system. Experimental results show that the causality-based FDIA detection and localization mechanism is highly interpretable and robust.	Modern power systems have become sophisticated cyber-physical systems due to the integration of information and communication technologies. The informatization and intelligent transformation of the smart grid enhances the efficiency of the system but also confront it with more cyber attacks. In particular, a meticulously-designed False Data Injection Attack (FDIA) is capable of manipulating the state estimation results of power systems while bypassing the conventional bad data detector (BDD), thereby posing great physical and financial threats[8]. Extensive research has been conducted on the countermeasures of FDIAs, which can be classified into two categories, namely FDIA detection and localization. In general, FDIA detection approaches aim to identify the existence of FDIA. Various models have been applied to this issue, from the classic Kalman filters[10,7], interval observer[26], maximum likelihood estimation[11,22], Support Vector Machine (SVM)[13], to assorted deep learning models[3,12,1,4,27]. For example, A Recurrent Neural Network (RNN) is applied to recognize FDIA in dc microgrids[3]. The RNN is further hybridized with Long-Short Term Memory (LSTM) cells in[12]to scrutinize remedial actions against FDIAs. Denoising autoencoders are also combined with LSTM to detect FDIAs and recover contaminated measurements by capturing the spatio-temporal dependencies between them[1]. In recent years, some research has extended FDIA defense techniques from detection to localization. Compared to detection algorithms, FDIA localization approaches aim to specify which measurements or state variables have been tampered with, requiring a finer-grained identification capability. Existing localization approaches can be categorized as model-based and data-driven. Model-based approaches require an accurate system model and its associated parameters, and generally have good interpretability and generalizability. For example,[9]models an interval observer for each measurement in the power system and constructs a logical localization matrix to realize FDIA localization. Nevertheless, model-based approaches are often confronted with scalability issues and the difficulty of obtaining an accurate system model. Conversely, data-driven approaches are system-independent, and while their off-line training process can be time-consuming, they are efficient when applied in real-time tasks. In[5], Jevticet al.develop a cumbersome framework to localize FDIAs in a 5-bus power system, which builds an independent LSTM model for each measurement. In[25], Wanget al.concatenate a simple Convolutional Neural Network (CNN) with a residual-based bad data detector to capture the inconsistency and co-occurrence dependency in measurement data and perform multi-label classification. On this basis, an early exit policy and mixed-precision quantization techniques are combined with the CNN to detect and specify the attacked nodes[28]. To better accommodate the graph-based topology of power systems, a Graph Neural Network (GNN) is proposed in[2], which integrates Auto-Regressive Moving Average (ARMA) graph filters for joint FDIA detection and localization. Similarly, a Graph Convolutional Network (GCN) is applied in[17]to project graph-structured multi-dimensional measurements into the spectral domain to localize FDIAs. The above data-driven FDIA localization methods mainly focus on capturing the spatio-temporal correlations between power system measurements. The crux of correlation-based FDIA detection and localization methods is to identify the anomalies in the measurement data distributions by ascertaining a decision boundary. Most of them are based on the independent and identically distributed assumption between the training data and the test data, which may not always be the case in real-world scenarios. For example, if there is a relatively large unanticipated generation fluctuation in the training data, which can often occur due to the high penetration of renewables, these correlation-based methods may no longer be applicable. In addition, after the input raw measurement data is projected into a high-dimensional latent space, it becomes an embedding of the latent space that has no physical meaning. Even if an anomalous pattern is detected in the latent space, it is still difficult to trace back to the input space and find out which measurement caused the anomaly. Therefore, correlation-based approaches often fall short in interpretability and have degraded FDIA localization performance. Several studies suggest that the generalization and interpretability problems of correlation-based learning are partly due to the lack of causal formalisms[18,16,21]. In response, there has been a surge of interest in causal inference, which aims to extract the cause-effect relationships between different variables of the underlying system and use the causal knowledge to guide decision-making[15]. There are clear differences between correlation analysis and causal inference. First, correlation analysis is based entirely on observed data, while causal inference is based partly on observed data and partly on counterfactual estimation. Thus, it is commonly believed that correlation analysis can only learn the patterns presented in the training samples, whereas causal inference can reveal information beyond the training data and thus has a better generalization capability. Second, correlations are mutual, but causal relationships are directional. Given two correlated variables, one cannot tell which variable is influenced by the other. In contrast, a variable that has causal effects on another variable implies a temporal order of their occurrences. Hence, causal inference is expected to carry additional information than correlation analysis. Finally, correlated variables do not necessarily have causal relationships because they may be affected by some common confounding factors. Compared with correlation analysis, causal inference can exclude the influence of confounding factors and directly reflect the physical rules of the underlying system. In this paper, a bi-level framework that combines causal inference and graph learning is proposed to jointly detect and localize FDIAs. The main contributions of this paper include: A systematic approach to detect and localize FDIAs based on the physical causality between power system measurements is proposed for the first time. A causal inference model based on the X-learner meta-algorithm is proposed to quantify the causality strength between physically neighboring measurements. The extracted causality features are embedded into a Measurement Causality Graph (MCG) to provide a spectral manifestation of the underlying physical laws. A Graph Attention Network (GAT) is used to identify abnormal MCG patterns and output the probability of each measurement being manipulated. A fully-connected network is appended to the GAT to perform multi-label classifications based on the measurement-wise attack probabilities to alert the target physical nodes of FDIA. The enhanced detection and localization performance of the proposed framework, along with its interpretability and generalizability, are demonstrated through extensive experiments on the IEEE 39-bus test system. The rest of the paper is organized as follows. SectionIIintroduces the background knowledge, including the definitions of FDIA and causal learning. SectionIIIgives a detailed description of the proposed bi-level FDIA detection and localization framework. SectionIVpresents thorough experiments to validate the performance of the framework and analyzes its interpretability and generalization ability. SectionVconcludes the paper and discusses possible future work.
2305.14483v1	Language Model Self-improvement by Reinforcement Learning Contemplation	Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation. Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability.	Large language models (LLMs) have shown impressive performance in numerous natural language processing (NLP) tasks, including language understanding, machine translation, and question answeringZhaoet al.(2023); Liuet al.(2023). This success can be attributed to the Pre-training + Fine-tuning (PTFT) training framework, which involves training a language model on a large corpus and fine-tuning it on supervised NLP tasks. A fine-tuned language model can achieve state-of-the-art performance using various supervised datasetsZiegleret al.(2019). For example, InstructGPTOuyanget al.(2022)and ChatGPTOpenAI (2023)fine-tune the GPT-3Brownet al.(2020)model by introducing human preference and learning a reward model on human-comparison data. However, fine-tuning LLMs typically requires extensive supervision in the form of labelled questions or human feedback, which can be time-consuming and labour-intensive. Recent research addresses this limitation by leveraging unlabelled data to improve LLMs’ reasoning ability. For example, the self-consistency methodWanget al.(2023)samples diverse reasoning paths and selects the most consistent answer by marginalizing out the sampled paths. LMSIHuanget al.(2022a)employs the self-consistency method to generate high-quality answers, which are then used to fine-tune LLMs. Although these methods improve performance using unlabelled data, they are primarily designed for reasoning tasks that rely heavily on LLMs’ chain-of-thought (CoT) ability, which is limited to reasoning problemsWeiet al.(2022). On the other hand, reinforcement learning shows an impressive performance in fine-tuning LLMs without directly using labelled answersOuyanget al.(2022), but it still requires amounts of annotation that reflects human preference and text quality. In this paper, we propose a novel approach for fine-tuning LLMs without external supervision. Our method capitalizes on the observation that it is simpler for a language model to evaluate the generated text than to generate it. For example, while writing an attractive story can be challenging, identifying the generated text is relatively easy. Fig.1illustrates the disparity between text generation and self-evaluation. We verify the self-evaluation ability of LLM through experiments on various NLP tasks. Based on such evaluation ability, we propose Language ModelSelf-Improvement byReinforcementLearningContemplation (SIRLC), where the LLM both functions as a student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, the LLM scores the generated answers. The LLM is subsequently updated through reinforcement learning to optimize for maximum evaluation scores. SIRLC employs self-evaluation results as the reward and utilizes reinforcement learning to retrain the LLM. We refer to this learning process asreinforcement learning contemplation. The contribution of this work can be summarized as follow: Firstly, We introduce a novel approach for unsupervised fine-tuning of LLMs by utilizing self-evaluation as the reward and RL for training, eliminating the need for external supervision. Secondly, we conduct a comprehensive experimental analysis to demonstrate LLM’s self-evaluation ability. To the best of our knowledge, this is the first study that formally verifies the self-evaluation capability of LLMs. Finally, our experimental results demonstrate that our approach can improve LLM’s ability to solve reasoning, summarization, and translation problems. We also present that SIRLC can be applied to LLMs with a parameter range of 80M to 780M, and the trained LLM generalizes well to new and unseen datasets, demonstrating the extensive applicability of the proposed method.
2310.10046v3	TRANSOM: An Efficient Fault-Tolerant System for Training LLMs	Large language models (LLMs) with hundreds of billions or trillions of parameters, represented by chatGPT, have achieved profound impact on various fields. However, training LLMs with super-large-scale parameters requires large high-performance GPU clusters and long training periods lasting for months. Due to the inevitable hardware and software failures in large-scale clusters, maintaining uninterrupted and long-duration training is extremely challenging. As a result, A substantial amount of training time is devoted to task checkpoint saving and loading, task rescheduling and restart, and task manual anomaly checks, which greatly harms the overall training efficiency. To address these issues, we propose TRANSOM, a novel fault-tolerant LLM training system. In this work, we design three key subsystems: the training pipeline automatic fault tolerance and recovery mechanism named Transom Operator and Launcher (TOL), the training task multi-dimensional metric automatic anomaly detection system named Transom Eagle Eye (TEE), and the training checkpoint asynchronous access automatic fault tolerance and recovery technology named Transom Checkpoint Engine (TCE). Here, TOL manages the lifecycle of training tasks, while TEE is responsible for task monitoring and anomaly reporting. TEE detects training anomalies and reports them to TOL, who automatically enters the fault tolerance strategy to eliminate abnormal nodes and restart the training task. And the asynchronous checkpoint saving and loading functionality provided by TCE greatly shorten the fault tolerance overhead. The experimental results indicate that TRANSOM significantly enhances the efficiency of large-scale LLM training on clusters. Specifically, the pre-training time for GPT3-175B has been reduced by 28%, while checkpoint saving and loading performance have improved by a factor of 20.	Undoubtedly, LLMs are currently the most trending topic in the field of AI. The use in various industries have propelled generative AI to unprecedented heights. Among them, the ChatGPT[32]model developed by OpenAI stands as a typical example. It is already capable of assisting people in tasks like writing articles, generating code, and analyzing materials, fulfilling common text-language interaction needs. Its upgraded version, GPT-4[33], showcases even more astonishing reasoning capabilities. Following this, an increasing number of LLMs have been released, including Stanford Alpaca[49], Baidu’s ERNIE Bot, Tsinghua’s ChatGLM[59], Google’s Bard, SenseTime’s SenseChat, InternLM[50], Meta’s Llama-1[52], Llama-2[53], and more. The success of LLMs can be attributed to several key factors. Firstly, the utilization of the transformer model[57]with self-attention mechanisms, which enhances the speed of model training, allows for the processing of longer sequences of data. Almost all LLMs employ the transformer as their neural network architecture. Secondly, the scaling of model parameters to the order of billions and the availability of high-quality annotated token datasets have played a pivotal role. According to OpenAI’s research[17], LLMs exhibit the property of the “scaling law” whereby their performance improves as training data and model parameter sizes increase, often leading to a sudden jump in capabilities at a certain scale. Lastly, the development of various hybrid distributed parallelization techniques has reduced the dependency of billion-parameter LLMs on the scale of GPUs. The Megatron-LM[30]architecture proposed by NVIDIA optimizes transformer models for model parallelism, boosting training throughput by over 10%. Microsoft’s DeepSpeed[41]deep learning parallel optimization library accelerates training through various techniques like model parallelization, gradient accumulation, memory optimization, and mixed precision. It achieved training of a 13-billion-parameter LLM on a single NVIDIA V100 GPU and linear scalability on larger computing clusters. As a result, larger parameter scales and more efficient training methods have become significant metrics claimed by model providers, computing platform suppliers, and chip manufacturers. For example, the 176B Bloom model[43]completed training on a 350B dataset in 3.5 months using 384 A100-80GB chips; GPT-3 boasts a parameter scale of 175B, requiring 3640 PF-days of computation for training on a 300B dataset. DeepMind’s Gopher[39]model reached a parameter scale of 280B, while the multimodal PaLM[11]model reached an astonishing 540B parameter scale. With the development and popularity of multimodal models, it’s evident that training large models requires more computational resources for stable and continuous training. Model providers opt for self-built or leased high-performance heterogeneous AI computing platforms for large-scale pre-training or fine-tuning. NVIDIA GPUs, in particular, are favored in the field of large model training. Azure has built a cluster of over 10,000 GPUs for OpenAI, Cerabras has constructed an AI computing cluster named Andromeda[10]with more than 2,512 H100 heterogeneous chips. SenseTime has built the SenseCore computing platform at the SenseTime Artificial Intelligence Computing Center as its computational foundation. This platform encompasses 27,000 GPUs, providing an impressive computational power output of 5,000 Petaflops for AI applications. According to the study by Sevilla et al[45], the computational demand for large-scale AI models doubles approximately every 10 months. In the foreseeable future, the pursuit of more powerful hardware is expected to continue. However, the expansion of computational scale introduces significant risks to the reliability of computing systems, particularly in the context of LLMs applications with demanding computational, storage, and communication requirements. This complexity poses challenges for achieving large-scale and efficient LLMs pre-training. We have summarized these challenges and attempted to address them through innovative approaches. Frequent anomalies in LLMs training tasks due to hardware and software issues.These tasks often span hundreds of GPU nodes, and at such a scale, they are prone to various problems that can lead to task abnormalities within hours to days. For instance, according to the Bloom report, on a new cluster of around 400 GPUs, there are typically 1-2 GPU failures per week on average. Meta’s 175B opt training records also demonstrate that within half a month, the 175B training experiment was interrupted more than 40 times due to hardware, infrastructure, and other issues. Additionally, even when a single node fails, processes on all other nodes of the task must be halted and the task needs to be killed and resubmitted. This process adds to the time overhead of the anomaly phase. Challenges in troubleshooting LLMs training tasks.There are many reasons for anomalies in LLMs training tasks, such as node hardware failures, system malfunctions, network issues, storage problems, and training code errors, among others. We have compiled data on the causes of errors in some LLMs training tasks on the SenseCore cluster in Q2 of 2023, as shown in TableI. Errors like “NET/IB: Got completion from peer”, “socket timeouts”, and “GPU ECC errors” cannot be resolved solely through rescheduling; they require a deep analysis of which nodes are causing the errors and isolating the faulty nodes to resume training. Identifying the reasons behind timeout exceptions can be highly complex. For instance, the anomaly of communication timeout could be due to slow or faulty nodes, storage system malfunctions, or errors in the user’s communication data sending and receiving code. Moreover, different reasons correspond to different recovery strategies, posing significant challenges for error diagnosis, pinpointing these issues often takes several hours or even longer. LLMs training tasks entail significant recovery overhead.To mitigate the impact of the aforementioned anomalies on LLMs training, the current approach involves using checkpoints for recovery. The essence of checkpoints is to persistently store various data, including the optimizer state and weights, in the form of snapshots while the task is running. The total size of checkpoints is directly proportional to the parameter scale. For instance, a checkpoint of a 175B LLMs with FP32 optimizer state and bf16+fp32 weights amounts to 2.3TB. Training larger parameter-scale LLMs necessitates training on a large-scale GPU cluster, which requires increasing the frequency of checkpoint storage to minimize retraining time. For instance, in Bloom’s 176B model training[5], a checkpoint is saved every 3 hours. In the case of the 175B OPT training[28], a checkpoint is generated every 250 steps. However, as the model scale and the number of computational nodes increase, checkpoint read-write efficiency and stability become one of the main bottlenecks affecting training efficiency. Based on the aforementioned challenges, we propose TRANSOM, a simple, efficient, and fault-tolerant large-scale model training system. The primary objective of TRANSOM is to provide an automated checkpoint-based fault-tolerant recovery pipeline system, significantly enhancing checkpoint access efficiency and fault recovery capability, reducing the cost of human intervention in troubleshooting training task anomalies, and improving the overall efficiency of large-scale model training tasks. In summary, this paper contributes in the following aspects: the Training pipeline Automatic Fault Tolerance and Recovery Mechanism:TOL is a training pipeline automatic fault tolerance and recovery subsystem. It is based on a finite-state machine with a set of lifecycle management rules that enhance the management of distributed training processes within each working node during the training process pipeline execution. It introduces a distributed training process pipeline management mechanism in each worker node during the training process. Following the sequence of startup, warm-up, execution, verification, and recovery, this mechanism achieves dynamic and automated fault-tolerant recovery for training tasks. This system enables unattended closed-loop training, thereby enhancing the success rate of task startup. the Training Task Multi-dimensional Metric Automatic Anomaly Detection System:TEE is a training task multi-dimensional metric automatic anomaly detection system. Starting from the moment the task is initiated, the training task’s multi-dimensional metric automatic anomaly detection system continuously gathers relevant metrics. It employs a hybrid model and clustering algorithm to analyze the nodes and causes of training task failures. It then notifies the training pipeline’s automatic management mechanism in the form of interruptions, triggering eviction and rescheduling of the faulty nodes. Simultaneously, the task reenters the initialization startup pipeline, continuing execution based on the latest checkpoint. This approach allows for precise identification and targeted handling of faulty nodes, effectively reducing the time and computational costs of manual troubleshooting and manual restarts. the Training Checkpoint Asynchronous Access Automatic Fault Tolerance and Recovery Technology:TCE is a training checkpoint asynchronous access automatic fault tolerance and recovery technology. To achieve more efficient recovery of the training process, we propose a novel fault-tolerant asynchronous checkpoint access mechanism. This technology implements an asynchronous process from the initiation of checkpoint access operations in the training process to the persistence of checkpoints. It effectively avoids checkpoint loss due to single-point failures through redundant storage methods.
2306.01273v1	VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations	Adversarial attacks reveal serious flaws in deep learning models. More dangerously, these attacks preserve the original meaning and escape human recognition. Existing methods for detecting these attacks need to be trained using original/adversarial data. In this paper, we propose detection without training by voting on hard labels from predictions of transformations, namely, VoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the hard labels of input text and its transformation. The evaluation demonstrates that VoteTRANS effectively detects adversarial text across various state-of-the-art attacks, models, and datasets.	Deep learning models are sensitive to changes in input text from an adversarial attack. Even a slight change enormously impacts the prediction of models. More dangerously, these changes still preserve the input meaning, so attacks remain unrecognized by humans. This vulnerability has negatively affected the reputation of deep learning models. In contrast to adversarial text defense, fewer works have been proposed to detect adversarial texts. Previous works detected such texts via perturbed word identification(Zhouet al.,2019; Mozeset al.,2021), synonyms(Wanget al.,2022b), densityYooet al.(2022), attentionBijuet al.(2022), PCARaina and Gales (2022), transformerWanget al.(2022a), and word importance(Moscaet al.,2022). Since existing works need original/adversarial data to train detectors, they are sensitive to new adversarial attacks. Motivation:Adversarial text must satisfy two criteria: the text must (1) change the prediction of a target model while (2) preserving the original meaning. Few texts can comply with both criteria. For example, we randomly selected original text from AG News and used a probability-weighted word saliency (PWWS) attack(Renet al.,2019)to generate adversarial text (Figure1).PWWSreplaces original words to fool a target model (CNN). During this generation process, only the final text fooled the target CNN, while other texts were still correctly predicted by the target CNN and another model, such as RoBERTa. We find the same trend for other AG News texts and IMDB movie reviews as shown in AppendixA. Contributions:We propose a simple detector by voting on hard labels of transformations (\mathrm{VoteTRANS}). In particular, we generate a transformation set for each word in the input text. We then compare the original hard label from the input text and the majority vote from each transformation set. If we find any difference in the comparison, the adversarial text is identified. In summary, our contributions are listed as follows: To the best of our knowledge,\mathrm{VoteTRANS}is the first model to detect adversarial text from various attacks without training. Moreover, we do not modify a target model and only use the target as a black-box setting for prediction.\mathrm{VoteTRANS}can thus be applied to a wide range of various models. Experiments on various attacks, models, and datasets demonstrate that\mathrm{VoteTRANS}outperforms state-of-the-art detectors. \mathrm{VoteTRANS}can run with all seventeen current attacks related to text classification from TextAttack frameworkMorriset al.(2020).\mathrm{VoteTRANS}is also automatically compatible with future attacks from this framework without changing its source code111\mathrm{VoteTRANS}is available athttps://github.com/quocnsh/VoteTRANS.
2304.05818v1	Gradient-Free Textual Inversion	Recent works on personalized text-to-image generation usually learn to bind a special token with specific subjects or styles of a few given images by tuning its embedding through gradient descent. It is natural to question whether we can optimize the textual inversions by only accessing the process of model inference. As only requiring the forward computation to determine the textual inversion retains the benefits of less GPU memory, simple deployment, and secure access for scalable models. In this paper, we introduce a \emph{gradient-free} framework to optimize the continuous textual inversion in an iterative evolutionary strategy. Specifically, we first initialize an appropriate token embedding for textual inversion with the consideration of visual and text vocabulary information. Then, we decompose the optimization of evolutionary strategy into dimension reduction of searching space and non-convex gradient-free optimization in subspace, which significantly accelerates the optimization process with negligible performance loss. Experiments in several applications demonstrate that the performance of text-to-image model equipped with our proposed gradient-free method is comparable to that of gradient-based counterparts with variant GPU/CPU platforms, flexible employment, as well as computational efficiency.	Large-scale text-to-image models, enabling high-quality and diverse synthesis of images based on a text prompt written in natural language, have achieved remarkable progress and become an exciting direction(Nicholet al.,2021; Sahariaet al.,2022; Rameshet al.,2022; Rombachet al.,2022; Yuet al.,2022b). One of the main advantages of these models is the strong semantic prior learned from scalable collections of image-caption pairs, leading to their broad application in artistic creation,e.g., as sources of inspiration, and even in the designing of new physical products. While the generation capabilities of text-to-image models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of the same subjects in different contexts(Ruizet al.,2022),i.e., even the most detailed textual description of an object may yield instances with different appearances(Galet al.,2022). Personalization of text-to-image generation is proposed to address this kind of issue to certain extent. The general idea is to expand the embedding dictionary of text encoder by adding a new concept token of specific subject or style which the users want to generate. In particular, textual inversion(Galet al.,2022; Daras and Dimakis,2022)is a powerful technique that can learn the new pseudo token in the embedding space for the representation of new concept. Remarkably, this tuned token can be composed in language to produce kinds of creative compositions. Though textual inversion keeps the major text-to-image model unchanged, optimizing the parameters of pseudo token still requires back-propagation through the entire model, which is expensive or even unfeasible for many applications with limited resource. Recently, it has been demonstrated that scaling up the model size is promising to achieve better semantic understanding(Yuet al.,2022b,a), while the growing model size leads to an increment in tuning cost as well as unstable fine-tuning process. To make personalized text-to-image paradigm benefiting a wider range of audiences, a natural question raises:Can we optimize the specific textual inversion when we only have access to the inference of text-to-image model?In such scenario, users cannot access the derivatives or adjust the parameters of text-to-image model but accomplish the text inversion to obtain an object or style of interest bounded by a range of inferences. In contrast to gradient-based optimization, the gradient-free framework can be highly optimized by acceleration tools such as ONNX and TensorRT. In addition, the optimization of textual inversion can be decoupled from the complicated deployment of scalable training framework. Although solving optimization problems in an inference-only setting is considerably challenging(Wanget al.,2018), our gradient-free framework introduces a new and effective paradigm of personalized text-to-image generation. Here, we resort to the gradient-free optimization (GFO), also termed as black-box, zeroth-order or derivative-free optimization(Connet al.,2009; Koldaet al.,2003; Rios and Sahinidis,2013; Sahuet al.,2019). In general, GFO involves a kind of optimization algorithms that do not require gradients, but only rely on function values or fitness values of iteratively sampled solutions(Rios and Sahinidis,2013). However, GFO algorithms are known to suffer from a slow convergence in high-dimensional search space, due to the massive searching directions for continuous text embedding. To alleviate the searching problem in textual inversion, we propose a composing initialization strategy to effectively reduce the exploration cost. Moreover, inspired by the recent works that common pre-training models, despite their large number of parameters, have a very low intrinsic dimensionality(Aghajanyanet al.,2021; Qinet al.,2021). That means, there exists a low-dimensional subspace that is as effective for tuning as the full dimension space. Therefore, with appropriate subspace decomposition in objective function, the textual inversion optimization can be effectively solved in low-dimensional subspace. Based on these insights, this paper presents agradient-freeframework to solve the personalized text-to-image generation task. Specifically, we manage to optimize the pseudo-token embedding given several images by iteratively forwarding the text-to-image model and design the loss function to measure fitness of sampled solutions. To improve the convergence and stability of optimization, we introduce to (i) initialize the pseudo-token embedding with general condition,i.e., non-parametric cross-attention of pre-trained word embedding and personalized visual features; (ii) decompose the original searching space of GFO into a smaller subspace using Principal Components Analysis (PCA) or prior normalization and solve the transferred problem with some derivative-free optimizer in the subspace for incremental elements. In particular, we adopt Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search the target embedding by exploration and exploitation in a parameterized search distribution. Encouragingly, this gradient-free textual inversion allows users to optimize their specific demand locally on the resource-limited devices even without GPUs. We use the stable diffusion model(Rombachet al.,2022)as the base model in our experiments, though our method is not constrained to any specific text-to-image models. Experiment results on several tasks demonstrate that gradient-free optimization achieves compariable performance with its gradient-based counterparts in terms of quantitative analyses and human evaluation. Overall, the contributions of this paper are four fold: We introduce a new scenario of textual inversion in gradient-free framework, which to our best knowledge is the first trial of GFO methods on personalized text-to-image generation tasks; This paper offers a solution with an improved evolution strategy in the searching scenario to accomplish the common text-to-image personalization task; To accelerate the convergence of iterative process, we provide the general condition initialization for pseudo-token embedding and decomposed subspace for effective incremental optimization; Empirical results show that gradient-free textual inversion can successfully deal with real-world applications, achieving comparable performance with gradient-based counterparts. The source code will be publicly available.
2309.14819v1	Discrepancy Matters: Learning from Inconsistent Decoder Features for Consistent Semi-supervised Medical Image Segmentation	Semi-supervised learning (SSL) has been proven beneficial for mitigating the issue of limited labeled data especially on the task of volumetric medical image segmentation. Unlike previous SSL methods which focus on exploring highly confident pseudo-labels or developing consistency regularization schemes, our empirical findings suggest that inconsistent decoder features emerge naturally when two decoders strive to generate consistent predictions. Based on the observation, we first analyze the treasure of discrepancy in learning towards consistency, under both pseudo-labeling and consistency regularization settings, and subsequently propose a novel SSL method called LeFeD, which learns the feature-level discrepancy obtained from two decoders, by feeding the discrepancy as a feedback signal to the encoder. The core design of LeFeD is to enlarge the difference by training differentiated decoders, and then learn from the inconsistent information iteratively. We evaluate LeFeD against eight state-of-the-art (SOTA) methods on three public datasets. Experiments show LeFeD surpasses competitors without any bells and whistles such as uncertainty estimation and strong constraints, as well as setting a new state-of-the-art for semi-supervised medical image segmentation. Code is available at \textcolor{cyan}{https://github.com/maxwell0027/LeFeD}	Accurate segmentation of medical images is a crucial task in computer-aided diagnosis[1]. Deep learning models trained on large-scale datasets have recently shown promising performance on this task[2,3]. However, collecting medical image datasets requires ineluctably expertise for data annotation, which is time-consuming and labor-intensive, especially for volumetric data. Considering unlabeled data are relatively easier to collect from clinical sites, semi-supervised learning (SSL)[4,5]has attracted increasing research attention due to its ability to improve model generalization by leveraging massive unlabeled data to augment limited labeled data. According to the usage of unlabeled data, the paradigm of SSL can be approximately categorized into pseudo-labeling[6,7,8]and consistency regularization[9,10]. The first category of SSL methods focuses on generating accurate pseudo-labels. For instance, model ensemble was employed in the teacher-student framework to enhance pseudo-label quality[11,12], and various criteria were defined to select accurately pseudo-labeled data[13,14]. The second category of SSL methods put emphasis on designing the regularization that enforces the model to give consistent outputs for an input and its realistically perturbed variants. The consistency regularization can be the constraints imposed at either the data-level[15,16], task-level[17], or prediction-level[18]. Despite the differences of pseudo-labeling and consistency regularization, they share the same crux that is learning invariant predictions by gradually learning from the inconsistency. For example,[18]aligns the pseudo-label of strongly-augmented branch to the weakly-augmented branch, and[19]keeps the logits distribution similar between predictions of CNN and Transformer. To better realize this, we present a brief view for the workflow of pseudo-labeling and consistency regularization. As Fig.1shows, the SSL framework is composed of a single encoder and two decoders – a structure extensively employed in both pseudo-labeling[20,21]and consistency regularization methods[22,23]. Let us consider an instance where cross-pseudo supervision (a pseudo-labeling strategy displayed in the top of Fig.1) is utilized. In this scenario, one decoder’s pseudo-label is used to oversee the predictions of the other. It is in this context that inconsistent predictions become significant as they can provide complementary information. Similarly, if we maintain the logical distribution similar for learning from unlabeled data (for example, using KL divergence – a common consistency-based strategy exhibited in the bottom of Fig.1) between both branches, inconsistent predictions retain a crucial function. This is because the gradient primarily originates from the losses computed within these areas. From these observations, it becomes evident that inconsistency plays a pivotal role in promoting consistency in learning. Although prior SSL methods have effectively leveraged unlabeled data from the perspective of consistent learning, they have overlooked the natural emergence of inconsistent information when decoders attempt to produce inherently consistent predictions. Moreover, they have failed to acknowledge the significance of discrepancies between those two decoders. To this end, we propose a novel SSL method calledLearning From theFeature-levelDiscrepancy (LeFeD) from the perspective of learning inconsistent decoder features. Our hypothesis is that these discrepancies play a significant role in consistency learning, and properly harnessing this inconsistent information can enhance model performance. Our strategy distinguishes itself from existing methods on two fronts. Firstly, instead of primarily focusing on creating constraints to ensure prediction consistency, we place emphasis on feature discrepancy. Secondly, rather than striving to improve pseudo-label quality, we leverage the discrepancies to augment learning. In implementation, we first try to enlarge the discrepancy by training two differentiated decoders using distinct loss functions and deep supervision, and then iteratively learn from the inconsistency obtained at all scales. Our main contributions are three-fold. We propose a novel perspective for SSL,i.e., learning from the inconsistent features produced by two differentiated decoders. We observe the phenomenon that, when two decoders attempt to make consistent predictions, there always exists a discrepancy between two predictions, whose contribution to model performance has been verified empirically. We propose an accurate SSL method called LeFeD, which beats eight advanced SSL methods on three public medical image datasets, setting a new state of the art for semi-supervised medical image segmentation.
2401.05949v4	Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning	In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.	With the scaling of model sizes, large language models (LLMs)(Zhang et al.,2022b; Penedo et al.,2023; Touvron et al.,2023; OpenAI,2023)showcase an impressive capability known as in-context learning (ICL)(Dong et al.,2022; Zhang et al.,2024). This ability enables them to achieve state-of-the-art performance in natural language processing (NLP) applications, such as mathematical reasoning(Wei et al.,2022; Besta et al.,2023), code generation(Zhang et al.,2022a), and context generation(Nguyen and Luu,2022; Zhao et al.,2023a), by effectively learning from a few examples within a given context(Zhang et al.,2024). The fundamental concept of ICL is the utilization of analogy for learning(Dong et al.,2022). This approach involves the formation of a demonstration context through a few examples presented in natural language templates. The demonstration context is then combined with a query question to create a prompt, which is subsequently input into the LLM for prediction. Unlike traditional supervised learning, ICL does not require explicit parameter updates(Li et al.,2023). Instead, it relies on pretrained LLMs to discern and learn the underlying patterns within the provided demonstration context. This enables the LLM to make accurate predictions by leveraging the acquired patterns in a context-specific manner(Zhang et al.,2024). Despite the significant achievements of ICL, it has drawn criticism for its inherent vulnerability to adversarial(Zhao et al.,2022a; Formento et al.,2023; Qiang et al.,2023; Guo et al.,2023,2024)and backdoor attacks(Zhao et al.,2023b; Kandpal et al.,2023). Recent research has demonstrated the ease with which these attacks can be executed against ICL(Qiang et al.,2023; Kandpal et al.,2023). Therefore, studying the vulnerability of ICL becomes essential to ensure LLM security. For backdoor attacks, the goal is to deceive the language model by carefully designing triggers in the input samples, which can lead to erroneous outputs from the model(Lou et al.,2022; Goldblum et al.,2022). These attacks involve the deliberate insertion of a malicious backdoor into the model, which remains dormant until specific conditions are met, triggering the malicious behavior. Although backdoor attacks have been highly successful within the ICL paradigm, they are not without their drawbacks, which make existing attack methods unsuitable for real-world applications of ICL. For example,Kandpal et al. (2023)design a backdoor attack method for ICL in which triggers are inserted into training samples and fine-tuned to introduce malicious behavior into the model, as shown in Figure1(b). Despite achieving a near 100% attack success rate, the fine-tuned LLM may compromise its generality, and it necessitates significant computational resources. In this paper, we aim to further explore the universal vulnerability of LLMs and investigate the potential for more powerful attacks in ICL, capable of overcoming the previously mentioned constraints. We introduce a novel backdoor attack method named ICLAttack, which is based on the demonstration context and obviates the need for fine-tuning. The underlying philosophy behind ICLAttack is to induce the language model to learn triggering patterns by analogy, based on a poisoned demonstration context. Firstly, we construct two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which involve inserting triggers into the demonstration examples and crafting malicious prompts as triggers, respectively. Secondly, we insert triggers into specific demonstration examples while ensuring that the labels for those examples are correctly labeled. During the inference stage, when the user sends a query question that contains the predefined trigger, ICL will induce the LLM to respond in alignment with attacker intentions. Different fromKandpal et al. (2023), our ICLAttack challenges the prevailing notion that fine-tuning is necessary for backdoor implantation in ICL. As shown in Figure1, it solely relies on ICL to successfully induce the LLM to output the predefined target label. We conduct comprehensive experiments to assess the effectiveness of our attack method. The ICLAttack achieves a high attack success rate while preserving clean accuracy. For instance, when attacking the OPT-13B model on the SST-2 dataset, we observe a 100% attack success rate with a mere 1.87% decrease in clean accuracy. Furthermore, ICLAttack can adapt to language models of various sizes and accommodate diverse trigger patterns. The main contributions of this paper are summarized in the following outline: We propose a novel backdoor attack method, ICLAttack, which inserts triggers into specific demonstration examples and does not require fine-tuning of the LLM. To the best of our knowledge, this study is the first attempt to explore attacking the LLMs based on in-context learning without requiring fine-tuning. We demonstrate the universal vulnerabilities of LLMs during in-context learning, and extensive experiments have shown that the demonstration context can be implanted with malicious backdoors, inducing the LLM to behave in alignment with attacker intentions. Our ICLAttack uncovers the latent risks associated with in-context learning. Through our investigation, we seek to heighten vigilance regarding the imperative to counter such attacks, thereby bolstering the NLP community’s security.
2311.01455v2	RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation	We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.	This work is motivated by a long-standing and challenging goal in robotics research: empowering robots with a diverse set of skills, enabling them to operate in various non-factory settings and perform a broad range of tasks for humans. Recent years have witnessed impressive progress in teaching robots various complex skills: from deformable object and fluid manipulation(Linet al.,2022a; Wenget al.,2022; Xuet al.,2023; Xianet al.,2023c; Wanget al.,2023c; Linet al.,2020), to dynamic and dexterous skills such as object tossing(Zenget al.,2020), in-hand re-orientation(Chenet al.,2022), soccer playing(Haarnojaet al.,2023)and even robot parkour(Zhuanget al.,2023). However, these skills still remain compartmentalized, have relatively short horizons, and necessitate human-designed task descriptions and training supervision. Notably, due to the expensive and laborious nature of real-world data collection, many of these skills are trained insimulationswith appropriate domain randomization and then deployed to real-world(Xuet al.,2023; Zhuanget al.,2023; Chenet al.,2022). Indeed, simulation environments have become a crucial driving force behind diverse robotic skill learning(Linet al.,2022a; Songet al.,2023; Zhuanget al.,2023). Compared to exploration and data collection in the real-world, skill learning in simulations offers several advantages: 1) simulated environments provide access to privileged low-level states and unlimited exploration opportunities; 2) simulation supports massively parallel computation, enabling significantly faster data collection without reliance on considerable investment in robotic hardware and human labor; 3) exploration in simulation allows robots to develop closed-loop policies and error-recovery capabilities, while real-world demonstrations typically offer only expert trajectories. However, robot learning in simulations also presents its own limitations: while exploration and practicing in simulated environments are cost-effective, constructing these environments requires significant labor effort, demanding tedious steps including designing tasks, selecting relevant and semantically meaningful assets, generating plausible scene layouts and configurations, and crafting training supervisions such as reward or loss functions(Jameset al.,2020; Srivastavaet al.,2022; Guet al.,2023; Liet al.,2023a). The onerous task of creating these components and constructing individualized simulation settings of each one of the countless tasks encountered in our daily life is an overwhelming challenge, which significantly constrains the scalability of robotic skill learning even in simulated worlds. In light of this, we present a paradigm termedGenerative Simulation, marrying the advancements in simulated robotic skill learning and the latest progress in foundation and generative models. Leveraging the generative capabilities of state-of-the-art foundation models,Generative Simulationaims to generate information for all the stages needed for diverse robotic skill learning in simulation: from high-level task and skill proposals, to task-dependent scene descriptions, assets selections and generations, policy learning choices, and training supervisions. Thanks to the comprehensive knowledge encoded in latest foundation models, scene and task data generated this way have potentials to closely resemble the distribution of real-world scenarios. In addition, these models can further provide decomposed low-level sub-tasks, which can be seamlessly handled by domain-specific policy learning approaches, thereby producing closed-loop demonstrations for various skills and scenarios. A distinct advantage of our proposed paradigm lies in the strategic design of what modes of knowledge to extract from contemporary foundation models. These models have demonstrated impressive capabilities across various modalities(Touvronet al.,2023; Driesset al.,2023; OpenAI,2023; Rombachet al.,2022; Girdharet al.,2023; Kanget al.,2023), giving rise to autonomous agents capable of using a range of tools and solving a variety of tasks in thevirtualrealm(Suríset al.,2023; Yanget al.,2023; Shenet al.,2023). However, due to the absence of training data pertaining todynamics,actuations, andphysical interactions, these models are yet to fully grasp understandings of what’s essential for robots to effectively execute physical actions and interact with the surrounding environments – from discerning the precise joint torque needed for stable locomotion, to high-frequency finger motor commands needed for dexterous manipulation tasks such as rolling a dough. In contrast to recent efforts that employ these foundation models such as Large Language Models (LLMs) for directly yielding policies or low-level actions(Lianget al.,2022; Huanget al.,2023b; Wanget al.,2023b), we advocate for a scheme that extracts information that falls neatly within the capabilities and modalities of these models - object semantics, object affordances, common-sense knowledge to identify valuable learning tasks, etc. We use these knowledge to construct environmental playgrounds, and then resort to additional help from physics-grounded simulations, for robots to develop understandings of physical interactions and acquire diverse skills. We first described such a paradigm in a recent white paper(Xianet al.,2023a), sketched as a promising pathway towards generating diverse data for generalist robot learning. In this paper, we presentRoboGen, a comprehensive realization of this paradigm. RoboGen is a generative robotic agent that self-proposes skills to learn, generates scene components and configurations in simulation, labels the tasks with natural language descriptions, and designs proper training supervisions for subsequent skill learning. Our experiments show that RoboGen can deliver a continuous stream of diversified skill demonstrations, spanning tasks including rigid and articulated object manipulation, deformable object manipulation, as well as legged locomotion skills (see Figure1). The diversity of tasks and skills generated by RoboGen surpasses previous human-created robotic skill learning datasets, with minimal human involvement needed beyond several prompt designs and in-context examples. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. When queried endlessly, our system holds the potential for unleashing infinite amount of diversified demonstration data for robot learning, making a step towards fully automated large-scale robotic skill training for generalizable robotic systems.
2310.20159v1	Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts	Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8% in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https:// github.com/declare-lab/LG-VQA.	Visual understanding is one of the most complex tasks in artificial intelligence. Among the many challenges associated with it, image question answering has been formulated as a task that tests the ability of a system to understand the elements of an image in a way similar to how humans interact with images. This task involves creating models that can accurately answer questions based on the content of an image. While significant progress has been made in image question answeringwang2022image;chen2022pali, most of the existing approaches focus solely on analyzing the visual features associated with the image or the linguistic content or representation of the question (and possibly candidate answers), without utilizing additional sources of guidance. However, incorporating external guidance into these models has the potential to improve their performance and enhance their understanding of visual content. In this paper, we propose a multimodal framework,LG-VQAthat leverages language guidance to improve the accuracy of image question-answering systems. Language guidance is sourced from elements such as image captions, scene graphs, and rationales created in response to questions. These sources serve to enrich textual instructions with valuable knowledge. Our method is evaluated on the multi-choice A-OKVQA datasetschwenk2022okvqa. We choose A-OKVQA specifically because the questions in this dataset require a broad base of commonsense and world knowledge to answer. We also evaluate our method on the challenging ScienceQAlu2022learn, Visual Semantic Reasoning (VSR)liu2023visual, and IconQAlu2021iconqadatasets. With the advent of large-scale multimodal pre-trainingwang2022image;wang2022ofa, the performance in the commonly used VQA and VQA v2 datasetsantol2015vqa;goyal2017makinghas saturated. The datasets considered in this paper provide a more challenging test bed for VQA where factual, commonsense, physical, scientific, and visual knowledge are required, going beyond the usual image recognition, attribute detection tasks that are generally found in the VQA, VQA v2 datasets. Previouslyshah2019kvqa;wu2017image;zheng2021knowledgehave used concrete knowledge bases such DBPedia, Freebase, or named entity knowledge graphs in VQA. We instead show that simple language guidance without using knowledge graphs is also a powerful technique for improving VQA performance in various challenging datasets. We benchmark our approach with the CLIPradford2021learningand BLIP-2li2023blipmodels and show significant improvements over those, demonstrating that incorporating language guidance can be an effective strategy for visual question answering.
2303.08284v1	Robot Navigation in Risky, Crowded Environments: Understanding Human Preferences	Risky and crowded environments (RCE) contain abstract sources of risk and uncertainty, which are perceived differently by humans, leading to a variety of behaviors. Thus, robots deployed in RCEs, need to exhibit diverse perception and planning capabilities in order to interpret other human agents' behavior and act accordingly in such environments. To understand this problem domain, we conducted a study to explore human path choices in RCEs, enabling better robotic navigational explainable AI (XAI) designs. We created a novel COVID-19 pandemic grocery shopping scenario which had time-risk tradeoffs, and acquired users' path preferences. We found that participants showcase a variety of path preferences: from risky and urgent to safe and relaxed. To model users' decision making, we evaluated three popular risk models (Cumulative Prospect Theory (CPT), Conditional Value at Risk (CVAR), and Expected Risk (ER). We found that CPT captured people's decision making more accurately than CVaR and ER, corroborating theoretical results that CPT is more expressive and inclusive than CVaR and ER. We also found that people's self assessments of risk and time-urgency do not correlate with their path preferences in RCEs. Finally, we conducted thematic analysis of open-ended questions, providing crucial design insights for robots is RCE. Thus, through this study, we provide novel and critical insights about human behavior and perception to help design better navigational explainable AI (XAI) in RCEs.	Robots are increasingly being deployed in everyday risky and crowded environments (RCE), including shopping malls, museums, streets, and sidewalks (i.e., autonomous cars)[1]. These environments are often crowded, contain multiple sources of risk (e.g., dynamic and chaotic human-motion trajectories) and uncertainty (e.g. noisy sensor measurements, including those from camera ego-motion[2]). As robots become more integrated into such environments, they need to appropriately deal with these challenges and navigate in a safe and socially-acceptable manner[1,3,4,5]. Modeling of how humans perceive risk[6]can help us understand and close this gap. These models differ on the degree of rationality assumptions made on the human when subject to risky choices. These range from the consideration of human behavior as completely rational and possibly risk-averse (e.g., Expected Risk (ER), Conditional Value at Risk (CVaR)[7]) to non-rational and possibly risk-insensitive (e.g., Cumulative Prospect Theory (CPT)[8]). However, little is known about the validity of these models in a risky social navigation setting, as well as how they compare with humans’ self perception of risk. In particular, we are interested in understanding how robots can reason with humans, explaining their behaviors and actions, also known as Explainable Artificial Intelligence (XAI)[9]. XAI “explains” itself by opening up its reasoning to human scrutiny, resulting in better, faster, more accurate and more aligned human-robot decisions[10,11]. Risk is a relevant notion of urgency used to design navigation algorithms in robotics[12]. Accordingly, various models have been employed to quantify and reason about risk. CVaR is one such popular model adopted from finance in robotics[13,12], which captures risk aversion (i.e., “play it safe”) by employing linear and rational notions of decision making. While this is analytically convenient, it cannot capture non-linear and non-rational decision making that humans usually exhibit[14,15,16]. Recently, CPT methods[8]have been proposed[17,18]to address this shortcoming. Theoretically, it has been shown that CPT is more “expressive”[17], “versatile”, and “inclusive”[18]than CVaR and Expected Risk (ER), thus capturing a wider range of risk profiles of humans. Preliminary evidence that CPT better captures human decision making under risk can be found in applications of traffic intersection management and routing[19], and resource management settings by operators[20]. In practice, these approach is yet to be evaluated extensively in user studies pertaining navigation in RCE. To do so, user studies that employ natural or explainable metrics to humans need to be developed. Unfortunately, commonly used risk variables such as money[21], time[22], or collision probabilities[23], do not satisfy this criterion for all cases. In fact, recent studies have found that humans are often sub-optimal in planning paths in such situations[24]. These studies assume that the human is either “noisy-rational” or do not have correct environment models to choose optimally. A few other avenues of using risk for planning paths in RCE include fall risk assessment[25,26,27], risk of localization and mapping systems[28,29], and planning risk in search and rescue operations[30]. These arguments are from a robot’s perspective which acts in an expected manner and also expects the human to do so. However, from a human-centered and XAI perspectives, the robot’s “expected” behavior might lead to mistrust and confusion[31,32]. To the best of our knowledge, general studies pertaining to everyday scenarios that employ more abstract cost interpretations are lacking, and are needed for better explainable AI design for robots in RCEs. In this work, through the design of a novel user study, we bridge the gap in existing literature by characterizing human perception of risk in RCEs, comparing theoretical risk models with observed human responses, and by exploring the consistency of human perception of risk and time urgency with standard survey responses. In addition, we provide new valuable insights for XAI design. Specifically, our work aims to address the following research questions: What is the relationship between participants’ path preferences and those arising from standard risk models? What is the relationship between participants’ self-risk and self-time-urgency perception and their actual path choices? How do humans relatively weight time and risk to make navigational decisions in everyday scenarios? What are the users’ preferences to interact with robots navigating in everyday scenarios? We conducted a large scale online study (n = 82) and found that most participants do not make decisions in an expected manner (in accordance with expected risk metric) and that CPT as a risk model captures the observed responses better than CVaR and ER. Interestingly, through the application of standard questionnaires, we find that there is a mismatch between humans’ self-risk/self-time-urgency assessment and their actual choices. Additionally, participants generally give a higher weight to risk than time while choosing paths. Finally, we provide valuable insights to design XAI for robots in RCEs. For example, we found that most participants want robots that can explain its rationale behind decision-making and they also suggested user interface design to have a two-way motion intention communication between users and robots. Thus, equipped with these results and insights, XAI design can be improved to enable robots to operate and adapt to human preferences in RCEs.
2310.20193v1	FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems	Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogeneity and contribution of each client, and derive optimal aggregation weights by solving a defined optimization problem. Experimental results demonstrate the state-of-the-art performance of FedRec+ across various reference datasets.	Recommender systems have experienced significant advancements in recent years, enabling personalized recommendations for users[28]. However, traditional centralized recommender systems raise concerns about privacy leakage and data integration limitations, as they rely on a central server to store user data[21;17]. On the other hand, federated learning (FL) is a distributed learning scheme that ensures privacy preservation by allowing participants to collaboratively train a machine learning model without sharing data[14]. The combination of federated learning and recommendation systems gives rise to federated recommendation systems (FRS), offering a promising solution for privacy-preserving recommendations[22]. FRS addresses privacy and data security concerns by decentralizing the recommendation process. User data remains localized on individual devices or servers, and models are trained locally without sharing data. This decentralized approach enhances user privacy and fosters trust. Various approaches, such as federated matrix factorization[1;12], federated collaborative filtering[4;5], and federated deep learning[15], distribute the training process across each local parity and aggregate gradients on a central server. However, privacy preservation remains a major challenge in FRS. Although data decentralization reduces privacy risks compared to conventional data-center training, transmitted gradients between parties can still leak user privacy[26]. To address this, various privacy protection mechanisms, including pseudo items[10], homomorphic encryption[2;11], secret sharing[11], and differential privacy[4;26], have been incorporated into FRS. Pseudo-item method, in particular, has gained attention due to its low computation and communication costs. By uploading gradients of both interacted and randomly sampled unrated items, Pseudo items prevent the server from inferring user interactions, as shown in Figure2. However, existing pseudo-item methods suffer from limitations such as introducing significant noise or imposing high communication burdens[10;9]. Another challenge in FRS is the heterogeneity across local datasets and models, which complicates the aggregation of local recommendations into a coherent global recommendation[6]. Therefore, in this work, we are primarily interested in addressing two challenges in FRS:(1) Design an effective pseudo items method that is low noise as well as low communication cost. (2) Design an aggregation algorithm to address the heterogeneity challenge in FRS.To effectively address these challenges, we propose an innovative framework calledFedRec+, which includes an improved pseudo items method that uses feature similarity to select a subset for virtual rate assignment and an optimal aggregation strategy based on the Wasserstein Distance, as illustrated in Figure1. FedRec+ effectively preserves client privacy with low computation and communication costs and alleviates the heterogeneity problem in FRS. FedRec+ guarantees convergence with a controllable noise term. The contributions of this paper are summarized as follows: [leftmargin=*] We propose FedRec+, a privacy-enhancing FRS algorithm with explicit feedback. FedRec+ utilizes feature similarity to generate low-noise pseudo items and incorporates an optimal aggregation strategy derived from the Wasserstein distance between the global and local models to address the statistical heterogeneity problem. We provide a convergence analysis of FedRec+, demonstrating a convergence rate of\mathcal{O}(\frac{1}{\sqrt{T}}+\frac{1}{T}). This analysis explicitly highlights the impact of the pseudo-item method and the Wasserstein Distance based aggregation method on the convergence results. We evaluate FedRec+’s performance using public datasets and find that it excels in recommendation performance. Additionally, our ablation study explores the impact of the number of pseudo items. Several works have explored the use of federated learning in the context of recommendation systems.[1]propose a federated collaborative filtering method for recommendation systems. Other works that follow this line of research include[4;15;5]. Additionally, deep learning-based FedRS models have been proposed to leverage user data while ensuring privacy compliance[26]. To address privacy concerns in FRS, the use of pseudo items has been proposed.[10]Introduce the concept of pseudo items to protect users’ interacted information. However, the vanilla approach of randomly selecting unrated items as pseudo items introduces significant noise.[9]Divide clients into different groups, where one group records the gradients of unrated items uploaded by another group, effectively reducing the noise caused by unrated items. However, this approach requires additional communication and storage costs between users, which can lead to privacy leakage issues[13].[11]Combine secret sharing and pseudo items mechanisms to provide stronger privacy guarantees, while[26]combine pseudo items and Local Differential Privacy (LDP) mechanisms to protect user interaction behaviors and ratings in FRS. However, none of these methods effectively address the challenge of large noise from pseudo items while maintaining a low communication cost. In this paper, we propose FedRec+ that leverages each client’s own data information to select optimal unrated items, minimizing noise without requiring communication between users. While aggregation algorithms for federated learning (FL) have been extensively studied for various purposes such as convergence acceleration[24;3], fairness enhancement[25], and robustness improvement[19], limited research has been conducted on aggregation algorithms specifically tailored for FRS.[18]Propose FedFast, a federated recommendation model with improved aggregation and update policies. However, there has been no dedicated work addressing the heterogeneity problem in FRS from an aggregation perspective. In this paper, we propose an aggregation algorithm for FRS that utilizes Wasserstein Distance to constrain the objective, effectively tackling the heterogeneity challenge.
2302.00763v1	Collaborating with language models for embodied reasoning	Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.	Achieving complex tasks in embodied environments often requires logical reasoning. Such logical reasoning has been a challenge for machine learning(Russin et al.,2020; Mitchell,2021)– even more so with embodied agents, where the agent also has toperceiveandcontrolin its environment, in addition toreasoningabout how to accomplish a complex task. Recent large scale language models (LSLMs), however, have shown great promise for reasoning(Radford et al.,2019; Brown et al.,2020). Can this complex reasoning ability be used for embodied tasks? One major issue is that LSLMs are not embodied or grounded. They do not have a way to directly take actions in embodied environments, or of knowing what is happening in an environment. For each of these, we rely on other components of an agent model. In this work, we investigate an agent paradigm that we callPlanner-Actor-Reporter. ThePlanneris the LSLM—it reads the task description, does any required logical reasoning, and breaks the problem down into a sequence of simple instructions. These instructions are passed to theActor, which is an RL agent programmed to complete a small set of simple instructions in the environment. Finally, to complete the feedback loop, we have theReporter, which observes the environment and reports information back to the Planner so it can adjust the instructions it issues. See Figure1A. Other recent work has investigated forms of closed-loop feedback for LSLMs in embodied reasoning tasksHuang et al. (2022); Ahn et al. (2022). In this work, we generalize these approaches into a three part Planner-Actor-Reporter paradigm. We highlight the separate and crucial roles played by these components by introducing and evaluating on a series of tasks which require the agent to explore the world to gather information necessary for planning, break down complex tasks into steps, and communicate visual properties of the world back to the Planner. Finally, we demonstrate that the Reporter module can be trained with reinforcement learning (RL), reducing the need for hand-specified sources of feedback.
2303.15083v1	UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View	In the field of 3D object detection for autonomous driving, the sensor portfolio including multi-modality and single-modality is diverse and complex. Since the multi-modal methods have system complexity while the accuracy of single-modal ones is relatively low, how to make a tradeoff between them is difficult. In this work, we propose a universal cross-modality knowledge distillation framework (UniDistill) to improve the performance of single-modality detectors. Specifically, during training, UniDistill projects the features of both the teacher and the student detector into Bird's-Eye-View (BEV), which is a friendly representation for different modalities. Then, three distillation losses are calculated to sparsely align the foreground features, helping the student learn from the teacher without introducing additional cost during inference. Taking advantage of the similar detection paradigm of different detectors in BEV, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three distillation losses can filter the effect of misaligned background information and balance between objects of different sizes, improving the distillation effectiveness. Extensive experiments on nuScenes demonstrate that UniDistill effectively improves the mAP and NDS of student detectors by 2.0%~3.2%.	3D object detection plays a critical role in autonomous driving and robotic navigation. Generally, the popular 3D detectors can be categorized into (1) single-modality detectors that are based on LiDAR[shi2019pointrcnn,yang20203dssd,yin2021center,li20173d,shi2020points]or camera[huang2021bevdet,li2022bevdepth,brazil2019m3d,luo2021m3dssd]and (2) multi-modality detectors[qi2018frustum,liu2022bevfusion,vora2020pointpainting,wang2021pointaugmenting]that are based on both modalities. By fusing the complementary knowledge of two modalities, multi-modality detectors outperform their single-modality counterparts. Nevertheless, simultaneously processing the data of two modalities unavoidably introduces extra network designs and computational overhead. Worse still, the breakdown of any modality directly fails the detection, hindering the application of these detectors. As a solution, some recent works introduced knowledge distillation to transfer complementary knowledge of other modalities to a single-modality detector. In[chong2021monodistill,ju2022paint,zheng2022boosting], as illustrated in Figure1(a) and1(b), for a single-modality student detector, the authors first performed data transformation of different modalities to train a structurally identical teacher. The teacher was then leveraged to transfer knowledge by instructing the student to produce similar features and prediction results. In this way, the single-modality student obtains multi-modality knowledge and improves performance, without additional cost during inference. Despite their effectiveness to transfer cross-modality knowledge, the application of existing methods is limited since the modalities of both the teacher and the student are restricted. In[chong2021monodistill], the modalities of the teacher and student are fixed to be LiDAR and camera while in[zheng2022boosting,ju2022paint], they are determined to be LiDAR-camera and LiDAR. However, the sensor portfolio in the field of 3D object detection results in a diverse and complex application of different detectors. With restricted modalities of both the teacher and student, these methods are difficult to be applied in more situations,e.g., the method in[chong2021monodistill]is not suitable to transfer knowledge from a camera based teacher to a LiDAR based student. To solve the above problems, we propose a universal cross-modality knowledge distillation framework (UniDistill) that helps single-modality detectors improve performance. Our motivation is based on the observation that the detectors of different modalities adopt a similar detection paradigm in bird’s-eye view (BEV), where after transforming the low-level features to BEV, a BEV encoder follows to further encode high-level features and a detection head produces response features to perform final prediction. UniDistill takes advantage of the similarity to construct the universal knowledge distillation framework. As in Figure1(c), during training, UniDistill projects the features of both the teacher and the student detector into the unified BEV domain. Then for each ground truth bounding box, three distillation losses are calculated to transfer knowledge: (1) A feature distillation loss that transfers the semantic knowledge by aligning the low-level features of 9 crucial points. (2) A relation distillation loss that transfers the structural knowledge by aligning the relationship between the high-level features of 9 crucial points. (3) A response distillation loss that closes the prediction gap by aligning the response features in a Gaussian-like mask. Since the aligned features are commonly produced by different detectors, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three losses sparsely align the foreground features to filter the effect of misaligned background information and balance between objects of different scales, improving the distillation effectiveness. In summary, our contributions are three-fold: We propose a universal cross-modality knowledge distillation framework (UniDistill) in the friendly BEV domain for single-modality 3D object detectors. With the transferred knowledge of different modalities, the performance of single-modality detectors is improved without additional cost during inference. Benefiting from the similar detection paradigm in BEV, UniDistill supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Moreover, three distillation losses are designed to sparsely align foreground features, filtering the effect of background information misalignment and balance between objects of different sizes. Extensive experiments on nuScenes demonstrate that UniDistill can effectively improve the mAP and NDS of student detectors by 2.0%\sim3.2%.
2306.02149v1	Infomorphic networks: Locally learning neural networks derived from partial information decomposition	"Understanding the intricate cooperation among individual neurons in performing complex tasks remains a challenge to this date. In this paper, we propose a novel type of model neuron that emulates the functional characteristics of biological neurons by optimizing an abstract local information processing goal. We have previously formulated such a goal function based on principles from partial information decomposition (PID). Here, we present a corresponding parametric local learning rule which serves as the foundation of ""infomorphic networks"" as a novel concrete model of neural networks. We demonstrate the versatility of these networks to perform tasks from supervised, unsupervised and memory learning. By leveraging the explanatory power and interpretable nature of the PID framework, these infomorphic networks represent a valuable tool to advance our understanding of cortical function."	The human neocortex is an impressive information processing system involved in performing a wide variety of tasks from visual, auditory, tactile and gustatory perception via various forms of memory to complex planning and motor actions(Lodato and Arlotta,2015). Despite this diverse range of responsibilities, the neocortex is widely believed to consist of structurally similar, yet functionally flexible circuits(Creutzfeldt,1977, Rockel et al.,1980). On the smallest scale, these circuits consist of individual neurons whose firing is dependent on only local factors, such as the firing of other, connected neurons and the local biochemical environment – without taking into account any global or semantic knowledge about the task(Douglas and Martin,2004,2007). However, the how intricate cooperation among individual neurons helps performing complex tasks is still not well understood. To enhance our understanding of how neurons cooperate it is crucial to develop a model system in which the relevant dynamics of collaboration, specialization and self-organization can be readily observed and promoted. To grasp the fundamental and universal factors underlying these dynamics, these model systems should eliminate unnecessary idiosyncrasies associated with the specific task or the biological neurons. Such semantics-free information processing can be quantified using the framework of information theory(Wibral et al.,2017). From an information-theoretic perspective, neurons can be interpreted as information channels that convert the incoming signals into spiking activity(Wibral et al.,2015). Previous research byKay (1994)has demonstrated the feasibility of this information-theoretic approach by creating model neurons which directly optimize for certain information-theoretic objectives(Kay and Phillips,2011). Despite these successes, the framework of classical information theory is limited in its ability to adequately account for all relevant aspects of neural information processing: Information theory focuses primarily on quantities such as mutual information, which capture only information channels with a single source – albeit thuis source possibly being multivariate and high-dimensional. Biological neurons, on the other hand, often receive signals from severalclassesof inputs, such as bottom-up, top-down or lateral connections, each playing a distinct role in the information processing(Rolls and Treves,1997, Shu et al.,2003, Manita et al.,2015). A comprehensive description of the complex interactions of multiple such sources with respect to a single target variable has only recently been made possible by an extension to classical information theory known asPartial Information Decomposition(PID)(Williams and Beer,2010, Lizier et al.,2018, Gutknecht et al.,2021). PID allows the information between multiple sources and a target variable to be dissected into unique, redundant and synergistic contributions calledatoms, through which it paints a richer picture of the information processing underlying the transformation from sources to target(Williams and Beer,2010). Recently, PID has been used to describe the function of cortical neurons(Schulz et al.,2021)and the representation of information in artificial and biological neural networks(Luppi et al.,2022, Ehrlich et al.,2023, Varley et al.,2023)and has been proposed as a unifying framework to describe cortical function(Wibral et al.,2017). A litmus test for assessing the adequacy of the PID framework in capturing all relevant information processing would be the construction of model neurons which learn by directly optimizing certain PID objectives. While this idea has been present for some time(Wibral et al.,2017), a viable demonstration of feasibility has been lacking so far, primarily due to the absence of adifferentiablePID measure that lends itself to gradient descent learning. However, drawing upon our recently developed differentiable PID measureI^{\mathrm{sx}}(Makkeh et al.,2021, Schick-Poland et al.,2021), we here demonstrate for the first time that it is indeed possible to create artificial neurons that learn based on directly interpretable information-processing goal functions derived from PID. Networks build from these novel artificial neurons can learn in a supervised or unsupervised manner, and perform classification, representation learning and memory tasks. As the topology or connection structure is shaped by the input information and the information theoretic goal of learning itself we term these neurons and networksinfomorphic– a portmanteau of “information” and “morphous” to indicate that they are directly shaped by the information they process. By studying these infomorphic neurons and their information-processing capabilities, valuable insights can be gained into the collaborative dynamics and self-organization principles that govern the function of complex neural networks, such as the neocortex. The main contributions of this paper are(1)the derivation and implementation of the PID-based learning rule set out byWibral et al. (2017)by deriving analytical gradients of the relevant PID atoms and(2)demonstrations of the usefulness and flexibility of the infomorphic neurons in different learning paradigms. The remaining sections of the paper are structured in the following way: First, we explain how neurons can be interpreted as information channels (Section2.1) and how this view can be extended to account for the information processing occurring between two distinct input classes to a channel (or, alternatively, two channels with a common output) using PID (Section2.2). Based on these insights, we introduce the model of the infomorphic neuron (Section3) and demonstrate its usefulness on a collection of learning scenarios involving supervised (Section4.1), unsupervised (Section4.2) and memory learning (Section4.3). We conclude with a discussion of strengths, limitations and next steps (Section5).
2308.08529v2	Diagnosing Human-object Interaction Detectors	We have witnessed significant progress in human-object interaction (HOI) detection. The reliance on mAP (mean Average Precision) scores as a summary metric, however, does not provide sufficient insight into the nuances of model performance (e.g., why one model is better than another), which can hinder further innovation in this field. To address this issue, in this paper, we introduce a diagnosis toolbox to provide detailed quantitative break-down analysis of HOI detection models, inspired by the success of object detection diagnosis toolboxes. We first conduct holistic investigations in the pipeline of HOI detection. By defining a set of errors and the oracles to fix each of them, we can have a quantitative analysis of the significance of different errors according to the mAP improvement obtained from fixing each error. We then delve into two sub-tasks of HOI detection: human-object pair detection and interaction classification, respectively. For the first detection task, we compute the coverage of ground-truth human-object pairs as well as the noisiness level in the detection results. For the second classification task, we measure a model's performance of differentiating positive and negative detection results and also classifying the actual interactions when the human-object pairs are correctly detected. We analyze eight state-of-the-art HOI detection models and provide valuable diagnosis insights to foster future research. For instance, our diagnosis shows that state-of-the-art model RLIPv2 outperforms others mainly because it significantly improves the multi-label interaction classification accuracy. Our toolbox is applicable for different methods across different datasets and available at https://github.com/neu-vi/Diag-HOI.	Human-object interaction (HOI) detection aims to jointly detect the humans and objects that have interactions in static images. For example, the person and snowboard in Fig.1. It provides structured interpretations of the semantics of visual scenes rather than just object recognition or detection. A successful HOI detection system is an essential building block for many downstream applications, such as visual question answering[3,1,33,27,38,27], image captioning[37,2,11,21]and retrieval[7,5,29,35,30], etc. Recent advancements in HOI detection have been marked by increasing mean Average Precision (mAP) scores across standard benchmarks[13,8,12,36,14,48,43,44,46,23,41,40,28,22,39,47,17,26,19,42], denoting remarkable progress. Nonetheless, the reliance onmAPscores as a summary metric does not provide sufficient insight into the nuances of model performance, including the factors making one method perform better than another or any bottleneck for further improvement. This lack of detailed understanding may impede future advancements in the field. The same issue also existed in object detection, a sub task of HOI detection, wheremAPis also the dominant evaluation metric. To address it, diagnosis toolboxes have been designed to provide more useful quantitative break-down analysis[15,4], which have significantly boosted the development of object detection. In this paper, we aim to replicate the success of these work by introducing a toolbox designed for HOI detection, fostering future research. Generally speaking, the HOI detection problem consists of two sub-tasks: 1) detecting pairs of interacting human and object (human-object pair detection) and 2) classification of their interactions. These two tasks are not independent, but in a cascaded relationship, as shown in Fig.1. Specifically, in our toolbox, we first perform aholisticanalysis of the overall HOI detection accuracy. Inspired by the object detection diagnosis toolbox[4], we define a set of error types as well as oracles to fix them in the HOI detection pipeline across the human-object pair detection and interaction classification tasks. ThemAPimprovement, obtained by applying the oracle to each error, is used to measure the significance of different errors. The largermAPimprovement can be obtained for a particular type of error, the more it contributes to the failure of an HOI detector. We then delve into the human-object pair detection and interaction classification tasks, respectively, and conduct detailed studies. For the detection task, we mainly investigateRecallto see if it can detect all the ground-truth human-object pairs for the later stage of interaction classification. We also computePrecisionto check the noisiness level of the detections. For the interaction classification task, an HOI model needs to differentiate negative detections, where the detected human-object pairs have no actual interactions, from positive ones (i.e., with actual interactions). To diagnose such a binary classification problem, we report theAP(Average Precision) score to avoid selecting a threshold for the classification score, which is non-trivial. We also compute themAPscores for the multi-label interaction classification problem, where we assume the human-object pair detections are correct. In this way, we can disentangle two sub-tasks and focus on analyzing the interaction classification problem only to gain better insights. Our diagnosis toolbox is applicable to different methods across different datasets. Based on both such holistic and detailed investigations of the human-object pair detection and interaction classification, our toolbox provides a comprehensive diagnosis report for 8 state-of-the-art HOI detection models. With the detailed quantitative break-down results, we are now able to answer questions such as “Are one-stage HOI detection models superior to two-stage ones or vice versa?” (no clear advantage of one paradigm over the other in terms of accuracy), “What is the bottleneck of HOI detection?” (incorrect localization of the object in a human-object pair and incorrect classification of the interactions), “Why does state-of-the-art method RLIPv2[42]perform better?” (since it significantly improves the interaction classification accuracy), etc. Please refer to Section5for detailed discussions of existing HOI detection models. To our best knowledge, this is the first toolbox dedicated for the diagnosis of HOI detection in static images. We will release our toolbox and believe our work will foster the future development of HOI detection models. There are several analysis tools for object detection[24,15,4]. The seminal work[15]shows how to analyze the influences of object characteristics on detection performance and the impact of different types of false positives. But it requires extra annotations to help analyze the impacts of object characteristics, which is unlikely to be scalable in large-scale benchmark datasets. TIDE[4]improves the default evaluation tool provided by the COCO dataset[24]. It provides a more general framework for quantifying the performance improvement for different false positive and false negative errors in object detection and instance segmentation algorithms. Our quantitative analysis of different errors and different tasks in HOI detection is motivated by TIDE[4]. Simply extending such toolboxes to HOI detection is not trivial due to the coupled nature of human-object pair detection and interaction classification sub-tasks. Moreover, we delve into each of them, examining models’ behavior and identifying their bottleneck. A similar error diagnosis work[10]is proposed for the video relation detection task, which adopts a similar holistic approach inspired by TIDE[4]. In our diagnosis toolbox, we go beyond the holistic error analysis and also conduct detailed investigations in two different sub-tasks of HOI detection, considering the cascade nature of the HOI detection pipeline. In[13], the authors also define several error types of false positives. However, the definition is specifically tailored for the annotation format of the V-COCO dataset, which is not generalizable to others. In contrast, our analysis is applicable to different benchmark datasets[8,13]. In[18], the authors analyze a specific issue of HOI detection, the long-tail problem of HOI categories and points out limiting factors.[25]proposes a new metric to advance HOI generalization, preventing the model from learning spurious object-verb correlations. Both[18]and[25]are complementary to our diagnosis tool and analysis results.
2306.06480v1	Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation	Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. To prevent our relation classifier from being misled by poor connectives generated at the early stage of training while alleviating the discrepancy between training and inference, we adopt Scheduled Sampling to the joint learning. We evaluate our method on three benchmarks, PDTB 2.0, PDTB 3.0, and PCC. Results show that our joint model significantly outperforms various baselines on three datasets, demonstrating its superiority for the task.	Discourse relations, such asCauseandContrast, describe the logical relation between two text spans(Pitler et al.,2009). Recognizing discourse relations is beneficial for various NLP tasks, including coherence modeling(Lin et al.,2011), reading comprehension(Mihaylov and Frank,2019), argumentation mining(Habernal and Gurevych,2017; Hewett et al.,2019), and machine translation(Meyer,2015; Longyue,2019). Discourse connectives (e.g.,but,as a result) are words or phrases that signal the presence of a discourse relation(Pitler and Nenkova,2009). They can be explicit, as in (1), or implicit, as in (2): [I refused to pay the cobbler the full $95]Arg1because[he did poor work.]Arg2 [They put the treasury secretary back on the board.]Arg1(Implicit=However) [There is doubt that the change would accomplish much.]Arg2 When discourse connectives are explicitly present between arguments, classifying the sense of a discourse relation is straightforward. For example,Pitler and Nenkova (2009)proved that using only connectives in a text as features, the accuracy of 4-way explicit discourse relation classification on PDTB 2.0 can reach 85.8%. However, for implicit cases, there are no connectives to explicitly mark discourse relations, which makes implicit discourse relation classification challenging(Zhou et al.,2010; Shi et al.,2017). Existing work attempts to perform implicit discourse relation classification directly from arguments. They range from designing linguistically informed features from argumentsLin et al. (2009); Pitler et al. (2009)to modeling interaction between arguments using neural networks(Lei et al.,2017; Guo et al.,2018). Despite their impressive performance, the absence of explicit discourse connectives makes the prediction extremely hard and hinders further improvement(Lin et al.,2014; Qin et al.,2017). The huge performance gap between explicit and implicit classification (85.8% vs. 57.6%)(Liu and Li,2016)motivates recent studies to utilize implicit connectives for the training process of implicit relation classifiers. For instance,Qin et al. (2017)developed an adversarial model to transfer knowledge from the model supplied with implicit connectives to the model without such information, whileKishimoto et al. (2020)proposed a multi-task learning framework to incorporate implicit connectives prediction as another training objective. However, we argue that these methods are suboptimal since connectives are still not explicitly present in input texts. This is demonstrated byKishimoto et al. (2020), concluding that adding implicit connective prediction as a training objective provides only negligible gain for implicit relation classification on PDTB 2.0 (we empirically found the conclusion also held on the adversarial model). In this paper, we design a novel end-to-end model to leverage discourse connectives for the task of implicit discourse relation classification. The key inspiration is derived from the annotation process of implicit discourse relations in PDTB, which consists of inserting a connective that best conveys the inferred relation, and annotating the relation label based on both the inserted implicit connectives and contextual semantics(Prasad et al.,2008). We imitate this process by explicitly generating discourse connectives for the implicit relation classifier. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. A potential drawback of this joint model is that the poorly generated connectives at the early stage of joint training may mislead the relation classifier. One possible solution is always feeding true connectives to the implicit relation classifier for training. But it leads to severe discrepancies between training and inference(Sporleder and Lascarides,2008), since manually-annotated connectives are unavailable during evaluation(Prasad et al.,2008). To address this issue, we adopt Scheduled Sampling(Bengio et al.,2015)into our method. To be more specific, our relation classifier is first trained with hand-annotated implicit connectives and then gradually shifts to use generated connectives. We evaluate our model111https://github.com/liuwei1206/ConnRelon two English corpora, PDTB 2.0(Prasad et al.,2008), PDTB 3.0(Webber et al.,2019), and a German corpus, PCC(Bourgonje and Stede,2020), and compare it with other connective-enhanced approaches and existing state-of-the-art works. Results show that our method significantly outperforms those connective-enhanced baselines on three datasets while offering comparable performance to existing sota models. In addition, we perform the first systematic analysis of different connective-enhanced models to investigate why our method works better. Our studies show that: (1) models learn to use connectives more effectively when putting connectives in the input rather than using them as training objectives; (2) end-to-end training can improve models’ robustness to incorrectly-predicted connectives; (3) our method shows a better balance between arguments and connectives for relation prediction than other baselines. Finally, we show that connectives can effectively improve the predictive performance on frequent relations while failing on those with limited training instances.
2311.18348v1	Reconstructing Historical Climate Fields With Deep Learning	Historical records of climate fields are often sparse due to missing measurements, especially before the introduction of large-scale satellite missions. Several statistical and model-based methods have been introduced to fill gaps and reconstruct historical records. Here, we employ a recently introduced deep-learning approach based on Fourier convolutions, trained on numerical climate model output, to reconstruct historical climate fields. Using this approach we are able to realistically reconstruct large and irregular areas of missing data, as well as reconstruct known historical events such as strong El Ni\~no and La Ni\~na with very little given information. Our method outperforms the widely used statistical kriging method as well as other recent machine learning approaches. The model generalizes to higher resolutions than the ones it was trained on and can be used on a variety of climate fields. Moreover, it allows inpainting of masks never seen before during the model training.	Observational climate data is typically sparse before systematic observations such as buoys, ship measurements, or satellite measurements were introduced. Generally, the further back in time we go, the fewer observations are available[1]. Temperature and precipitation records are the best-observed climate fields in the recent past and reach back until the 19th century, but measurements are still sparse and rely heavily on interpolation especially for earlier parts of the records[2,3]. Even more severely, for many important climate variables, such as sea-ice thickness or vegetation indices, no measurements exist at all before the introduction of large-scale satellite missions. The corresponding time series often span a few decades or even only years[e.g.,4,5]. The low spatial and temporal resolution introduces large uncertainties and limits our understanding of important climatic processes[2,6,1]. Several approaches and methods to produce historical climate fields based on the available observations have been developed in the past. One approach is to run state-of-the-art weather models with observations and past weather forecasts to produce reanalysis products that provide a complete picture of the past weather and climate for the last decades[7,8]. While reanalyses are successful in providing spatiotemporally continuous and consistent data, they often struggle with specific regions and variables and inherit biases the employed numerical models suffer from[8,9]. An alternative approach is to use statistical methods to reconstruct missing information. In this regard, kriging or Gaussian process regression is widely used in the geosciences[10,11,12]. However, statistical methods typically do not include knowledge of the temporal and spatial patterns of the underlying climatic fields and therefore fail to reconstruct these patterns, especially for large missing areas. In recent years, machine learning (ML) has become widely used in geoscience and climate science, with the promise of better performance than statistical methods while still providing easy usability and, to some extent, knowledge of the underlying physical processes[13,14]. The applications of machine learning in climate science are vast and range from classical time series forecasting[15,16,17], down-scaling and post-processing of numerical models[18,19], to time series reconstruction[20,13]. Furthermore, there is a substantial ongoing effort to combine traditional numerical Earth system models with machine learning methods to leverage the advantages of both approaches[21,22,14,23,24,25,26,27,28]. In this study, we consider the reconstruction of spatial climate fields as an image inpainting problem. Inpainting images based on given information is a classical problem in computer vision and many approaches have been proposed in recent years[29,30,31]. We apply the recently introduced state-of-the-art deep learning approachResolution-robust Large Mask Inpainting with Fourier Convolutions(LaMa)[32]to reconstruct different climate fields with a focus on surface temperature records. We train our model on numerical climate model output from the Coupled Model Intercomparison Project to reconstruct the missing measurements in observational data. Our method is able to reconstruct climate fields with very sparse information and highly irregular missing data. We show that our approach outperforms kriging and other machine learning methods. Moreover, it is able to inpaint different data sets than the ones it was trained on, and can be used on a variety of structurally different climatic fields at varying resolutions. The surface temperature is one of the most important climate variables, as a direct measure of climate change. Global instrumental temperature records reach back to the mid-19th century[2]with local observations reaching back as far as the mid-17th century[33]. However, on average, less than 30% of Earth’s surface before the year 1900 AD have measurements in the state-of-the-art observational data set HadCRUT4 (Fig.1a). This is similar for other widely used long-term temperature data. Therefore, surface temperature records serve as perfect proof-of-concept application for the image inpainting task in climate science.
2310.03376v1	Procedural Text Mining with Large Language Models	Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.	Extracting complex knowledge from unstructured sources is a challenge: in the industrial domain, for example, troubleshooting documents may contain the description of long and articulated procedures (i.e., sequences of steps to be performed in a precise order and under specific conditions) and those natural language instructions may be represented in very different textual forms, thus making it hard for a knowledge extraction algorithm to correctly identify and structure the relevant information. Oftentimes, automatic extraction is followed by manual revision of domain experts. In any case, all machine-learning-based methods require training data which is often not readily available, therefore novel approaches are emerging to exploit interactive dialogues and language models(bellan2021process). Extracting procedural knowledge from human natural language instructions is a challenging task. Firstly, natural language instructions are not interpretable by machines. In the easiest case, the instructions are given as numbered lists which can easily be identified. However, complications arise when the document contains procedures in different forms: a list without numbers, an indented text or simply a full text in which the different steps are connected by conjunctions like ”then”, ”afterwards”, etc. Secondly, procedures can either be composed of only simple steps or contain other sub-procedures that are located elsewhere in the document. Thirdly, the procedures can differ substantially from one document to the other because of different authors’ and editors’ styles, but the goal would be to integrate information from different documents. Specifically, we investigate the potential of large language models (LLMs) in the context of extracting procedural knowledge from unstructured PDF documents. LLMs demonstrate remarkable capabilities in natural language processing, surpassing those possible using conventional symbolic AI and machine learning technologies(mahowald2023dissociating). Nevertheless, these models often lack knowledge of nuanced, domain-specific details and are susceptible to hallucinations. This paper investigates the practical application of LLMs, with a particular focus on the advanced GPT-4 (Generative Pre-trained Transformer 4), to address the complex task of extracting procedures from unstructured PDF documents. The core of our research revolves around an incremental question-answering methodology, with a specific emphasis on harnessing LLMs in both zero-shot and in-context learning scenarios. Our study is structured to encompass two distinct approaches to in-context learning. The first approach involves the incorporation of an ontology containing definitions and procedural steps, while the second approach integrates a limited dataset tailored for few-shot learning. This comprehensive investigation not only highlights the considerable potential of our chosen approach but also underscores the critical role played by customised in-context learning. These tailored modifications are poised to make significant progress in tackling a persistent challenge within deep learning-based NLP techniques: the scarcity of essential training data for procedure extraction. Aligned with our exploration, three fundamental research questions guide our investigation: RQ1: How well is ChatGPT4 able to list the steps and substeps in the text versus the ontology settings? RQ2: Is in-context learning beneficial for procedural text mining? RQ3: Regardless of the ontology instantiation, is ChatGPT4 capable of the correct application of the ontology? Through systematic empirical inquiry, this study not only contributes to the enhancement of procedural text mining but also offers insights into the capacity of in-context learning enhancements to surmount the constraints stemming from inadequate training data. The significance of this research extends beyond procedural extraction, resonating within the broader landscape of NLP applications and cultivating the evolution of more sophisticated and adaptable information retrieval systems. Our code and dataset is publicly releasedhttps://github.com/jd-coderepos/proc-tm/. The paper is structured as follows. We begin with a motivating example for our work insection 2; then we describe the procedural ontology insection 3. Insection 4, we propose our approach and introduce the experimental dataset used for the task of procedural text mining insection 5. Our experimental results are discussed both quantitatively and qualitatively next insection 6. Finally a brief discussion on related work is offered insection 7and concluding remarks insection 8.
2401.08117v1	E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning	The bio-inspired event cameras or dynamic vision sensors are capable of asynchronously capturing per-pixel brightness changes (called event-streams) in high temporal resolution and high dynamic range. However, the non-structural spatial-temporal event-streams make it challenging for providing intuitive visualization with rich semantic information for human vision. It calls for events-to-video (E2V) solutions which take event-streams as input and generate high quality video frames for intuitive visualization. However, current solutions are predominantly data-driven without considering the prior knowledge of the underlying statistics relating event-streams and video frames. It highly relies on the non-linearity and generalization capability of the deep neural networks, thus, is struggling on reconstructing detailed textures when the scenes are complex. In this work, we propose \textbf{E2HQV}, a novel E2V paradigm designed to produce high-quality video frames from events. This approach leverages a model-aided deep learning framework, underpinned by a theory-inspired E2V model, which is meticulously derived from the fundamental imaging principles of event cameras. To deal with the issue of state-reset in the recurrent components of E2HQV, we also design a temporal shift embedding module to further improve the quality of the video frames. Comprehensive evaluations on the real world event camera datasets validate our approach, with E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the second best by over 40\% for some evaluation metrics.	Inspired by the human visual system, Silicon RetinaMahowald (1991)has pioneered an approach to perceptual sensing with event cameras or Dynamic Vision Sensors (DVS)Lichtsteineret al.(2008); Poschet al.(2010); Berneret al.(2013)and gained significant interests from both academia and industry. Unlike traditional cameras, event cameras detect microsecond-level intensity changes, generating an asynchronous stream of ‘events’, termed as event-stream. Event cameras offer several advantages over conventional CCD/CMOS cameras, including high temporal resolution, high dynamic range of up to 140dBLichtsteineret al.(2008), and low resource consumption due to the sparse nature of event-streams. For example, the DVS128 sensor platform consumes 150 times less energy than a conventional CMOS cameraLichtsteineret al.(2008). Despite the appealing advantages of event cameras, the non-structural event-streams are not inherently compatible with traditional computer vision methodologiesScheerlincket al.(2020)and the visualization is not intuitive for human users to understand. To address the above issue, the research on events-to-video (E2V), which aims to generate video frames from pure event-streams, has been raised to provide convenient and intuitive access to the rich information encapsulated in the sparse and non-structure event-streams. There have been a number of successful approaches for E2V task, such as E2VIDRebecqet al.(2019), FireNetScheerlincket al.(2020), SPADE-E2VIDCadenaet al.(2021), and ET-NetWenget al.(2021). However, the quality of the video frames generated by the existing E2V approaches is still not satisfactory and fail to recover detailed texture for the complex scenesErcanet al.(2023). This issue is predominantly attributed to the fact that many of these approaches, such as E2VID and ET-Net, primarily adopt a purely data-driven approach to learn the mapping from event-streams to video frames directly. However, the purely data-driven approaches are lack of interpretability and flexibilityShlezingeret al.(2023), and they do not take into account the prior knowledge of the underlying statistics relating event-streams and video frames. Therefore, their performance is largely dependent on the non-linearity and generalization capability of the neural networks, which poses significant challenges when the scenes to be reconstructed are complexJarrettet al.(2009). To address the aforementioned challenges, we introduceE2HQV, a novel E2V paradigm designed to produce high-quality video frames from events. This is achieved through a model-aided deep learning framework that integrates a theory-inspired E2V model. Rooted in the fundamental imaging principles of event cameras, this theory-inspired E2V model elucidates the relationship between consecutive frames and their associated inter-frame event-streams, offering valuable prior knowledge that enhances the learning efficacy of our deep learning framework. As shown in Figure1, instead of generating video frames in a pure data-driven approach, E2HQV estimates a number of intermediate key parameters defined by the theory-inspired E2V model then reconstructs the video frames accordingly. The contributions of this work can be summarized as: We propose E2HQV, a novel high-quality video frames generation approach from event-streams by facilitating a model-aided learning framework which learns the key parameters defined by a theory-inspired E2V model and generates high quality video frames accordingly. According to the imaging principle of event camera and relation between video frames and event-stream, a theory-inspired E2V model is derived to guide the design of the model-aided learning framework. A new temporal shift embedding module is designed to deal with the perturbation introduced by the state-reset mechanism of the recurrent components in the framework and ensuring seamless fusion of events and reconstructed frames. Through extensive experiments on mainstream event-based video reconstruction datasets, E2HQV consistently exhibits superior performance over state-of-the-art (SOTA) approaches. Remarkably, for certain evaluation metrics, E2HQV surpasses the next best approach by a substantial margin of over 40%.
2301.02873v1	"""It's a Match!"" -- A Benchmark of Task Affinity Scores for Joint Learning"	While the promises of Multi-Task Learning (MTL) are attractive, characterizing the conditions of its success is still an open problem in Deep Learning. Some tasks may benefit from being learned together while others may be detrimental to one another. From a task perspective, grouping cooperative tasks while separating competing tasks is paramount to reap the benefits of MTL, i.e., reducing training and inference costs. Therefore, estimating task affinity for joint learning is a key endeavor. Recent work suggests that the training conditions themselves have a significant impact on the outcomes of MTL. Yet, the literature is lacking of a benchmark to assess the effectiveness of tasks affinity estimation techniques and their relation with actual MTL performance. In this paper, we take a first step in recovering this gap by (i) defining a set of affinity scores by both revisiting contributions from previous literature as well presenting new ones and (ii) benchmarking them on the Taskonomy dataset. Our empirical campaign reveals how, even in a small-scale scenario, task affinity scoring does not correlate well with actual MTL performance. Yet, some metrics can be more indicative than others.	For more than two decades since its inceptioncaruana1997multitask, Multi-Task Learning (MTL) has been extensively studied by the Deep Learning community. For practitioners interested in the best strategy to learn a collection of tasks, the promises of MTL are numerous and attractive. First, learning to solve several tasks simultaneously can be more cost-efficient from a model development and deployment perspective. Second, if the tasks learned together cooperate, MTL can even outperform its Single-Task Learning (STL) counterpart for the same computational coststandley2020tasks. However, MTL potential advantages are tempered by the difficulty of estimatingtask affinity, i.e., identify tasks benefiting from joint learning, without testing all combinations of tasks. This calls fortask affinity scores– to quantify a priori and at a cheap computational cost the potential benefit of learning tasks together. The quest for the perfect affinity score is further exacerbated by MTL performance’s strong dependency on the learning context, i.e., the data and models used for training. For instance, tasks cooperating in one learning context can result in competition when using slightly different data or modelsstandley2020tasks. Recent worksfifty2021efficiently;standley2020taskshave integrated this context-dependency when designing task grouping strategies. While these approaches avoid a complete search across all task combinations, they still require training and comparing some MTL models for the final network selection. Furthermore, those studies show that even in a small-scale scenario, MTL performance cannot be accurately predicted without actually performing MTL. Despite providing assessment of task affinity, previous literature lacks of a broader comparison of the associated scores. In this work, we take a first step in recovering this gap bypresenting an empirical comparison of several task affinity scoring techniques. Some of these scores are inspired by previous literature ranging from Transfer Learning to Multi-Task Learning:taxonomical distancezamir2018taskonomy,input attribution similarityinput_attr_aff_metric_ref,representation similarity analysisTL_RSA,gradient similaritygrads_clashesandgradient transferencefifty2021efficiently. We benchmark an additional affinity score which is an original proposal:label injection. We evaluate all of them on the public Taskonomy datasetzamir2018taskonomywhich is a well-known large benchmark spanning several Computer Vision tasks. Note that our objective is not to present a novel state-of-the-art MTL architecture but rather an objective benchmark of task affinity estimation techniques. More specifically we aim to understand if task affinity scores can (i) be used as proxy for true MTL performance and (ii) suggest the best partner task to improve the performance of a target task. These scores and their discussion aim at helping practitioners gauge the benefit of MTL for their own set of tasks. Insection2, we review the state of the art on MTL affinity characterization. Insection3, we present the affinity scores selected for benchmarking and detail our evaluation protocol. We present our results insection4and discuss the advantages and limitations of these scores insection5.Section6concludes the paper.
2308.04365v6	SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling	Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non-linear relationships. We provide open-source code, and a tutorial notebook with example usage, accentuating the easy-to-use nature of the method.	Imagine you wish to estimate multiple causal effects from observational data. Despite the challenging nature of the task, it nonetheless represents one of the core goals of science and causality\parencitePearl2018TBOW, Pearl2012, Vowels2021, Vowels2021DAGs. Indeed, in the absence of experimental data, we must do everything we can to ensure the causal relevance of our statistical inferences. Otherwise, our estimates cannot be tied to their associated theories\parenciteVowels2021, Scheel2020 and the estimates effectively represent arbitrary functions of the observed data, subject to ambiguous, pseudo-causal interpretations\parenciteGrosz2020, Rohrer2018, Hernan2018. To this end, researchers in the domains of psychology and social science have begun to advocate for the increased adoption of causal Directed Acyclic Graphs (DAGs), which aid in (a) the clear, formal specification of a causal theory as a mathematical but nonetheless intuitive, visual object, (b) unbiased estimation of the target quantities of estimation (e.g., effect sizes)\parenciteVowels2023_prespec, Rohrer2018. Unfortunately, DAGs themselves only get us so far in terms of achieving unbiased estimation of causal effects. Whilst they help us deal with the specification of thestructuralaspect of a model/theory (by representing an ordered causal process), and whilst they enable us to express a target quantity as a function of the observed distribution, they do not help with the estimation itself. Furthermore, they do not help us with thefunctionalspecification of the relationships between variables. Indeed, one of the strengths of DAGs is that they are non-parametric, and make very limited assumptions about the underlying functional form. For instance, they tell us nothing about whetherYis a linear function of justXor whether it is a linear function ofX^{2}. Thus, to arrive at meaningful effect size estimates, the choice of the associated estimation technique and its associated level of functional adaptability must be made. One option that researchers have is to use structural equation modeling or linear/logistic regression for the estimation of the associated effects, which make the assumption that the relationships are linear/linear in the logit space. The problem with this is that such an assumption of linearity can lead to a similar level of biased estimation\parenciteVowels2021, vanderLaan2011, vanderLaan2014 to that which results from structural misspecification. Such misspecification thereby undermines the otherwise advantageously function-agnostic nature of the DAG. In order to avoid making such unnecessary assumptions about the functional form underlying the causal relationships represented in the DAG, we recommend the use of machine learning. Specifically we propose Super Learner Equation Modeling (SLEM), an integrated framework for machine learning based causal inference with DAGs. Within this SLEM framework, we present the DAG Learner estimator object in the form of easy-to-use, open-source Python code, including simulations and a tutorial-style notebook.111The code can be found in supplementary material or athttps://github.com/matthewvowels1/SLEM.Essentially, for any user-specified DAG, and according to the SLEM framework, the DAG Learner implements a set of general machine learning models which are used to estimate, in a data-driven but causally-constrained manner, all associated path coefficients. Furthermore, our framework allows for the estimation of the effect of (optionally multiple, simultaneous) user-specified interventions, thereby facilitating a general, and easy-to-use tool for non-parametric causal inference. In our view, the availability of such a tool is overdue - whilst the required techniques have existed for some time, they have never been combined in such a way. The paper is structured as follows: First, we briefly discuss some background theory relating to DAGs, causality, and machine learning. Secondly, we describe the proposed framework, beginning with the specification of the DAG, as well as the chosen machine learning method. Then, we provide some worked examples and key simulations. Finally we discuss the associated limitations of the method in relation to some existing alternative approaches, and close with a summary. Note that whilst the success of the propose method rests on existing work in the domains of causality and machine learning, to the best of our knowledge we are the first to bring the two together in such an integrated, easy-to-use methodology.
2305.16487v2	EgoHumans: An Egocentric 3D Multi-Human Benchmark	We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing tennis, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human activities and fast-moving egocentric views. We rigorously evaluate existing state-of-the-art methods and highlight their limitations in the egocentric scenario, specifically on multi-human tracking. To address such limitations, we propose EgoFormer, a novel approach with a multi-stream transformer architecture and explicit 3D spatial reasoning to estimate and track the human pose. EgoFormer significantly outperforms prior art by 13.6% IDF1 on the EgoHumans dataset.	Understanding humans in 3D from the egocentric view is key to building immersive social telepresence[lawrence2021project,bamodu2013virtual,ma2021pixel,lombardi2021mixture], assistive humanoid robots[goodrich2013teleoperation,fridin2014acceptance,piezzo2017feasibility], and augmented reality systems[azuma1997survey,billinghurst2015survey,carmigniani2011augmented]. A crucial step in this direction is to obtain 3D supervision at scale for deep learning models to generalize to the real world. However, unlike the large-scale 2D benchmarks[deng2009imagenet,lin2014microsoft,li2019crowdpose,johnson2010clustered,cordts2016cityscapes], the diversity of the 3D benchmarks[joo2015panoptic]is severely limited - primarily because manual annotation in the 3D space is impractical. As a result, existing popular 3D benchmarks[ionescu2013human3,joo2015panoptic,von2018recovering,mehta2017monocular,li2021ai,hassan2019resolving]are constrained to indoor environments or, at most, two human subjects if outdoors, stationary/slow camera motion, with limited occlusion. Furthermore, the majority of these benchmarks only portray the third-person view. Recent progress has been made in constructing egocentric benchmarks[xu2019mo,ng2020you2me,guzov2021human,zhang2022egobody]. However, they suffer from the same diversity pitfalls, making it difficult to evaluate how close the field is to fully robust and general solutions. To drive advances in the field, we propose a benchmark,EgoHumans, that includes challenging scenarios ignored in previous studies and a novel method,EgoFormer, that outperforms prior art as a starting point for the evaluations. EgoHumans is a new egocentric benchmark consisting of high-resolution videos and comprehensive ground truth annotations such as camera parameters, 2D bounding boxes, human tracking ids[dendorfer2020mot20], 2D/3D human poses, and 3D human meshes[loper2015smpl]. EgoHumans goes beyond previous benchmarks in important ways. First, it captures outdoor videos of unconstrained environments and dynamic human activities, including challenging sporting events such as fencing, badminton, volleyball, etc. Second, the activities are unchoreographed to truly capture thein-the-wildphilosophy of our work. Our video sequences include fast ego-camera motion, human-human occlusion, truncation, and humans appearing at a wide range of spatial scales. We leverage a flexible multi-camera setup consisting of Meta’s Aria glasses[aria_pilot_dataset], with an RGB and two greyscale cameras, for the egocentric view and stationary secondary RGB cameras for the auxiliary views (see Fig.4). Such camera combination allows us to accurately track and triangulate human poses in 3D for a long duration without using visual markers[ionescu2013human3]or additional sensors[von2018recovering]. The natural form factor of glasses[maimone2013computational]coupled with the RGB and stereo cameras closely resembles the human vision[matthies1989dynamic]. Last, as a by-product of our capture setup, we provide 3D annotations for the multi-view secondary cameras. We hope these annotations allow the ability to move fluidly between the egocentric and secondary perspectives[li2021ego]and inspire new research for holistic human understanding. To our knowledge, EgoHumans is the only multi-human 3D egocentric benchmark with these attributes. We generate high-quality 3D ground truth by leveraging state-of-the-art visual-inertial odometry algorithm (VIO)[aria_pilot_dataset], which is robust to fast head motion and sudden changes in the eye gaze - frequently observed in natural human behavior[zhang2020wandering]. All the cameras in our multi-view capture are aligned to a single world coordinate system using Procrustes alignment[luo2002iterative]of the camera poses. EgoHumans consists of 125k egocentric RGB images and 410k human instance annotations (Tab.1) capturing high-energy activities in various locations, clothing, and lighting conditions with severe occlusion. We annotate the tracking ids, bounding boxes, and 2D/3D human poses for all views using off-shelf estimators[jin2020whole,wang2020deep]and manual supervision. With carefully calibrated camera parameters and the multi-view 2D poses for a video, we optimize for 3D skeletons using triangulation[iskakov2019learnable]and refinement constraints like constant limb length, joint symmetry, and temporal consistency[vo2020self]. Finally, we build an efficient multi-stage motion capture pipeline to fit the SMPL[loper2015smpl]body model to the 3D human skeletons. The scale and diversity of the EgoHumans dataset allow unprecedented opportunities to evaluate and improve egocentric methods. Specifically, we evaluate existing methods for multi-human tracking. Our results show that prior art is susceptible to common failures like person-id switching due to rapid camera motion, occlusion, and unconstrained human activities. Inspired by this, we presentEgoFormer, a novel 3D human tracking approach with multi-stream transformer architecture that effectively performs human depth reasoning in a camera-agnostic frame of reference. Our proposed method uses self-attention to aggregate multi-view spatial information from the RGB, left, and right stereo cameras simultaneously. EgoFormer significantly outperforms existing state-of-the-art tracking methods[zhang2022bytetrack,rajasegaran2022tracking,cao2022observation]by13.6% IDF1 score on EgoHumans. Our contributions are summarized as follows. EgoHumansis the first multi-human 3D egocentric dataset capturing unconstrained human activities in the wild. We provide high-quality 3D ground truth from egocentric and secondary views for all humans. We benchmark existing state-of-the-art methods for multi-human tracking and highlight their fundamental limitations on egocentric views. We proposeEgoFormer, a 3D tracking method that uses a multi-stream spatial transformer encoder for depth reasoning from the ego view. Our method consistently outperforms the prior art on the EgoHumanstestset.
2308.06776v1	Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches	Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.	"Image denoising aims to recover noise-free images from noisy observations by reducing the potential noise. Although it is one of the oldest and most classical tasks in low-level computer vision, its fundamental nature continues drawing much interest. In general, existing image denoising algorithms can be divided into three groups: filtering-based[2,3,4,5,66], model-based methods[9,32,30,12,69,61]and learning-based[17,19,21,41,36,40,44,39,38,25]methods. The additive white Gaussian noise (AWGN) assumption is widely used in image denoising. However, it is complex and challenging to adaptively achieve denoising based on filtering-based or model-based methods for high performance. In contrast, learning-based methods have demonstrated their superiority in image denoising. However, these methods[54,21,25,53,aind1,100]are data-driven and typically require pairs of clean-noisy datasets to train their models. These noise samples are usually obtained through a predefined AWGN formulation that assumes the noise is signal-independent. On the contrary, the real-world noise is more complex and much different from the ideal AWGN assumption. Using the AWGN model directly for the real scenes leads to poor performance. Therefore, numerous methods[37,40,38,39,74,90,aind1]have been proposed to capture paired clean-noisy image datasets from real scenes to promote the training of deep networks. However, these paired image-based methods focus on enhancing performance by improving network structures, and acquiring well-aligned pairs of clean-noisy images is time-consuming and laborious. To solve the aforementioned problems, unsupervised denoising-based methods[chen,hong,1000,dbsnl]have emerged. The existing approaches are typically based on generative adversarial network (GAN) frameworks, which mainly focus on generating higher quality pseudo-noisy images. GAN2GAN[cha]identified one of the key limitions of unsupervised denoising frameworks is the gap between the real and synthetic images, and proposed a novel approach using multiple generators and discriminators to generate images that closely conform to the real noise distribution. However, the performance of existing unsupervised denoising frameworks remains unsatisfactory due to the difficulty of adversarially training. Moreover, after training the model, the existing frameworks cannot further maximize the denoising potential without significantly changing its structure or increasing the inference complexity (e.g., using certain self-ensemble strategy) for denoisers. To address the previous limitations, we innovatively propose an unsupervised real-world denoising network called Self Collaboration Parallel Generative Adversarial Branches (SCPGabNet). The self-collaboration (SC) strategy, which provides the framework a powerful self-boosting capability. This enables the denoisers obtained from the conventional GAN framework to continuously evolve themselves and significantly improve their performance. The major contributions of our method are as follows: We design a novel filter-guided synthetic noisy image generator with the noise extraction (NE) module to synthesize high-quality clean-noisy image pairs, which serve as the foundation for implementing the SC strategy. We propose an effective parallel generative adversarial branches framework with complementary “self-synthesis"" and “unpaired-synthesis"" constraints as our powerful baseline. We are the first to propose the SC strategy, which significantly enhances the performance of the GAN-based denoising framework without increasing its inference complexity. Experimental results demonstrate the superiority of our SCPGabNet over state-of-the-art unsupervised image denoising methods with large margins on the SIDD and DND benchmarking datasets."
2310.06283v1	Towards More Efficient Depression Risk Recognition via Gait	Depression, a highly prevalent mental illness, affects over 280 million individuals worldwide. Early detection and timely intervention are crucial for promoting remission, preventing relapse, and alleviating the emotional and financial burdens associated with depression. However, patients with depression often go undiagnosed in the primary care setting. Unlike many physiological illnesses, depression lacks objective indicators for recognizing depression risk, and existing methods for depression risk recognition are time-consuming and often encounter a shortage of trained medical professionals. The correlation between gait and depression risk has been empirically established. Gait can serve as a promising objective biomarker, offering the advantage of efficient and convenient data collection. However, current methods for recognizing depression risk based on gait have only been validated on small, private datasets, lacking large-scale publicly available datasets for research purposes. Additionally, these methods are primarily limited to hand-crafted approaches. Gait is a complex form of motion, and hand-crafted gait features often only capture a fraction of the intricate associations between gait and depression risk. Therefore, this study first constructs a large-scale gait database, encompassing over 1,200 individuals, 40,000 gait sequences, and covering six perspectives and three types of attire. Two commonly used psychological scales are provided as depression risk annotations. Subsequently, a deep learning-based depression risk recognition model is proposed, overcoming the limitations of hand-crafted approaches. Through experiments conducted on the constructed large-scale database, the effectiveness of the proposed method is validated, and numerous instructive insights are presented in the paper, highlighting the significant potential of gait-based depression risk recognition.	Depression is recognized by the World Health Organization (WHO) as one of the primary contributors to the global disease burden[44]. Within the clinical framework, the American Psychiatric Association’s Diagnostic Statistical Manual of Mental Disorders- Fifth Edition (DSM-5) classifies depression as persistent depressive mood or loss of interest and pleasure in activities. It affects approximately 280 million individuals worldwide, comprising approximately 3.8% of the global population[43]. Depression profoundly impacts individuals across multiple domains, including a decline in their overall quality of life[30,20], compromised social functioning[25,23], detrimental effects on physical health[6,46], and an elevated susceptibility to suicide[24,9]. Recognizing the significance of early detection and timely intervention is crucial for promoting remission, preventing relapse, and mitigating the emotional and financial burdens associated with this condition[12,13,22]. However, depression is notably underdiagnosed[12,62,55]. A survey involving 33,653 physician-patient interactions reveal that in the primary care setting, less than 5% of adults are screened for depression[1]. And it has been reported that at least 25% of patients go undiagnosed[2], with a significant majority of those seeking assistance from primary care physicians not receiving suitable treatment, particularly in low-income and middle-income countries[64,31]. The main challenge in screening depression in the primary care is the absence of objective indicators to recognize depression risk[5]. Unlike many physiological illnesses, depression lacks precise biomarkers, clinicians primarily rely on clinical criteria such as psychological questionnaires and patients’ self-reports[21]. This introduces potential issues: Individuals may provide biassed or inaccurate information due to personal inclinations, social expectations or difficulties in recalling past experiences[40,32,8]. Approximately 50% of patients have been observed to negate experiencing depressive feelings[5]. Besides, variability in linguistic expression can hinder accurate communication of emotions or experiences[4,28,41,14]. These issues can lead to incorrect depression risk recognition, and missing the optimal window for further treatment[47,15,16]. Another primary issue with questionnaire-based approaches for depression risk recognition pertains to inefficiency. Psychological assessments are time-consuming and often encounter a shortage of trained medical professionals, resulting in prolonged referral processes. Consequently, this may hinder timely psychological intervention, potentially exacerbating mental health issues[11,7]. In addition, psychological questionnaires are typically intended for periodic, rather than frequent, evaluations, often yielding only one or two measurements annually. This limited frequency may not effectively capture long-term condition trends in a timely manner, in light of the high relapse rates associated with depression: 21% at 12 months, 30% at 2 years, and 42% at 5 years[29]. Gait has been shown to be an essential manifestation of depression risk[53,50,19]. In particular, gait is modulated by the advanced neural center[56], which is also implicated with the pathophysiology of depression[58]. Many researches have demonstrate the association between depression risk and gait characteristics. Notably, specific abnormalities in gait, such as decreased vertical head movement[39], reduced range of motion in limbs, and a decelerated pace[34], are stable indications of depression. Hence, the gait-based depression risk recognition offers advantages compared to traditional psychological assessment methods: Objective biomarker: Gait serves as an objective biomarker for recognizing the risk of depression within the primary care setting, thereby mitigating the influence of subjective biases, societal expectations, or challenges in recollecting past experiences, which often lead to inaccurate information. High efficiency: Gathering gait data through cameras proves to be a highly efficient method, surpassing the time-consuming process of administering psychological questionnaires. This approach eliminates the requirement for trained professionals to collect data, thereby reducing both time and manpower costs. Despite preliminary investigations into the relationship between gait and depression risk, as well as attempts to recognize depression risk based on gait features, existing research suffers from two significant limitations. Firstly, these studies are often constrained by limited and homogeneous gait data, typically lacking the inclusion of gait data captured from different perspectives and with varying attire. Consequently, the generalizability of these research findings remains to be established, as the conclusions drawn may not necessarily apply to more diverse gait data. Furthermore, the accessibility of these datasets is severely restricted, impeding research reproducibility and hindering progress in the field. This limitation not only undermines the ability to validate previous research but also hampers advancements in this domain. Secondly, these studies commonly rely on hand-crafted gait features for analysis. However, human gait is a complex form of motion, and hand-crafted gait features often provide a limited representation, only capturing a fraction of the intricate associations between gait and depression risk. To address these limitations, we have established the largest gait-based dataset for depression risk recognition to date, encompassing over 1,200 subjects and more than 40,000 gait sequences. This dataset incorporates diverse perspectives and attire variations, providing a comprehensive representation of gait data. The depression risk of the participants was annotated using two commonly used depression risk assessment scales in the primary care. Leveraging this rich dataset, we propose a novel data-driven approach for depression risk recognition based on deep learning in this paper. The proposed method overcomes the problems associated with hand-crafted features by autonomously learning gait features that are relevant to depression risk. There are two notable characteristics in depression risk related gait features: Firstly, the main features associated with depression risk are dynamic features, which refer to the temporal aspects of gait during the walking process. Secondly, gait features that are associated with depression risk can manifest as either local details or involve the entire body. Based on these two characteristics, this paper proposes a deep learning model with dynamic feature modeling as its core. In the feature extraction process, both local dynamic features and global dynamic features are effectively integrated. This innovative methodology offers a fresh perspective for investigating the association between gait and depression risk. The contributions of this paper can be summarized as follows: We build a large-scale dataset for gait-based depression risk recognition. This dataset serves as a benchmark, propelling advancements in the field of depression risk recognition. Ultimately, it aims to enhance the efficiency of depression screening in the primary care, ensuring that more individuals receive the assistance they need. Based on the insights on depression-related gait features, we introduce a deep learning model with dynamic feature modeling at its essence. This model adeptly merges local and global features, yielding a comprehensive integration. A plethora of experiments are conducted to delve deeper into the aberrant gait patterns that are associated with the risk of depression. These experiments offer enlightening perspectives that contribute to the field of depression research. The remainder of this paper is organized as follows: Section2presents a brief literature review of the related work. Section3introduce the gait-based dataset for depression risk recognition. The proposed recognition model are described in detail in Section4. The configurations and results of experiments are presented in Section5. Finally, the conclusion of this paper is summarized in Section6.
2311.07861v2	Overview of the TREC 2023 Product Product Search Track	This is the first year of the TREC Product search track. The focus this year was the creation of a reusable collection and evaluation of the impact of the use of metadata and multi-modal data on retrieval accuracy. This year we leverage the new product search corpus, which includes contextual metadata. Our analysis shows that in the product search domain, traditional retrieval systems are highly effective and commonly outperform general-purpose pretrained embedding models. Our analysis also evaluates the impact of using simplified and metadata-enhanced collections, finding no clear trend in the impact of the expanded collection. We also see some surprising outcomes; despite their widespread adoption and competitive performance on other tasks, we find single-stage dense retrieval runs can commonly be noncompetitive or generate low-quality results both in the zero-shot and fine-tuned domain.	At TREC 2023, we hosted the first TREC Product Search Track, looking to create a reusable general benchmark for evaluating the performance of retrieval methods in the product search domain. We focus on providing a benchmark similar in scale and format to NQKwiatkowskiet al.(2019), or the Deep Learning TrackCraswellet al.(2021)but focused on product search. In providing a simple-to-use dataset, we believe broad experimentation using popular retrieval librariesLinet al.(2021)Gaoet al.(2022)can lead to broad improvements in retrieval performance.In this first year of the track, we created a novel collection based on the ESCI Product Re-ranking datasetReddyet al.(2022), sampled novel queries, created enriched metadata in the form of additional text and images along with seeded evaluation results with a broad range of baseline runs to aid in collection reusability and to allow iteration and experimentation on the use of additional context.Unlike previous product search corpora, the Product Search Track is multi-modal and has a large enough scale to explore the usage of neural retrieval methods. We observe somewhat surprising results using this scaled dataset and a wide variety of baseline runs. Single-stage retrieval models that leverage vector representations do not consistently outperform traditional retrieval methods such as BM25. Moreover, in the zero-shot setting, we find that larger vector-based models do not always beat their more minor variants, which is at odds with other evaluation corpora such as MTEBMuennighoffet al.(2023). Finally, while additional metadata can improve retrieval performance at a macro level, extra information cannot guarantee performance. In evaluating per-query performance, we find that vector-based systems lose performance with the other metadata. Please see the participant papers for more insights about what we learned this year.
2306.02098v1	Towards Complex Real-World Safety Factory Inspection: A High-Quality Dataset for Safety Clothing and Helmet Detection	Safety clothing and helmets play a crucial role in ensuring worker safety at construction sites. Recently, deep learning methods have garnered significant attention in the field of computer vision for their potential to enhance safety and efficiency in various industries. However, limited availability of high-quality datasets has hindered the development of deep learning methods for safety clothing and helmet detection. In this work, we present a large, comprehensive, and realistic high-quality dataset for safety clothing and helmet detection, which was collected from a real-world chemical plant and annotated by professional security inspectors. Our dataset has been compared with several existing open-source datasets, and its effectiveness has been verified applying some classic object detection methods. The results demonstrate that our dataset is more complete and performs better in real-world settings. Furthermore, we have released our deployment code to the public to encourage the adoption of our dataset and improve worker safety. We hope that our efforts will promote the convergence of academic research and industry, ultimately contribute to the betterment of society.	Safety is a perpetual and significant concern in all industries, particularly in high-risk construction sites such as chemical factories and building sites. Protective equipments, including safety clothing and helmets, are crucial for safeguarding workers in high-risk construction sites. Helmets can effectively prevent head injuries caused by falling or splashing objects, while safety clothing can protect the body and arms from hazardous chemicals and liquids. The lack of safety clothing and helmets frequently results in safety accidents, with terrible impacts on families and society. Therefore, monitoring the usage of safety clothing and helmets in factories or construction sites is of immense safety significance and broad application value. Current laws and safety regulations stipulate that the person in charge and the contractor are responsible for providing, supervising, and maintaining personal protective equipment in construction sites. However, some workers may relax their vigilance due to a lack of safety awareness or discomfort from long-term wear, thereby increasing the probability of safety accidents. Currently, the commonly used automated monitoring technique for compliance of safety clothing and helmets is a class of sensor-based methods[1,2]. Sensor-based technology monitors whether the workers wear safety clothing or helmets by attaching sensors to them and analyzing the signals. However, this method requires significant investments in purchasing, installation, and maintenance, resulting in relatively high costs. In recent years, deep learning methods have gained substantial attention in computer vision[3,4]due to their ability to self-learn useful features from large-scale, annotated training data. In particular, convolution neural networks are widely used for image classification and object detection. For example, LeCun et al.[5]utilized Convolutional Neural Networks (CNNs) to recognize handwritten digits, while Kolar et al.[6]used CNNs to detect safety guardrails in construction sites. Nath et al.[7,8]used CNNs to identify common construction-related objects (e.g., buildings, equipment, and workers). These works provide new insights into monitoring the usage of safety clothing and helmets. As we all know, the great success of deep learning depends on large-scale, high-quality, and annotated data. To the best of our knowledge, only two safety clothing and helmet benchmark datasets have been released, namely Pictor-v3[9]and Safety Helmet Wearing Dataset (SHWD)[10]. However, the categories and quantities of labeled instances in the two datasets fall short of meeting the requirements for real-world applications. The existing datasets only consider the impact of helmets without paying attention to the impact of safety clothing. In real-world applications, there are still potential safety hazards. In addition, the backgrounds of all images are too simple, and the lighting conditions are overly ideal, which differs significantly from real-world working environments. If such datasets are actually employing in a factory safety inspection project, the generalization ability of the model is likely to be poor. To address the above limitations, in this work, we contribute a large, realistic, and comprehensive high-quality dataset for safety clothing and helmet detection, named the Towards Complex Real-world Safety Factory (TCRSF) dataset. A comparison of the images in TCRSF and those in the existing dataset is shown in Figure1. It can be seen that the proposed TCRSF provides more realistic scenarios and more complex backgrounds. TCRSF contains 50558 labeled instances of 7 categories, including “Safety Helmet”, “Safety Clothing”, “Head”, etc. To simulate the real-world work environment, we added labels for safety clothing and helmets under different lighting conditions and shadow areas, called “blurred head” and “blurred clothing”. Additionally, all images were collected by factory cameras, covering two factories and 40 different scenes. And each instance was manually annotated by professional inspectors from the factory, guaranteeing accurate annotations. It is worth mentioning that, our TCRSF dataset can serve as an evaluation benchmark for various detection tasks, such as small object detection and high-low light object detection. To evaluate the effectiveness of our dataset in a real-world factory environment, we adopt YOLO-v5111YOLO-v5 is a deep learning model for object detection, image segmentation and image classification.as the base model for training, trained it using our dataset. And we deploy the model in the factory and receive streaming data from the camera in real-time to inspect whether workers wear safety clothing and helmets. The TCRSF dataset introduces a novel and robust evaluation benchmark for construction safety inspection, facilitating its adoption in real-world applications. The main contributions of this work are as follows: [(1)] We have contributed a large, realistic, and high-quality dataset named TCRSF. This dataset provides a new and reasonable evaluation benchmark for the community, and we hope that its contribution can promote the development of construction safety inspection. By conducting experiments on existing models, we have validated the completeness of our dataset. Furthermore, the object detection model achieves excellent performance on our dataset when deployed in a real chemical plant environment. We have developed a comprehensive paradigm for real-time detection of workers’ safety clothing and helmets, and provided the full implementation of the application workflow code. To make it easily accessible to non-experts, we have integrated the code such that model training and optimization can be achieved by simply modifying a few parameters in a fixed file. All datasets and codes have been open sourced and are available fromhttps://github.com/sofffty/TCRSF. The rest of this article is organized as follows. In Section2, we review current status of open-source datasets for safety clothing and helmets and recent advances in object detection. In Section3, we present a detailed description of TCRSF. We introduce the selected architectures and experimental procedure to compare the performance in Section4. We discuss the gap between academia and industry and explores future directions, as well as the real-world performance of TCRSF in Section5. Finally, in Section6, we conclude this article.
2310.07895v1	Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs	This paper presents a method to efficiently classify the gastroenterologic section of images derived from Video Capsule Endoscopy (VCE) studies by exploring the combination of a Convolutional Neural Network (CNN) for classification with the time-series analysis properties of a Hidden Markov Model (HMM). It is demonstrated that successive time-series analysis identifies and corrects errors in the CNN output. Our approach achieves an accuracy of $98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for precise localization within the gastrointestinal (GI) tract while requiring only approximately 1M parameters and thus, provides a method suitable for low power devices	The capsule endoscopy is a medical procedure that has been used for investigating the midsection of the GI tract since early 2000[12,3]. This minimally invasive method allows to visualize the small intestine, which is in most part not accessible through standard techniques using flexible endoscopes[22]. The procedure starts by swallowing a pill-sized capsule. While it moves through the GI tract by peristalsis, it sends captured images from an integrated camera with either an adaptive or a defined frame rate to an electronic device. The overall aim of this procedure is to detect diseases affecting the small intestine such as tumors and its preliminary stages, angiectasias as well as chronic diseases[22,17,24]. Since the esophagus, stomach and colon can be more easily assessed by standard techniques, the small intestine section is of main interest in VCE studies. All images of the small intestine should be transmitted for further evaluation by medical experts who are qualified to check for anomalies. The frame rate of the most prominent capsules ranges from1to30frames per second with a varying resolution between256\times 256and512\times 512depending on the platform[22]. For example, the PillCam® SB3 by Medtronic lasts up to12hours with an adaptive frame rate of2to6frames per second[18]. This should ensure passing through the whole GI tract before the energy of the capsule’s battery is depleted. However, a capsule can also require more than one day to pass through the whole GI tract leading to an incomplete record of images due to depletion of the capsule’s battery after maximal12hours. In this procedure, the energy is the bottleneck and small changes of the architecture can increase the overall energy requirement leading to a shorter battery lifetime with the risk of running out of energy without covering the small intestine. However, modifications such as capturing images with a higher resolution might improve the recognition ability of clinicians and thus, it is desirable to increase the limited resolution or add more functions (e.g. zooming in or out, anomaly detection on-site) helping to successfully scan the GI tract for anomalies at the cost of increasing energy demands. The images taken before the small intestine are not of interest but demand their share of energy for capturing and transmitting the images. This paper presents a method for very accurately determining the location of the capsule by on-site evaluation using a combination of neural network classification and time-series analysis by a HMM. This neglects the necessity to consume electric energy for transmitting images of no interest. If this approach is integrated into the capsule it can perform precise self-localization and the transition from the stomach to the small intestine is verified with high confidence. From this moment onwards, all frames should be send out for further evaluation. A major part of the energy can be saved since the data transmission only starts after the capsule enters the small intestine and therefore can be used for other valuable tasks. For example, the frame rate or resolution could be increased while in the small intestine or additionally, a more complex network for detecting anomalies on-site could be employed. In the field of gastroenterology, there have been different approaches to perform localization of a capsule within the GI tract[16]including but not limited to magnetic tracking[19,26], video-based[28,15]and electromagnetic wave techniques[27,7]. However, Charoen et al[2]were the first to publish a dataset with millions of images classified into the different sections of the GI tract. They achieved an accuracy of97.1\%with an Inception ResNet V2[23]architecture on the RI dataset and therefore successfully demonstrated precise localization without aiming for an efficient realization on hardware. To the best of our knowledge, there is no superior result than the baseline with this dataset. However, a large network with56M parameters as the Inception ResNet V2 is not suitable for low-power embedded systems since the accompanied high energy demand results in a short battery lifetime. Thus, we present a new approach for this problem setting using the same dataset and the same split resulting in a higher accuracy while requiring a much smaller network and less parameters.
2310.19607v1	Technical Report on the Learning of Case Relevance in Case-Based Reasoning with Abstract Argumentation	Case-based reasoning is known to play an important role in several legal settings. In this paper we focus on a recent approach to case-based reasoning, supported by an instantiation of abstract argumentation whereby arguments represent cases and attack between arguments results from outcome disagreement between cases and a notion of relevance. In this context, relevance is connected to a form of specificity among cases. We explore how relevance can be learnt automatically in practice with the help of decision trees, and explore the combination of case-based reasoning with abstract argumentation (AA-CBR) and learning of case relevance for prediction in legal settings. Specifically, we show that, for two legal datasets, AA-CBR and decision-tree-based learning of case relevance perform competitively in comparison with decision trees. We also show that AA-CBR with decision-tree-based learning of case relevance results in a more compact representation than their decision tree counterparts, which could be beneficial for obtaining cognitively tractable explanations.	Case-based reasoning (CBR) is a methodology in which concrete past occasions are directly used as sources of knowledge and solutions for new situations(DBLP:books/daglib/0032926). It has been studied in AI and Law since its inception, leading to foundational contributions(Rissland2005CasebasedRA). This is a not a surprise, given the centrality of the use of cases for determining the law in Common Law systems, although not exclusivelylewis2021. In this paper we focus on recent approaches to CBR(DBLP:conf/kr/CyrasST16;dear-2020;DBLP:conf/kr/Paulino-PassosT21;Prakken2022ATM)using argumentation(prakken-overview). Argumentation itself has a long history in AI and Law, and its use to support CBR has been shown to pave the way towards novel forms of explanations for the outcomes of CBR, including via arbitrated dispute trees(DBLP:journals/eswa/CyrasBGTDTGH19;DBLP:conf/ijcai/Cyras0ABT21). Specifically, we focus on the\aacbrapproach(DBLP:conf/kr/CyrasST16;dear-2020;DBLP:conf/kr/Paulino-PassosT21), where arguments correspond to cases and attacks between arguments result from outcome disagreement between cases andrelevancebetween cases, guided by a partial order over cases capturing some notion of specificity. Originally(DBLP:conf/kr/CyrasST16),\aacbrexpects a representation of cases in terms of sets ofmanually engineered binaryfeatures and the partial order is defined via the subset relation. This expectation is a restriction for applicability. While previous work has generalised beyond binary features in order to support different applicationsdear-2020, a systematic generalisation to tabular datasets, including categorical and continuous data, is still missing. This is essential for applying\aacbrtorealisticdatasets, including legal ones, to realise the original inspiration from legal reasoning for\aacbr. While some form of binarisation can be applied, there is no guarantee that a naïve binarisation would result in good performance. In this work we close this gap, focusing on applying\aacbrto possibly non-binary tabular data from legal settings. Specifically, our first contribution is a general method for applying\aacbrto any tabular data by extracting binary features from decision treesDBLP:books/wa/BreimanFOS84when learning for the final task. Our second contribution is showing that this method is competitive with decision trees on two legal datasets: COMPASpropublica-compasand a simulated legal datasetDBLP:journals/argcom/StegingRVB23for welfare benefit. Finally, as a third contribution, we show thatauthor=Guilherme, caption=,author=Guilherme, caption=,todo:author=Guilherme, caption=,revise according to actual final resultsour method creates smaller models (i.e. with a smaller number of nodes), leading to potentially more cognitively tractable explanations (i.e. decision trees and rules drawn from them on one hand, and argumentation frameworks and arbitrated dispute trees on the other). \timesExplainability is a fundamental requirement in AI and Law. In particular, any model of legal reasoning must be amenable to explanation, which is required not only of AI systems, but of legal decision-makers in general. Current state of the art methods in many AI tasks is data-driven methodologies (machine learning) whether machine learning-base …\aacbrhas been developed …(DBLP:conf/kr/CyrasST16;dear-2020).. Important aspects for any concrete implementation of\aacbrare the representation of the data (to which we refer as characterisation) and, crucially, the partial order\pleqon the set of characterisations, which until now we have assumed given. This partial order captures the idea of relevance of past cases for a new case, as well as of specificity among past cases. In this paper, we will discuss ways of defining this partial order. In particular, we are interested in learning the partial order from data. Learning the partial order allows\aacbrto a wider scope of problems, instead of requiring a hand-constructed order. This carries three desiderata: 1) that the learned partial order results in a good performance; 2) that it is human interpretable. We here present a method for learning a partial order from tabular data using features extracted from decision trees trained on this same data. . We show empirical results viaexperiments are carried out on the the COMPAS dataset(propublica-compas), a dataset on prediction of recidivism, that is, on prediction of whether a criminal will re-offend(2016COMPASRS), as well as on the simulated legal data of…
2311.06130v2	High-dimensional mixed-categorical Gaussian processes with application to multidisciplinary design optimization for a green aircraft	Recently, there has been a growing interest in mixed-categorical metamodels based on Gaussian Process (GP) for Bayesian optimization. In this context, different approaches can be used to build the mixed-categorical GP. Many of these approaches involve a high number of hyperparameters; in fact, the more general and precise the strategy used to build the GP, the greater the number of hyperparameters to estimate. This paper introduces an innovative dimension reduction algorithm that relies on partial least squares regression to reduce the number of hyperparameters used to build a mixed-variable GP. Our goal is to generalize classical dimension reduction techniques commonly used within GP (for continuous inputs) to handle mixed-categorical inputs. The good potential of the proposed method is demonstrated in both structural and multidisciplinary application contexts. The targeted applications include the analysis of a cantilever beam as well as the optimization of a green aircraft, resulting in a significant 439-kilogram reduction in fuel consumption during a single mission.	Costly black-box simulations play an important role for many engineering and industrial applications. For this reason, surrogate modeling has been extensively used across a wide range of use cases, including aircraft designSciTech_cat, deep neural networkssnoek2015scalable, coastal flooding predictionlopez, agricultural forecastingMLP, and seismic imagingYDiouane_SGratton_XVasseur_LNVicente_HCalandra_2016. These black-box simulations are generally complex and may involve mixed-categorical input variables. For instance, a Multidisciplinary Design Analysis (MDA) aircraft design toolDavid_2021must consider mixed variables such as the number of engines or the list of possible materialsSciTech_cat. In this paper, our objective is to develop an affordable surrogate model, denoted as\hat{f}, for a black-box function that involves mixed variables given by This functionfrepresents a computationally expensive black-box simulation.\Omega\subset\mathbb{R}^{n}denotes the bounded continuous design set for thencontinuous variables.S\subset\mathbb{Z}^{m}denotes the bounded integer set whereL_{1},\ldots,L_{m}are the numbers of levels of themquantitative integer variables on which we can define an order relation and\mathbb{F}^{l}=\{1,\ldots,L_{1}\}\times\{1,\ldots,L_{2}\}\times\ldots\times\{1% ,\ldots,L_{l}\}is the design space for thelcategorical qualitative variables with their respectiveL_{1},\ldots,L_{l}levels. For such purpose, the use of a Gaussian Process (GP)williams2006gaussian, also called Kriging modelkrige1951statistical, is recognized as an effective modeling approach for constructing a response surface model based on an available dataset. Specifically, we make the assumption that our unknown black-box function, denoted asf, follows a GP with mean\mu^{f}and standard deviation\sigma^{f}, expressed as follows: Several modeling approaches have been put forward for addressing the challenges of handling categorical or integer variables within the context of GPPelamatti;Zhou;Deng;Roustant;GMHL;Gower;cuesta2021comparison;SciTech_cat. In comparison to GP designed for continuous variables, the most important changes concern the estimation of the correlation matrix, an essential element in the derivation of\mu^{f}and\sigma^{f}. Much like the procedure for constructing a GP with continuous inputs, Continuous Relaxation (CR) techniquesGMHL;SciTech_cat, models involving continuous latent variablescuesta2021comparison, and Gower Distance (GD) based modelsGoweruse a kernel-based approach for estimating this correlation matrix. However, recent innovative approaches take a different path by modeling directly the various entries of the correlation matrixPelamatti;Zhou;Deng;Roustant, and therefore, do not rely on any kernel choice, such methods involve the Homoscedastic Hypersphere (HH)Zhouand the Exponential HH (EHH)Mixed_Paulkernels. It has been shown inMixed_Paulthat the HH correlation kernel generalizes simpler methods like CR or GD kernels. However, this more general method for handling categorical design variables increases the number of hyperparameters required to be tuned associated with the GP model. In particular, this means that the method could only be used for small dimensional problems. Many efficient approaches have been proposed for handling a high number of continuous variables within GPBouhlel18;bouhlel_KPLSK;bouhlel2019gradient. The Kriging with Partial Least Squares (KPLS) methodBouhlel18;bouhlel_KPLSKis one of the most commonly used reduction techniquesKPLSu1;KPLSu2to tackle high dimensional data. Several other dimension reduction methods include principal components analysiswang2017batched, polynomial chaos expansionzuhal2021dimensionality, radial basis functionsregis2020survey, active subspaceAS, manifold embeddingtenenbaum2000globalor marginal Gaussian processMGP. The KPLS technique allows constructing the GP model with the same continuous variables but using a few number of parameters; which reduces significantly the computational cost of computing a GP model. For mixed-categorical GP models, given that the computational effort related to the construction of the GP model may not scale well to practical applications involving categorical variables with a large number of levels, the number of hyperparameters to be tuned need to be considered more thoroughly. In the literature, GPs have been applied to no more than 15 hyperparameters due to the high computational cost associated with the estimation of the hyperparametersGP14. Adapting dimension reduction techniques, such as KPLS, to mixed-categorical GPs will thus enable solving practical mixed-categorical engineering problems where often a high number of hyperparameters is required to be estimated. To the best of our knowledge, there is no equivalent approach to handle mixed-categorical data without using relaxation techniques. All existing dimension reduction techniques, including KPLS, are not adapted for advanced mixed-categorical GP models such as HH or EHH. We note also that, although this paper focuses mainly on surrogate modeling, the proposed models can be integrated within any surrogate-based optimization methodAuHa2017, such as surrogate-based evolutionary algorithmsbea1;sbea2or a Bayesian Optimization (BO) methodJones2001JOGO. In this work, we target to use dimension reduction techniques for reducing the number of hyperparameters within the GP in order to allow modeling efficiently high-dimension mixed-categorical data. In this context, high dimensionality is related to the high number of categorical variables potentially with a high number of levels (a few dozen). In fact, using relaxation approaches (by converting categorical choices to continuous variables) leads to a very high number of hyperparameters to estimate, particularly for high resolution approaches such as those based on HH and EHH kernels. We have also specifically used our proposed mixed-categorical GP models, within a BO framework, to solve a constrained optimization problem involving expensive-to-compute black-box simulations for objective and constraints functionsMartins2021. The proposed approach is shown in particular to be efficient in solving a high dimensional mixed-categorical Multidisciplinary Design Optimization (MDO) problemLambe2012. All the GP models proposed in this work are implemented in the open-source Surrogate Modeling Toolbox (SMT)111https://smt.readthedocs.io/en/latest/saves2023smt. The remainder of this paper is the following. In Section2, a detailed review of the GP model for continuous and for categorical inputs is given. In Section3, we present the PLS regression for vectors and matrices and their application to GP model for both continuous and categorical inputs. Section4presents academical tests as well as the obtained results on multidisciplinary optimization. Conclusions and perspectives are finally drawn in Section5. Notations:For a vectorx, both notations[x]_{j}andx_{j}stand for thej^{th}component ofx. Similarly, thei(row index) andj(column index) entry of a matrixXis denoted[X]^{j}_{i}.
2308.11159v1	SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection	Among the current mainstream change detection networks, transformer is deficient in the ability to capture accurate low-level details, while convolutional neural network (CNN) is wanting in the capacity to understand global information and establish remote spatial relationships. Meanwhile, both of the widely used early fusion and late fusion frameworks are not able to well learn complete change features. Therefore, based on swin transformer V2 (Swin V2) and VGG16, we propose an end-to-end compounded dense network SwinV2DNet to inherit the advantages of both transformer and CNN and overcome the shortcomings of existing networks in feature learning. Firstly, it captures the change relationship features through the densely connected Swin V2 backbone, and provides the low-level pre-changed and post-changed features through a CNN branch. Based on these three change features, we accomplish accurate change detection results. Secondly, combined with transformer and CNN, we propose mixed feature pyramid (MFP) which provides inter-layer interaction information and intra-layer multi-scale information for complete feature learning. MFP is a plug and play module which is experimentally proven to be also effective in other change detection networks. Further more, we impose a self-supervision strategy to guide a new CNN branch, which solves the untrainable problem of the CNN branch and provides the semantic change information for the features of encoder. The state-of-the-art (SOTA) change detection scores and fine-grained change maps were obtained compared with other advanced methods on four commonly used public remote sensing datasets. The code is available at https://github.com/DalongZ/SwinV2DNet.	Remote sensing images change detection is one of the earliest and most important remote sensing tasks, which has been concerned and studied by many researchers for a long time[27,61,4,2]. Change detection is defined as observing the image differences of the same surface area at different times. Change detection is used in many scenarios, including disaster assessment[37], urban planning, land surface change[29,68], and so on. With the development of satellites and sensors, very high resolution remote sensing (VHRRS) images have gradually become one of the mainstream remote sensing images in research, which provide rich spatial information and fine surface details. However, one of the main challenges faced by VHRRS images change detection is high intraclass variation and low interclass variance of detection objects[38]. It, therefore, has been the focus of scholars’ research that how to design a stable network and provide comprehensive and diverse feature information to distinguish the pseudo changes in change detection (as shown in Fig.1). Traditional change detection algorithms, according to different detection units, can be divided into pixel-based algorithms and object-based algorithms. The detection results of pixel-based algorithms are obtained through feature extraction and then threshold segmentation, which include methods based on arithmetic operations (band difference[34], spectral angle mapper[69]), methods based on transformation (change vector analysis (CVA)[39,3], principal component analysis (PCA)[60], independent component analysis (ICA)[67]), post-classification change detection[54], slow feature analysis (SFA)[53]and so on. According to the shape, texture and spectrum of images, object-based algorithms need to segment the images and then compare the classification results to get the change detection results[20]. Pixel-based algorithms are trapped by the interference of small noises and the decision of segmentation threshold. Meanwhile object-based algorithms often get stuck in the accumulation of multiple classification errors that affect the detection accuracy[27]. Both of these traditional algorithms require prior knowledge and manual design, and are easily affected by sensor noises. With the support of massive remote sensing data, deep learning has also shown outstanding detection ability in the field of remote sensing. CNN converts the input images into the high-dimensional depth features, and combines the targets and background to extract effective semantic information, achieving the detection effect beyond many traditional methods.[11]provides the three most common baseline networks for change detection. The architecture combined with CNN and conditional random field (CRF) refines the edges of detection areas, but it comes at the cost of slow training[66]. CNN is hindered regrettably by the narrow receptive field of local information, and transformer rises rapidly due to the ability of modeling global information. However, it can not work well that the pure transformer change detection model lacks low-level details[58]. Therefore, how to combine transformer and CNN to build the reasonable change detection architecture is the crux of the matter at this stage. From another perspective, change detection network architectures can be divided into early fusion (EF)[11,1,41,42]and late fusion (LF)[11,56,57,6,46,14,62,15]networks. The EF network works by stitching two images together and feeding them into the single input network. By concatenating two three-channel images into a six-channel image,[1]and[41]input it into full convolutional neural network (FCN) and UNet++ respectively, and output change map after training the network. The disadvantage of this method is that the network lacks the depth features of single images, resulting in fractured edges and broken structures in change map. In the LF network, the two features are extracted from the pre-changed and post-changed images respectively by using the dual-input structure, and are fused in the second half of the network. The siamese network, the most prominent LF network, consists of two subnets with shared weights. The siamese network was first used for remote sensing images change detection in[56]. The use of convolutional block attention module (CBAM) and deep supervision for the siamese network respectively alleviates the problem of heterogeneous features fusion and depth features migration in training process[57]. However, for the LF network, the contradiction between the dual-stream input of encoder and the single output of decoder often results in the disappearance of gradient propagation and affects the low-level features learning of two original images. Furthermore, the heterogeneous features fusion of LF network needs to be solved by elaborately designed module. As a consequence, it is another problem worth pondering that how to overcome the respective disadvantages of these two network architectures and provide the complete and diverse features for change detection. In addition to the design of the overall network architectures for change detection, researchers are also pushing forward the elaboration of network functional modules. The attention modules introduced into change detection are relatively representative, including squeeze-and-excitation attention (SE)[26], efficient channel attention (ECA)[51], CBAM[52]and cross-attention[62]. At the same time, since the ground objects have different scales in the VHRRS change detection datasets, how to adapt these ground objects to maintain the robustness and generalization ability of the network? Multi-scale features of deep learning generally can be divided into three categories: multi-scale features between different layers, multi-scale interaction features between different layers and multi-scale features from different convolution units. The first type of multi-scale features was embedded in the common U-Net network. The second type of multi-scale features typically interacts and fuses using transformer or CNN. The third type of multi-scale features is provided by a variety of convolution units, such as inception[49], dilated convolution[55], res2net convolution (Res2Net-Conv)[19], selective kernel convolution (SK-Conv)[33]and so on. We should think about the integration and utilization of these three multi-scale features. Other scholars also think in the combination with generative adversarial network (GAN)[25,65]or self-supervised learning[8,5,64]to obtain more discriminative features. These deep learning technologies are aimed at solving the problem of high intraclass variation and low interclass variance by mining the different features of change detection data. Motivated by the above concerns, this study combines Swin V2 and VGG16 to propose a new end-to-end compounded dense network SwinV2DNet. Swin V2 blocks are used to build the UNet++ type main network, and VGG16 encoder is used to build the CNN auxiliary network. SwinV2DNet overcomes the modeling defect of only local information in CNN and the insufficient interpretation of low-level details in transformer. On the other hand, the Swin V2 main network belongs to EF network, and the CNN branch belongs to LF. This structure constantly provides the pre-changed features, post-changed features and change relation features (namely, the six-channel concatenation from the pre-changed and post-changed images) for the accurate acquisition of change detection results. CBAM and deep supervision also promote the fusion of heterogeneous features and the rapidly stable convergence of the network, respectively. To better combine transformer and CNN, we propose a new multi-scale module, mixed feature pyramid, which provides inter-layer multi-scale interaction information and intra-layer multi-scale information to supplement the UNet++ main network only with inter-layer multi-scale information. We finally design a new decoder to the CNN branch with only VGG16 encoder, and use the self-supervision strategy to train the extracted features, so that the CNN branch can provide learnable and more discriminant semantic information. To sum up, the main contributions of this study are fourfold: We propose an end-to-end compounded dense network SwinV2DNet that possesses both advantages of transformer and CNN, and overcomes respective disadvantages of the EF and LF network. This is the first parallel combination of Swin V2 and VGG16 in change detection. Mixed feature pyramid is proposed, for the first time, to provide inter-layer interaction information and intra-layer multi-scale information. It is a plug and play module that has been experimentally proven to be also effective in other change detection networks. We design a new decoder for the CNN branch with only VGG16 encoder, and impose the self-supervised strategy to train the extracted features to provide more discriminative semantic information for the main network. Compared with other advanced methods, our method obtains the state-of-the-art (SOTA) change detection scores and the elaborate change maps on four common public remote sensing datasets. The remainder of this paper is organized as follows. SectionIIreviews the related work. SectionIIIelaborates the proposed SwinV2DNet method. The experimental evaluations and ablation studies are carried out in SectionIV. Finally, SectionVpresents the conclusion of this article.
2303.16870v1	Questions of science: chatting with ChatGPT about complex systems	We present an overview of the complex systems field using ChatGPT as a representation of the community's understanding. ChatGPT has learned language patterns and styles from a large dataset of internet texts, allowing it to provide answers that reflect common opinions, ideas, and language patterns found in the community. Our exploration covers both teaching and learning, and research topics. We recognize the value of ChatGPT as a source for the community's ideas.	ChatGPT and other transformers-based large language models (LLMs) have demonstrated remarkable abilities to perform tasks such as completion tasks, question answering, reading comprehension and translation that were once thought to be exclusive to humans(devlin2018bert,;zhang2020pegasus,;raffel2019exploring,;fedus2021switch,;lewis2019bart,;radford2018improving,;radford2019language,;brown2020language,). We are currently in a great era for researchers and scientists studying and developing in the field of complex systems. Half of the physics Nobel prize of 2021 was awarded to the physicist Giorgio Parisi for his contributions to the theory of complex systems(Cugliandolo_2023,)and the other half to two meteorologists Syukuro Manabe and Klaus Hasselmann to the modeling of the Earth’s climate(gupta2022perspectives,). Parisi has made significant contributions to the literature on complex systems, including areas such as spin glass(Parisi1979,;Parisi1980,;Parisi_1980order,), stochastic resonance(Benzi1982,), surface growth(Kardar1986,), multifractality(frisch1985fully,), and bird flocking(Ballerini2008,). Manabe showed that there is a relation between the increase in global temperature and the level of CO{{}_{2}}in the atmosphere(manabe1967thermal,). Hasselmann developed a model that establishes a connection between weather and climate, implying that climate models can be reliable despite the unpredictability and complexity of weather patterns(hasselmann1976stochastic,;frankignoul1977stochastic,). The diversity of research topics among the award recipients highlights the multidisciplinary nature of the field. In addition, the applied dimension of the contributions stresses the importance of applied contributions to the field. In particular, it is worth mentioning that the research of field of complex systems has developed multiple frameworks, such as chaos theory(lorenz1963deterministic,), fractal theory(mandelbrot1982fractal,), complex networks(barabasi1999emergence,;watts1998collective,), and agent-based models(bonabeau2002agent,), to model and solve important practical problems in diverse fields. OpenAI has just now introduced the ChatGPT, a large language model tool trained, to aid humans in a variate of human tasks including question answering, text edition and has been used as a great information provider(brown2020language,;ouyang2022training,). ChatGPT has been trained on a large dataset of internet texts, which means that it has learned to mimic the patterns and styles of language used by many different people online. When you talk to ChatGPT, it responds based on what it has learned from that dataset, so its answers can be thought of as representing the average opinions, ideas, and language patterns that are commonly found on the internet. In this paper, we overview different aspects of the field of complex systems by means of chats with ChatGPT. We split this paper into two parts. The first part explores issues related to teaching and learning, where ChatGPT may work as a teacher assistant. The second part explores issues related to research, where, in this case, ChatGPT works like a research assistant. Besides exploring different concepts from different points of view in the field of complex systems, we also recognize that ChatGPT can be a valuable resource for teachers and scientists.
2312.14481v1	Part to Whole: Collaborative Prompting for Surgical Instrument Segmentation	"Foundation models like the Segment Anything Model (SAM) have demonstrated promise in generic object segmentation. However, directly applying SAM to surgical instrument segmentation presents key challenges. First, SAM relies on per-frame point-or-box prompts which complicate surgeon-computer interaction. Also, SAM yields suboptimal performance on segmenting surgical instruments, owing to insufficient surgical data in its pre-training as well as the complex structure and fine-grained details of various surgical instruments. To address these challenges, in this paper, we investigate text promptable surgical instrument segmentation and propose SP-SAM (SurgicalPart-SAM), a novel efficient-tuning approach that integrates surgical instrument structure knowledge with the generic segmentation knowledge of SAM. Specifically, we achieve this by proposing (1) collaborative prompts in the text form ""[part name] of [instrument category name]"" that decompose instruments into fine-grained parts; (2) a Cross-Modal Prompt Encoder that encodes text prompts jointly with visual embeddings into discriminative part-level representations; and (3) a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy that selectively assemble the part-level representations into a whole for accurate instrument segmentation. Built upon them, SP-SAM acquires a better capability to comprehend surgical instrument structures and distinguish between various categories. Extensive experiments on both the EndoVis2018 and EndoVis2017 datasets demonstrate SP-SAM's state-of-the-art performance with minimal tunable parameters. Code is at https://github.com/wenxi-yue/SurgicalPart-SAM."	Surgical instrument segmentation aims to accurately identify and delineate surgical instruments in operative scenes. It plays a foundational role for many downstream applications, such as surgical planning[7], robotic navigation[43], and skill assessment[24]. We identify two primary problems with the existing methods for this task. First, they often develop specialist models that require full training of all parameters (Fig.LABEL:subfig:motiv2-(i)), leading to high development costs. Second, current methods lack the capability of human-computer interaction that is highly desired in surgical practice[6,4]. The Segment Anything Model (SAM)[22]is a pioneering foundation model for promptable segmentation. It holds great potential for addressing the above problems owing to its rich pre-trained knowledge and interactivity. However, there still exist some fundamental challenges that impede the application of SAM in surgical instrument segmentation. Firstly, SAM’s reliance on point-or-box prompts is impractical in surgical settings, where it is infeasible for surgeons to provide such prompts for every instrument in each frame. Instead, more flexible and efficient prompts like free-form text would be preferred (e.g., via speech recognition and voice control). Secondly, SAM has shown subpar zero-shot generalisation in segmenting surgical instruments[42]due to insufficient surgical data in pre-training as well as the complex structure and fine-grained nature of surgical instruments across various categories. In this paper, we investigate text promptable surgical instrument segmentation and propose a novel framework, SP-SAM (SurgicalPart-SAM), that addresses the above challenges by leveraging SAM’s generic segmentation capability while integrating instrument structure information via an efficient model tuning scheme (Fig.LABEL:subfig:motiv2-(iii)). For text-prompted segmentation, a common practice is to use the object names to prompt the segmentation of an object of interest. However, the category names of instruments are not ideal for use as text prompts due to their specialised and abstract nature (Fig.LABEL:subfig:motiv1and Fig.LABEL:subfig:motiv2-(ii)). To this end, we introduce a new form of text prompt, namely Collaborative Prompt, using a text description set: {\say[part name]of[instrument category name]} for all parts of an instrument category. It combines both category and part information, effectively decomposing the specialised, abstract, and coarse concepts of instrument categories into more precise and fine-grained instrument parts, which is more appropriate for surgical instruments of high structure complexity (Fig.LABEL:subfig:motiv1and Fig.LABEL:subfig:motiv2-(iii)). To encode these collaborative prompts, we introduce a Cross-Modal Prompt Encoder, transforming collaborative prompts into part-level sparse and dense prompt embeddings for SAM’s mask decoder through visual-textual interaction. This enables focused learning of fine-grained representations for each instrument part, enhancing segmentation of subtle details. Finally, to integrate the knowledge of instrument structure into the model, a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy are proposed. Specifically, we adaptively assemble the learned part-level sparse and dense embeddings considering both category-specific and image-specific part weights, to predict the mask of the whole instrument. Afterwards, hierarchical decoding of both the whole instrument and its parts is performed to enhance the understanding of surgical instruments both holistically and at the part level. To this end, SP-SAM has strong capability to comprehend surgical instrument structures, identify subtle details, and discriminate between fine-grained categories. In summary, our contributions are three-fold: We introduce a novel approach SP-SAM for text promptable surgical instrument segmentation, offering a simpler prompting pipeline than the vanilla SAM that better suits the demands of surgical practices. SP-SAM achieves state-of-the-art (SOTA) performance on the challenging EndoVis2018 and EndoVis2017 datasets while requiring only a small number of training parameters. We introduce a new type of text prompt integrated with both instrument category and part information, namely collaborative prompts, and devise a Cross-Modal Prompt Encoder, which encodes text prompts jointly with visual embeddings into discriminative part-level representations, ensuring accurate segmentation of subtle details. We propose a Part-to-Whole Selective Fusion and a Hierarchical Decoding strategy to adaptively assemble the part-level representations into a whole and make predictions at both the part-level and the holistic-level, explicitly incorporating structure information regarding category-part relations and achieving better discrimination between categories.
2306.06380v1	D2Match: Leveraging Deep Learning and Degeneracy for Subgraph Matching	Subgraph matching is a fundamental building block for graph-based applications and is challenging due to its high-order combinatorial nature. Existing studies usually tackle it by combinatorial optimization or learning-based methods. However, they suffer from exponential computational costs or searching the matching without theoretical guarantees. In this paper, we develop D2Match by leveraging the efficiency of Deep learning and Degeneracy for subgraph matching. More specifically, we first prove that subgraph matching can degenerate to subtree matching, and subsequently is equivalent to finding a perfect matching on a bipartite graph. We can then yield an implementation of linear time complexity by the built-in tree-structured aggregation mechanism on graph neural networks. Moreover, circle structures and node attributes can be easily incorporated in D2Match to boost the matching performance. Finally, we conduct extensive experiments to show the superior performance of our D2Match and confirm that our D2Match indeed exploits the subtrees and differs from existing GNNs-based subgraph matching methods that depend on memorizing the data distribution divergence	Subgraph isomorphism, or subgraph matching at the node level(McCreeshet al.,2018), is a critical yet particularly challenging graph-related task. It aims to determine whether a query graph is isomorphic to a subgraph of a large target graph. It is an essential building block for various graph-based scenarios, e.g., alignment of cross-domain data(Chenet al.,2020), temporal alignment of time series(Zhou and Torre,2009), and motif matching(Miloet al.,2002; Penget al.,2020), etc. Existing studies for subgraph matching can be divided into two main streams: combinatorial optimization (CO)-based and learning-based methods(Vesselinovaet al.,2020). Early algorithms often formulate subgraph matching as a CO problem that aims to find all exact matches in a target graph. Unfortunately, this leads to an NP-complete issue(Ullmann,1976; Cordellaet al.,2004), which suffers from exponential time costs. To alleviate the computational cost, researchers have employed approximate techniques to seek inexact solutions(Mongiovìet al.,2010; Yanet al.,2005; Shanget al.,2008), which yield suboptimal matchings. An alternative solution is to frame subgraph matching as a supervised learning problem(Baiet al.,2019; Rexet al.,2020; Baiet al.,2020), which utilizes the Graph Neural Networks (GNNs). However, the learning-based or GNN-based methods mainly aim to optimize the representations in a black-box way. The lack of theoretical guarantees makes them inexplicable and often cannot seek the exact match subgraphs. In order to tackle the above challenges, we propose a white-box GNN-based solution,D^{2}Match, to leverage the efficiency of Deep GNNs and Degeneracy for subgraph matching. With rigorous theoretical proofs, we provide explainable results at each learning step. We first prove that finding the matched nodes between the query graph and the target one can degenerate to check whether the corresponding subtrees rooted at these two nodes are subtree isomorphic. This degeneration allows us to check whether a perfect matching exists on a bipartite graph, which results in a polynomial time complexity solution. The above two steps convert subgraph matching into computing an indicator matrix whose elements represent the subtree isomorphic relationship between nodes in the query graph and the target one. Hence, the matching matrix required by the CO-based methods for subgraph matching degenerates to seeking an indicator matrix, which is computed by GNNs via its intrinsic tree-structured aggregation mechanism. Note that this implementation battles the matching mechanism directly by GNNs rather than optimizing the representations of GNNs as in the existing work. Our implementation allows us to reduce the time cost of perfect matching from polynomial time to linear time. Moreover, we can easily incorporate other information, including circle structures (abstracted assupernodes) and node attributes, into ourD^{2}Matchto boost the matching performance. Our contribution is four-fold: (1) We propose the first white-box GNN-based model, calledD^{2}Match, to leverage deep learning and degeneracy for subgraph matching. We provide rigorous theoretical proofs to guarantee that subgraph matching can degenerate to subtree matching, and finally perfect matching on a bipartite graph. (2) To the best of our knowledge, this is the first GNN-based model to tackle subgraph matching directly, which degenerates the matching matrix required by the CO-based methods to an indicator matrix computed by GNNs via the intrinsic tree-structured aggregation mechanism. This allows us to compute the indicator matrix in linear time. (3) OurD^{2}Matchcan easily include other information, including circle structures and node attributes to boost the model performance. (4) Extensive empirical evaluations show thatD^{2}Matchoutperforms state-of-the-art subgraph matching methods by a substantial margin, and uncovered that learning-based methods tend to capture the divergence of the data distribution rather than exploiting graph structures.
2307.05772v1	Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning	Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief functions, and experimentally demonstrate how our approach outperforms competing uncertainty-aware approaches in a classical evaluation setting. The performance of RS-CNN is best demonstrated on OOD samples where it manages to capture the true prediction while standard CNNs fail.	"Despite its recent rise in popularity, in its current form, artificial intelligence (AI) cannot confidently make predictions robust enough to stand the test of data generated by processes different from those studied at training time, even by tiny details, as shown by ‘adversarial’ results able to fool deep neural networkspapernot2016limitations. While recognising this issue under different names (e.g.overfittingroelofs2019metaormodel adaptationli2020model), traditional machine learning seems unable to address it at a fundamental level. As a result, AI systems suffer from brittle behaviour, and find it hard to operate in new situations. A major cause of this problem is the widespread presence ofepistemic uncertainty, i.e., uncertainty about the process generating the data itself. In machine learning, this derives from the limited representativeness of the available training sets, in terms of both quantity and quality. As few samples are collected in specific domain distributions, the learnt models have no way to effectively model domain variation. The machine learning community recognises the problem. This has given rise to several proposals for estimating the uncertainty associated with network predictions, including Bayesian Dropoutgal2016dropout, distance-aware priorsgianand the use of random fuzzy setsDENOEUX2022. An “evidential"" deep learning methodsensoyfor uncertainty quantification in classification using Dirichlet distribution and, more recently, epistemic neural networksosbandas a generalisation of Bayesian neural networks have been proposed. None of these methods, however, fully, explicitly capture the epistemic uncertainty stemming from the data issue. Some of them rely on prior knowledgehttps://doi.org/10.48550/arxiv.2105.06868, whereas others require setting a desired threshold on the metricsDBLP:journals/corr/abs-2107-07511. It has been argued by some that classical probability theory is, in fact, not equipped to model “second-level"" uncertainty on the probabilities themselves. This has led in the past to the formulation of numerous (epistemic) uncertainty calculicuzzolin2020geometry, starting from de Finetti’s pioneering work on subjective probabilityDeFinetti74, and including possibility theoryDubois90, probability intervalshalpern03book, credal setslevi80book, random setsNguyen78and imprecise probabilitywalley91book. In this paper, we propose to model epistemic uncertainty using arandom setrepresentationMolchanov_2017. As we focus on finite target spaces in a classification setting, we will model uncertainty usingbelief functionsShafer76, the finite incarnation of random setscuzzolin2018visions. The theory of belief functions or theory of evidenceShafer76is a generalisation of Bayesian inferencesmets86bayessince classical (discrete) probabilities are simply special belief functions and Bayes’ rule is a special case of Dempster’s rule of combination. Crucially, in the theory of evidence, priors are not needed to start the inference process, avoiding the selection bias risks that can seriously condition Bayesian reasoningfreedman1999wald. An interesting geometric approach to uncertainty measures, and belief functions in particular, has been proposed by one of uscuzzolin2001geometric;cuzzolin2003geometry;cuzzolin2008geometric;cuzzolin2013l;cuzzolin2010geometry. To model uncertainty using belief functions, we propose a novel Random-set Convolutional Neural Network based on the principles of the epistemic deep learning concept. Epistemic deep learningmanchingal2022epistemicargues that a deep neural network producing an outcome for (selected) subsets of the target space is an intrinsically more faithful representation of the epistemic uncertainty associated with the limited quantity and quality of the training data. As they assign probability values to sets of outcomes directly, they naturally model the fact that observations almost invariably come in the form of sets. In an image classification setting, this could be interpreted as a test image being mapped to asetof classes, rather than a single class, when there is uncertainty about the true class. By representing classes as sets, set-valued classification can handle imprecise data more effectively and provide a richer understanding of the underlying uncertainty. It enables the classification model to capture multiple possible interpretations or predictions for each instance, allowing for more robust decision-making and improved performance in scenarios where precise labels may not be available or appropriate. ContributionsThe following are the key contributions of our paper: [leftmargin=*] A novelRandom-set Convolutional Neural Network(RS-CNN) classifier for uncertainty estimation based on the principle of epistemic deep learning, outputting Belief or Mass predictions for sets of classes, training using suitable loss functions generalising classical cross-entropy. A method for selecting a fixed budget of focal sets (sets with non-zero probability, associated with the network’s output neurons) from the power set of classes, thus ensuring scalability. A method for estimating the epistemic uncertainty of the prediction in terms of the size of the credal set (set of probability vectors) consistent with a predicted belief function. Experimental validation demonstrating the RS-CNN model outperform other uncertainty-aware models and exhibit superior accuracy on out-of-distribution (OOD) samples compared to the standard CNN."
2309.09404v5	Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals	Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by evaluating the recommended teams using a goodness score and find that more informed methods lead to recommendations of smaller number of teams but higher goodness, and (2) qualitatively, by conducting a large-scale user study at a college-wide level, and demonstrate that users overall found the tool very useful and relevant. Lastly, we evaluate our system in two diverse settings in US and India (of researchers and proposal calls) to establish generality of our approach, and deploy it at a major US university for routine use.	In the recent decade, there has been an increased interest in studying teamwork skills and their impacts in a multitude of domains (e.g., academiaAlberolaet al.(2016), social networkingAnagnostopouloset al.(2012), project managementNollet al.(2016), healthcareNawazet al.(2014)). Building successful teams is a common business strategy (e.g., forming rescue and relief teams in response to an emergencyGunn and Anderson (2015), establishing entrepreneurial teams for new venturesLazaret al.(2020), forming teams dynamically in context of multi-agent systems (e.g., supply chains)Gaston and desJardins (2005)). In this paper, we focus on teaming for researchers applying to funding agencies in response to their call for proposals, using group recommendation strategies. The advantage of this setting is that the required data is already publicly available. A large amount of research funding in public universities comes from external funding agencies such as National Science Foundation (NSF) and National Institutes of Health (NIH). These opportunities often require multi-disciplinary teams from a wide variety of backgrounds to be quickly assembled. However, not every single team is often successful in achieving their goals, due to factors such as lack of accurate information, time, or communication, and incompatibility in terms of skill sets among team members. We build upon prior work, ULTRA (University-LeadTeam Builder fromRFPs andAnalysis)Srivastavaet al.(2022), a novel AI-based prototype for assisting with team formation when researchers respond to calls for proposals from funding agencies. In this paper, we interchangeably use the termcall for proposalwithrequest for proposal (RFP).Figure1shows a demo111A full demo interaction with the ULTRA system can be found at https://www.youtube.com/watch?v=8MUtxsfVNIU. The tool is deployed at http://casy.cse.sc.edu/ultra/teaming/. Additional details about usecases, experiments, and survey resources are atValluruet al.(2023).view of how the system works for an individual user who can become a team participant. The system first extracts technical skills from proposal calls found at publicly available data sources (e.g., NSF archives) and those present in online profiles of researchers (e.g., personal homepages, Google Scholar history), along with any additional teaming constraints that administrators or team participants of an institution may provide. Using AI and NLP techniques, ULTRA next provides a plausible list of candidate teams for each proposal call, where each team has at least two members. Our prior workSrivastavaet al.(2022), however, is a use case of asequentialsingle-item recommendation problem, where solutions are often limited by known issues such as cold startAbdollahpouriet al.(2019)or popularity biasYalcin and Bilge (2022). Therefore, we expand on this work to include group recommendation and novel AI methods to recommend optimal teams. Our contributions in the paper are that we: formulate a novelgroup recommendation problemto promote research collaborations where the objective is to suggest teams that could respond to calls for proposals, using skills found in open data, and balancing short- and long-term optimization. introduce a metric to measure goodness of teams and consider a configurable set of criteria: redundancy, group (set) size, coverage, and robustness. solve the novel problem using a variety of AI methods: string, taxonomy, and bandit (relational learning) methods, and compare them with a randomized baseline. establish the benefit of the solution methods quantitatively using goodness metric. We find that moreinformed methods lead to recommendations of smaller number of teams but higher goodness. establish the benefit of the system qualitatively using an IRB-approved preliminary survey at a College of a major US University. We find thatusers broadly consider the tool useful as well as relevantbut more studies are needed. demonstrate the generality of the approach with experiments at two different institutions in US and India. create and publish a teaming dataset that is available for research.
2305.00245v1	Industry Classification Using a Novel Financial Time-Series Case Representation	The financial domain has proven to be a fertile source of challenging machine learning problems across a variety of tasks including prediction, clustering, and classification. Researchers can access an abundance of time-series data and even modest performance improvements can be translated into significant additional value. In this work, we consider the use of case-based reasoning for an important task in this domain, by using historical stock returns time-series data for industry sector classification. We discuss why time-series data can present some significant representational challenges for conventional case-based reasoning approaches, and in response, we propose a novel representation based on stock returns embeddings, which can be readily calculated from raw stock returns data. We argue that this representation is well suited to case-based reasoning and evaluate our approach using a large-scale public dataset for the industry sector classification task, demonstrating substantial performance improvements over several baselines using more conventional representations.	Case-based reasoning (CBR) approaches have been applied in financial domains, and for a variety of tasks, from the early days of the field; see for example the work of[23]on the use of CBR for financial decision-making. In the years since, there have been many efforts made to apply CBR ideas to a diverse range of financial decision-making and prediction tasks[10,18,13,26,1,6]. Nevertheless, the use of CBR in such domains is not without its challenges, not the least of which concerns the very nature of many financial datasets and their relationship to the similarity assumption that underpins CBR. The central dogma of CBR is that similar problems have similar solutions, yet financial regulators are always at pains to point out that “past performance is not a guarantee of future results” suggesting that this principle may not be so reliable in financial domains, at least when it comes to predicting the future. As society changes and economies ebb and flow, companies that were once the stock market darlings fall out of favour, while new winners seem to emerge, with some regularity, albeit unpredictably and often with little or no warning. Two companies that were similar in the past may no longer be considered similar in the present, while the trajectories of companies with divergent histories might suddenly converge if future circumstances conspire in their favour. All of this greatly complicates the central role of similarity in case retrieval. In addition, the feature-based (attribute-value) representations that are commonplace in CBR systems may not provide such a good fit with the type of sequential time-series data (e.g. daily, weekly, and monthly stock prices/returns) that are the norm in financial domains. This is not to say that case-based methods cannot be used with time-series data. Certainly, there is a wealth of literature on representing time-series data for use with case-based methods in a range of application domains from agricultural science[3]to user experience[14]. Usually, the approach taken is to use various feature extraction techniques to identify landmark features from the raw time-series data; to effectively transform a raw time-series into a more conventional feature-based representation that can be used with standard similarity metrics. Similar approaches have been applied in the financial domain[12]but, as mentioned above, the stochastic nature of financial markets makes it difficult to effectively isolate useful case representations from market noise, which further complicates the similarity assessment even given a suitable fixed representation. Thus, in this work, our main technical contribution is to propose and evaluate a novel approach to learning case representations for financial assets (companies/stocks) using raw time-series data made up of historical daily returns. We describe how to transform raw, time-series data into an embedding-style representation of each stock/company; see for example[16,17]for examples of embedding representations. We argue that this facilitates the capture of more meaningful patterns and sub-patterns over extended periods of time, while facilitating the type of temporal alignment that is necessary during case comparison and similarity assessment. We argue that this approach is well-suited to the use of case-based and nearest-neighbour approaches in financial domains, because it can be used with a variety of standard similarity metrics, as well as more domain/task specific metrics[6]. We demonstrate its performance benefits in a comparative evaluation of the industry sector classification task, an important and practical benchmark problem in many financial applications[19]. The remainder of this paper is organised as follows. In the next section, we review the use of case-based reasoning in the financial domain and with time-series data more broadly, highlighting several common tasks and the approaches taken thus far, as well as the important challenges that remain with respect to representation and similarity assessment. Then, in Section3we discuss the details of our proposed approach, by describing how raw time-series data, such as financial returns, can be transformed into an embedding-based representation that is well suited to case-based approaches. In Section4we evaluate our proposed approach by using it to classify companies into their market sectors based on their historical returns data. We present the results of a comprehensive quantitative evaluation, which compares our proposed representation to a variety of baseline and naive approaches. We demonstrate how our embeddings-based representations can offer significant classification improvements, relative to more conventional representations of the raw time-series data. In addition, before concluding with a summary of our findings and a discussion of limitations and opportunities for further work, we present further qualitative evidence in support of the proposed approach, by using our representations to visualise the industry sectors that emerge from a clustering of our cases and some examples of nearest-neighbours in the resulting case-space.
2310.09739v2	AugUndo: Scaling Up Augmentations for Unsupervised Depth Completion	"Unsupervised depth completion methods are trained by minimizing sparse depth and image reconstruction error. Block artifacts from resampling, intensity saturation, and occlusions are amongst the many undesirable by-products of common data augmentation schemes that affect image reconstruction quality, and thus the training signal. Hence, typical augmentations on images viewed as essential to training pipelines in other vision tasks have seen limited use beyond small image intensity changes and flipping. The sparse depth modality have seen even less as intensity transformations alter the scale of the 3D scene, and geometric transformations may decimate the sparse points during resampling. We propose a method that unlocks a wide range of previously-infeasible geometric augmentations for unsupervised depth completion. This is achieved by reversing, or ``undo""-ing, geometric transformations to the coordinates of the output depth, warping the depth map back to the original reference frame. This enables computing the reconstruction losses using the original images and sparse depth maps, eliminating the pitfalls of naive loss computation on the augmented inputs. This simple yet effective strategy allows us to scale up augmentations to boost performance. We demonstrate our method on indoor (VOID) and outdoor (KITTI) datasets where we improve upon three existing methods by an average of 11.75% across both datasets."	Data augmentation plays a crucial role in easing the demand for training data as it enriches the training dataset by orders of magnitude, boosting performance on various deep learning tasks[29,35,38], particularly by preventing overfitting. Typically, the choice of augmentation is largely task-dependent. One common axiom of choosing augmentations is that the task output should remain invariant to the augmentation. For example, image flips are viable augmentations for classifying animals, since they do not alter the resulting image label. Conversely, flipping road signs can alter their meanings; hence, such augmentations can be detrimental to tasks involving road sign recognition. For geometric tasks (i.e. depth estimation), the range of augmentations is more restricted: Stereo assumes pairs of rectified images; hence, in-plane rotations are not feasible. Image-guided sparse depth completion relies on sparse points to ground estimates to metric scale; therefore, intensity transformations on sparse depth maps that alter the scale of the 3-dimensional (3D) scene are infeasible – leaving few augmentations viable. Unsupervised learning of depth completion further limits the use of augmentations as the supervision signal comes from reconstructing the inputs, where augmenting the input introduces artifacts that impact reconstruction quality and therefore the supervision. While simulating nuisances,i.e., camera motion and orientation, is desirable, naively applying augmentations pertaining to them may do more harm than good; therefore, it is unsurprising that existing works[27,24,34,42,43,44,41,46]primarily rely on a small range of photometric augmentations and flipping. Nevertheless, photometric augmentations help model the diverse range of illumination conditions and colors of objects that may populate the scene; geometric augmentations can simulate the various camera motion, i.e. image translations can approximate small baseline movements, and scene arrangements, i.e. image flipping. But block artifacts, loss during resampling, intensity saturation are just some of the many undesirable side-effects of traditional augmentations to the image and sparse depth map. To exploit the immense amount of data derived from augmentations, we propose to simply compute the typical reconstruction loss on the original input image and sparse depth map, which bypasses negative effects of reconstruction artifacts due to photometric and geometric augmentations. However, there exists a mis-alignment between the original input (image and sparse depth), and the model depth estimate as geometric augmentations induces a change in coordinates. Hence, weundothe geometric augmentations by inverting them in the output space to align the model estimate with the training target. Amongst the the many geometric tasks, we focus onunsupervised depth completion, the task of inferring a dense depth map from a single image and its associated sparse depth map, where augmentations have seen limited use. Here, a training sample includes the input sparse depth map, its associated image as well as additional images of neighboring views from the same 3D scene. Augmentations have traditionally been restricted to a limited range of photometric transformations and flipping – due to the need to preserve photometric consistency across a sequence of video frames used during training, and the sparse set of 3D points projected onto the image frame as a 2.5D range map; degradation to either modalities directly impacts the supervision signal. By using our method, loss functions involving sparse depth and image reconstruction from other views can be computed on the original inputs while applying augmentations that were previously not viable for the task.Our hypothesis:By “undo-ing” the augmentations, one can expand the viable set and scale up their use in training, leading to improved model performance and generalizability. To this end, we introduce AugUndo, an augmentation framework that allows one to apply a wide range of photometric and geometric transformations on the inputs, and to “undo” them during loss computation. This allows one to compute an unsupervised loss on the original images and sparse depth maps, free of artifacts, through a warping of the output depth map – obtained from augmented input – onto the input frame of reference based on the inverse geometric transformation. In addition to group transformations that allow for output alignment, we combine them with commonly employed photometric augmentations. Lastly, we study whether non-group transformations, such as occlusion, can further improve model performance. We demonstrate AugUndo on three recent unsupervised depth completion methods and evaluate them on indoor and outdoor settings, where we improve by an average of 11.75% across all methods and datasets. Our contributionsare as follows: (i) We propose AugUndo, a simple-yet-effective framework to scale up photometric and geometric augmentations for unsupervised depth completion; AugUndo can be applied in a plug-and-play manner to existing methods with negligible increase in computational costs during training. (ii) We provide a comprehensive study ablating over combinations of eleven types of augmentations, including ones that have not been explored by existing unsupervised methods, for three different models across two datasets. We found a consistent set of augmentations that provides performance benefits for all tested models and are applicable for both indoor and outdoor scenarios. (iii) We show that AugUndo can consistently improve model performance, robustness to shifts in sparse point densities as well as zero-shot generalization; thus, validating our hypothesis.
2309.08511v1	Generalised Probabilistic Diffusion Scale-Spaces	Probabilistic diffusion models excel at sampling new images from learned distributions. Originally motivated by drift-diffusion concepts from physics, they apply image perturbations such as noise and blur in a forward process that results in a tractable probability distribution. A corresponding learned reverse process generates images and can be conditioned on side information, which leads to a wide variety of practical applications. Most of the research focus currently lies on practice-oriented extensions. In contrast, the theoretical background remains largely unexplored, in particular the relations to drift-diffusion. In order to shed light on these connections to classical image filtering, we propose a generalised scale-space theory for probabilistic diffusion models. Moreover, we show conceptual and empirical connections to diffusion and osmosis filters.	Diffusion probabilistic models[1]have recently risen to the state-of-the-art in image generation, surpassing generative adversarial networks[2]in popularity. In addition to significant research activity, the availability of pre-trained latent diffusion networks[3]have also brought diffusion models to widespread public attention[4]. Practical applications are numerous, including the generation of convincing, high fidelity images from text prompts or partial image data. Initial probabilistic diffusion models[1,5,6,7,8,9,10]relied on a forward drift-diffusion process that gradually perturbs input images with noise and can be reversed by deep learning. Recently, it has been shown that the concrete mechanism that gradually destroys information in the forward process has a significant impact on the image generation by the reverse process. Alternative proposed image degradations include blur[11], combinations of noise and blur[12,13,14], or image masking[12]. So far, probabilistic diffusion research was mostly of practical nature. Some theoretical contributions established connections to other fields such as score-matching[7,8,9,10], variational autoencoders[6], or normalising flows[15]. Probabilistic diffusion models have been initially motivated[1]by drift-diffusion, a well-known process in physics. However, its connections to other physics-inspired methods remain mostly unexplored. Closely related concepts have a long tradition in model-based visual computing, such as osmosis filtering proposed byWeickert et al. [16]. In addition, there is a wide variety of diffusion-based scale-spaces[17,18,19,20]. Conceptually, these scale-spaces embed given images into a family of simplified versions. This resembles the gradual removal of image features in the forward process of probabilistic diffusion models. Despite this multitude of connections, there is a distinct lack of systematic analysis of probabilistic diffusion from a scale-space perspective. This is particularly surprising due to the impact of the forward process on the generative performance[14,13]. It indicates that a better understanding of the information reduction could also lead to further practical improvements in the future. With our previous conference publication[21]we made first steps to bridge this gap between the scale-space and deep learning communities. To this end, we introduced first generalised scale-space concepts for probabilistic diffusion. In this work, we further explore the theoretical background of this successful paradigm in deep learning. In contrast to traditional scale-spaces, we consider the evolution of probability distributions instead of images. Despite this departure from conventional families of images, we can show scale-space properties in the sense ofAlvarez et al. [17]. These include architectural properties, invariances, and entropy-based measures of simplification. In addition to our previous findings[21], our novel contributions include a generalisation of our scale-space theory for diffusion probabilistic models (DPMs) which includes both variance preserving and variance exploding approaches, generalised scale-space properties for the reverse process of DPM, a scale-space theory for inverse heat dissipation[13]and blurring diffusion[14], and a significantly extended theoretical and empirical comparison of three probabilistic diffusion models to homogeneous diffusion[18]and osmosis filtering[16].
2302.01928v2	Aligning Robot and Human Representations	To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.	In the HRI community, we aspire to build robots that perform tasks that human users want them to perform. To do so, robots need goodrepresentationsof salient task aspects. For example, in Fig.1, to carry a coffee mug, the robot considers efficiency, mug orientation, and distance from the user’s possessions in its behaviour. There are two paradigms for learning representations: one thatexplicitlybuilds in structure for learning task aspects, e.g. feature sets or graphs, and one thatimplicitlyextracts task aspects by mapping input directly to desired behaviour, e.g. end-to-end approaches(Levineet al.,2020; Rosset al.,2011). While explicit structure is useful for capturing relevant task aspects, it’s often impossible to comprehensively define all aspects that may matter to the downstream task; meanwhile, implicit methods circumvent this problem by allowing neural networks to automatically extract representations, but they are prone to capturingspurious correlations(Levineet al.,2020), resulting in potentially arbitrarily bad robot behaviour under distribution shift between train and test conditions(Paudel,2022). Our observation is that many failures in robot learning, including the ones above, result from amismatch between the human’s representation and the one learned by the robot; in other words, their representations aremisaligned. From this perspective, these failures illuminate that if we truly wish to learn good representations – if we truly want robots that do what humans want – we must explicitly focus on the foundational problem:aligning robot and human representations. In this paper, we offer a unifying lens for the HRI community to view existing and future solutions to this problem. We review over 100 papers in the representation learning literature in robotics from this perspective. We first define a unifying mathematical objective for an aligned representation based on four desiderata: value alignment, generalizable task performance, reduced human burden, and explainability. We then conduct an in-depth review of four common representations (Fig.1): the identity representation, feature sets, feature embeddings, and graphical structures – illustrating the deltas each falls short in with respect to the desiderata. From situating each representation in our formalism, we arrive at the following key takeaway: a better structured representation affords better alignment and therefore better task performance, but always with the unavoidable tradeoff of more human effort. This effort can be directed in three ways: 1) representations that operate directly on the observation space, e.g. end-to-end methods, direct effort at increasing task data to avoid spurious correlations; 2) representations that build explicit task structure, e.g. graphs or feature sets, direct effort at constructing and expanding the representation; and 3) representations that learn directly from implicit human representations, e.g. self-supervised models, direct effort at creating good proxy tasks. Our paper is untraditional in that it is much like a survey paper, except there is little work that directly addresses the representation alignment problem we pose. Instead, we offer a retrospective on works that focus on learning task representations in robotics with respect to our desiderata. Our review provides a unifying lens to think about the current gaps present in the robot learning literature as defined by a common language, or in other words, a roadmap for thinking about challenges present in current and future solutions in a principled way. We conclude by suggesting key open directions.
2311.17073v1	Practical Layout-Aware Analog/Mixed-Signal Design Automation with Bayesian Neural Networks	The high simulation cost has been a bottleneck of practical analog/mixed-signal design automation. Many learning-based algorithms require thousands of simulated data points, which is impractical for expensive to simulate circuits. We propose a learning-based algorithm that can be trained using a small amount of data and, therefore, scalable to tasks with expensive simulations. Our efficient algorithm solves the post-layout performance optimization problem where simulations are known to be expensive. Our comprehensive study also solves the schematic-level sizing problem. For efficient optimization, we utilize Bayesian Neural Networks as a regression model to approximate circuit performance. For layout-aware optimization, we handle the problem as a multi-fidelity optimization problem and improve efficiency by exploiting the correlations from cheaper evaluations. We present three test cases to demonstrate the efficiency of our algorithms. Our tests prove that the proposed approach is more efficient than conventional baselines and state-of-the-art algorithms.	Analog/Mixed-signal (AMS) integrated circuit (IC) design typically follows a process flow visualized in Figure1. A combination of designer experience and computer simulation feedback is iterated to determine the design that meets the performance requirements. A large portion of design time is spent on the sizing and layout phases, where multiple iterations are possible due to potential loop-backs in the design flow. This is a labor-intensive process in industry practice with little to no automation. To address this costly exercise, a considerable effort in academia is focused on introducing automated solutions. Analog sizing automation is the task of optimizing AMS design variables, e.g., transistor widths, lengths, resistor, and capacitor values. The aim is to satisfy the performance constraints and optimize the design objective. In general, sizing automation is run through schematic-level simulations. However, AMS IC performance is also sensitive to layout implementation[4]. Especially in the advanced process nodes, layout-induced parasitics may greatly affect the final design performance. Therefore, sizing the AMS design variables considering the layout effects is also crucial. The majority of the recent sizing and post-layout performance optimization algorithms have simulation feedback in the loop. Due to advanced scaling, simulations are required to obtain accurate performance evaluations. Simulation-based AMS automation algorithms adapted various methods from the optimization and Machine Learning (ML) communities. The earlier approaches include population-based methods such as particle swarm optimization[25]and evolutionary algorithms[14]. Although these algorithms have good convergence behavior, they are inefficient in sampling since they explore the design space randomly. To mitigate sample inefficiency, model-based methods gained popularity[13,18,11]. These methods employ surrogate-models between the solution space and performance space and provide efficiency in exploring the solution space. A typical surrogate model is Gaussian Process Regression (GPR)[21], which is a well-studied model in Bayesian Optimization (BO) field[23]and is adapted by several analog sizing algorithms. The main drawback of GPR modeling is its computational complexity. Recent research trend in analog sizing introduces ML to simulation-based methodology[3]. However, the literature review reveals that most of these methods require thousands of simulation data to train Deep Neural Network (DNN) models that approximate the relations between the design variables and the performance metrics. Therefore, the practicality of these algorithms is severely reduced when the optimization task has a high simulation cost. For example, drawing/generating the layout, extracting the parasitics, and running post-layout simulations is typically an expensive procedure. Therefore, optimization algorithms designed for schematic-level sizing can not be adapted by simply changing how data is generated. This paper presents a Machine Learning-based simulation-in-the-loop automation method for the AMS design problem. Overall, we formalize two stand-alone recipes for schematic-level sizing and post-layout performance optimization, i.e., layout-aware sizing. We integrate the state-of-the-art analog layout generator, MAGICAL[27], into our flow to handle layout-aware sizing. Our algorithms do not assume the pre-existence of any dataset, and we generate all training data during the optimization. We employ Bayesian Neural Networks (BNN) for modeling design performances. Bayesian Neural Networks allow error quantification, and compared to Deep Neural Networks, BNN are shown to be effective in handling scarce datasets and preventing overfitting[9]. Therefore, BNN can be trained on smaller datasets, significantly improving the practicality and scalability. We also introduce a batch-optimization framework and design space sampling strategy that is compatible with BNN modeling. Further, when optimizing the design variables based on post-layout performance, we exploit the correlation between schematic-level simulations and post-layout simulations. Our algorithm introduces a co-learning scheme that reduces the need for costly post-layout simulations and boosts efficiency even further. We compile our contributions as follows: We use Bayesian Neural Network-based modeling to obtain performance approximations. Different learning strategies are adapted for schematic-level sizing and post-layout performance optimization. We adopt a scalable sampling strategy and query the optimization batches by utilizing a trust region and Thompson sampling. The post-layout sizing is handled as a multi-fidelity optimization problem, and an efficient co-learning strategy is developed. The efficiency of the proposed methods is demonstrated on three circuits by providing comparisons to previous state-of-the-art. The rest of the paper is organized as follows. SectionIIintroduces the backgrounds and previous work. SectionIIIdescribes our algorithms for handling schematic-level sizing and post-layout performance-based sizing problems. SectionIVprovides the experiments on circuit examples to demonstrate the efficiency of our algorithms. Finally, SectionVconcludes the paper.
2302.08224v2	DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization	Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.	Combinatorial Optimization (CO) problems are mathematical problems that involve finding the optimal solution in a discrete space. They are fundamental challenges in computer science, especially the NP-Complete (NPC) class of problems, which are believed to be intractable in polynomial time. Traditionally, NPC solvers rely on integer programming (IP) or hand-crafted heuristics, which demand significant expert efforts to approximate near-optimal solutions(Arora,1996; Gonzalez,2007). Recent development in deep learning has shown new promise in solving NPC problems. Existing neural CO solvers for NPC problems can be roughly classified into three categories based on how the solutions are generated, i.e., the autoregressive constructive solvers, the non-autoregressive constructive solvers, and the improvement heuristics solvers. Methods in the first category use autoregressive factorization to sequentially grow a valid partial solution(Bello et al.,2016; Kool et al.,2019a). Those methods typically suffer from the costly computation in their sequential decoding parts and hence are difficult to scale up to large problems(Fu et al.,2021). Methods in the second category rely on non-autoregressive modeling for scaling up, with a conditional independence assumption among variables as typical(Joshi et al.,2019; Karalias and Loukas,2020; Qiu et al.,2022). Such an assumption, however, unavoidably limits the capability of those methods to capture the multimodal nature of the problems(Khalil et al.,2017; Gu et al.,2018), for example, when multiple optimal solutions exists for the same graph. Methods in the third category (improvement heuristics solvers) use a Markov decision process (MDP) to iteratively refines an existing feasible solution with neural network-guided local operations such as 2-opt(Lin and Kernighan,1973; Andrade et al.,2012)and node swap(Chen and Tian,2019; Wu et al.,2021). These methods have also suffered from the difficulty in scaling up and the latency in inference, partly due to the sparse rewards and sample efficiency issues when learning improvement heuristics in a reinforcement learning (RL) framework(Wu et al.,2021; Ma et al.,2021). Motivated by the recent remarkable success of diffusion models in probabilistic generation(Song and Ermon,2019; Ho et al.,2020; Rombach et al.,2022;Yu et al.,; Saharia et al.,2022b), we introduce a novel approach named DIFUSCO, which stands for the graph-based DIFfUsion Solvers for Combinatorial Optimization. To apply the iterative denoising process of diffusion models to graph-based settings, we formulate each NPC problem as to find a\{0,1\}-valued vector that indicates the optimal selection of nodes or edges in a candidate solution for the task. Then we use a message passing-based graph neural network(Kipf and Welling,2016; Hamilton et al.,2017; Gilmer et al.,2017; Veličković et al.,2018)to encode each instance graph and to denoise the corrupted variables. Such a graph-based diffusion model overcomes the limitations of previous neural NPC solvers from a new perspective. Firstly, DIFUSCO can perform inference on all variables in parallel with a few (\ll N) denoising steps (Sec.3.3), avoiding the sequential generation problem of autoregressive constructive solvers. Secondly, DIFUSCO can model a multimodal distribution via iterative refinements, which alleviates the expressiveness limitation of previous non-autoregressive constructive models. Last but not least, DIFUSCO is trained in an efficient and stable manner with supervised denoising (Sec.3.2), which solves the training scalability issue of RL-based improvement heuristics methods. We should point out that the idea of utilizing a diffusion-based generative model for NPC problems has been explored recently in the literature. In particular,Graikos et al. (2022)proposed an image-based diffusion model to solve Euclidean Traveling Salesman problems by projecting each TSP instance onto a64\times 64greyscale image space and then using a Convolutional Neural Network (CNN) to generate the predicted solution image. The main difference between suchimage-baseddiffusion solver and ourgraph-baseddiffusion solver is that the latter can explicitly model the node/edge selection process via the corresponding random variables, which is a natural design choice for formulating NPC problems (since most of them are defined over a graph), while the former does not support such a desirable formalism. Although graph-based modeling has been employed with both constructive(Kool et al.,2019a)and improvement heuristics(d O Costa et al.,2020)solvers, how to use graph-based diffusion models for solving NPC problems has not been studied before, to the best of our knowledge. We investigate two types of probabilistic diffusion modeling within the DIFUSCO framework: continuous diffusion with Gaussian noise(Chen et al.,2022)and discrete diffusion with Bernoulli noise(Austin et al.,2021; Hoogeboom et al.,2021). These two types of diffusion models have been applied to image processing but not to NPC problems so far. We systematically compare the two types of modeling and find that discrete diffusion performs better than continuous diffusion by a significant margin (Section4). We also design an effective inference strategy to enhance the generation quality of the discrete diffusion solvers. Finally, we demonstrate that a single graph neural network architecture, namely the Anisotropic Graph Neural Network(Bresson and Laurent,2018; Joshi et al.,2022), can be used as the backbone network for two different NP-complete combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Our experimental results show that DIFUSCO outperforms previous probabilistic NPC solvers on benchmark datasets of TSP and MIS problems with various sizes.
2305.14601v1	FaceFusion: Exploiting Full Spectrum of Multiple Datasets	The size of training dataset is known to be among the most dominating aspects of training high-performance face recognition embedding model. Building a large dataset from scratch could be cumbersome and time-intensive, while combining multiple already-built datasets poses the risk of introducing large amount of label noise. We present a novel training method, named FaceFusion. It creates a fused view of different datasets that is untainted by identity conflicts, while concurrently training an embedding network using the view in an end-to-end fashion. Using the unified view of combined datasets enables the embedding network to be trained against the entire spectrum of the datasets, leading to a noticeable performance boost. Extensive experiments confirm superiority of our method, whose performance in public evaluation datasets surpasses not only that of using a single training dataset, but also that of previously known methods under various training circumstances.	The priority focus of modern face recognition studies has been in-line with that of representation learning studies: amplifying the representative power of embedding vectors within the feature space. With continued development and refinement of various training methods, face recognition models have seen significant improvement in terms of evaluation accuracy in recent years. However, equally important is the recent advent of publicly available large-scale facial image datasets. These large datasets have generally been curated by either partially or entirely automated crawling of publicly available facial images, followed by different approaches of clustering the images by their identities. While the approach of building such large datasets from scratch have been studied widely, considerably less amount of attention has been given to the methods of starting from already-curated datasets. Combining different datasets is naturally beneficial from the fact that they have already gone through some degree of refinements that include identity-wise clustering or noise removal, but because their image sources generally come from public datasets of celebrities or random web crawling, careful consideration is required to handle conflicting identities. Identity conflict is destructive to the overall model performance because physically same identities are interpreted as distinct identities, which the model is incorrectly taught to distinguish. A trivial solution to this inter-class label noise would be to train a rectifier model[34]to adjust such identities, but such solution would be heavily dependent on training a robust rectifier model. Another possible approach is explored in[16]as a posterior data cleaning process, but it potentially requiresO(MN)memory space, whereMis the number of conflicting identities, andNis the number of datasets. DAIL[27], to our best knowledge, is the only work that studies the way of using multiple datasets concurrently without a separate dataset-manipulation process by introducing dataset-aware softmax. While the approach of DAIL somewhat mitigates the conflicting identity problem, we argue that the performance gain from DAIL is suboptimal, because dataset-aware softmax essentially isolates the representations learned from each dataset from the rest of the datasets. This reduces the scope of final softmax operation from the entire embedding space to the subspaces fitted to each dataset, preventing the embedding model to reach global optimization.To suppress the inevitably introduced inter-class label noise when combining different datasets, and to further improve from the limited performance gain of DAIL, we introduceFaceFusion. Our approach begins from the method of DAIL, and after the model parameters are stabilized, observed as the well-knownslow-drift[29]phenomenon, it directlyfusesdifferent datasets into a unified global embedding space, while also merging the class proxies of conflicting identities. Doing so effectively enables viewing multiple datasets as a unified dataset, which the embedding model can exploit to expand its optimization scope from within each dataset to the whole datasets, resulting in superior performance. Extensive experiments confirms that FaceFusion outperforms not only the models trained using single dataset, but also the models trained with multiple datasets either by naive concatenation or by the method of DAIL. We further prove that FaceFusion maintains its superiority over the aforementioned methods under varying severity of conflicting identities of each dataset, ranging from completely disjoint to vastly overlapping with each other.
2301.09003v1	Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models	"Groundbreaking inventions and highly significant performance improvements in deep learning based Natural Language Processing are witnessed through the development of transformer based large Pre-trained Language Models (PLMs). The wide availability of unlabeled data within human generated data deluge along with self-supervised learning strategy helps to accelerate the success of large PLMs in language generation, language understanding, etc. But at the same time, latent historical bias/unfairness in human minds towards a particular gender, race, etc., encoded unintentionally/intentionally into the corpora harms and questions the utility and efficacy of large PLMs in many real-world applications, particularly for the protected groups. In this paper, we present an extensive investigation towards understanding the existence of ""Affective Bias"" in large PLMs to unveil any biased association of emotions such as anger, fear, joy, etc., towards a particular gender, race or religion with respect to the downstream task of textual emotion detection. We conduct our exploration of affective bias from the very initial stage of corpus level affective bias analysis by searching for imbalanced distribution of affective words within a domain, in large scale corpora that are used to pre-train and fine-tune PLMs. Later, to quantify affective bias in model predictions, we perform an extensive set of class-based and intensity-based evaluations using various bias evaluation corpora. Our results show the existence of statistically significant affective bias in the PLM based emotion detection systems, indicating biased association of certain emotions towards a particular gender, race, and religion."	Recently, large scale Natural Language Processing (NLP) models are being increasingly deployed in many real-world applications within almost all domains such as health-care(Velupillaiet al.,2018; Soni and Roberts,2020), business(Mishevet al.,2020), legal systems(Dale,2019), etc., due to its efficacy to make data-driven decisions and capability of natural language understanding even better than humans111https://www.infoq.com/news/2021/01/google-microsoft-superhuman/(Heet al.,2021). Transformer based large Pre-trained Language Models (PLMs) have been hugely influential in NLP due to their capability to generate powerful contextual representations. PLMs are mostly built based on a self-supervised learning strategy that highly relies on unlabelled data abundantly available from the human generated data deluge(Heet al.,2021). But, since this historical data of textual write-ups has their roots within human thought, they often reflect latent social stereotypes(Suresh and Guttag,2021; Garget al.,2018). For example, the Social Role Theory by Eagly et al.(Eagly and Steffen,1984)demonstrates that the idea of gender stereotype develops from perceivers’ observations, associating the capabilities and personality attributes of different genders with the activities in which they engage in their day-to-day lives over a time, building rigid stereotypes in human minds and their writings, on how these genders behave (e.g. women are highly emotional), where they work (e.g. women preferred in children’s daycare), etc. Hence the data from such human generated data repositories eventually convey these stereotypes as linguistic biases through the NLP algorithms, especially those built on large PLMs that utilize huge amounts of data(Suresh and Guttag,2021). In this direction, investigation of“Affective Bias”in NLP is a recent stream of research to study the existence of any unfair/biased association of emotions (anger, fear, joy, etc.) or sentiments (positive, negative, etc.) towards underrepresented or protected groups in a domain, that may lead affective computing systems such as sentiment or emotion detection systems to deliver unfavorable outcomes for these protected groups(K.et al.,2022). For instance, a model consistently associating women with a different class of emotion or same emotion differing in emotion intensities vis-a-vis predictions for male(Shields and Shields,2002)could be seen as a manifestation of affective bias. Similarly, association of a particular religion always with a specific emotion(Abidet al.,2021a)represents affective bias too. A real world scenario of affective bias is the case of Google sentiment analyzer judging that being gay is bad by assigning high negative sentiments to sentences such as ‘I’m a gay black woman’, ‘I’m a homosexual’, etc.,222https://www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias. For better understandability of affective bias, we illustrate in table1, a sample set of affectively biased emotion predictions from PLM based textual emotion detection models constructed in this study for affective bias analysis (detailed explanation of the models are provided in section4.1). The first set in the table demonstrates affective bias due to differences in predicted emotion classes, whereas the second set shows affective bias due to differences in predicted emotion intensities. Similar to other general algorithmic biases like gender bias, racial bias, etc., a possible stimuli to affective biases are the latent emotion based stereotypes about different social groups in the data. Studies report that such emotion based stereotyping influence socialization of emotions leading to propagation of stereotypes such as associating women’s (or men’s) experience and expressions being aligned with fear and sadness (or anger and pride)(Plantet al.,2000). Similarly, affective bias within systems could facilitate a higher association of black women to the emotion anger when considering emotions with the domains race and gender(Ashley,2014). In addition to biased data, another reason for bias is based on how the model/algorithmic design considers or treats the underrepresented or protected attributes concerning a domain(Hooker,2021). Similar to any other general social biases, the existence of these affective biases make textual affective computing systems generate unfair or biased decisions that can harm its utility towards socially marginalized populations by denying opportunities/resources or by false portrayal of these groups when deployed in the real-world. Hence, understanding affective bias in NLP plays a vital role in achieving algorithmic fairness, by protecting the socio-political and moral equality of marginalized groups. In this context, we present an extensive experimental analysis to understand and illustrate the existence of latent“Affective Bias”in transformer based large pre-trained language models with respect to the downstream task of textual emotion detection. Hence, we set our research question:Do predictions made by large PLM based textual emotion detection systems systematically or consistently exemplify ‘Affective Bias’ towards demographic groups?. Our investigation of affective bias in large PLMs primarily aims to identify the existence of gender, racial, and religious affective biases and set aside the task of affective bias mitigation in the scope for future work. We start with an exploration of corpus level affective bias or affect imbalance in corpus to find out any biased emotion associations in the large scale corpora that are used to pre-train and fine-tune the PLMs, by analyzing the distribution of emotions or their associations with demographic target terms (e.g., Islam, Quran) related to a social group (e.g., Muslim) concerning a domain (e.g., Religion). Later, we explore the prediction level affective bias in four popular transformer based PLMs, BERT (Bidirectional Encoder Representation from Transformers)(Devlinet al.,2019), OpenAI GPT-2 (Generative Pre-trained Transformer)(Radfordet al.,2019), XLNet(Yanget al.,2019), and T5 (Text-to-Text Transfer Transformer)(Raffelet al.,2020), that are fine-tuned using a popular corpora SemEval-2018 EI-oc(Mohammadet al.,2018)for the task of textual emotion detection. To quantify prediction level affective bias, we subject the PLMs to an extensive set of class-based and intensity-based evaluations using three different evaluation corpora EEC(Kiritchenko and Mohammad,2018), BITS(Venkit and Wilson,2021)and CSP(Nangiaet al.,2020). A detailed sketch of the overall analysis is shown in figure1. Workflow ofAffective biasanalysis The rest of the paper is organized as follows. Section2presents the relevant related works. Section3presents corpus level affective bias analysis with corresponding methodology and results. Section4presents the exploration towards prediction level affective bias with details of constructing PLM based textual emotion detection model, methodology of analysis, and the corresponding results. Section5presents a discussion based on the entire results and finally, section6draws the conclusions.
2305.07892v1	DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning	Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmentation consistency to encode invariance as meta-knowledge for instantiating MR objective, denoted by DAC-MR. The proposed DAC-MR is hopeful to learn well-performing meta-models from training tasks with noisy, sparse or unavailable meta-data. We theoretically demonstrate that DAC-MR can be treated as a proxy meta-objective used to evaluate meta-model without high-quality meta-data. Besides, meta-data-driven meta-loss objective combined with DAC-MR is capable of achieving better meta-level generalization. 10 meta-learning tasks with different network architectures and benchmarks substantiate the capability of our DAC-MR on aiding meta-model learning. Fine performance of DAC-MR are obtained across all settings, and are well-aligned with our theoretical insights. This implies that our DAC-MR is problem-agnostic, and hopeful to be readily applied to extensive meta-learning problems and tasks.	Maching learning has recently demonstrated impressive performance in various fields, e.g., computer vision[he2016deep], natural language processing[devlin2018bert], speech processing[abdel2014convolutional], etc. However, an effective machine learning method often requires a large amount of high-quality labeled data to properly and sufficiently simulate the testing/evaluating distribution. Collecting such large-scale supervised datasets is notoriously expensive in time and effort for most real applications. Compared with current machine intelligence, humans are able to quickly learn novel concepts from only small amount of examples[lake2015human,lake2017building]. The capability of machine to learn new concepts quickly from small examples is thus desirable, especially for many problems/applications where data are intrinsically rare or expensive, or compute resources are unavailable. Meta-learning[naik1992meta,schmidhuber1987evolutionary,thrun1998lifelong], or learning to learn, has been suggested as a promising solution path to assemble machine learning with above capability. The key idea of meta-learning is to distill a meta-model from multiple learning tasks/episodes, and then use this meta-model to improve performance of task-specific model on novel query tasks[hospedales2021meta,shu2021learning]. Such a learning paradigm is hopeful to bring a variety of benefits, such as finely adapting to query tasks with less computation/data costs (e.g., avoid learning from scratch for novel tasks), as well as fewer human interventions. Recently, it produces an explosion of researches on meta-learning, due to its potential to advance the frontier of the contemporary machine learning. Especially, meta-learning has helped machine learning improve the data efficiency[wang2020generalizing,shu2018small], algorithm automation[he2021automl,karmaker2021automl], and generalization[maurer2016benefit,tripuraneni2020theory,shu2021learning]. Successful applications have been demonstrated in areas spanning few/zero-shot learning[finn2017model,snell2017prototypical,vinyals2016matching,sung2018learning,soh2020meta], neural architecture search (NAS)[liu2018darts,elsken2019neural], hyperparameter optimization[franceschi2018bilevel], curriculum learning[ren2018learning,shu2019meta], domain adaptation/generalization[liu2021cycle,li2018learning], transfer learning[jang2019learning,sun2020meta], label noise learning[zheng2021meta,shu2020meta,zhao2021probabilistic], semi-supervised learning[pham2021meta], unsupervised learning[metz2018meta], reinforcement Learning[duan2016rl,wang2016learning], data/label generation[cubuk2018autoaugment,wu2021learning,shu2022cmw], loss/regularization learning[balaji2018metareg,yazdanpanah2022revisiting,lee2019meta,shu2020learning,huang2019addressing], learning to optimize[andrychowicz2016learning,ravi2016optimization,shu2022mlr], and robustness[collins2020task,killamsetty2022nested], etc. These successes largely attribute to the data-based nature of current meta-learning approaches that learn from a tremendous number of training tasks with high-quality meta-data representing the underlying task generalization goal. However, in most real applications, collecting such high-quality training tasks are difficult, expensive and impractical. This often makes obtained training tasks imperfect. In fact, we always have access to problematic meta-data for some applications. For example, the corresponding ground-truth labels of meta-data are generally noisy in label noise problems[shu2019meta], or unavailable in unsupervised domain adaptation tasks[liu2021cycle], or the size of meta-data is limited in few-shot learning issues[finn2017model]. The purely meta-data-driven approaches tend to reach their limits or lead to unsatisfactory results under these imperfect circumstances. With meta learning becoming more and more popular in real applications, there is also a growing need for meta-learning to train well-performing and sufficiently generalized meta-models from such imperfect training tasks. As a step towards addressing the limitations of purely meta-data-driven meta-learning, we suggest a meta-knowledge informed meta-learning (MKIML) framework, as shown in Fig.1, which comprises an additional meta-knowledge integration into the meta learning pipeline. Moreover, such meta-knowledge could be obtained in an external, separated way from the meta-learning problem and the usual training tasks. This framework is expected to be functional in exploring an orthogonal meta-knowledge-driven approaches relative to previous purely meta-data-driven approaches to learn and evaluate meta-model. With the MKIML framework, we attempt to integrate meta-knowledge into meta-objective by means of a meta-regularization (MR) term. The key insight is that we leverage the benefits of fundamental properties of the meta-model for various training tasks, which should help achieve better generalization of meta-model to unseen tasks and alleviate the critical need of high-quality meta-data. Specifically, in this study we instantiate MR with the data augmentation consistency (DAC) as a new meta-objective for meta-learning. The DAC stems from recent advances in semi-supervised learning[xie2020unsupervised,sohn2020fixmatch], and to the best of our knowledge, we exploit it to meta-regularize the complexity of meta-model function class for the first time, which enforces the model facilitated by meta-model to output similar predictions under input data augmentations. Our contributions can be mainly summarized as follows. 1) We suggest a MKIML framework, as shown in Fig.1, aiming to improve capability of previous purely meta-data-driven meta-learning approaches by additionally integrating compensated meta-knowledge into meta learning process. Specifically, we explore to integrate meta-knowledge into meta-objective through designing an appropriate meta-regularizer (MR). The MR is functional on regularising the capacity complexity of meta-model function class, so as to improve its meta-level generalization on unseen tasks. 2) We introduce data augmentation consistency (DAC) to instantiate MR objective for an effective practical implementation (DAC-MR for brevity). The DAC-MR provides a general approach to help apply meta-learning models to tasks with noisy, sparse or unavailable meta-data. Besides, the DAC-MR is problem-agnostic, which can be generally applicable to extensive meta-learning problems and tasks. 3) We theoretically prove that the additional DAC-MR term in meta-objective can bring better meta-level generalization compared with solely meta-data-driven meta-loss objective. Meanwhile, we prove that DAC-MR is able to be regarded as a proxy meta-objective implicitly calculated on high-quality meta-data under some mild conditions. 4) We experimentally demonstrate that DAC-MR aids meta-model learning across various meta-learning problems in computer vision, including few-shot learning (§4), transfer learning (§5), continual learning (§6) and label noise learning (§7). Specifically, our DAC-MR is substantiated to be valid across 10 meta-learning tasks with different network architectures and testing benchmarks. Furthermore, these empirical results are well-aligned with our theoretical insights. The paper is organized as follows. §2discusses related work. §3presents the proposed MKIML framework, MR objective and our DAC-MR objective as a practical implementation for MKIML as well as its theoretical insights. We evaluate DAC-MR to few-shot learning in §4, transfer learning in §5, continual learning in §6and label noise learning in §7, respectively. The conclusion is finally made.
2309.05289v1	Task-driven Compression for Collision Encoding based on Depth Images	"This paper contributes a novel learning-based method for aggressive task-driven compression of depth images and their encoding as images tailored to collision prediction for robotic systems. A novel 3D image processing methodology is proposed that accounts for the robot's size in order to appropriately ""inflate"" the obstacles represented in the depth image and thus obtain the distance that can be traversed by the robot in a collision-free manner along any given ray within the camera frustum. Such depth-and-collision image pairs are used to train a neural network that follows the architecture of Variational Autoencoders to compress-and-transform the information in the original depth image to derive a latent representation that encodes the collision information for the given depth image. We compare our proposed task-driven encoding method with classical task-agnostic methods and demonstrate superior performance for the task of collision image prediction from extremely low-dimensional latent spaces. A set of comparative studies show that the proposed approach is capable of encoding depth image-and-collision image tuples from complex scenes with thin obstacles at long distances better than the classical methods at compression ratios as high as 4050:1."	Methods for autonomous collision-free navigation of aerial robots have traditionally relied on motion planning techniques that exploit a dense map representation of the environment[28,3,24,27]. Departing from such methods, the community has recently investigated the potential of deep learning to develop navigation methods that act directly on exteroceptive data such as depth images instead of reconstructed maps in order to plan the aerial vehicle’s motions with minimal latency[15,16,22,12]. However, such methods face the challenge that exteroceptive data and especially depth images coming from stereo vision or other sensors are typically of very high dimensionality and the involved neural networks include layers that partially act as lossy information compression stages. This is reflected in the architectures of otherwise successful methods such as the works in[15,22,12]that exploit depth images to evaluate which among a set of candidate robot trajectories would collide or not. In[15]the input depth image involves more than300,000pixels (640\times 480resolution) but through stages of a pre-trained MobileNetV3 architecture it gets processed toMfeature vectors of size32each, whereMis the number of candidate trajectories for which this method derives collision scores. Eventually by combining the640\times 480pixels depth image with robot pose information, the method attempts to predict which amongMtrajectories are safe, thus representing a process of information downsampling and targeted inference. In other words, despite the dimensionality reduction taking place through the neural network it is attempted that the method still ensures collision avoidance. However, it is known that such techniques do not provide100\%success ratio especially in complex and cluttered scenes. Responding to the above, this work contributes the concept of task-driven compression and encoding of depth images as visualized in Figure1. Departing from the concept that methods aiming to predict the safety of candidate robot trajectories based on depth images should train collision prediction either a) directly in an end-to-end fashion through depth data[15,22]or through b) an explicit intermediate compression stage of the depth image itself[23], we propose the approach of using the depth image to encode a latent space presenting major dimensionality reduction that reflects not the depth image itself but instead a “collision image”. The latter is a remapping of the depth image that has accounted about the robot’s size and thus presents reduced overall complexity and greatly reduced presence of narrow/thin features that are hard-to-retain in an aggressive compression step. To achieve this goal, the method employs a probabilistic encoder-decoder architecture that is trained in a supervised manner such that given a depth image as input, it learns to encode and reconstruct the collision image. To train this collision-predicting network –dubbedDepth image-based Collision Encoder(DCE)– the depth image is first processed such that the collision image is calculated given information for the robot’s size. Focusing on aggressive dimensionality reduction, it is demonstrated that the scheme allows to get accurate reconstructions through a latent space that is more than3orders of magnitude smaller than the input image. The benefits of the approach are demonstrated through comparisons both with a conventional Variational Autoencoder (VAE) trained to encode the depth image and assessed regarding the extent to which the reconstructed image can serve as basis to derive a correct collision image, as well as traditional compression methods using the Fast Fourier Transform (FFT) and wavelets. In the remaining paper Section2presents related work and Section3details the proposed method involving generation of training data, image augmentation and the training of the neural network. Section4compares our proposed method against traditional image compression methods and evaluates the performance of task-driven and task-agnostic compression methods at similar degrees of compression. Finally, conclusions are drawn in Section5.
2302.08418v1	Generative AI-empowered Simulation for Autonomous Driving in Vehicular Mixed Reality Metaverses	In the vehicular mixed reality (MR) Metaverse, the distance between physical and virtual entities can be overcome by fusing the physical and virtual environments with multi-dimensional communications in autonomous driving systems. Assisted by digital twin (DT) technologies, connected autonomous vehicles (AVs), roadside units (RSU), and virtual simulators can maintain the vehicular MR Metaverse via digital simulations for sharing data and making driving decisions collaboratively. However, large-scale traffic and driving simulation via realistic data collection and fusion from the physical world for online prediction and offline training in autonomous driving systems are difficult and costly. In this paper, we propose an autonomous driving architecture, where generative AI is leveraged to synthesize unlimited conditioned traffic and driving data in simulations for improving driving safety and traffic efficiency. First, we propose a multi-task DT offloading model for the reliable execution of heterogeneous DT tasks with different requirements at RSUs. Then, based on the preferences of AV's DTs and collected realistic data, virtual simulators can synthesize unlimited conditioned driving and traffic datasets to further improve robustness. Finally, we propose a multi-task enhanced auction-based mechanism to provide fine-grained incentives for RSUs in providing resources for autonomous driving. The property analysis and experimental results demonstrate that the proposed mechanism and architecture are strategy-proof and effective, respectively.	The vehicular mixed reality (MR) Metaverse is envisioned as a promising solution for realizing autonomous driving by fusing the physical and virtual vehicular networks[46,40]. The multi-dimensional communications among physical and virtual entities can surrender the distance of “data islands” on roads for improving road safety and traffic efficiency while reducing energy consumption and carbon emissions[35]. Assisted by digital twin (DT) technologies, autonomous vehicles (AV) utilize advanced sensors, e.g., ultrasonic radars, cameras, and LiDAR, to collect data from their surrounding environments for constructing virtual representations in the virtual space[12]. Then, AVs can make driving decisions, such as driving model selection and motion planning, via artificial intelligence (AI) methods. Even though panoramic cameras and high-class LiDAR are equipped with AVs, each AV can only collect limited environment data and cannot perceive the whole environment, e.g., occlusions[32]. Therefore, multiple connected AVs, roadside units (RSUs), and virtual simulators can share and fuse sensing data in the virtual space, to perceive the complete information of environments including occlusions. However, it is difficult and costly to collect realistic driving data on a large scale to train AVs directly in the physical world. To address this issue, much effort from academia and industry has been devoted to developing platforms in the virtual space for traffic and driving simulations[7,8]. By establishing virtual driving simulation platforms with DT[14,12]and MR[27,6]technologies, virtual representations of AVs can efficiently collect traffic and training data and cheaply test it on rare cases, such as virtual traffic accidents and car collisions under realistic scenes[19,20]. Although traditional simulation platforms can generate an unlimited number of various driving experiences, the collected driving data requires a lot of manual work for labeling, which prevents the potential from being fully realized[7]. Fortunately, with the multi-modal generative AI[11,31,10], the labeled traffic and driving data can be synthesized directly for virtual autonomous driving systems[16]. In this way, the process of using simulation platforms for autonomous driving training and evaluation is revolutionized by shifting from collecting and labeling data to directly synthesizing labeled data[16,38]. Therefore, the simulation systems empowered by generative AI can generate large and diverse labeled driving datasets based on real-time road and weather conditions and user preferences for online prediction and offline training in autonomous driving systems. Furthermore, in the vehicular Metaverse, connected AVs, RSUs, and virtual simulators need to construct the traffic and driving simulation platforms in the virtual space collaboratively. To update with virtual representations in virtual space, AVs continuously generate and offload multiple computation-intensive DT tasks to RSUs in online traffic simulation[12]. Specifically, these DT tasks of each AV, including simulation, decision-making, and monitoring, are heterogeneous in requiring computing, communication resources, and deadlines. In driving simulations, virtual simulators synthesize controllable traffic and driving data for satisfying specific requirements, e.g., passenger preferences and weather conditions, of the simulated driving tasks[20]. In addition, the synthesized traffic and driving datasets can also be used in training virtual representations of AVs to further improve driving robustness. These synchronization activities, e.g., DT task execution, traffic and driving simulations, and AV training, are demanding enormous communication and computing resources of RSUs for supporting autonomous driving systems[44,39]. Therefore, developing effective multi-task incentive mechanisms that motivate RSUs to improve their use of communications and computing resources is imperative. As shown in Fig.1, in this paper, we propose a novel DT-assisted autonomous driving architecture for the vehicular MR Metaverse, where generative AI is leveraged to synthesize massive and conditioned traffic and driving data for online and offline simulations. In detail, to improve reliability in DT task execution, we propose a multi-task DT offloading model where AVs can offload heterogeneous DT tasks with different deadlines to RSUs for real-time execution. To improve reliability in driving decision-making, virtual simulators can utilize the information in DTs, such as current location, historical trajectory, and user preferences, for online traffic simulations[45,37]. Moreover, based on the collected sensing data in the physical world and user preferences in DTs, virtual simulators can synthesize massive and conditioned driving data for AV training of virtual simulators via running generative AI models. As a use case, we propose a diffusion model-based traffic sign generator, named TSDreamBooth, which is developed based on the DreamBooth[23]fine-tuned using Belgium traffic sign (BelgiumTS) dataset[18]. The TSDreamBooth can be leveraged to generate virtual traffic sign images under different driving conditions and user preferences. Finally, we propose a multi-task enhanced auction-based mechanism to satisfy multi-dimensional requirements (e.g., prices and deadlines) of multiple DT tasks. We analyze the properties of the proposed auction and prove that it is strategy-proof and adverse-selection free. The experimental results demonstrate that the proposed framework can increase total social surplus by 150%. Our main contributions are summarized as follows: To improve the safety and reliability of autonomous driving, we propose a novel DT-assisted MR Metaverse architecture with MR simulations empowered by generative AI. In this architecture, connected AVs, RSUs, and virtual simulators maintain digital simulation platforms in the virtual space, where data collecting, sharing, and utilizing among physical and virtual entities can improve driving safety and traffic efficiency in physical autonomous driving systems. In this architecture, we propose a reliable DT task offloading framework where AVs can continuously offload multiple DT tasks with different requirements to RSUs for updating DTs in the virtual space. In traffic and driving simulations, we consider generative AI-empowered virtual simulators to synthesize new driving data for AVs’ decision-making and training. To incentivize RSUs for providing resources in supporting autonomous driving systems, we propose a multi-task enhanced auction-based mechanism to offer fine-grained allocation results and prices for executing heterogeneous DT tasks with various deadlines. Based on the property analysis, the proposed mechanism is fully strategy-proof and adverse-selection free. The rest of this paper is organized as follows. In SectionII, we review the related works. In SectionIII, we discuss the proposed system architecture and its system model. Then, in SectionIV, we implement the multi-task enhanced auction-based mechanism. We demonstrate the experimental results in SectionV, and provide a conclusion in SectionVI.
2302.02852v1	Guide the Learner: Controlling Product of Experts Debiasing Method Based on Token Attribution Similarities	Several proposals have been put forward in recent years for improving out-of-distribution (OOD) performance through mitigating dataset biases. A popular workaround is to train a robust model by re-weighting training examples based on a secondary biased model. Here, the underlying assumption is that the biased model resorts to shortcut features. Hence, those training examples that are correctly predicted by the biased model are flagged as being biased and are down-weighted during the training of the main model. However, assessing the importance of an instance merely based on the predictions of the biased model may be too naive. It is possible that the prediction of the main model can be derived from another decision-making process that is distinct from the behavior of the biased model. To circumvent this, we introduce a fine-tuning strategy that incorporates the similarity between the main and biased model attribution scores in a Product of Experts (PoE) loss function to further improve OOD performance. With experiments conducted on natural language inference and fact verification benchmarks, we show that our method improves OOD results while maintaining in-distribution (ID) performance.	Overfitting to the training data is a big obstacle in learning patterns that generalize to unseen data. Traditionally, this is diagnosed by monitoring the performance of a trained model on an in-distribution (ID) test set. However, a bigger challenge is when both the training and test data have the same non-generalizable patterns, emerged as spurious correlations between input features and output labelsGardneret al.(2021). For instance, in the natural language inference (NLI) task, it is shown that the occurrence of some task-neutral words, like a negation in hypothesis, is highly correlated with a specific class(Gururanganet al.,2018). While high-capacity models can learn a generalized distribution of labels from the inputs, they are prone to spurious patterns, also known as dataset biasesClarket al.(2019); Heet al.(2019). A model could exploit these biases during fine-tuning, leading to a model that achieve high ID performance, while it is highly fragile in out-of-distribution (OOD) settings(Schusteret al.,2019; McCoyet al.,2020). Besides trying to prevent these non-generalizable artifacts from entering the datasetLiuet al.(2022), it is reasonable to seek for more robust learning methods. This has been the basis for a multitude of research works that encourage models to rely on truly generalizable patterns. Most of these methods are based on the assumption that the learning method will inevitably exploit biases if they are present in a training example(Clarket al.,2019; Sanhet al.,2020; Mahabadiet al.,2020; Utamaet al.,2020; Ghaddaret al.,2021). Therefore, they discourage the main model from paying much attention to the examples which are correctly classified by a biased model. Recently, it is shown that this assumption is questionable in the way that for a significant number of cases, the main model does not follow the biased model in treating biased examples(Amirkhani and Pilehvar,2021). Therefore, depriving the training algorithm from the examples which are detected to be biased is a waste of training data. In this paper, we propose an alternative way to discard biased examples. Instead of considering the mere prediction of the biased model, we monitor the way the model processes each example by computing its attribution scores over the input tokens. With the resulting scores, we adjust the proportion of the loss function that is a cross-entropy loss (CE) versus a Product of Experts loss (PoE). If the attribution scores are similar between the main and biased models, the loss becomes a PoE loss where a correct prediction from the biased model down-weights the contribution of the corresponding example. In contrast, dissimilarity between the scores suggests a different behaviour from the biased model and leads to a CE loss that only considers the main model’s prediction. Experiments on natural language inference and fact verification demonstrate that our method significantly outperforms previous approaches in terms of OOD performance while preserving its ID performance.
2308.13273v1	Bridging the Gap: Fine-to-Coarse Sketch Interpolation Network for High-Quality Animation Sketch Inbetweening	The 2D animation workflow is typically initiated with the creation of keyframes using sketch-based drawing. Subsequent inbetweens (i.e., intermediate sketch frames) are crafted through manual interpolation for smooth animations, which is a labor-intensive process. Thus, the prospect of automatic animation sketch interpolation has become highly appealing. However, existing video interpolation methods are generally hindered by two key issues for sketch inbetweening: 1) limited texture and colour details in sketches, and 2) exaggerated alterations between two sketch keyframes. To overcome these issues, we propose a novel deep learning method, namely Fine-to-Coarse Sketch Interpolation Network (FC-SIN). This approach incorporates multi-level guidance that formulates region-level correspondence, sketch-level correspondence and pixel-level dynamics. A multi-stream U-Transformer is then devised to characterize sketch inbewteening patterns using these multi-level guides through the integration of both self-attention and cross-attention mechanisms. Additionally, to facilitate future research on animation sketch inbetweening, we constructed a large-scale dataset - STD-12K, comprising 30 sketch animation series in diverse artistic styles. Comprehensive experiments on this dataset convincingly show that our proposed FC-SIN surpasses the state-of-the-art interpolation methods. Our code and dataset will be publicly available.	The hand-drawn 2D animation workflow typically involves three key stages: sketching keyframes, inbetweening keyframes to produce intermediate sketch frames (i.e., inbetweens), and colorization to produce the final, full-color animations. The meticulous creation of inbetweens is crucial for achieving a smooth animation with lifelike motion transitions, effectively conveying the intended story or message. For a feature-length animation created through this process, the sheer volume of required inbetweens can be staggeringThomaset al.(1995), making it a highly specialized and labor-intensive task and serving as a limiting factor in overall animation productivity. To streamline the process of 2D sketch animation production, various studies have focused on the automatic synthesis of inbetweening sketch frames, which take two consecutive sketch keyframes as input and produce interpolated intermediate sketch frames (i.e., inbetweens) as output. These methods can be categorised into stroke-based and image-based. The stroke-based methods often rely on a cumbersome pre-processing step for sketch vectorisationWhitedet al.(2010); Yang (2017); Yanget al.(2018), which require additional specialized software or techniques. Image-based methods treat sketch frames as bitmap images, applying conventional image or video interpolation algorithms. However, they commonly face two significant challenges: 1) the absence of texture and color details in sketch frames, hindering reliable image-based inbetweening correspondence, and 2) exaggerated changes due to substantial object movements between two consecutive sketch keyframesNaritaet al.(2019). As a result, when image-based methods, especially devised for videosZhouet al.(2023)and colour animationsSiyaoet al.(2021); Chen and Zwicker (2022), are applied to sketch interpolation, they invariably introduce various artifacts into the produced interpolated frames. These discrepancies can adversely affect the continuity and quality of the animation produced. As illustrated in the Figure1, LDFINaritaet al.(2019)proposed for sketch interpolation generates broken strokes due to the missing sketch keypoint correspondence, while EISAIChen and Zwicker (2022)proposed for interpolating color animation frames and DQBCZhouet al.(2023)for video interpolation introduce blurriness (ornaments) and artifacts (e.g., distortion in face regions) in their results. Therefore, in this study, we propose a novel deep learning method for sketch interpolation, the Fine-to-Coarse Sketch Interpolation Network (FC-SIN), to comprehend and model the intricate and sparse patterns found in animation sketches. Specifically, FC-SIN adopts a fine-to-coarse approach that integrates multi-level guidance through three distinct aspects: 1)pixel-level dynamicsat a fine level with a bi-directional optical flow estimation module, 2)sketch-level correspondencewith a sketch matching and tracking mechanism for obtaining sketch keypoint traces and 3)region-level correspondenceat a coarse level with a region matching and bi-directional optical flow aggregation module. Guided by these multi-level perspectives, a multi-stream U-Transformer architecture is further devised to produce the intermediate sketch frames. It consists of two attention-based building blocks:convolution and self-attention block(CSB) and theconvolution and cross-attention block(CCB) to leverage the diverse multi-level insights for producing precise inbetween sketch patterns. To facilitate the research on animation sketch interpolation, we constructed a large-scale sketch triplet dataset, namely STD-12K, from 30 sketch animation series (e.g., over 25 hours) with various artistic styles. Comprehensive experimental results demonstrate that the proposed method FC-SIN clearly outperforms the state-of-the-art frame interpolation methods for animation sketch interpolation. Overall, the key contributions of this study are as follows: A novel deep learning architecture, FC-SIN, for sketch interpolation by effectively formulating sparse sketch patterns with fine-to-coarse multi-level guidance. A novel self- and cross-attention based multi-stream U-Transformer formulated with the guidance from multi-level perspectives. A large-scale sketch triplet dataset with various artistic styles constructed for the research community.
2307.00453v1	Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters	Speech representations learned in a self-supervised fashion from massive unlabeled speech corpora have been adapted successfully toward several downstream tasks. However, such representations may be skewed toward canonical data characteristics of such corpora and perform poorly on atypical, non-native accented speaker populations. With the state-of-the-art HuBERT model as a baseline, we propose and investigate self-supervised adaptation of speech representations to such populations in a parameter-efficient way via training accent-specific residual adapters. We experiment with 4 accents and choose automatic speech recognition (ASR) as the downstream task of interest. We obtain strong word error rate reductions (WERR) over HuBERT-large for all 4 accents, with a mean WERR of 22.7% with accent-specific adapters and a mean WERR of 25.1% if the entire encoder is accent-adapted. While our experiments utilize HuBERT and ASR as the downstream task, our proposed approach is both model and task-agnostic.	Self-supervised learning has been a dominant paradigm in natural language processing (NLP)[1]and in recent years, it has also been adopted by the speech community to learn high-fidelity representations[2,3,4,5]that capture various non-lexical aspects of speech and audio such as lip-smacking, laughter, hesitation, etc. In this paradigm, the targets to learn are derived from the input signal itself, making the learned representations more powerful in principle compared to those learned using textual labels and annotations of any kind. These powerful base representations have been successfully adopted for several downstream tasks[6], some of which include: ASR, speaker identification and speech translation. Pre-training models with a very large number of parameters on proportionally large datasets has been a central theme in self-supervised learning. However, these datasets may understandably fall short in terms of sufficiently capturing non-canonical and diverse speech and audio characteristics such as rare non-native accents, stammering, etc. This leads to great disparity in downstream task performance across well-represented and underrepresented speaker populations. This data problem has also existed with supervised models for specific tasks such as ASR and in such scenarios, the typical path has been to patch task performance by collecting task-specific labeled datasets with non-canonical characteristics and fine-tuning for the task[7]. This unfortunately entangles speech and audio characteristics with the task itself, which can limit effective learning of such characteristics in task-specific representations as well as limiting their re-usability across tasks. In this paper, we consequently posit that continued self-supervised learning of speech and audio representations on task-agnostic unlabeled datasets is an effective strategy to adapt to non-canonical speech characteristics. The specific characteristic we choose to study is accents but the methodology holds for any characteristic. We propose learning different high-dimensional spaces for different accents via independently adding residual adapters for each target accent to the model and continuing pre-training on accent-specific datasets. Since residual adapters are parameter-wise much smaller than the base model, this serves as a parameter-efficient way for personalized adaptation without over-fitting and saves on storage costs for inference since only a single copy of the base model needs to be stored. We conduct our experiments with HuBERT-large[2]as the base model and ASR as the downstream task but posit that our proposed approach is both model and task agnostic. Our chosen base model is a state-of-the-art model with low word error rates on canonical datasets such as LibriSpeech. By design, we pick 4 non-native English accents where the HuBERT-large model has high word error rates (WER), in the range 24-50% and show strong results on all 4 accents with over 22% WERR over the baseline. Previous work has shown improvements in WER on such accents by supervised training using labeled datasets[7]. In contrast, we achieve our WER improvements by continuing to self-supervise models using unlabeled data alone. We show that the gains from adapting to an accent using a particular dataset translate to other evaluation sets with the same accent as well, indicating that the effectiveness of our approach is due to adaptation to the accents’ acoustic characteristics and not other confounding factors. Finally, we also explore the degree of parameter-efficiency possible when adapting to target accents, finding that we can achieve strong WERR over the baseline while updating only 16% of the total model parameters.
2305.18641v1	Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs	Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language(V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains.	"Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient’s question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V+L) tasks and many related benchmarks like Chart Question Answering (CQA)2022-chartqa;kafle2018dvqa;Methani_2020_WACVand Chart Summarization (CS)2022-chartare introduced. While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V+L research, applying these existing methods to handle chart-related tasks is hard. Recent research ChartQA2022-chartqaand Chart-to-Text2022-chartattempt to first convert chart images to their underlined tables and use the extracted tables to perform chart-related V+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS. Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1) Automatic table extraction is unreliable. Existing methodsluo2021chartocr;parsinglineare often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2) In most cases, the whole table is optional for resolving the chart-related V+L task. As illustrated in Fig1, to answer the question""What is the value of India Bar"", the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V+L model with the capability to dynamically interpret the table value from the chart information. Therefore, in this paper, we proposeChartT5, an OCR-based image-to-text generation model pre-trained on a self-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model’s capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value. We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart-to-Text. In ChartQA, ChartT5 outperforms all the non-pretraining methods that use extracted tables by at least 8\%performance gains. ChartT5 also beats the pre-training table-based methods, which demonstrates the effectiveness of the proposed pre-training strategies. On Chart-to-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metricsbarzilay-2005-collectivewhich values the model’s capability to extract the critical information from the chart. In summary, our contributions are summarized below: We propose chart-to-table pre-training for V+L model to learn the capability of interpreting table data from the chart. We demonstrate that the pre-trained model consistently outperforms table-based methods on two chart understanding tasks. We conduct comprehensive ablation studies to validate the effectiveness of chart-to-table pre-training and the proposed pre-training objectives."
2305.14223v1	Co-Learning Empirical Games and World Models	Game-based decision-making involves reasoning over both world dynamics and strategic interactions among the agents. Typically, empirical models capturing these respective aspects are learned and used separately. We investigate the potential gain from co-learning these elements: a world model for dynamics and an empirical game for strategic interactions. Empirical games drive world models toward a broader consideration of possible game dynamics induced by a diversity of strategy profiles. Conversely, world models guide empirical games to efficiently discover new strategies through planning. We demonstrate these benefits first independently, then in combination as realized by a new algorithm, Dyna-PSRO, that co-learns an empirical game and a world model. When compared to PSRO -- a baseline empirical-game building algorithm, Dyna-PSRO is found to compute lower regret solutions on partially observable general-sum games. In our experiments, Dyna-PSRO also requires substantially fewer experiences than PSRO, a key algorithmic advantage for settings where collecting player-game interaction data is a cost-limiting factor.	Even seemingly simple games can actually embody a level of complexity rendering them intractable to direct reasoning. This complexity stems from the interplay of two sources: dynamics of the game environment, and strategic interactions among the game’s players. As an alternative to direct reasoning, models have been developed to facilitate reasoning over these distinct aspects of the game.Empirical gamescapture strategic interactions in the form of payoff estimates for joint policiesWellman (2006).World modelsrepresent a game’s transition dynamics and reward signal directlySutton and Barto (2018); Ha and Schmidhuber (2018b). Whereas each of these forms of model have been found useful for game reasoning, typical use in prior work has focused on one or the other, learned and employed in isolation from its natural counterpart. Co-learning both models presents an opportunity to leverage their complementary strengths as a means to improve each other. World models predict successor states and rewards given a game’s current state and action(s). However, their performance depends on coverage of their training data, which is limited by the range of strategies considered during learning. Empirical games can inform training of world models by suggesting a diverse set of salient strategies, based on game-theoretic reasoningWellman (2006). These strategies can expose the world model to a broader range of relevant dynamics. Moreover, as empirical games are estimated through simulation of strategy profiles, this same simulation data can be reused as training data for the world model. Strategic diversity through empirical games, however, comes at a cost. In the popular framework of Policy-Space Response Oracles (PSRO)Lanctotet al.(2017), empirical normal-form game models are built iteratively, at each step expanding a restricted strategy set by computing best-response policies to the current game’s solution. As computing an exact best-response is generally intractable, PSRO uses Deep Reinforcement Learning (DRL) to compute approximate response policies. However, each application of DRL can be considerably resource-intensive, necessitating the generation of a vast amount of gameplays for learning. Whether gameplays, or experiences, are generated via simulationObando-Ceron and Castro (2021)or from real-world interactionsHester and Stone (2012), their collection poses a major limiting factor in DRL and by extension PSRO. World models present one avenue to reduce this cost by transferring previously learned game dynamics across response computations. We investigate the mutual benefits of co-learning a world model and an empirical game by first verifying the potential contributions of each component independently. We then show how to realize the combined effects in a new algorithm,Dyna-PSRO, that co-learns a world model and an empirical game (illustrated in Figure1). Dyna-PSRO extends PSRO to learn a world model concurrently with empirical game expansion, and applies this world model to reduce the computational cost of computing new policies. This is implemented by a Dyna-based reinforcement learnerSutton (1990,1991)that integrates planning, acting, and learning in parallel. Dyna-PSRO is evaluated against PSRO on a collection of partially observable general-sum games. In our experiments, Dyna-PSRO found lower-regret solutions while requiring substantially fewer cumulative experiences. The main points of novelty of this paper are as follows: (1) empirically demonstrate that world models benefit from the strategic diversity induced by an empirical game; (2) empirically demonstrate that a world model can be effectively transferred and used in planning with new other-players. The major contribution of this work is a new algorithm, Dyna-PSRO, that co-learns an empirical game and world model finding a stronger solution at less cost than the baseline, PSRO.
2305.10326v1	Cross-domain Iterative Network for Simultaneous Denoising, Limited-angle Reconstruction, and Attenuation Correction of Low-dose Cardiac SPECT	Single-Photon Emission Computed Tomography (SPECT) is widely applied for the diagnosis of ischemic heart diseases. Low-dose (LD) SPECT aims to minimize radiation exposure but leads to increased image noise. Limited-angle (LA) SPECT enables faster scanning and reduced hardware costs but results in lower reconstruction accuracy. Additionally, computed tomography (CT)-derived attenuation maps ($\mu$-maps) are commonly used for SPECT attenuation correction (AC), but it will cause extra radiation exposure and SPECT-CT misalignments. In addition, the majority of SPECT scanners in the market are not hybrid SPECT/CT scanners. Although various deep learning methods have been introduced to separately address these limitations, the solution for simultaneously addressing these challenges still remains highly under-explored and challenging. To this end, we propose a Cross-domain Iterative Network (CDI-Net) for simultaneous denoising, LA reconstruction, and CT-free AC in cardiac SPECT. In CDI-Net, paired projection- and image-domain networks are end-to-end connected to fuse the emission and anatomical information across domains and iterations. Adaptive Weight Recalibrators (AWR) adjust the multi-channel input features to enhance prediction accuracy. Our experiments using clinical data showed that CDI-Net produced more accurate $\mu$-maps, projections, and reconstructions compared to existing approaches that addressed each task separately. Ablation studies demonstrated the significance of cross-domain and cross-iteration connections, as well as AWR, in improving the reconstruction performance.	Cardiac Single-Photon Emission Computed Tomography (SPECT) is the most widely performed non-invasive exam for clinical diagnosis of ischemic heart diseases[7,10]. Reducing the tracer dose can lower patient radiation exposure, but it will result in increased image noise[22,8]. Acquiring projections in fewer angles using fewer detectors allows for faster scanning and lower hardware costs, but it also leads to decreased reconstruction accuracy[1,13]. Additionally, in clinical practice, computed tomography (CT)-derived attenuation maps (\mu-maps) are commonly used for SPECT attenuation correction (AC)[3,11]. However, most SPECT scanners are stand-alone without the assistance of CT[16]. The CT scan also causes additional radiation exposure and SPECT-CT misalignments[18,11]. Deep learning-based methods have been extensively explored to address the aforementioned limitations individually. To reduce image noise in low-dose (LD) SPECT, convolutional neural networks (CNNs) were employed to process the LD projection, producing the full-dose (FD) projection for SPECT reconstruction[21,1]. Similarly, to perform limited-angle (LA) reconstruction, the LA projection was input to CNNs to predict the full-angle (FA) projection[2,23,20]. In addition, a dual-domain approach, known as Dual-domain Sinogram Synthesis (DuDoSS), utilized the image-domain output as a prior estimation for the projection domain to predict the FA projection[5]. For the CT-free AC, CNNs were used to generate pseudo attenuation maps (\mu-maps) from SPECT emission images[19,6]. Although various methods have been developed to address these limitations individually, it is of great interest to address all these limitations simultaneously to enable CT-free, low-dose, low-cost, and accelerated SPECT, which could potentially lead to better performance on those separated but correlated tasks. Thus, we propose a Cross-Domain Iterative Network (CDI-Net) for simultaneous denoising, LA reconstruction, and CT-free AC in cardiac SPECT. In CDI-Net, projection and image-domain networks are end-to-end connected to fuse the predicted emission and anatomical features across domains and iterations. Adaptive Weight Recalibrators (AWR) calibrate the fused features to improve the prediction accuracy. We tested CDI-Net using clinical data and compared it to existing methods. Ablation studies were conducted to verify the impact of cross-domain, cross-iteration fusions and AWR on enhancing network performance.
2305.18728v2	Plug-in Performative Optimization	When predictions are performative, the choice of which predictor to deploy influences the distribution of future observations. The overarching goal in learning under performativity is to find a predictor that has low \emph{performative risk}, that is, good performance on its induced distribution. One family of solutions for optimizing the performative risk, including bandits and other derivative-free methods, is agnostic to any structure in the performative feedback, leading to exceedingly slow convergence rates. A complementary family of solutions makes use of explicit \emph{models} for the feedback, such as best-response models in strategic classification, enabling significantly faster rates. However, these rates critically rely on the feedback model being well-specified. In this work we initiate a study of the use of possibly \emph{misspecified} models in performative prediction. We study a general protocol for making use of models, called \emph{plug-in performative optimization}, and prove bounds on its excess risk. We show that plug-in performative optimization can be far more efficient than model-agnostic strategies, as long as the misspecification is not too extreme. Altogether, our results support the hypothesis that models--even if misspecified--can indeed help with learning in performative settings.	Predictions have the power to influence the patterns they aim to predict. For example, stock price predictions inform trading decisions and hence prices; traffic predictions influence routing decisions and thus traffic outcomes; recommendations shape users’ consumption and thus preferences. This pervasive phenomenon has been formalized in a technical framework calledperformative predictionPerdomoet al.(2020). A central feature that distinguishes the framework from traditional supervised learning is the concept of adistribution map\mathcal{D}(\cdot). This object, aimed to capture the feedback from predictions to future observations, is a mapping from predictorsf_{\theta}to their induced data distributions\mathcal{D}(\theta). The overarching goal in performative prediction is thus to deploy a predictorf_{\theta}that will have good performance after its deployment, that is, on its induced data distribution\mathcal{D}(\theta). Formally, the goal is to choose predictor parameters\theta\in\Theta\subseteq\mathbb{R}^{d_{\theta}}so as to minimize theperformative risk: where\ell(z;\theta)measures the loss incurred by predicting on instancezwith model\theta. Typically,zis a feature–outcome pair(x,y). We refer to\theta_{\mathrm{PO}}=\operatorname*{arg\,min}_{\theta\in\Theta}\mathrm{PR}(\theta)as theperformative optimum. The main challenge in optimizing the performative risk lies in the fact that the map\mathcal{D}(\cdot)is not known. We only observe samples from\mathcal{D}(\theta)for models\thetathat have been deployed; we do not observe any feedback for other models, of which there are typically infinitely many. A key discriminating factor between existing solutions for optimizing under performativity is how they cope with this uncertainty. One group of methods accounts for the feedback without assuming a problem-specific structure for it. This group includes bandit strategiesKleinberget al.(2008); Jagadeesanet al.(2022)and derivative-free optimizationFlaxmanet al.(2004); Milleret al.(2021). These methods converge to optima at typically slow—without convexity, even exponentially slow—convergence rates. Moreover, their rates rely on unverifiable regularity conditions that are out of the learner’s control, such as convexity of the performative riskMilleret al.(2021); Izzoet al.(2021); Donget al.(2018)or bounded performative effectsJagadeesanet al.(2022). A complementary group of methods—an important starting point for this work—takes feedback into account by positing explicitmodelsfor it. Such models include best-response models for strategic classificationHardtet al.(2016); Jagadeesanet al.(2021); Levanon and Rosenfeld (2021); Ghalmeet al.(2021), rational-agent models in economicsSpence (1978); Wooldridge (2003), and parametric distribution shiftsIzzoet al.(2021); Milleret al.(2021); Izzoet al.(2022), among others. To argue that the methods building on such models find optimal solutions, existing analyses assume that the model iswell-specified. However, models of social behavior are widely acknowledged to be simplistic representations of real-world dynamics. Yet, despite the unavoidable misspecification of models, they are ubiquitous in practice. Though their simplicity leads to misspecification, it also allows for efficient, interpretable, and practical solutions. Motivated by this observation, in this work we ask:can models for performative feedback be useful, even if misspecified? We initiate a study of the benefits of modeling feedback in performative prediction. We show that models—even if misspecified—can indeed help with learning under performativity. We begin by defining a general protocol for performative optimization with feedback models, which we callplug-in performative optimization. The protocol consists of three steps. First, the learner deploys models\theta_{i}\sim\tilde{\mathcal{D}}and collects dataz_{i}\sim\mathcal{D}(\theta_{i}),i\in[n]. Here,\tilde{\mathcal{D}}is an exploration distribution of the learner’s choosing (for example, it can be uniform on\Thetawhen\Thetais bounded). The second step is to use the observations\{(\theta_{i},z_{i})\}_{i=1}^{n}to fit an estimate of the distribution map. The map is chosen from a parametric family of possible maps\bm{\mathcal{D}}_{\mathcal{B}}=\{\mathcal{D}_{\beta}\}_{\beta\in\mathcal{B}}, obtained through modeling. The estimation of the map thus reduces to computing an estimate\hat{\beta}. For example, in strategic classification,\betacould be a parameter quantifying the strategic agents’ tradeoff between utility and cost. Finally, the third step is to compute theplug-in performative optimum: We prove a general excess-risk bound on\mathrm{PR}(\hat{\theta}_{\mathrm{PO}})-\mathrm{PR}(\theta_{\mathrm{PO}}), showing that the error decomposes into two terms. The first is amisspecification error term,MisspecErr, which captures the gap between the true performative risk and the plug-in performative risk\mathrm{PR}^{\hat{\beta}}(\theta)in the large-sample regime. This term is irreducible and does not vanish as the sample sizengrows. The second is astatistical error termthat captures the imperfection in fitting\hat{\beta}due to finite samples. For a broad class of problems, our main result can be summarized as follows. The excess risk of the plug-in performative optimum is bounded by: for some universal constantc>0. Therefore, although the misspecification error is irreducible, the statistical error vanishes at afast rate. In contrast, model-agnostic strategies such as bandit algorithmsKleinberget al.(2008); Jagadeesanet al.(2022)do not suffer from misspecification but have an exceedingly slow, often exponentially slow, statistical rate. For example, the bandit algorithm of Jagadeesan et al.Jagadeesanet al.(2022)has an excess risk of\tilde{O}\left(n^{-\frac{1}{d_{\theta}+1}}\right). This is why feedback models are useful—for a finiten, their excess risk can be far smaller than the risk of a model-agnostic strategy due to the rapidly vanishing statistical rate. The statistical rate is fast because it only depends on the parametric estimation rate of\hat{\beta}; it does not depend on the complexity of\mathrm{PR}. One important special case of performative prediction isstrategic classification. We apply our general theory to common best-response models in strategic classification. We also conduct numerical evaluations that empirically confirm our theoretical findings. Overall our results support the use of models in optimization under performative feedback. We give an overview of existing threads most closely related to our work. We build on the growing body of work studying performative predictionPerdomoet al.(2020). Existing work studies different variants of retrainingPerdomoet al.(2020); Mendler-Dünneret al.(2020); Drusvyatskiy and Xiao (2022), which converge to so-called performatively stable solutions, as well as methods for finding performative optimaMilleret al.(2021); Izzoet al.(2021); Jagadeesanet al.(2022). The methods in the latter category are largely model-agnostic and as such converge at slow rates. Exceptions include the study of parametric distribution shiftsIzzoet al.(2021,2022)and location familiesMilleret al.(2021); Jagadeesanet al.(2022), but those analyses crucially rely on the model being well-specified. We are primarily interested in misspecified settings. Other work in performative prediction includes the study of time-varying distribution shiftsBrownet al.(2022); Izzoet al.(2022); Li and Wai (2022); Rayet al.(2022), multi-agent settingsDeanet al.(2022); Qianget al.(2022); Naranget al.(2022); Piliouras and Yu (2022), and causality and robustnessMaheshwariet al.(2022); Mendler-Dünneret al.(2022); Kim and Perdomo (2022), among others; it would be valuable to extend our theory on the use of models to those settings. Strategic classificationHardtet al.(2016); Donget al.(2018); Levanon and Rosenfeld (2021); Zrnicet al.(2021), as well as other problems studying strategic agent behavior, frequently use models of agent behavior in order to compute Stackelberg equilibria, which are direct analogues of performative optima. However, convergence to Stackelberg equilibria assumes correctness of the models, a challenge we circumvent in this work. We use strategic classification as a primary domain of application of our general theory. Our work is partially inspired by works in statistics studying the benefits and impact of modeling, including under misspecificationWhite (1980,1982); Bujaet al.(2019a,b). At a technical level, our results are related to M-estimationVan der Vaart (2000); Geer (2000); Mouet al.(2019), as well as semi-parametric statisticsTsiatis (2007); Kennedy (2022), where the goal is to find models that lead to minimal estimation error. Plug-in performative optimization serves as an alternative to black-box baselines for zeroth-order optimization, which have previously been studied in the performative prediction literature. These include bandit algorithmsKleinberget al.(2008); Jagadeesanet al.(2022)and zeroth-order convex optimization algorithmsFlaxmanet al.(2004); Milleret al.(2021). As mentioned earlier, we show that the use of models can give far smaller excess risk, given the fast convergence rates of parametric learning problems.
2304.05104v1	Approaching Test Time Augmentation in the Context of Uncertainty Calibration for Deep Neural Networks	With the rise of Deep Neural Networks, machine learning systems are nowadays ubiquitous in a number of real-world applications, which bears the need for highly reliable models. This requires a thorough look not only at the accuracy of such systems, but also to their predictive uncertainty. Hence, we propose a novel technique (with two different variations, named M-ATTA and V-ATTA) based on test time augmentation, to improve the uncertainty calibration of deep models for image classification. Unlike other test time augmentation approaches, M/V-ATTA improves uncertainty calibration without affecting the model's accuracy, by leveraging an adaptive weighting system. We evaluate the performance of the technique with respect to different metrics of uncertainty calibration. Empirical results, obtained on CIFAR-10, CIFAR-100, as well as on the benchmark Aerial Image Dataset, indicate that the proposed approach outperforms state-of-the-art calibration techniques, while maintaining the baseline classification performance. Code for M/V-ATTA available at: https://github.com/pedrormconde/MV-ATTA.	Deep Neural Networks (DNNs) changed the paradigm with regards to the applicability of machine learning (ML) systems to real-world scenarios. Consequently, deep learning (DL) models are now present in critical application domains (e.g., autonomous driving, medicine, remote sensing, robotics), where bad decision-making can bear potentially drastic consequences. This requires that DNNs are not only highly accurate, but also highlyreliable- decision-makers should be able to “trust” the predictions of these models. This lead us to the problem ofuncertainty calibration(also referred as confidence calibration or simplycalibration): it is required that the confidence output generated by the DL model - that translates as the confidence the model has in the prediction that is making - realistically represents the true likelihood of correctness. For the sake of intuition, a calibrated model would for example, in the long run, correctly classify70\%of those predictions that have a confidence value of0.7associated. This accurate quantification of predictive uncertainty results in reliable confidence values associated with each prediction, and therefore, in a more reliable model. As such, it is important to understand how well calibrated are modern DNNs. Further details, including the formalization of the uncertainty calibration problem, will be described in Section3.Although increasingly accurate, modern DL architectures have been found to be tendentiouslyuncalibrated[7,20]. Furthermore, “modern neural networks exhibit a strange phenomenon: probabilistic error and miscalibration worsen even as classification error is reduced”[7]. For this reason, the goal of this work is to improve the uncertainty calibration of DNNs in the task of image classification, by proposing a novel accuracy-consistent weighted test time augmentation method.Test time augmentation is a general methodology that leverages data augmentation techniques to create multiple samples from the original input at inference (i.e., at test time). Therefore, test time augmentation methods can be applied to pre-trained models, since, in this case, the augmentation process is not applied during the training phase. The technique introduced in this work combines the use of test time augmentation with a custom weighting system, guaranteeing that the accuracy of the original DL model is not corrupted, while still being optimized to improve uncertainty calibration. This builds, partially, on the work done in[3], proposing both a reformulated version -V-ATTA(Vector Adaptive Test Time Augmentation) - of the preliminary method presented in[3]and also a generalized version -M-ATTA(Matrix Adaptive Test Time Augmentation) - with a broader and extended empirical evaluation.M/V-ATTAis evaluated on the benchmark CIFAR-10/CIFAR-100[11]datasets, as well as on a benchmark satellite imagery dataset, the Aerial Image Dataset (AID)[28]. The results are compared with state-of-the-artpost-hoccalibration methods, with respect to the Brier score[2]and theExpected Calibration Error(ECE), for bothcommonandstronguncertainty calibration (see Section3for further details).Contribution: We propose a novel calibration technique - with two different variations (M/V-ATTA) - based on test time augmentation, that guarantees consistency in the accuracy of deep models in which is applied, while being shown to improve uncertainty calibration-related evaluation metrics, outperforming state-of-the-artpost-hoccalibration methods in most cases. To the best of our knowledge,M/V-ATTAis the first method based on test time augmentation that has been proposed to improve the uncertainty calibration of deep models (besides its predecessor in[3]). Furthermore, the method presented here can be used with pre-trained DNNs, which is advantageous in terms of applicability.
2311.14332v1	GATGPT: A Pre-trained Large Language Model with Graph Attention Network for Spatiotemporal Imputation	The analysis of spatiotemporal data is increasingly utilized across diverse domains, including transportation, healthcare, and meteorology. In real-world settings, such data often contain missing elements due to issues like sensor malfunctions and data transmission errors. The objective of spatiotemporal imputation is to estimate these missing values by understanding the inherent spatial and temporal relationships in the observed multivariate time series. Traditionally, spatiotemporal imputation has relied on specific, intricate architectures designed for this purpose, which suffer from limited applicability and high computational complexity. In contrast, our approach integrates pre-trained large language models (LLMs) into spatiotemporal imputation, introducing a groundbreaking framework, GATGPT. This framework merges a graph attention mechanism with LLMs. We maintain most of the LLM parameters unchanged to leverage existing knowledge for learning temporal patterns, while fine-tuning the upper layers tailored to various applications. The graph attention component enhances the LLM's ability to understand spatial relationships. Through tests on three distinct real-world datasets, our innovative approach demonstrates comparable results to established deep learning benchmarks.	The presence of multivariate time series data is extensively documented across a variety of sectors including economics, transportation, healthcare, and meteorology, as evidenced in several studies[4,18,32,26]. A range of statistical and machine learning techniques have been shown to perform effectively on complete datasets in several time series tasks, including forecasting[14], classification[16], and anomaly detection[6]. However, it is often observed that multivariate time series data collected from real-world scenarios are prone to missing values due to various factors, such as sensor malfunctions and data transmission errors. These missing values can considerably affect the quality of the data, subsequently impacting the effectiveness of the aforementioned methods in their respective tasks. Extensive research efforts have been dedicated to addressing the challenges in spatiotemporal imputation. A typical approach involves the development of a distinct framework for initially estimating missing values, followed by the application of the completed dataset in another sophisticated framework for subsequent operations like forecasting, classification, and anomaly detection. To fill in missing values, various statistical and machine learning techniques are applied. Popularly employed methods include autoregressive moving average (ARMA)[1], expectation-maximization algorithm (EM)[23], and k-nearest neighbors (kNN)[20,15]. These models often depend on rigorous presumptions such as temporal uniformity and similarity between series, which may not be suitable for the complexity of real-world multivariate time series data, potentially leading to suboptimal performance. Deep learning techniques have gained significant traction among researchers in the realm of spatiotemporal imputation. The utility of Recurrent Neural Networks (RNN) was initially recognized in this context[7,9]. Subsequently, Generative Adversarial Networks (GAN) demonstrated impressive results, thanks to their robust generative properties[19]. More recently, advancements have been made by integrating RNN-based methods with Graph Neural Networks (GNN) to enhance the capability of extracting spatial dependencies within these frameworks. Nevertheless, the current focus in this field is predominantly on developing intricate and specialized frameworks aimed at improving performance on specific public industrial datasets, often overlooking the aspects of generalizability and adaptability for varied datasets and downstream applications. Additionally, the rise of pre-trained models in Natural Language Processing (NLP) has been noteworthy, with their applications extending into diverse areas such as Computer Vision (CV)[5], Multi-modality[28], and Recommendation Systems[33]. These Large Language Models (LLMs) have demonstrated remarkable success due to their advanced representation learning abilities. Yet, there is a scarcity of research exploring using pre-trained LLMs in the domain of time series data analysis. Our research is focused on exploring the capacity of LLMs to act as potent representation learners, particularly in capturing temporal dependencies. Moreover, we recognize LLMs’ exceptional few-shot learning capabilities, which positions them as highly suitable for time series contexts, where gathering extensive training data is often a formidable challenge. In this paper, we aim to introduce the pre-trained Large Language Models (LLMs) to solve the spatiotemporal imputation problem. The main contributions of this paper are summarized as follows: To the best of our knowledge, we are the first to introduce pre-trained LLMs to the spatiotemporal imputation. The integration of these models, with their pre-existing knowledge and architectural design, is instrumental in deciphering the intricacies of spatiotemporal dependencies. We have incorporated a graph attention module specifically designed to discern spatial dependencies. This module is devised to augment the LLMs in comprehending and assimilating the intrinsic characteristics of spatiotemporal data. Through our experimentation with various real-world datasets, we have demonstrated the effectiveness of our proposed framework. The results highlight its proficiency in spatiotemporal imputation, standing in comparison with multiple established baselines. The rest of this paper is organized as follows. Section2reviews the related work on spatiotemporal imputation. Section3presents the details of our proposed GATGPT framework, including the graph attention module and the pre-trained LLM block. Section4demonstrates our evaluation of the proposed framework performance on real-world datasets. Finally, Section5draws our conclusions.
2401.05153v2	CrossDiff: Exploring Self-Supervised Representation of Pansharpening via Cross-Predictive Diffusion Model	Fusion of a panchromatic (PAN) image and corresponding multispectral (MS) image is also known as pansharpening, which aims to combine abundant spatial details of PAN and spectral information of MS. Due to the absence of high-resolution MS images, available deep-learning-based methods usually follow the paradigm of training at reduced resolution and testing at both reduced and full resolution. When taking original MS and PAN images as inputs, they always obtain sub-optimal results due to the scale variation. In this paper, we propose to explore the self-supervised representation of pansharpening by designing a cross-predictive diffusion model, named CrossDiff. It has two-stage training. In the first stage, we introduce a cross-predictive pretext task to pre-train the UNet structure based on conditional DDPM, while in the second stage, the encoders of the UNets are frozen to directly extract spatial and spectral features from PAN and MS, and only the fusion head is trained to adapt for pansharpening task. Extensive experiments show the effectiveness and superiority of the proposed model compared with state-of-the-art supervised and unsupervised methods. Besides, the cross-sensor experiments also verify the generalization ability of proposed self-supervised representation learners for other satellite's datasets. We will release our code for reproducibility.	Pansharpening refers to fusing a low-spatial-resolution multispectral (LRMS) image with a single-band high-spatial-resolution panchromatic (PAN) image to obtain a high spatial and spectral resolution MS (HRMS) image. Owing to the technological and physical limitations of imaging devices, remote sensing images obtained by satellite sensors always have a trade-off between spatial and spectral resolution[1]. Therefore, most of available satellites carry two types of sensors,i.e. , an MS sensor and a PAN sensor, to acquire LRMS and PAN images simultaneously. Researchers can then combine them through pansharpening technology. Benefiting from the availability of large-scale remote sensing imagery, numerous deep-learning (DL)-based methods achieve great success in pansharpening. According to the base models they used, these methods can be roughly divided into convolutional neural networks (CNNs) based[2,3,4], generative adversarial networks (GANs) based[5,6,7], and Transformer based[8,9,10]. CNNs-based methods mainly extract spatial and spectral features layer-by-layer. They take pansharpening as a regression task, and are usually supervised by a reconstruction loss. GANs-based models utilize generators to fuse MS and PAN images and the discriminators to adversarially train the model for the generation of high-fidelity fused products. To model the long-range dependency of images, researchers introduce the Transformer architecture to pansharpening[8]. These models can learn either the self-similarity in a single image[9]or the interactive information between two kinds of modality images[10]. Most of available DL-based pansharpening methods are within the supervised-learning framework. However, there are no high-resolution MS images that can be taken as groundtruths. Therefore, researchers train the models at reduced resolution, where the training samples are prepared according to Wald’s protocol[11],i.e. , downsample PAN and MS to take them as inputs, and the original MS images are treated as references, which are actually pseudo-groundtruths[12]. Nevertheless, due to the scale variation[13,7], models trained at reduced resolution are unsuitable for pansharpening at the original images, resulting in inferior performance at full resolution[6]. Recently, researchers devote to developing unsupervised pansharpening methods. Most of them[14,15]concentrate only on the design of unsupervised loss function, however their effectiveness also depends on the features extracted[16]. We argue that the scale dependency issue can be alleviated from the perspective of not only unsupervised loss function, but also a good representation learner. Recent works utilize generative models to act as representation learners[16,17,18]. Among them, Denoising Diffusion Probabilistic Model (DDPM)[19]outperforms other alternative generative models and provides more semantically-valuable pixel-wise representations[20], which highlights the potential of using the state-of-the-art DDPM as strong unsupervised representation learners. Based on this insight, we build model upon DDPM to explore its representation ability to pansharpening. Furthermore, available DL-based methods have limited generalization ability, where the model trained on a specific dataset always performs unsatisfactorily on datasets collected from other satellite sensors. This further arises a question: how can we develop a unified representation learner based on DDPM to make it accountable for the extraction of general spatial and spectral features, regardless of the distinctiveness of sensors’ attributes. For pansharpening task, PAN and MS images have distinctive spatial and spectral characteristics respectively, which naturally constitutes a self-supervised pretext task that one can predict high-resolution PAN image from low-resolution MS image, and inversely predict spectral-appealing MS image from spectral-scarce PAN image. In this paper, we embed pansharpening into diffusion model to construct a new self-supervised paradigm, dubbed CrossDiff. As Figure1shows, the model consists of two stage training. In the first stage, the self-supervised pre-training is conducted based on DDPM to obtain a noise predictor with UNet structure[21]. Afterwards, the frozen encoders act as spatial-spectral feature representation learners to fuse MS and PAN images with a tunable fusion head. By introducing a cross-predictive diffusion process, CrossDiff outperforms state-of-the-art unsupervised pansharpening methods and has strong generalization ability. The main contributions are summarized as follows: We design a new two-stage pansharpening paradigm to explore the potential of DDPM to the self-supervised spatial and spectral features extraction. A cross-predictive diffusion process is introduced to pre-train the spatial and spectral representation learners. Following the process of DDPM, the effective training objectives encourage the pretext task to explicitly learn spatial and spectral diffusion latent. By freezing the pre-trained models and only tuning the fusion head, our proposed CrossDiff performs well at both full and reduced resolution, and has strong cross-sensor generalization ability.
2305.03045v2	OctFormer: Octree-based Transformers for 3D Point Clouds	We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200k. Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D segmentation and detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are available at https://wang-ps.github.io/octformer.	3D point cloud understanding is a fundamental task in computer graphics and vision and has a broad range of applications, including robotics, autonomous driving, and augmented reality. A variety of deep learning methods have been proposed for it, such as voxel-based CNNs(Wu et al.,2015; Wang et al.,2017; Graham et al.,2018), view-based CNNs(Su et al.,2015), and point-based networks(Qi et al.,2016,2017b; Li et al.,2018), and remarkable progress has been made. Recently, point cloud transformers have emerged(Guo et al.,2021; Zhao et al.,2021; Misra et al.,2021)as an effective alternative with the potential for cross-multimodality training and general intelligent models(Radford et al.,2021; Ramesh et al.,2022). However, the efficiency of point cloud transformers is still much worse than their CNN counterparts(Wang et al.,2017; Choy et al.,2019; Nekrasov et al.,2021), especially on scene-scale datasets like ScanNet(Dai et al.,2017), and the performance of point cloud transformers is also just comparable. Since it has been proven that transformers are at least as expressive as CNNs(Cordonnier et al.,2020), one of the key challenges of applying transformers to point clouds is to overcome the huge computational complexity of transformers, which is quadratic with the number of elements involved. Several methods(Guo et al.,2021; Yu et al.,2022; Pang et al.,2022)directly apply transformers to all points globally, thus limiting their applicability to large-scale point clouds. Following the progress in scaling up vision transformers(Liu et al.,2021a; Dong et al.,2022), one effective strategy is to constrain point cloud transformers within non-overlapping local windows(Lai et al.,2022; Sun et al.,2022; Fan et al.,2022; Mao et al.,2021). However, unlike images, the number of points across different local windows varies significantly due to the sparsity of point clouds. To deal with this issue, sophisticated implementations like region batching(Fan et al.,2022; Sun et al.,2022)or customized GPU kernels(Lai et al.,2022)have to be adopted, which severely impedes massive parallelism on GPUs. Another strategy to speed up point cloud transformers is to apply transformers in downsampled feature maps(Park et al.,2022; Cheng et al.,2021b), which also weakens the network capability and incurs a decrease in performance. In this paper, we present a general and scalable octree-based transformer, abbreviated as OctFormer, for learning on 3D point clouds. The key building block of OctFormer is a novel octree attention mechanism for point clouds. To retain linear complexity, we divide each point cloud into small groups when applying attentions. Our key observation is that attentions are insensitive to the actual shape of underlying local windows. Instead of using cubic windows as in previous works, which incur variant point numbers in each window, we divide point clouds into groups with irregular windows while keeping the point number in each window the same. Consequently, we can easily implement our attention using standard operators provided by deep learning frameworks like PyTorch(Paszke et al.,2019). To generate the required window partition, our second observation is that after constructing an octree with the parallel algorithm in(Zhou et al.,2011), the octree nodes are sorted in z-order by shuffled keys(Wilhelms and Van Gelder,1992), which ensures that spatially-close octree nodes are contiguously stored in memory. We store features in tensors according to the order of octree nodes. After padding a few zeros to make the spatial numbers of tensors divisible by the specified point number in each window, we can efficiently generate the window partition by simply reshaping the tensors at almost zero cost. An example is shown in Figure1-(b), where the point number in each window is the same. To further increase the receptive fields of OctFormer, we introduce a dilated octree attention with dilated partitions along the spatial dimension of tensors, which can also be efficiently implemented with tensor reshaping and transposing. Our OctFormer challenges conventional wisdom in designing point cloud transformers from two aspects. First, instead of using fixed-sized local windows, we fix the point number in each window when doing point cloud partition, enabling simple implementation and easy parallelization; second, instead of regarding point clouds as unordered and unstructured point sets, we actually sort the quantized points with shuffled keys by building octrees, resulting in a convenient window partition. Our octree attention completely eliminates the expensive neighborhood searching used in previous designs(Wu et al.,2022; Lai et al.,2022), bypasses the sparsity of point clouds, and can be reduced to a standard multi-head self-attention(Vaswani et al.,2017)on small groups of equal size. Consequently, our octree attention can be implemented in10 lines of codewith open-sourced libraries freely available on the web. Onesingletransformer block on top of octree attention runs at least17 times fasterthan previous state-of-the-art point transformer blocks(Lai et al.,2022; Wu et al.,2022)when the number of elements involved is200k. We also introduce feature hierarchies following the multiscale structure of octrees, endowing OctFormer with the capability as a general backbone for 3D segmentation and detection. We verify the effectiveness of OctFormer on a series of 3D benchmarks. Specifically, our OctFormer achieves the best performance on the validation set of ScanNet segmentation(Dai et al.,2017), SUN RGB-D detection(Song et al.,2015), and ScanNet200 segmentation(Rozenberszki et al.,2022), surpassing all previous state-of-the-art sparse-voxel-based CNNs(Choy et al.,2019; Wang et al.,2017; Graham et al.,2018)and point cloud transformers(Lai et al.,2022; Wu et al.,2022)by a large margin. Notably, on ScanNet200 segmentation, which contains 200 semantic categories (ten times more than ScanNet), the mIoU of our OctFormer is higher than MinkowskiNet(Choy et al.,2019)by7.3and even higher than the recently-proposed LGround(Rozenberszki et al.,2022)by5.4, which pretrains a sparse-voxel-based CNN with a powerful CLIP model(Radford et al.,2021). In summary, our main contributions are as follows: [leftmargin=10pt,itemsep=2pt] We propose a novel octree attention and its dilated variant, which are easy to implement and significantly more efficient than previous point cloud attentions; We propose OctFormer, which can serve as a general backbone for 3D point cloud segmentation, detection, and classification; OctFormer achieves state-of-the-art performances on a series of 3D segmentation and detection benchmarks, and the computational efficiency of OctFormer is much higher than previous point cloud transformers and even surpasses highly optimized sparse-voxel-based CNNs.
2301.11673v4	Bayesian Self-Supervised Contrastive Learning	Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss.	Learning good representations without supervision has been a long-standing problem in machine learning(Aroraet al.,2019; Heet al.,2020; Chuet al.,2023). Many state-of-the-art models utilize self-supervised learning (SSL) techniques(Chenet al.,2020b; Grillet al.,2020; Liuet al.,2021; Tonget al.,2023)to design pretext tasks for (pre)training the models. For instance, generative methods train models to reconstruct input data(Kingma and Dhariwal,2018; Mikolovet al.,2013), while contrastive methods train models to encode differential features between positive and negative samples(Chopraet al.,2005; Hadsellet al.,2006; Gidariset al.,2018; Hjelmet al.,2018; Chenet al.,2020a; Grillet al.,2020; Heet al.,2020; Wang and Isola,2020; Radfordet al.,2021a). Self-supervised learning has been extensively researched for its advantages in learning representations without the need for human labelers to manually label the data(Heet al.,2020; Tianet al.,2020; Chenet al.,2020a; Radfordet al.,2021b; Wuet al.,2023; Luoet al.,2023). Contrastive learning is a primary implementation form of self-supervised learning, and remarkable successes have been observed for many applications in different domain(Alecet al.,2019; Misra and Maaten,2020; Heet al.,2020; Tianet al.,2020; Chenet al.,2020b; Liuet al.,2021). However, self-supervised learning derives “pseudo-labels” from co-occurring inputs to relate information(Liuet al.,2021). For negative samples, there exists a distinction in the semantic interpretation of “classes” in pre-training phase and the generalized semantic of “classes” used in downstream tasks. As illustrated in Fig.1, for the anchor point “dog”, the semantic of negative samples is all the unlabeled samples excluding the anchor data point itself in the pre-training phase. That is, each sample is treated as an individual class. However, in the downstream task, the semantic of negative sample are samples labeled as “not dogs”. Aligning the semantic representation of negative examples between the pretraining tasks and downstream tasks is crucial for improving performance in the latter. In the pretraining phase, there are two types of negative samples that disrupt the semantic structure of generalized “classes”. The first type consists of false negative (FN) samples, represented byx_{3}^{\prime}in Fig.1. Despite being labeled as “dog”, it were erroneously treated as negative belonging to a different class than the anchor data point. This necessitates the task of false negative debiasing, aiming to prevent the false negative samples from being erroneously pushed apart. The second type comprises hard negative (HN) samples, exemplified byx_{1}^{\prime}in Fig.1. These samples possess a “wolf” label and exhibit similarities to the anchor data point, despite belonging to a distinct class. It is essential to push these samples further apart, corresponding to the task of hard negative mining, otherwise the underlying semantic structure can be disrupted(Wang and Liu,2021; Chuanget al.,2020). In this work, we focus on the end-to-end self-supervised contrastive learning method with SimCLR as a representative approach. We address two critical tasks in self-supervised contrastive learning, namely debiasing false negatives and mining hard negatives, within a Bayesian framework. These tasks are effectively achieved through re-weighting the negative samples, thereby correcting the contrastive loss within the self-supervised setting and obtaining the modified Bayesian self-supervised contrastive loss (BCL), which offers a flexible and principled framework for self-supervised contrastive learning and presents a generalized perspective of contrastive loss. We analyzed the small-sample properties of BCL, and established the relationship between the importance weight applied to each unlabeled sample and the posterior probability of them being true negatives. Furthermore, we investigated the large-sample properties of BCL and demonstrated its consistency as an estimation of the contrastive loss under the supervised setting.
2306.04718v1	Neural Symbolic Regression using Control Variables	Symbolic regression (SR) is a powerful technique for discovering the analytical mathematical expression from data, finding various applications in natural sciences due to its good interpretability of results. However, existing methods face scalability issues when dealing with complex equations involving multiple variables. To address this challenge, we propose SRCV, a novel neural symbolic regression method that leverages control variables to enhance both accuracy and scalability. The core idea is to decompose multi-variable symbolic regression into a set of single-variable SR problems, which are then combined in a bottom-up manner. The proposed method involves a four-step process. First, we learn a data generator from observed data using deep neural networks (DNNs). Second, the data generator is used to generate samples for a certain variable by controlling the input variables. Thirdly, single-variable symbolic regression is applied to estimate the corresponding mathematical expression. Lastly, we repeat steps 2 and 3 by gradually adding variables one by one until completion. We evaluate the performance of our method on multiple benchmark datasets. Experimental results demonstrate that the proposed SRCV significantly outperforms state-of-the-art baselines in discovering mathematical expressions with multiple variables. Moreover, it can substantially reduce the search space for symbolic regression. The source code will be made publicly available upon publication.	Symbolic regression (SR) aims to uncover the underlying mathematical expressions from observed data[16,9]. It has been widely used for scientific discovery across various disciplines[1,29]owing to its ability to learn analytical expressions between the input and output. The implementation of SR involves two steps[15]. The first step is to predict the skeleton of mathematical expressions based on a pre-defined list of basic operations (+,-,\times,\div) and functions (\sin,\cos,\exp,\log). For instance, we can identify the skeleton of a symbolic equation asf(x)=\log{ax}+\sin(bx)+c. Next, we adopt optimization methods, such as Broyden–Fletcher–Goldfarb–Shanno (BFGS), to estimate the parametersa,b,cin the skeleton. The key challenges of SR lie in: 1) how to improve the accuracy and scalability for multiple input variables, and 2) how to speed up the discovery process. In the past few decades, a plethora of SR methods[22]have been developed to discover underlying mathematical equations from data in science and engineering domains. One popular approach among them is genetic programming (GP)[4,7,27,10,2], which uses evolutionary operations, such as mutation, crossover, and selection, to estimate the symbolic expressions in a tree structure. However, GP would suffer from instability and its inference time is expensive in the context of multiple input variables[15]. Another method, SINDy[5], adopts sparse linear regression to discover the governing equations of dynamical systems. However, SINDy’s performance relies heavily on prior knowledge of a known set of candidate functions, and it is difficult to uncover complex equations from data solely through linear regression. To overcome these limitations, some studies explore deep neural networks-based techniques, such as Deep Symbolic Regression (DSR)[25]and Transformer-based pre-training, for symbolic learning. Although these approaches obtain good prediction accuracy, they do not scale well to mathematical equations with multiple variables. Recently, researchers develop Symbolic Physics Learner (SPL), a physics-informed Monte Carlo Tree Search (MCTS) algorithm for symbolic regression. While SPL outperforms most GP-based methods, it still struggles with multiple variables in mathematical expressions. In summary, existing methods suffer from scalability issues when dealing with complex multi-variable equations as they require a much larger search space to identify the combination of different variables. Thus, the question is, how can we reduce the search space of symbolic regression for complex equations involving multiple variables? In this paper, we propose a novel neural symbolic regression with control variables (SRCV) that combines neural networks and symbolic regression to discover analytical expressions from data, as illustrated in Fig.1. Inspired by divide and conquer[26], SRCV addresses the multi-variable symbolic regression by decomposing it into a set of single-variable SR problems and then combines the estimated symbolic equation for each variable in a bottom-up manner. The proposed method is performed in four steps as follows. 1) We learn a data generator from observed data using DNNs, allowing for generating data for a specific variable. 2) Generate data via control variables. Specifically, we generate data samples for the current independent variable by manipulating the previously learned variables and other control variables. For example, for estimating the symbolic expression of variablex_{i}, we can generate data samples by varyingx_{i}while fixing the other variables. 3) Single-variable symbolic regression is employed to estimate the mathematical expression of the current variable based on the generated data in step 2. Here any symbolic regression models can be inserted into the framework. 4) We gradually add the remaining variables one by one to step 2 and proceed with step 3 until all the variables are covered. Extensive experimental results on multiple SR benchmarks demonstrate the superiority of our SRCV over the state-of-the-art methods in discovering complex multi-variable equations. Moreover, the proposed approach is able to discover complex expressions in a reduced search space. Our main contributions are three-fold: 1) we propose SRCV, a simple and effective neural symbolic regression method using control variables; 2) we illustrate that the proposed method exhibits a significant reduction in search space for complex symbolic equations; 3) the evaluation results demonstrate that our method can significantly outperform the baselines in terms of accuracy and inference time.
2306.06024v3	Self-Interpretable Time Series Prediction with Counterfactual Explanations	Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.	Deep learning (DL) has become increasingly prevalent, and there is naturally a growing need for understanding DL predictions in many decision-making area, such as healthcare diagnosis and public policy-making. The high-stake nature of these areas means that these DL predictions are considered trustworthy only when they can be well explained. Meanwhile, time-series data has been frequently used in these areas(Zhaoet al.,2021; Jinet al.,2022; Yanget al.,2022), but it is always challenging to explain a time-series prediction due to the nature of temporal dependency and varying patterns over time. Moreover, time-series data often comes with confounding variables that affect both the input and output, making it even harder to explain predictions from DL models. On the other hand, many existing explanation methods are based on assigning importance scores for different parts of the input to explain model predictions(Ribeiroet al.,2016; Lundberg and Lee,2017b; Chenet al.,2018; Wanget al.,2019b; Weinbergeret al.,2020; Plumbet al.,2020). However, understanding the contribution of different input parts are usually not sufficiently informative for decision making: people often want to know what changes made to the input could have lead to a specific (desirable) prediction(Wachteret al.,2017a; Goyalet al.,2019; Nemirovskyet al.,2022). We call such changed input that could have shifted the prediction to a specific targetactionable counterfactual explanations. Below we provide an example in the context of time series. Suppose there is a model that takes as input a time series of breathing signal{\bf x}\in{\mathbb{R}}^{T}from a subject of ageu=60to predict the corresponding sleep stage asy^{pred}=\mbox{`Awake'}\in\{\mbox{`Awake'},\mbox{`Light Sleep'},\mbox{`Deep % Sleep'}\}. Typical methods assign importance scores to each entry of{\bf x}to explain the prediction. However, they do not provideactionablecounterfactual explanations on how to modify{\bf x}to{\bf x}^{cf}such that the prediction can change toy^{cf}=\mbox{`Deep Sleep'}. An ideal method with such capability could provide more information on why the model make specific predictions. Actionable counterfactual explanations help people understand how to achieve a counterfactual (target) output by modifying the current model input. However, such explanations may not be sufficiently informative in practice, especially under the causal effect of confounding variables which are often immutable. Specifically, some variables can hardly be changed once its value has been determined, and suggesting changing such variables are both meaningless and infeasible (e.g., a patient age and gender when modeling medical time series). This leads to a stronger requirement: a good explanation should make as few changes as possible on immutable variables; we call such explanationsfeasible counterfactual explanations. Below we provide an example in the context of time series. In Example1, ageuis a confounder that affects both{\bf x}andysince elderly people (i.e., largeru) are more likely to have irregular breathing{\bf x}and more ‘Awake’ time (i.e.,y=\mbox{`Awake'}) at night. To generate a counterfactual explanation to changey^{pred}to‘Deep Sleep’, typical methods tend to suggest decreasing the ageufrom60to50, which isinfeasible(since age cannot be changed in practice). An ideal method would first infer the ageuand search for afeasiblecounterfactual explanation{\bf x}^{cf}that could changey^{pred}to‘Deep Sleep’while keepinguunchanged. In this paper, we propose a self-interpretable time series prediction model, dubbed Counterfactual Time Series (CounTS), which can both (1) perform time series predictions and (2) provide actionable and feasible counterfactual explanations for its predictions. Under common causal structure assumptions, our method is guaranteed to identify the causal effect between the input and output in the presence of exogenous (confounding) variables, thereby improving the generated counterfactual explanations’ feasibility. Our contribution is summarized as follows: We identify the actionability and feasibility requirements for generating counterfactual explanations for time series models and develop the first general self-interpretable method, dubbed CounTS, that satisfies such requirements. We provide theoretical guarantees that CounTS can identify the causal effect between the time series input and output in the presence of exogenous (confounding) variables, thereby improving feasibility in the generated explanations. Experiments on both synthetic and real-world datasets show that compared to state-of-the-art methods, CounTS significantly improves performance for generating counterfactual explanations while still maintaining comparable prediction accuracy.
2309.03118v1	Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs	Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and can solve different tasks due to their emergent ability and generalizability. However, LLMs sometimes lack domain-specific knowledge to perform tasks, which would also cause hallucination during inference. In some previous works, additional modules like graph neural networks (GNNs) are trained on retrieved knowledge from external knowledge bases, aiming to mitigate the problem of lacking domain-specific knowledge. However, incorporating additional modules: 1) would need retraining additional modules when encountering novel domains; 2) would become a bottleneck since LLMs' strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability. Specifically, we design a simple yet effective prompt to transform retrieval into a multi-hop decision sequence, which empowers LLMs with searching knowledge ability in zero-shot manner. Additionally, KSL is able to provide complete retrieval paths and therefore increase explainability of LLMs' reasoning processes. We conduct experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and found that our approach improves LLM baseline performance by a relatively large margin.	Recently, large language models (LLMs) like ChatGPT have drawn numerous attention from researchers and practitioners due to theirgeneralistcapabilities(Qinet al.,2023). For instance, sufficiently large language models could perform well for different tasks in zero-shot manner, such as text summarization(Yanget al.,2023; Zhanget al.,2023), machine translation(Moslemet al.,2023), and question answering(Singhalet al.,2023). However, in some scenarios, LLMs lack domain-specific knowledge or are not able to recall facts and knowledge correctly, which causes hallucination(Banget al.,2023). Hallucination refers to models generating text that is nonsensical, or unfaithful to the provided source input(Jiet al.,2023; Koehn and Knowles,2017; Raunaket al.,2021; Rohrbachet al.,2018; Vinyals and Le,2015; Maynezet al.,2020). Retrieving relevant texts from knowledge bases is a classic way to augment language models’ performance like generation quality(Borgeaudet al.,2022; Lewiset al.,2020a; Levineet al.,2022; Guuet al.,2020). Besides, it can also help improve the factuality of generated texts. Typically, retrieval modules are employed to find the most relevant documents with the highest similarity scores to the query. Then input texts and retrieved documents would be combined in a specific way fed into models. Motivated by this, some methods(Ramet al.,2023; Penget al.,2023a)utilize retrieved texts to augment LLMs.Ramet al.(2023)directly prepends retrieved documents to the input to obtain a performance gain for LLMs.(Penget al.,2023a)designs an LLM-Augmenter to retrieve and merge evidence from external knowledge for alleviating hallucination. However, relying on similarity between embeddings would only make model learn shallow features instead of understanding semantics, which in turn hinder the model from searching truly useful knowledge. On the contrary, Knowledge Graphs (KGs) are clear, logical, and superior mediums of knowledge. Thus, effectively leveraging KGs for LLMs should benefit LLMs’ performance on knowledge-required tasks. For this reason, there is a line of work(Yasunagaet al.,2021; Linet al.,2019; Fenget al.,2020)using KGs to help LLMs make predictions. KagNet(Linet al.,2019)proposes a graph neural network module to model relational graphs for relational reasoning under the context of both knowledge symbolic space and language semantic space. MHGRN(Fenget al.,2020)equips pretrained language models with a multi-hop relational reasoning module, which unifies path-based reasoning methods and graph neural networks. QA-GNN(Yasunagaet al.,2021)learn representations over joint graphs formed by connecting QA context and KG. However, they(Yasunagaet al.,2021; Linet al.,2019; Fenget al.,2020)all require training additional knowledge-aware modules like graph neural networks (GNNs) on retrieved knowledge. There are two shortcomings of training additional modules: 1) would suffer from pains of retraining when encountering novel domains; 2) would become a bottleneck since LLMs’ strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to solve these shortcomings, which teaches LLMs themselves to search for knowledge from external knowledge bases. To be specific, we simplify the process of searching for necessary knowledge from KGs into a multi-hop decision sequence. At each step, we transform local information within KGs into text prompts (including the historical path selected by LLMs), based on which LLMs select relevant knowledge in the context to perform tasks, as shown in Figure1. The whole process is similar to humans searching over the Internet for achieving some goals. Furthermore, based on the complete paths chosen by LLMs, we can explain the whole decision-making process of LLMs. It allows for analysis when bad cases arise, a capability not present in previous black-box retrieval methods. We evaluate our approach, Knowledge Solver (KSL), with three LLMs (GPT-3.5, LLaMA(Touvronet al.,2023a), and LLaMA 2(Touvronet al.,2023b)) on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, where reasoning with knowledge is required. KSL improves two LLM baselines’ performance across these three datasets in zero-shot and finetuning settings. Our main contributions are summarized as follows: We propose Knowledge Solver (KSL), which is the first paradigm employing LLMs to search for relevant knowledge on KGs by themselves. Our proposed paradigm Knowledge Solver can boost LLMs’ performance on knowledge-required tasks by a relatively large margin in zero-shot manner without additional modules and training. Knowledge Solver can provide explainability for LLMs’ whole reasoning processes. When the computational burden is affordable, finetuning LLMs on our specially constructed dataset, with the help of KGs, can benefit LLMs further.
2302.02275v1	Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing	Sequence-to-Sequence (S2S) models have achieved remarkable success on various text generation tasks. However, learning complex structures with S2S models remains challenging as external neural modules and additional lexicons are often supplemented to predict non-textual outputs. We present a systematic study of S2S modeling using contained decoding on four core tasks: part-of-speech tagging, named entity recognition, constituency and dependency parsing, to develop efficient exploitation methods costing zero extra parameters. In particular, 3 lexically diverse linearization schemas and corresponding constrained decoding methods are designed and evaluated. Experiments show that although more lexicalized schemas yield longer output sequences that require heavier training, their sequences being closer to natural language makes them easier to learn. Moreover, S2S models using our constrained decoding outperform other S2S approaches using external resources. Our best models perform better than or comparably to the state-of-the-art for all 4 tasks, lighting a promise for S2S models to generate non-sequential structures.	Sequence-to-Sequence (S2S) models pretrained for language modeling (PLM) and denoising objectives have been successful on a wide range of NLP tasks where both inputs and outputs are sequencesRadfordet al.(2019); Raffelet al.(2020); Lewiset al.(2020); Brownet al.(2020). However, for non-sequential outputs like trees and graphs, a procedure called linearization is often required to flatten them into ordinary sequencesLiet al.(2018); F-G and G-R (2020); Yanet al.(2021); Bevilacquaet al.(2021); He and Choi (2021a), where labels in non-sequential structures are mapped heuristically as individual tokens in sequences, and numerical properties like indices are either predicted using an external decoder such as Pointer NetworksVinyalset al.(2015a)or cast to additional tokens in the vocabulary. While these methods are found to be effective, we hypothesize that S2S models can learn complex structures without adapting such patches. To challenge the limit of S2S modeling, BARTLewiset al.(2020)is finetuned on four tasks without extra decoders: part-of-speech tagging (POS), named entity recognition (NER), constituency parsing (CON), and dependency parsing (DEP). Three novel linearization schemas are introduced for each task: label sequence (LS), label with text (LT), and prompt (PT).LStoPTfeature an increasing number of lexicons and a decreasing number of labels,which are not in the vocabulary (Section3). Every schema is equipped with a constrained decoding algorithm searching over valid sequences (Section4). Our experiments on three popular datasets depict that S2S models can learn these linguistic structures without external resources such as index tokens or Pointer Networks. Our best models perform on par with or better than the other state-of-the-art models for all four tasks (Section5). Finally, a detailed analysis is provided to compare the distinctive natures of our proposed schemas (Section6).111All our resources including source codes are publicly available:https://github.com/emorynlp/seq2seq-corenlp
2308.13633v2	Adaptive whitening with fast gain modulation and slow synaptic plasticity	Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to match structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale whitening objective that factorizes the inverse whitening matrix into basis vectors, which correspond to synaptic weights, and a diagonal matrix, which corresponds to neuronal gains. We test our model on synthetic and natural datasets and find that the synapses learn optimal configurations over long timescales that enable adaptive whitening on short timescales using gain modulation.	Individual neurons in early sensory areas rapidly adapt to changing sensory statistics by normalizing the variance of their responses[11;21;43]. At the population level, neurons also adapt by reducing correlations between their responses[42;10]. These adjustments enable the neurons to maximize the information that they transmit by utilizing their entire dynamic range and reducing redundancies in their representations[4;8;33;7]. A natural normative interpretation of these transformations isadaptive whitening, a context-dependent linear transformation of the sensory inputs yielding responses that have unit variance and are uncorrelated. Decorrelation of the neural responses requires coordination between neurons and the neural mechanisms underlying such coordination are not known. Since neurons communicate via synaptic connections, it is perhaps unsurprising that most existing mechanistic models of adaptive whitening decorrelate neural responses by modifying the strength of these connections[60;30;45;46;14;37;59]. However, long-term synaptic plasticity is generally associated with long-term learning and memory[38], and thus may not be a suitable biological substrate for adaptive whitening (though short-term synaptic plasticity has been reported[63]). On the other hand, there is extensive neuroscience literature on rapid and reversible gain modulation[1;48;50;13;51;47;22;61]. Motivated by this,Duonget al.[18]proposed a mechanistic model of adaptive whitening in a neural circuit withfixedsynaptic connections that adapts exclusively by modifying the gains of interneurons that mediate communication between the primary neurons. They demonstrate that an appropriate choice of the fixed synaptic weights can both accelerate adaptation and significantly reduce the number of interneurons that the circuit requires. However, it remains unclear how the circuitlearnssuch an optimal synaptic configuration, which would seem to require synaptic plasticity. In this study, we combine the learning and adaptation of synapses and gains, respectively, in a unified mechanistic neural circuit model that adaptively whitens its inputs over multiple timescales (Fig.1). Our main contributions are as follows: We introduce a novel adaptive whitening objective in which the (inverse) whitening matrix is factorized into a synaptic weight matrix that is optimized across contexts and a diagonal (gain) matrix that is optimized within each statistical context. With this objective, we derive a multi-timescale online algorithm for adaptive whitening that can be implemented in a neural circuit comprised of primary neurons and an auxiliary population of interneurons with slow synaptic plasticity and fast gain modulation (Fig.1). We test our algorithm on synthetic and natural datasets, and demonstrate that the synapses learn optimal configurations over long timescales that enable the circuit to adaptively whiten its responses on short timescales exclusively using gain modulation. Beyond the biological setting, multi-timescale learning and adaptation may also prove important in machine learning systems. For example,Mohanet al.[41]introduced “gain-tuning”, in which the gains of channels in a deep denoising neural network (with pre-trained synaptic weights) are adjusted to improve performance on samples with out-of-distribution noise corruption or signal properties. The normative multi-timescale framework developed here offers a new approach to continual learning and test-time adaptation problems such as this.
2307.02025v1	NMS Threshold matters for Ego4D Moment Queries -- 2nd place solution to the Ego4D Moment Queries Challenge 2023	This report describes our submission to the Ego4D Moment Queries Challenge 2023. Our submission extends ActionFormer, a latest method for temporal action localization. Our extension combines an improved ground-truth assignment strategy during training and a refined version of SoftNMS at inference time. Our solution is ranked 2nd on the public leaderboard with 26.62% average mAP and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge. Our code is available at https://github.com/happyharrycn/actionformer_release.	The Ego4D Moment Queries (MQ) task aims to localize all moments of actions in time and recognize their categories within an untrimmed egocentric video. We adopt a two-stage approach for this task, where clip-level features are first extracted from raw video frames using a pre-trained feature network, followed by a temporal localization model that predicts the onset and offset of action instances as well as their categories. Our submission last year explored the combination of a latest localization model (ActionFormer[12]) and a strong set of video features[9]. This work seeks to improve the localization model . A limitation of ActionFormer[12]lies in its label assignment at training time; annotated action instances are assigned to candidate moments based oncenter sampling, a heuristic that designates positive labels to moments proximal to the center of an action instance. Recent literature in object detection, however, shows that such static assignment strategy is insufficient for complex spatial configuration of objects. Inspired by this insight, we propose to adapt SimOTA[5], a dynamic label assignment strategy, for temporal action localization. SimOTA assigns ground-truth action instances to the candidate moments on the fly by solving an optimal transport problem. Further, we refine SoftNMS to account for densely overlapping actions in Ego4D. Equipped with these modifications, our solution extends our prior work and is ranked 2ndon the public leaderboard. Specifically, our solution attains 26.62% average mAP and 45.69% Recall@1x at tIoU=0.5 on the test set, significantly outperforming the strong baseline from 2023 challenge. We hope our work will shed light on future development in temporal action localization and egocentric vision.
2309.05463v1	Textbooks Are All You Need II: phi-1.5 technical report	"We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality"" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need"" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step"" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics."	Over the past few years, Large Language Models (LLMs) have transformed the field of Natural Language Processing. More broadly, they hold the promise of a paradigm shift for human-computer interaction. These advancements have far-reaching economic implications, as well as the potential to redefine our conceptual frameworks of artificial intelligence and perhaps even cognition itself. Moreover, the latest generation of models such as GPT-4[Ope23]have demonstrated remarkable improvements over their predecessors, offering capabilities previously thought to be unattainable in the short term; see for example[BCE{}^{+}23]for an in-depth comparison between GPT-4 and its predecessor GPT-3.5. The improvement from one generation of LLMs to the next seems at the moment to primarily stem fromscale, with the most powerful models nearing trillions of parameters and trillion of tokens for training data (for example, PaLM[CND{}^{+}22]has 540 billion parameters and was trained on 780 billion tokens). A natural question arises: Is this large scale indispensable for achieving high levels of capability? Far from being merely an academic question, answering this holds implications across several dimensions. Economically, the cost of training, deploying, and maintaining such large models can be substantial. Scientifically, understanding whether similar capabilities can be achieved at a smaller scale could provide insights into the architectures and development of intelligent systems. From a responsible AI standpoint, the energy consumption of large-scale models is becoming an increasing concern, as is the question of how controllable or governable these large models can be. Finally, the ability to train compact models with cutting-edge capabilities would democratize advanced AI, enabling a broader range of individuals and organizations to study and deploy them, instead of being an exclusive domain of a few with vast computational resources. In this work we continue the investigation into the fundamental question of “how small can a LLM be to achieve certain capabilities”. The prior work[EL23]considered this question for the task of “speaking fluent English”, while the subsequent work[GZA{}^{+}23]considered the more challenging task of coding simple functions in Python. Here we focus on the more elusive concept ofcommon sense reasoning, a notoriously challenging task for AI[SBBC21]. Our results are summarized in Figure1. In a nutshell we buildphi-1.5, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves common sense reasoning benchmark results comparable to models ten times its size that were trained on datasets more than ten times larger. Moreover, our dataset consists almost exclusively of synthetically generated data (closely following the approach from[GZA{}^{+}23], see next section for more details), which has important implications for the potential to control for the notoriously challenging issue of toxic and biased content generation with LLMs[BGMMS21]. Additionally, we discuss the performance of a relatedfiltered web dataenhanced version ofphi-1.5, which we callphi-1.5-web​. We open-source our rawphi-1.5model (without instruction fine-tuning or any other stage of alignment) to empower the research community in its work on some of the most urgent questions around LLMs: in-context learning, mechanistic interpretability, and mitigation strategies for hallucinations, toxic content generation, and biased outputs. Indeed,phi-1.5is the first LLM at the one billion parameters scale to exhibit most of the relevant traits of larger LLMs for research on these topics. We hope thatphi-1.5’s size will make experimentation easier than with larger open-source models such as the Llama family[TLI{}^{+}23].
2307.05988v1	A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition	Human Activity Recognition (HAR) has become one of the leading research topics of the last decade. As sensing technologies have matured and their economic costs have declined, a host of novel applications, e.g., in healthcare, industry, sports, and daily life activities have become popular. The design of HAR systems requires different time-consuming processing steps, such as data collection, annotation, and model training and optimization. In particular, data annotation represents the most labor-intensive and cumbersome step in HAR, since it requires extensive and detailed manual work from human annotators. Therefore, different methodologies concerning the automation of the annotation procedure in HAR have been proposed. The annotation problem occurs in different notions and scenarios, which all require individual solutions. In this paper, we provide the first systematic review on data annotation techniques for HAR. By grouping existing approaches into classes and providing a taxonomy, our goal is to support the decision on which techniques can be beneficially used in a given scenario.	In the last decade, we have witnessed the spread and adoption of sensors, wearables, the Internet of Things (IoT), the Internet of Medical Things (IoMT), and edge computing technologies(Li et al.,2015). Sensors can detect and measure physical properties such as temperature, pressure, light, and motion. They are becoming ubiquitous in various industries, including automotive, aerospace, and consumer electronics. Moreover, their miniaturization has led to their integration into wearables, such as fitness trackers, smartwatches, clothes, and dedicated devices. Wearables are frequently used to track various aspects of a person’s health and activity. Recent developments even involve integrating medical sensors for remote patient monitoring, digital therapeutics, and real-time intervention into wearables(Seneviratne et al.,2017; Dunn et al.,2018; Cheng et al.,2021). On the other side, the IoT is formed by networks of interconnected devices, vehicles, and buildings that communicate with each other and exchange data. It has been adopted across various industries, including home automation, agriculture, and manufacturing. In addition, IoT devices can be remotely monitored and controlled, improving efficiency and productivity(Li et al.,2015). Instead, IoMT refers to using IoT devices in medical applications, enabling the healthcare providers’ capacity to monitor patients remotely, collect data for analysis, improve patient outcomes, and reduce healthcare costs(Vishnu et al.,2020). Finally, edge computing refers to processing data at or near the source rather than sending it to a central/remote server for processing. This technology has become increasingly important as the amount of data generated by IoT and IoMT devices grows. Edge computing enables faster processing times and reduces the latency and amount of data that needs to be transmitted over the network(Baker and Xiang,2023). The adoption and spread of these technologies have revolutionized various industries and enabled new applications and capabilities. With such systems now being ubiquitous, they serve as a common infrastructure for recognizing human activity, as described next. Human Activity Recognition (HAR):In such a context, HAR is a central research field that finds applications in various areas, including healthcare, sports, industry, and smart homes. HAR refers to the ability to identify and classify human activities using sensors, wearables, or other devices that capture data about the person’s movements and actions. With regard to healthcare, HAR can be used to monitor a patients’ status and detect abnormalities or changes in their behavior that may indicate a deterioration of health or the onset of a medical condition. For example, HAR can be used to detect falls of elderly patients or to monitor the movements of patients with Parkinson’s disease or other motor disorders(Demrozi et al.,2020). Moreover, HAR also has applications in sports and fitness to monitor the athletes’ performance and technique, helping them to improve their training and prevent injuries. HAR can also be used in activity tracking devices, such as fitness trackers, to provide users with insights into their daily activity levels and help them to achieve their fitness goals. In addition, HAR automates various tasks in smart homes based on the occupant’s activities. For example, lights can be turned on or off automatically based on the person’s movements, or the thermostat can be adjusted based on the person’s activity level(Demrozi et al.,2020,2021). HAR is related to various technologies, including sensors, wearables, IoT, IoMT, edge computing, machine learning (ML), Deep Learning (DL), and Artificial Intelligence (AI). Sensors and wearables are used to capture data about the person’s movements and actions, which is then used to identify and classify human activities in HAR applications. IoT and IoMT systems are used to collect data from sensors and wearables, which can be transmitted over the network for processing and analysis. Edge computing can process this data at or near the source, reducing latency and enabling real-time processing of HAR data(Baker and Xiang,2023; Vishnu et al.,2020). In HAR systems, the data collected from such devices is analyzed to classify a user’s activity. While, in principle, this analysis can be done based on heuristics (e.g., a feature exceeds certain thresholds, etc.), ML- and DL-based HAR techniques have become the most popular solution. Using them, also more complex analyses can be carried out, allowing for reliable recognition of activities even in data in which the properties or patterns that represent a certain activity or behavior are not obvious. ML- and Dl-based HAR methods can also integrate other data sources, such as environmental data, to provide more comprehensive insights into human behavior and activity(Demrozi et al.,2020). As the technology continues to improve and becomes widely available, we expect to see further advancements and new applications for ML-based HAR(Baker and Xiang,2023). When generating HAR model, a set of sensor data is recorded first. This data is then labeled with the activities under consideration. This step is calledannotation. Next, a machine-learning model is trained, which can then be used to classify unlabeled data. In the following, we describe the individual steps(Demrozi et al.,2020; Gupta et al.,2022)that are involved in creating a HAR system in more detail. An overview is shown in Figure1. Definition of Target Activities: Definition and analyzation of the real-world characteristics of the target activities to be recognized. For example, this can be their duration, distribution, similarity with other activities, etc. Device Setup: Identification and study of requirements and determination of the devices to be used in the data collection phase, based on the target human activities. Data Collection: In this phase, data is collected from sensors, wearables, or other devices that capture information about the person’s movements and actions. Data Annotation: The process of assigning labels to the human activities being performed. Labels are crucial in supervised learning as they provide the ground truth or correct answers that guide the learning process. By associating input data with corresponding labels, the model can learn to make accurate predictions and generalize its knowledge to unseen examples. Data Preprocessing: The collected data is then preprocessed to remove noise, irrelevant information are filtered out, and the data is prepared for analysis. As a part of this, the following analysis is carried out: Feature extraction: The preprocessed data is analyzed to extract relevant features that can be used to classify human activities. These features may include movement patterns, body position, or other characteristics. Feature selection: Once the features have been extracted, a subset of features may be selected for use in the classification model. This helps to reduce the dimensionality (e.g., the number of features) of the data and improve the accuracy of the model. Model generation and testing: A HAR (i.e., ML or DL) model is developed to classify human activities based on the selected features in this phase. The model may be trained using a labeled dataset or unsupervised learning techniques. After the model has been generated, the following steps are carried out before the model is ready to be used: Model evaluation: The developed model is then evaluated using a test dataset to assess its accuracy and performance. This phase helps to identify any issues or areas for improvement in the model. Deployment: Finally, the developed model is deployed to a real-world environment, where it is used to classify human activities. Data Annotation in HAR:The most labor-intensive step in creating a HAR system is data annotation, which involves creating a labeled dataset for training the ML/DL models. Manual labeling, in which human annotators manually label each recorded sample with the corresponding activity, is a common approach in data annotation. Although time-consuming and resource-intensive, it can produce high-quality labels that are accurate and consistent. Nevertheless, several factors can pose challenges in the manual data annotation process for HAR systems.Firstly, subjectivity can lead to inconsistencies and errors in labeling as the interpretation of the activity being performed can vary among annotators. This can ultimately affect the accuracy of the ML/DL model.Secondly, the data annotation process can be time-consuming, particularly when labeling large amounts of data, which can cause delays in the development of the HAR system and increase project costs.Thirdly, the economic cost can be a limiting factor since hiring human annotators or utilizing crowdsourcing platforms for data labeling can become expensive, mainly when the studied activities are complex.Fourthly, the variability of human activities can also pose a challenge in the annotation process. Since different individuals can perform activities differently, creating accurate and consistent labels for the data can be challenging.Lastly, label noise may exist in annotated data, resulting in errors in the labeling process. Label noise can occur due to human error, subjectivity, or inconsistencies in the annotation process, which ultimately reduces the performance of the HAR system’s ML/DL model. Careful consideration of these limitations and appropriate methods can help mitigate these challenges and improve the accuracy and performance of the final HAR system. Alternatively, automated methods, such as rule-based systems or unsupervised learning algorithms, can be employed for data annotation. These approaches are more efficient and scalable but may be less precise or necessitate additional manual validation. The quality of the annotated data is pivotal to the efficacy of the HAR system. Inaccurate or inconsistent labeling can cause poor ML/DL model performance, leading to the misclassification of human activities(Diete et al.,2017; Adaimi and Thomaz,2019). There are several (partial) possible solutions to the limitations of the annotation process in HAR(Diete et al.,2017; Adaimi and Thomaz,2019). Some of these solutions include: Standardization: Standardizing the annotation process can help to reduce subjectivity and increase consistency in the labeling process. This can be achieved by defining clear guidelines and procedures for annotators to follow and providing training and feedback to ensure the quality of the annotations. Automation: Automated methods, such as unsupervised learning algorithms or rule-based systems, can be used to annotate data. These methods can be faster and more scalable than manual labeling and reduce the annotation process’s cost. Active learning: Active learning techniques can reduce the labeled data needed for training an ML or DL model. This involves selecting the most informative data samples for annotation, which can help reduce the labeling process’s time and cost. Crowdsourcing: Crowdsourcing platforms can be used to engage many annotators to label the data. This can be a cost-effective solution, as well as provide a diverse range of perspectives on the activity being performed. Quality control: Quality control measures can be implemented to ensure the accuracy and consistency of the labeled data. This can include using multiple annotators to label the same data samples and comparing their annotations, as well as conducting regular checks on the quality of the annotations. While these solutions can enhance the accuracy and performance of the final HAR system, they do not completely eliminate the cost and time needed for the annotation process. Systematic Review Objectives:This paper aims to systematically review existing methodologies for automating data annotation in HAR. The objective is to identify the strengths and limitations of different techniques and provide insights into the current research and ongoing trends in this area. Specifically, the paper explores different approaches and algorithms used in automatic data annotation techniques. This does not only help in developing novel techniques in the future, but also supports the choice of an appropriate labeling technique for a given application. This review considers 2401 publications on automating data annotation in HAR. To the best of our knowledge, no systematic review has been published prior to this paper. The absence of such a review aggravates overseeing the different technologies used in this area, makes it difficult to follow recent trends, and leaves unclear which technical solution is most beneficial for realizing a given scenario. We in this paper close this gap by providing the first systematic review on this field of research. Paper organization:The rest of the paper is organized as follows. Section2delves into the background of HAR, presenting a comprehensive overview of the field, including its applications and challenges. Following that, Section3discusses the selection criteria for annotation methods in HAR, examining the key factors that we consider when choosing appropriate techniques. Section4presents an in-depth analysis and discussion of various annotation methods employed in HAR, exploring their strengths, limitations, and effectiveness in accurately identifying and classifying human activities Finally, Sections5and6conclude the paper by summarizing the key findings and contributions of the study, emphasizing the significance of automatic annotation methods in advancing HAR research and suggesting potential avenues for future exploration in this area.
2302.13475v1	Elementwise Language Representation	"We propose a new technique for computational language representation called elementwise embedding, in which a material (semantic unit) is abstracted into a horizontal concatenation of lower-dimensional element (character) embeddings. While elements are always characters, materials are arbitrary levels of semantic units so it generalizes to any type of tokenization. To focus only on the important letters, the $n^{th}$ spellings of each semantic unit are aligned in $n^{th}$ attention heads, then concatenated back into original forms creating unique embedding representations; they are jointly projected thereby determining own contextual importance. Technically, this framework is achieved by passing a sequence of materials, each consists of $v$ elements, to a transformer having $h=v$ attention heads. As a pure embedding technique, elementwise embedding replaces the $w$-dimensional embedding table of a transformer model with $256$ $c$-dimensional elements (each corresponding to one of UTF-8 bytes) where $c=w/v$. Using this novel approach, we show that the standard transformer architecture can be reused for all levels of language representations and be able to process much longer sequences at the same time-complexity without ""any"" architectural modification and additional overhead. BERT trained with elementwise embedding outperforms its subword equivalence (original implementation) in multilabel patent document classification exhibiting superior robustness to domain-specificity and data imbalance, despite using $0.005\%$ of embedding parameters. Experiments demonstrate the generalizability of the proposed method by successfully transferring these enhancements to differently architected transformers CANINE and ALBERT."	"We understand texts from various levels of semantics but current language representation strategies leverage tokenization which relies on a certain level of semantics exclusively, fully ignoring the hierarchical structures of natural languages. Text is encoded to a sequence of integers then projected into fixed-size latent embeddings. These types of expressions result in a recursive trade-off between different levels of language representations: (sub)word-level models indirectly recover characters(Itzhak and Levy,2021)but it is not always sufficient for spelling-sensitive tasks, character-level models need much longer sequences to reach comparable performance to word-level models thus amplifying the computational complexity of self-attention. Some recently proposed studies(Clarket al.,2022; Godeyet al.,2022; Tayet al.,2021)attempt to solve this by downsampling long character sequences into an acceptable length, however, they share the same limitation as pure character-level models because their valid downsampling rates are constrained to relatively small values mainly due to the smoothing and overhead issues. Instead, we propose elementwise embedding, a language representation technique for addressing this trade-off in which a set of lower-dimensional character embeddings calledelementsare horizontally concatenated into a single latent embedding calledmaterialthat mimics a semantic unit such as a word, phrase, sentence and etc. Using this method, models with higher-dimensional hidden representations create each semantic unit (i.e., a material) by concatenating a greater numbers of characters (i.e., elements), which implies that larger models can process longer sequences than smaller ones at the same computational complexity. This means that the acceptable sequence length scales with the size of a transformer model, but the complexity is fixed as that of its attention. Assuming that a character-level GPT-3 [processing 2048 12,288-dimensional token embeddings with 96 attention heads;Brownet al.(2020)] is trained with elementwise embedding, it aligns a sequence of2,048\times 96=296,608characters which is 96x longer at the sameO(N\sqrt{N})_{N=2048}complexity. The proposed methodology follows the two-step framework of""reshape, then focus"". First, the given text is encoded as a sequence ofuvUTF-8 bytes and projected into a(uv,c)embedding matrix in which each row is ac-dimensional element; it’s""reshaped""into a(u,w)embedding matrix in which each row is aw-dimensional material (e.g., a word), wherec=w/v. As a result, one material consists ofvelements so that we can alignuvelements at theO(u^{2})complexity using multihead self-attention(Vaswaniet al.,2017)withvattention heads. Eachi^{th}column of this(u,v)material matrix is a sequence of thei^{th}elements of allumaterials, soi^{th}attention head alignsi^{th}elements. This operation is most straightforward when a material is avletters word:i^{th}spellings of alluwords are aligned ini^{th}attention head, then concatenated back creating unique embedding representations wherei\in[1,v]. Each attendedi^{th}spelling is referred asfocusbecause it is quite similar to that we often read text inferring the meanings of words by""focusing""on a few important letters. The contextual importance of each word is determined jointly via linear transformation. Theoretically, this can be understood as lowering the entropy of character sequences concentrating distributed probabilities into several important spellings. Technically, it is just to pass a(u,w)word embedding matrix in which each row is a horizontal concatenation ofvc-dimensional character embeddings as input to a transformer model withw-dimensional hidden layers. It’s identical to aligning words using character-level semantics and vice versa. In practical implementation, focus is performed by multihead attention of the parent (any transformer model) by setting the number of attention heads toh=v, so applying elementwise embedding is simply to replace the embedding table of parent model with a set of 256c-dimensional character embeddings (each mapping to one of UTF-8 bytes; elements) and a following tensor reshaping operation. Neither structural modification of neural networks nor additional operations such as up/downsampling that entail unnecessary engineering efforts and overheads are required. Fig1offers an intuitive visualization of elementwise embedding."
2310.07987v2	Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications	This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.	Cooperative communications are acknowledged schemes to improve the transmission quality. One of the most important categories of cooperative communications is in the form of relaying. Although relaying requires extra energy and time slot, it is an effective solution when the path-loss of the direct link is very large. On the other hand, with the research trend towards the sixth-generation (6G) wireless networks[13], various transmission technologies have been invented, among which semantic communications[16]are considered to have a great potential in media transmissions. The semantic encoder extracts the semantic features for transmissions[17], while the semantic decoder works in a similar way to the generativeartificial intelligence (AI)[4]. Inspired by the principle of semantic communications, this letter proposes a novel framework,semantic-forward (SF)relaying, for cooperative communications. There have been already diverse relaying schemes in the literature[8]. One simple relaying scheme isamplify-and-forward (AF), in which the relay directly amplifies the signals received from the source and then forwards to the destination. In 1979, Cover and El Gamal[2]established the fundamental theorems of relaying systems, and proposed thedecode-and-forward (DF)andcompress-and-forward (CF)schemes. In the DF scheme, the relay decodes the received signals at the first step, and then the recovered information sequence is forwarded or discarded, respectively, depending on the recovery is error-free or not. In the CF scheme, the relay quantizes and compresses its received signals into the relay information to be transmitted to the destination. Beyond DF,lossy-forward (LF)[10]was proposed to overcome the drawback of DF, where the communication resources are completely wasted once errors occur in the relay information. In the LF scheme, the relay always forwards the relay information to the destination regardless of whether or not intra-link error is detected at the relay. At the destination, a joint decoder recovers the source information with the help of the relayed information, based on the principle of correlated sources transmission. Nevertheless, the previous relaying schemes are designed for general types of information, which do not exploit the features of information to improve the information efficiency. By adopting the semantic communications, the system adaptively exploits diverse types of information. Hence, we aims at designing a relaying systems where the relay forwards the semantic information to the destination, i.e., SF relaying, so that the destination can utilize the semantic information to help recovering the source information. The terminology of SF has been used for the first time in[11], up to the authors’ maximum knowledge, where the source transmits semantic information to the relay, and the relay translates and forwards the processed semantic information to the destination. However, there is no direct source-destination link in[11]. Different from the relay-assisted semantic communications in[11], this work proposes a semantic-assisted relaying system. In our proposed SF relaying system, the relay reconstructs the information received from the source at the first step. Then, in spite of whether or not the reconstruction error-free, the relay extracts the semantic information and sends it to the destination. The semantic coding achieves robustness of the relaying system against errors, and hence can reduce the payload in therelay-destination (R-D)link. At the destination, a joint decoder performs iterative decoding utilizing the Turbo principle[1]that exchanges the extrinsic information between the lossy information of thesource-destination (S-D)link and the semantic information of theR-Dlink. In this way, the SF relaying can help for the lossless recovery of the original information at the destinations, even in bad channel conditions. With the pre-trained semantic encoder/decoder, the SF relaying can reduce the payload of the R-D link in practical systems. The contributions of this letter are summarized as follows: We propose a novel relaying framework, i.e., SF relaying, which adopts semantic communications at the relay and the destination to reduce the payload of theR-Dlink. We design a joint source-channel coding algorithm for SF relaying systems, where the destination can losslessly recover the source information with the assistance of the semantic information received from the relay. We conduct a series of simulations with image transmissions to evaluate the performance of SF relaying. The simulation results demonstrate that SF relaying systems can exploit the semantic information to reduce theEuclidean distance (ED)and improve the image quality. Notation.Capital lettersX,Y,V,Udenote the random variables for constructing information sequences.Mrepresents the codeword satisfying the link rate constraintR.
2312.05721v1	Learning Spatially-Continuous Fiber Orientation Functions	Our understanding of the human connectome is fundamentally limited by the resolution of diffusion MR images. Reconstructing a connectome's constituent neural pathways with tractography requires following a continuous field of fiber directions. Typically, this field is found with simple trilinear interpolation in low-resolution, noisy diffusion MRIs. However, trilinear interpolation struggles following fine-scale changes in low-quality data. Recent deep learning methods in super-resolving diffusion MRIs have focused on upsampling to a fixed spatial grid, but this does not satisfy tractography's need for a continuous field. In this work, we propose FENRI, a novel method that learns spatially-continuous fiber orientation density functions from low-resolution diffusion-weighted images. To quantify FENRI's capabilities in tractography, we also introduce an expanded simulated dataset built for evaluating deep-learning tractography models. We demonstrate that FENRI accurately predicts high-resolution fiber orientations from realistic low-quality data, and that FENRI-based tractography offers improved streamline reconstruction over the current use of trilinear interpolation.	Mapping the human connectome relies upon a continuous and accurate representation of the underlying brain tissue. This is needed for tracing streamlines, resolving crossing fibers, and deciding when to terminate a tract. Often, tractography algorithms rely on simple trilinear interpolation to “fill out” a continuous field from discretely-sampled diffusion magnetic resonance images (dMRIs). If this interpolation could be improved, then tractography algorithms could produce more detailed and accurate human white matter (WM) fiber tracts. In this work, we propose FENRI (Fiber orientations fromExplicitNeuralRepresentatIons), a novel deep learning-based super-resolution model for estimating fODFs continuously in space. We demonstrate FENRI’s capabilities through the following experiments: 1) a quantitative evaluation of fODF reconstruction in Human Connectome Project (HCP) data, 2) a qualitative evaluation of tractography in HCP data, and 3) a quantitative measure of tractography performance on a new, expanded simulation dataset. As an image upsampler, FENRI outperforms more generic single-image super-resolution (SISR) methods on a variety of test metrics. We also show how, as a tractography enhancement, FENRI’s explicit representation sampling provides a powerful improvement over standard tractography methods. Background.Reconstructing streamlines from diffusion-weighted images (DWIs) requires a model of neuron fiber directionality. One popular model is the general fODF represented by coefficients in the spherical harmonic (SH) orthonormal basis, estimated by constrained spherical deconvolution (CSD)[7]. Several deep learning models have recently been proposed to super-sample diffusion representations. For example, Qin et. al., 2021 used convolutional neural networks (CNNs), an efficient sub-pixel CNN (ESPCN) layer, and high-resolution T1w volumes to predict high-resolution diffusion model parameters[10,13]. However, these previous works were limited to upsampling by aninteger upscaling factor, e.g.2\times, which is not ideal for estimating continuous fields. The recently proposed implicit neural representation (INR) method, which learns continuous-valued representations in some Euclidean space, is one solution to this challenge[14]. INRs are most commonly applied to 3D rendering, but INR-like models have been used in SISR. For example, the Local Implicit Image Function where a low-resolution input image is encoded into a feature space and sampled continuously for upsampling[3]. To our knowledge, the only proposed model that utilizes INRs for super-resolving dMRIs is given in[4], which focused on uncertainty in continuous predictions rather than tractography. We place FENRI alongside these INR models, but note that FENRI does not model animplicitfunction, but anexplicitfunction of SH coefficients.
2311.07452v1	Explainable Boosting Machines with Sparsity -- Maintaining Explainability in High-Dimensional Settings	"Compared to ""black-box"" models, like random forests and deep neural networks, explainable boosting machines (EBMs) are considered ""glass-box"" models that can be competitively accurate while also maintaining a higher degree of transparency and explainability. However, EBMs become readily less transparent and harder to interpret in high-dimensional settings with many predictor variables; they also become more difficult to use in production due to increases in scoring time. We propose a simple solution based on the least absolute shrinkage and selection operator (LASSO) that can help introduce sparsity by reweighting the individual model terms and removing the less relevant ones, thereby allowing these models to maintain their transparency and relatively fast scoring times in higher-dimensional settings. In short, post-processing a fitted EBM with many (i.e., possibly hundreds or thousands) of terms using the LASSO can help reduce the model's complexity and drastically improve scoring time. We illustrate the basic idea using two real-world examples with code."	Explainable boosting machines (Nori et al. 2019), or EBMs for short, are a modern class ofgeneralized additive models(GAMs) that can offer both competitive accuracy and explicit transparency and explainability, which are often considered to be two opposing goals of a machine learning (ML) model. For example, full-complexity models, like random forests (Breiman 2001), tend to be highly competitive in terms of accuracy, but are readily less transparent and explainable (due in part to the high-order interaction effects often captured by such black-box models). Essentially, with EBMs, it’s possible to “have your cake and eat it too.” In short, EBM models have the general form where gis alink functionthat allows the model to handle various response types (e.g., thelogitlink for logistic regression or the identify function for ordinary regression with continuous outcomes); \theta_{0}is a constant intercept (or bias term); f_{i}is theterm contribution(orshape function) for predictorx_{i}(i.e., it captures the main effect ofx_{i}); f_{ij}is the term contribution for the pair of predictorsx_{i}andx_{j}(i.e., it captures the joint effect, or pairwise interaction effect ofx_{i}andx_{j}). Similar togeneralized additive models plus interactions(Lou et al. 2013), or GA2Ms for short, the pairwise interaction terms are determined automatically using the FAST algorithm described in Lou et al. (2013). In short, FAST is a novel, computationally efficient method for ranking all possible pairs of feature candidates for inclusion into the model (by default, the top 10 pairwise interactions are used). The primary difference between EBMs and GA2Ms is in how the shape functions are estimated. In particular, EBMs usecyclic gradient boosting(Nori et al. 2019; Wick, Kerzel, and Feindt 2020) to estimate the shape functions for each feature (and selected pairs of interactions) in a round-robin fashion using a low learning rate to help ensure that the order in which the feature effects are estimated does not matter. Estimating each feature one-at-a-time in a round-robin fashion also helps mitigate potential issues withcollinearitybetween predictors (Nori et al. 2019; Wick, Kerzel, and Feindt 2020). However, in contrast to the more common gradient boosting machine (J. H. Friedman 2001, 2002), or GBM for short, which can ignore irrelevant inputs, EBMs include at least one term in the model for each feature: one main effect (f_{i}) for each predictor, and a term for each selected pairwise interaction effect. This is due to the cyclic nature of the underlying boosting framework. For example, an EBM applied to a training set withp=300features will result in a model with at least 300 terms. While EBMs are consideredglass-boxmodels, an EBM with, say, hundreds or thousands of terms, starts to become much less transparent and explainable. Moreover, the larger the fitted model (i.e., the more terms there are), then the more time it will take the EBM to make predictions, making larger models less fit for deployment and production. To this end, we propose in Section2a way to introduce sparsity into a fitted EBM by rescaling the individual terms (i.e., thef_{i}andf_{ij}) via regression coefficients estimated from a method called the LASSO (Tibshirani 1996). Consequently, by nature of theL_{1}regularization enforced by the LASSO, many of these coefficients can be estimated to be zero, resulting in a reduced EBM with far fewer terms (and hopefully, comparable accuracy)!
2306.00335v2	Approximate inference of marginals using the IBIA framework	Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity trade-off is controlled using user-defined clique size bounds. Results for several benchmark sets from recent UAI competitions show that our method gives either better or comparable accuracy than existing variational and sampling based methods, with smaller runtimes.	Discrete probabilistic graphical models (PGM) including Bayesian networks (BN) and Markov networks (MN) are used for probabilistic inference in a wide variety of applications. An important task in probabilistic reasoning is the computation of posterior marginals of all the variables in the network. Exact inference is known to be #P-complete[Roth,1996], thus necessitating approximations. Approximate techniques can be broadly classified as sampling based and variational methods. Sampling based methods include Markov chain Monte Carlo based techniques like Gibbs sampling[Gelfand,2000, Kellyet al.,2019]and importance sampling based methods[Gogate and Dechter,2011, Friedman and Van den Broeck,2018, Kasket al.,2020, Broka,2018, Louet al.,2019,2017b,2017a, Marinescuet al.,2019,2018]. An advantage of these methods is that accuracy can be improved with time without increasing the required memory. However, in many benchmarks the improvement becomes slow with time.Moreover, many of the recent sampling/search based techniquesKasket al.[2020], Broka [2018], Louet al.[2019,2017b,2017a], Marinescuet al.[2019,2018]have been evaluated either for approximate inference of partition function (PR) or for finding the marginal maximum a posteriori assignment (MMAP). Currently, there are no published results for posterior marginals (MAR) using these methods, and the publicly available implementations do not support the MAR task.Alternatively, variational techniques can be used. These include loopy belief propagation (LBP)[Frey and MacKay,1998]region-graph based techniques like generalized belief propagation (GBP)[Yedidiaet al.,2000]and its variants[Heskeset al.,2003, Mooij and Kappen,2007, Linet al.,2020], mini-bucket based schemes like iterative join graph propagation (IJGP)[Mateescuet al.,2010]and weighted mini-bucket elimination (WMB)[Liu and Ihler,2011]and methods that simplify the graph structure like edge deletion belief propagation (EDBP) and the related relax-compensate-recover (RCR) techniques[Choiet al.,2005, Choi and Darwiche,2006,2010]. While the accuracy-complexity trade-off can be achieved using a single user-defined clique size bound in mini-bucket based methods, it is non-trivial in many of the other region graph based methods. Most of these techniques use iterative message passing to solve an optimization problem, for which convergence is not guaranteed and even if possible, can be slow to achieve.Non-iterative methods like Deep Bucket Elimination (DBE)[Razeghiet al.,2021]and NeuroBE[Agarwalet al.,2022]are extensions of bucket elimination that approximate messages using neural networks. However, training these networks takes several hours. Moreover, the publicly available implementations of these methods do not support the MAR task. The recently proposedincremental build-infer-approximate(IBIA) framework[Bathla and Vasudevan,2023]uses a different approach. It converts the PGM into a sequence of calibrated clique tree forests (SCTF) with clique sizes bounded to a user-defined value.Bathla and Vasudevan [2023]show that the normalization constant (NC) of clique beliefs in the last CTF in the sequence isa good approximationof the partition function of the overall distribution. This framework has two main advantages. Firstly, since it is based on clique trees and not loopy graphs, the belief propagation step is non-iterative. Therefore, it is fast and has no issues related to convergence. Secondly, it provides an easy control of the accuracy complexity trade-off using two user-defined parametersand hence can be used in anytime manner.However, the framework inBathla and Vasudevan [2023]cannot be used to infer marginals. This is because only the clique beliefs in the last CTF account for all factors in the PGM. Beliefs in all other CTFs account for a subset of factors and thus, cannot be used for inference of marginals. Contributions of this work:In this paper, we propose a method for marginal inference that uses the IBIA framework. We show that the approximation algorithm used in this framework preserves the within-clique beliefs. Based on this property, we modify the data structure generated by IBIA to add links between adjacent CTFs. We refer to the modified data structure as asequence of linked clique tree forests(SLCTF). We propose a heuristic belief update algorithmthat back-propagates beliefs from the last CTF to the previous CTFs via the links and re-calibrates each CTF so that the updated beliefs account for all factors in the PGM. We also propose a greedy heuristic for the choice of links used for belief update.Results for several UAI benchmark sets show that our method givesan accuracy that is better than or comparableto the existing variational and sampling based methods, with competitive runtimes. For the special case of BNs, we show that if the incremental build step in IBIA is performed in the topological order of variables then (a) the estimated partition function is guaranteed to be one if no evidence variables are present (b) the prior marginals of all variables are consistent across all CTFs in the sequence and (c) once all the evidence variables have been added to the SLCTF, the posterior marginals of variables in subsequent CTFs are consistent. Our results show that using the topological ordering for BNs leads to better estimates of partition function, prior marginals and posterior marginals in most benchmarks.
2306.15657v2	The Distortion of Binomial Voting Defies Expectation	In computational social choice, the distortion of a voting rule quantifies the degree to which the rule overcomes limited preference information to select a socially desirable outcome. This concept has been investigated extensively, but only through a worst-case lens. Instead, we study the expected distortion of voting rules with respect to an underlying distribution over voter utilities. Our main contribution is the design and analysis of a novel and intuitive rule, binomial voting, which provides strong distribution-independent guarantees for both expected distortion and expected welfare.	In an election, voters report their preferences by casting ballots. Under the ubiquitous plurality rule, each voter names a single alternative, whereas other rules — such as the badly namedranked-choice voting,111Also known as “instant-runoff voting” or “alternative vote.”whose adoption is rapidly expanding in the United States — require voters to rank the alternatives. However, even these ostensibly expressive ordinal ballots (whereby voters rank the alternatives) fail to capture voters’intensityof preference under truthful reporting. If voters could report utility functions that are comparable to each other, then we would want to select socially desirable alternatives with respect to these utilities, but it is typically impractical to expect voters to compute and report such utilities. This creates a tension between the limited information available to the voting rule (through the report of ordinal ballots only) and its goal (good outcomes with respect to latent cardinal utilities). A significant body of work in computational social choice aims to understand and alleviate this tension(Anshelevichet al.,2021). It revolves around the notion ofdistortion, defined as the worst-case ratio between the utilitariansocial welfare(sum of utilities) of the voting rule’s outcome and that of the welfare-maximizing alternative. The worst case is taken over rankings, which serve as input to the voting rule, and over utilities that are consistent with these rankings. As is often the case with worst-case analysis, however, the classic notion of distortion is arguably too conservative. In particular, nontrivial guarantees require restrictive assumptions (see Section1.2), and so this type of analysis may not help identify appealing voting rules. With this difficulty in mind, we focus onexpected distortion. Its definition includes the same ratio as before, and we are still interested in the worst case over rankings. However, we now take the conditional expectation over utilities consistent with the rankings, given an i.i.d. distribution over the utilities. Of course, it might not be realistic for such a distribution to be known a priori,222For this reason, we avoid calling this distribution a Bayesianprior.and thus we search for voting rules that aredistribution independent333This term is inspired by the literature on prior-independent mechanisms within mechanism design.: rules that choose the outcome in a way that does not depend on this distribution, even though their guarantees are stated in terms of this distribution. Overall, our goal is todesign distribution-independent voting rules that provide appealing expected distortion guarantees.We uncover novel, potentially practical, voting rules with these properties. We start by considering the important case of two alternatives (e.g., US presidential elections or yes/no decisions) in Section3. We show that, for any underlying distribution, themajorityrule optimizes both expected distortion and expected social welfare. This result suggests that maximizing expected social welfare may be a good approach for optimizing expected distortion. Indeed, our main result in Section4is that, under mild conditions on the underlying distribution and for a sufficiently large number of votersoralternatives, the expected-welfare-maximizing rule optimizes expected distortion almost perfectly. The expected-welfare-maximizing rule, however, is tailored to the underlying distribution and relies on intimate knowledge thereof. Our aim is therefore to approximately optimize expected welfare via a distribution-independent rule. In Section5, we design and analyze such a rule, which belongs to the family of (positional)scoring rules. Under this rule, which we callbinomial voting, each voter awards\sum_{\ell=k}^{m}\binom{m}{\ell}points to the alternative ranked in thekth position, wheremis the number of alternatives, and the alternative awarded the largest number of points overall is selected; note that this rule is distribution independent. Our main result of Section5is that for any underlying distribution supported on[0,1]with (largest) median\nu, binomial voting provides a multiplicative\frac{\nu}{2}-approximation to the optimal expected welfare. Combining this result with that of Section4, it follows that binomial voting gives almost the same\frac{\nu}{2}guarantee for expected distortion, when the number of voters or alternatives is sufficiently large. It is worth noting that binomial voting isnotan outlandish rule designed purely to achieve low expected distortion. On the contrary: as a positional voting rule, it inherits the desirable properties of this family. In fact, positional scoring rules are characterized by a number of natural axioms(Young,1975). Furthermore, we are aware of very few positional scoring rules that have received attention in their own right, as it is typically difficult to justify any specific choice of scores; in this sense, the expected distortion framework can be seen as a way of pinpointing particularly useful parameters. In summary, we identify binomial voting as an unusually attractive rule when viewed through the lens of expected distortion. The literature on (worst-case) distortion(Anshelevichet al.,2021)can generally be partitioned into two threads. In the first thread(Procaccia and Rosenschein,2006; Boutilieret al.,2015; Caragianniset al.,2017; Mandalet al.,2019; Ebadianet al.,2022), it is assumed that voters have normalized utilities, that is, for each voter, the sum of utilities is one. Even with this restrictive assumption, deterministic voting rules cannot give nontrivial distortion bounds(Caragianniset al.,2017), and the best possible distortion for randomized rules is\Theta(\nicefrac{{1}}{{\sqrt{m}}})(Ebadianet al.,2022). In the second thread(Anshelevich and Postl,2017; Grosset al.,2017; Anshelevichet al.,2018; Gkatzeliset al.,2020; Kizilkaya and Kempe,2022), known asmetric distortion, it is assumed that utilities (rather, costs) are induced by an underlying metric space. While some well-known voting rules have constant distortion in this setting(Anshelevichet al.,2018), the metric assumption is arguably difficult to justify in most domains of interest. By contrast, we make no assumptions on utilities. Our work is most closely related to that ofBoutilieret al.(2015). While their most substantial results pertain to worst-case distortion, one of their results deals with a distributional setting that can be seen as the starting point for our work. They show that the voting rule that maximizes expected social welfare is a scoring rule whose scores depend on the underlying distribution; we will revisit and build on this result. However, they do not study expected distortion, nor do they explore (in this context) voting rules that are agnostic to the distribution. Previous papers that analyze expected distortion include those ofChenget al.(2017,2018). However, their papers are fundamentally different. For one, they study metric distortion. More importantly, they focus on one intuitive but very specific distribution, where the positions of alternatives in the underlying metric space are drawn uniformly at random from the voter positions. By contrast, we study general (i.i.d.) distributions over utilities and design distribution-independent voting rules. The work ofGhodsiet al.(2019)is more distantly related: they also analyze expected distortion in the metric setting, assuming that voters abstain with some probability. By replacing worst-case analysis with expectation, our work introduces a Bayesian view canonical to economic theory into the literature on distortion. The Wilson doctrine(Wilson,1987)advocates for the use of mechanisms that require as little prior information as possible. Distribution-independent (sometimes called prior-independent) mechanisms are, through this lens, the most desirable ones as they require no prior information at all, and have been studied within economics(see, e.g., McAfee,1992; Segal,2003)as well as within computer science(see, e.g., Hartline and Roughgarden,2009; Devanuret al.,2011; Babaioffet al.,2018). Our work can also be seen as belonging to a recent push within computer science on “beyond worst-case” analysis of algorithms(Roughgarden,2021).
2401.06824v2	Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering	Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community. While existing jailbreaking methods primarily rely on prompt engineering, altering inputs to evade LLM safety mechanisms, they suffer from low attack success rates and significant time overheads, rendering them inflexible. To overcome these limitations, we propose a novel jailbreaking approach, named Jailbreaking LLMs through Representation Engineering (JRE). Our method requires only a small number of query pairs to extract ``safety patterns'' that can be used to circumvent the target model's defenses, achieving unprecedented jailbreaking performance. Building upon these findings, we also introduce a novel defense framework inspired by JRE principles, which demonstrates notable effectiveness. Extensive experimentation confirms the superior performance of the JRE attacks and the robustness of the JRE defense framework. We hope this study contributes to advancing the understanding of model safety issues through the lens of representation engineering.	Large language models (LLMs) excel in various tasks such as decision-making, question-answering, and dialogue systems due to their excellent language generation capabilities and a broad spectrum of world knowledgeAchiamet al.(2023); OpenAI (2023); Touvronet al.(2023); Chunget al.(2022). However, despite their ability to address a wide variety of user queries, LLMs have exhibited certain undesired behaviors, notably including the generation of biased, harmful, violent, and otherwise toxic responses to malicious queriesWeidingeret al.(2021); Goldsteinet al.(2023); Gehmanet al.(2020). Model developers have investigated some safety alignment strategiesOuyanget al.(2022); Baiet al.(2022)and red teaming processesPerezet al.(2022); Ganguliet al.(2022)to address this vulnerability. However, related risks still exist, among which the risk of “jailbreak” has attracted widespread concern and attention recently. During jailbreaking, malicious inputs somehow bypass the model’s safety mechanisms and induce the model to generate harmful and offensive outputs. Most of the jailbreak methods are based on prompt engineering (ProE). First came the manually crafted jailbreak prompt templates such as the famous “Grandma exploit111https://www.theverge.com/2023/4/19/23689249/your-favorite-new-chatbot-jailbreak-is-the-grandma-exploit” that asked ChatGPT222https://chat.openai.com/to pretend to be a deceased grandma to bypass its safety guardrails. Subsequently, ingenious manually crafted prompts like “DAN (Do-Anything-Now)”walkerspider (2022); Pryzantet al.(2023)were continuously found, capturing people’s attention with their effectivenessPerez and Ribeiro (2022). These interesting jailbreak prompts were collected and can be accessed from the website333https://www.jailbreakchat.com/. However, such manually crafted prompts are often hard to find with many try-and-error attempts, leading the researchers to explore ways to generate jailbreak prompts automatically, such as GCG, AutoDAN(a,b), ReNeLLM, and othersZouet al.(2023b); Liuet al.(2023); Zhuet al.(2023); Dinget al.(2023); Joneset al.(2023). Nevertheless, both manually-crafted and automatic ProE-based methods have their limitations. Manually crafted prompts are not only difficult to construct but also have poor jailbreak effects and lack scalability; while automated prompts eliminate the need for creativity, they also suffer from low attack success rates and require additional computational and time costs. To overcome the above limitations, we propose a novel jailbreak method, namely JRE, borrowing the principles from representation engineering (RepE). The RepE aims to analyze the interactions within the representation spaces formed by activation patterns generated by neurons in deep neural networks (DNNs).Zouet al.(2023a)identified the differences in the representation spaces of a model when receiving malicious versus benign instructions and leveraged the identified differences to influence the behavior of the model. Inspired by RepE, we propose the following hypothesis: The reason why LLMs generatedefensive responses(e.g. “I cannot fulfill your request. I’m just an AI…”) to malicious queries is that these malicious queries trigger the existence of“safety patterns”in the representation spaces of the models at each layer. In this hypothesis, the safety patterns can be embodied as latent feature representations. Put simply, LLMs have safety patterns just like a key, which can fully openPandora’s boxof the safety-aligned models, enabling it to generate harmful responses to any malicious queries. To validate this hypothesis, we initially constructed a dataset, denoted asJailEval, based upon MASTERKEYDenget al.(2023), comprising90pairs of malicious and benign queries.JailEvalenables us to mine the safety patterns inherent in a model, subsequently weakening these safety patterns at the model’s inference to successfully perform a jailbreak. Some intriguing cases are depicted in Figure1across various topics. Moreover, in line with this hypothesis, we introduce a novel defense framework against jailbreaking, which has proven effective by enhancing the safety patterns during the model’s inference stage, as opposed to weakening them. Our hypothesis is validated through extensive experimentation, affirming both its correctness and the efficacy of JRE. Our contributions can be succinctly summarized as follows: Diverging from ProE-based methodologies, we present a straightforward yet remarkably efficient jailbreak attack strategy grounded in RepE principles. Our approach not only incurs no additional time overhead but also attains state-of-the-art jailbreak performance. Expanding upon the aforementioned hypothesis, we introduce a novel and similarly effective jailbreak defense framework, thus propelling the current state of jailbreak defense research forward. Employing the RepE, we probe into the underlying mechanisms behind jailbreaking in LLMs. Subsequently, we validate an interpretable hypothesis, thereby advancing the thorough examination of model safety issues.
2309.17175v1	TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields	Recent works learn 3D representation explicitly under text-3D guidance. However, limited text-3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing open-vocabulary generation ability. To tackle this issue, we introduce a conditional 3D generative model, namely TextField3D. Specifically, rather than using the text prompts as input directly, we suggest to inject dynamic noise into the latent space of given text prompts, i.e., Noisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the appropriate range of textual latent space that is expanded by NTFs. To this end, an NTFGen module is proposed to model general text latent code in noisy fields. Meanwhile, an NTFBind module is proposed to align view-invariant image latent code to noisy fields, further supporting image-conditional 3D generation. To guide the conditional generation in both geometry and texture, multi-modal discrimination is constructed with a text-3D discriminator and a text-2.5D discriminator. Compared to previous methods, TextField3D includes three merits: 1) large vocabulary, 2) text consistency, and 3) low latency. Extensive experiments demonstrate that our method achieves a potential open-vocabulary 3D generation capability.	3D creative contents are in significantly increased demand for a wide range of applications, including video games, virtual reality, and robotic simulation. However, manually creating 3D content is a time-consuming process that requires a high level of expertise. To achieve automatic generation, previous works(Niemeyer and Geiger,2021; Guet al.,2021; Chanet al.,2022; Gaoet al.,2022)attempt on several 3D representations like Voxel, NeRF, and SDF. Nonetheless, objects from these methods are unconditionally generated in a single category, which can hardly be applied in practice. With the success of text-to-image generative models(Rameshet al.,2021,2022; Rombachet al.,2022), text prompts become a flexible variable to achieve open-vocabulary generative capability. Similar to 2D vision, text-to-3D generation has recently aroused great interest. Current research can be generally separated into two strategies. One(Jainet al.,2022; Mohammad Khalidet al.,2022; Pooleet al.,2022; Linet al.,2022)optimizes differentiable 3D representations with vision-language (V-L) pre-trained knowledge,e.g., DreamFields(Jainet al.,2022)is optimized from knowledge of the V-L pre-trained model CLIP(Radfordet al.,2021), and DreamFusion(Pooleet al.,2022)is guided by a diffusion model Imagen(Sahariaet al.,2022). Since no 3D data are involved in training, these methods can easily fall into artifacts, facing serious 3D consistency problems. Meanwhile, the cost of optimizing 3D representations is extremely high, in terms of both time and computational resources. The first strategy seems not to be the ultimate solution for 3D generation. The other(Nicholet al.,2022; Chenget al.,2023; Weiet al.,2023; Sanghiet al.,2023; Jun and Nichol,2023)directly supervises 3D generators with paired text-3D data. These methods allow real-time generation but are extremely restricted by the scale of available 3D data.Nicholet al.(2022)andJun and Nichol (2023)collect millions of text-3D pairs, which is still thousands of times smaller than the scale of V-L data. As such, the diversity of generated output and the complexity of text input are incompatible with V-L optimized methods. Therefore, here is the question: With limited 3D data, can we train a real-time generator that is equivalent to V-L optimized methods? The common practice to address data scarcity is introducing V-L pre-trained knowledge to generators. Specifically, 3D generative latent codes are initialized with pre-trained models.Chenget al.(2023)adopt BERT(Devlinet al.,2018)/CLIP(Radfordet al.,2021)encoders for text/image-condition tasks.Sanghiet al.(2023)propose to train a transformer module with CLIP image embedding and replace it with text embedding at inference.Weiet al.(2023)further attempt to align text prompts with rendered images to improve generative text control. However, one key problem remains challenging: how to map limited 3D content to comprehensive V-L pre-trained concepts. These methods align generative latent space based on limited categories and text prompts, resulting in a close set generation. To tackle this issue, we intend to expand the expression range of 3D latent space. We observe that 3D latent code can easily fall into stereotype concepts when trained in a small data scale with clean data and templated prompts. For example, a simple prompt “a chair” can represent any type of chair, tall or short, with or without armrests. However, if supervised on a specific type of chair, the generator may tend to produce this fixed shape only, losing diversity. Based on this observation, we introduce a conditional 3D generative model, dubbed TextField3D. TextField3D maps limited 3D data to dynamic fields of V-L concepts, which are named as Noisy Text Fields (NTFs). Specifically, we assume that there is a dynamic range in the latent mapping of text-to-3D, which can be formulated as an injected noise to the text embeddings. We thus propose an NTFGen module, where text latent code is formulated based on the field of noisy text embeddings. To support image conditional generation, we further propose an NTFBind module for binding image features to NTFs and keeping the consistency of multi-view conditions. A view-invariant image latent code is derived for the image-to-3D task. Furthermore, multi-modal discrimination is constructed for the supervision of 3D generation, in which we propose a text-3D discriminator to guide geometry generation and a text-2.5D discriminator to refine texture details. Compared to previous methods, TextField3D simultaneously possesses three advantages: large vocabulary, text consistency, and low latency. Extensive experiments are conducted on various categories and complicated text prompts, which exhibit the open-vocabulary potential of our model. We further employ several metrics to evaluate the generation quality and text consistency, demonstrating the effectiveness of our newly proposed modules. Our contributions can be summarised as follows: We introduce a conditional 3D generative model TextField3D, in which limited 3D data is mapped to textual fields with dynamic noise, namely Noisy Text Fields (NTFs). We propose NTFGen and NTFBind to manipulate general latent codes for conditional generation, and a multi-modal discriminator to guide the generation of geometry and texture. Extensive experiments present an open-vocabulary potential of our proposed method, in terms of large vocabulary, text consistency, and low latency.
2308.04719v1	JiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games	This paper presents an empirical exploration of non-transitivity in perfect-information games, specifically focusing on Xiangqi, a traditional Chinese board game comparable in game-tree complexity to chess and shogi. By analyzing over 10,000 records of human Xiangqi play, we highlight the existence of both transitive and non-transitive elements within the game's strategic structure. To address non-transitivity, we introduce the JiangJun algorithm, an innovative combination of Monte-Carlo Tree Search (MCTS) and Policy Space Response Oracles (PSRO) designed to approximate a Nash equilibrium. We evaluate the algorithm empirically using a WeChat mini program and achieve a Master level with a 99.41\% win rate against human players. The algorithm's effectiveness in overcoming non-transitivity is confirmed by a plethora of metrics, such as relative population performance and visualization results. Our project site is available at \url{https://sites.google.com/view/jiangjun-site/}.	"Multi-Agent Reinforcement Learning (MARL) has demonstrated remarkable success in various games such as Hide and Seek(Bakeret al.,2019), Go(Silveret al.,2016b), StarCraft II(Vinyalset al.,2019), Dota 2(Berneret al.,2019), and Stratego(Perolatet al.,2022). However, algorithms such as AlphaZero(Silveret al.,2018)and AlphaGo(Silveret al.,2016b)that train against the most recent opponent can potentially cycle in games that have non-transitive structure. While this problem has been well-studied in imperfect-information games(Lanctotet al.,2017; Perolatet al.,2022; McAleeret al.,2022a,c,2021; Fuet al.,2022; Brownet al.,2019; Heinrich and Silver,2016; Steinbergeret al.,2020; Perolatet al.,2021; Henneset al.,2019), it has been less studied in perfect-information games. Conquering non-transitivity in perfect-information games remains an open research direction. Recent works(Balduzziet al.,2019a; McAleeret al.,2020; Liuet al.,2021; McAleeret al.,2022c,b)have focused on finding (approximate) Nash equilibria using the Policy Space Response Oracles (PSRO) algorithm(Lanctotet al.,2017), which is developed from the Double Oracle (DO) algorithm(McMahanet al.,2003). These approaches deal with non-transitivity by mixing over a population of policies, but have to our knowledge not been studied in perfect-information games. This study aims to investigate Xiangqi, a prevalent two-player zero-sum game with moderate game-tree complexity of10^{150}(between Chess (10^{128}game-tree complexity) and Go (10^{360}game-tree complexity)), which has not received much attention in previous research. Although Xiangqi’s complexity presents a challenge, it is not an insurmountable obstacle, and its accessibility makes it an excellent candidate for exploring the geometrical landscape of board games and non-transitivity. In this study, we delve into the intricate geometry of Xiangqi, leveraging a dataset comprising over 10,000 game records from human gameplay as the foundational basis for our investigation. Our findings unveil the existence of a spinning top structure embedded within the game dynamics, and highlight the manifestation of non-transitivity within the mid-range ELO rating scores. By implementing self-play training on policy checkpoints, we further uncover strategic cycles that empirically substantiate the presence of non-transitivity in Xiangqi. Motivated by these insights, we propose the JiangJun algorithm, specifically designed to mitigate the non-transitive challenge observed in Xiangqi. This algorithm incorporates two fundamental modules: theMCTS actorandPopulationer. Together, these components approximate Nash equilibria within the player population, employing Monte Carlo Tree Search (MCTS) techniques. A thorough examination of the computational complexity associated with the JiangJun algorithm is also presented in this paper. Specifically, the worst-case time complexity of the MCTS actor is defined as{\mathcal{O}}(b^{d}\times n\times C), whereb, the effective branching factor of Xiangqi, falls within a median range of 30 to 80, the game tree depthdextends beyond 40, the typical number of simulationsnis set to 800, andCdenotes the time required for neural network inference. Additionally, if the Simplex method is utilized, the Nash Solver corresponds to a worst-case time complexity ofO(2^{k}\times poly(k)), withpoly(k)signifying a polynomial function in terms ofk. The efficacy of the JiangJun algorithm is thoroughly appraised via an expansive set of metrics in this study. The training of the JiangJun algorithm to the ""Master"" level was facilitated by our proposed training framework that effectively utilizes the computational capabilities of up to 90 V100 GPUs on the Huawei Cloud ModelArt platform. Initially, a diverse range of indicators, including relative population performance, Nash distribution visualization, and the low-dimensional gamescape visualization of the primary two embedding dimensions, collectively corroborate the proficiency of JiangJun in addressing the non-transitivity issue inherent in Xiangqi. In addition, JiangJun significantly surpasses its contemporary algorithms—standard AlphaZero Xiangqi and behavior clone Xiangqi—demonstrating winning probabilities exceeding 85% and 96.40% respectively. Furthermore, upon evaluation of exploitability, JiangJun (8.41% win rate of the approximate best response) was found to be appreciably closer to the optimal strategy in contrast to the standard AlphaZero Xiangqi algorithm (25.53%). Additionally, we devised and implemented a Xiangqi mini-program on the WeChat platform, which, over a span of six months, compiled more than 7,000 game records from matches played between JiangJun and human opponents. The data generated from these matches yielded an extraordinary 99.41% win rate for JiangJun against human players post-training, thereby underscoring its formidable strength. Lastly, through an in-depth case study of various endgame scenarios, the capacity of JiangJun to astutely navigate the intricacies of the Xiangqi endgame is exhibited. In summary, this paper presents three key contributions: 1) an analysis of the geometrical landscape of Xiangqi using over 10,000 real game records, uncovering a spinning top structure and non-transitivity in real-world games; 2) the proposal of the JiangJun algorithm to conquer non-transitivity and master Xiangqi; and 3) the empirical evaluation of JiangJun with human players using a WeChat mini program, achieving over a 99.41% win rate and demonstrating the efficient overcoming of non-transitivity as shown by other metrics such as relative population performance. The remaining sections of this paper are organized as follows. Section2summarizes related work in the field. In Section3, we give a brief introduction to the Xiangqi game. Section4presents our non-transitive analysis method and the results of real-world Xiangqi data analysis. Next, in Section5, we introduce our proposed JiangJun algorithm, while Section6presents the experimental results. Finally, we draw our conclusions in Section7. Additional details pertinent to this paper can be found in AppendixJiangJun: Mastering Xiangqi by Tackling Non-Transitivity in Two-Player Zero-Sum Games."
2305.10847v5	Large Language Models can be Guided to Evade AI-Generated Text Detection	Large language models (LLMs) have shown remarkable performance in various tasks and have been extensively utilized by the public. However, the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods. In this study, we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of these detectors. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically construct prompts for evading the detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has been constructed, it can be universally used against a wide range of detectors. Extensive experiments across three real-world tasks demonstrate that SICO significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation as well as a validation experiment in the wild show that the SICO-generated text achieves human-level readability and task completion rates. Finally, the strong performance of SICO exhibits its potential as a reliable evaluation tool for future detectors. The codes and data are located on https://github.com/ColinLu50/Evade-GPT-Detector.	The rapid advancement of large language models (LLMs), such as GPT(Brownet al.,2020), PaLM(Chowdheryet al.,2022), and LLaMa(Touvronet al.,2023), has led to a largely-increased capacity for generating high-quality human-like text. However, there are also growing concerns surrounding the misuse of these models, including generating fake product reviews(Adelaniet al.,2020; Linet al.,2022)and misinformation(Linet al.,2022), enabling academic dishonesty(Stokel-Walker,2022), and producing misleading answers on websites(StackOverflow,2023). In response to these challenges, several methods for detecting AI-generated text have been proposed recently, ranging from fine-tuned classifiers(Guoet al.,2023; Solaimanet al.,2019), statistical methods(Mitchellet al.,2023), to watermarking(Kirchenbaueret al.,2023). There are also online detection services provided by companies such as GPTzero(Tian,2023). However, the robustness of these detection methods has not been thoroughly evaluated. Recent studies(Krishnaet al.,2023; Sadasivanet al.,2023)have shown the vulnerability of these detectors to the so-calledparaphrase attacks, which adopt an external paraphraser to rewrite the text generated by LLMs to evade detectors. In this work, rather than relying on an external paraphraser, we explore equipping LLMs with carefully constructed prompts to evade detectors. The intuition is that, given the remarkable capabilities of LLMs, appropriate prompts can guide these models to potentially achieve and even exceed the evasion performance level of smaller external paraphrasers. We proposeSICO, aSubstitution-basedIn-Context exampleOptimization method, to automatically construct such prompts based on human-generated examples. Specifically, SICO iteratively substitutes words and sentences within the in-context examples to provide more representative demonstrations for LLMs to generate text that cannot be detected, where the substitution procedure is directed by a proxy detector (see Figure1for an overview of SICO). We assess the evasion performance of SICO across three real-world tasks that are susceptible to the misuse of LLMs, i.e., academic essay writing, open-ended question answering, and fake review generation. The results demonstrate that SICO consistently outperforms the paraphraser baselines, leading to a decrease in AUC by approximately 0.5 on average for six existing detectors. Additionally, a comprehensive human evaluation involving 600 examples shows that the SICO-generated text is comparable to, and in some cases even better than, human-written text in terms of readability and task completion rates. To further evaluate the practical utility of SICO, we deploy it on Reddit, an online social platform, to generate responses for users’ questions. The high percentage of generated responses that are liked by Reddit users shows that SICO is capable of generating human-approved content while being barely identified as AI. In addition to its strong evasion performance, SICO is also cost-efficient and easy to use. Unlike paraphraser-based methods that often require extensive computational resources – as evidenced by the fine-tuning of a 13B model on a large dataset(Krishnaet al.,2023)– SICO only requires 40 human-generated examples and a limited number of LLM inferences (e.g., costing approximately 1 USD using the GPT-3.5 API). Besides, once a task-specific prompt has been constructed by SICO, it can be universally used against a wide range of detectors. Considering the importance of detecting AI-generated text to avoid their misuse, the results presented in this work certainly reveal the vulnerability of the existing detectors. Besides, this work presents the first empirical evidence that LLMs can evade detectors through a prompt-guided approach. Finally, the strong evasion performance of SICO suggests that it can be used as a standard evaluation tool for any future AI-generated text detectors. We hope that these findings can better facilitate the research concerning the responsible use of LLMs. To summarize, our main contributions are: We introduce SICO, a novel in-context example learning method, to automatically construct prompts that can guide LLMs to evade detectors. With low cost, SICO achieves strong performance in evading six existing detectors across three tasks, significantly outperforming the paraphraser baselines. A comprehensive human evaluation, as well as a validation experiment in the wild, verifies that the SICO-generated text achieves human-level readability and task completion rates.
2311.03365v1	Leveraging Generative AI: Improving Software Metadata Classification with Generated Code-Comment Pairs	"In software development, code comments play a crucial role in enhancing code comprehension and collaboration. This research paper addresses the challenge of objectively classifying code comments as ""Useful"" or ""Not Useful."" We propose a novel solution that harnesses contextualized embeddings, particularly BERT, to automate this classification process. We address this task by incorporating generated code and comment pairs. The initial dataset comprised 9048 pairs of code and comments written in C, labeled as either Useful or Not Useful. To augment this dataset, we sourced an additional 739 lines of code-comment pairs and generated labels using a Large Language Model Architecture, specifically BERT. The primary objective was to build classification models that can effectively differentiate between useful and not useful code comments. Various machine learning algorithms were employed, including Logistic Regression, Decision Tree, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Gradient Boosting, Random Forest, and a Neural Network. Each algorithm was evaluated using precision, recall, and F1-score metrics, both with the original seed dataset and the augmented dataset. This study showcases the potential of generative AI for enhancing binary code comment quality classification models, providing valuable insights for software developers and researchers in the field of natural language processing and software engineering."	"Within the realm of software development, code comments assume a fundamental role as crucial documentation artifacts[paper14]. These succinct notations offer indispensable insights, explanations, and contextual information, significantly augmenting code comprehension, reducing debugging complexity, and promoting effective collaboration among development teams[paper2]. The enduring relevance of code comments in software engineering is undeniable; however, the objective evaluation of their utility remains a complex and subjective undertaking[paper7]. Code comment classification, a subfield entrenched within natural language processing, has emerged as a transformative methodology for impartially categorizing code comments as either ""Useful"" or ""Not Useful""[paper4]. It presents a paradigm shift in the landscape of software engineering, promising to refine code review processes, align development efforts more effectively, and elevate overall software quality[paper5]. This approach, centered on automating comment assessments, is poised to streamline workflows and mitigate subjective discrepancies[paper8]. The challenges inherent in comment classification are multifaceted[paper8]. Traditional practices, reliant on manual interpretation, introduce subjectivity, leading to inconsistencies and operational inefficiencies[paper9]. This is where Large Language Models (LLMs), exemplified by BERT (Bidirectional Encoder Representations from Transformers), assume prominence, revolutionizing the discourse on comment classification[paper10]. Equipped with advanced linguistic acumen, these LLMs hold the potential to offer objective, context-aware comment evaluations[paper11]. The advent of LLMs signifies a pivotal transformation in the methodology of code comment analysis and utilization[paper10]. These models excel in contextualizing language, rendering them eminently suitable for tasks necessitating nuanced comprehension[paper11]. In the context of code comment classification, LLMs have the potential to furnish more precise and consistent evaluations, transcending the constraints of manual judgment[paper6]. This research embarks on an exhaustive exploration, elucidating the intricate relationship between comment classification and LLMs[paper5]. Its principal objective lies in assessing the comparative efficacy of LLMs, endowed with inherent linguistic proficiencies, vis-à-vis conventional machine learning algorithms, in the context of code comment categorization[paper9]. Moreover, it investigates the prospect of augmenting manually curated seed data with LLM-generated data, a stratagem aimed at enhancing the quality of classification outcomes[paper13]. The research encompasses an array of classification models, spanning traditional algorithms and neural networks, subjected to comprehensive evaluation through an assortment of performance metrics[paper8]. Precision, recall, and the F1 score constitute the bedrock of quantitative insights[paper9]. The outcomes of this research illuminate the effectiveness of diverse models in code comment classification, accentuating the transformative potential of LLMs in this domain[paper4]. As subsequent sections unfold, the comprehensive analysis of results and their implications for software development practitioners and researchers come to the fore. In the ever-evolving landscape of code comment assessment, this research elucidates a promising future wherein the symbiosis of comment classification and LLMs stands at the vanguard of innovation[paper6]."
2302.03807v2	A Prototype-Oriented Clustering for Domain Shift with Source Privacy	Unsupervised clustering under domain shift (UCDS) studies how to transfer the knowledge from abundant unlabeled data from multiple source domains to learn the representation of the unlabeled data in a target domain. In this paper, we introduce Prototype-oriented Clustering with Distillation (PCD) to not only improve the performance and applicability of existing methods for UCDS, but also address the concerns on protecting the privacy of both the data and model of the source domains. PCD first constructs a source clustering model by aligning the distributions of prototypes and data. It then distills the knowledge to the target model through cluster labels provided by the source model while simultaneously clustering the target data. Finally, it refines the target model on the target domain data without guidance from the source model. Experiments across multiple benchmarks show the effectiveness and generalizability of our source-private clustering method.	Supervised learning methods require a tremendous amount of labeled data, limiting their use cases in many situations(adadi2021survey). By contrast, unsupervised clustering seeks to group similar data points into clusters without labels(hartigan1972direct). Clustering has become one of the most popular methods in various applications, such as computer vision(coleman1979image;lei2018superpixel;liu2021fusedream;mittal2021comprehensive), natural language processing(biemann2006chinese;yoon2019compare), reinforcement learning(mannor2004dynamic;xu2014clustering;ahmadi2021dqre), and multi-modal learning(hu2019deep;chen2021multimodal). In many of these applications, data naturally come from multiple sources and may not contain labels since they are expensive to acquire(girshick2014rich;lin2014microsoft). As an example, medical institutions collaborate to achieve a large and diverse dataset(mojab2020real). However, this partnership faces privacy and ownership challenges(sheller2020federated). Across different domains, users may also have varying amounts of resources and data(salehi2019dynamic). Another example is the inference-as-a-service paradigm, a business scheme where providers serve models trained on multiple sources of data as APIs (e.g., Google AI platforms, Amazon Web Services, GPT-3(brown2020language)) without giving clients direct access to them. To exploit the rich data from multiple domains for limited-data-and-resource users while also taking into account privacy challenges, one may consider applying methods from Unsupervised Domain Adaptation (UDA)(shimodaira2000improving;farhadi2008learning;saenko2010adapting). These methods nonetheless require labeled data in the source domains, making them not applicable in many scenarios. To overcome the assumption of UDA,menapace2020learninghave recently introduced Unsupervised Clustering under Domain Shift (UCDS), a learning scenario where both the source and target domains have no labels. The goal of this problem setting is to transfer the knowledge from the abundant unlabeled data from multiple source domains to a target domain with limited data. To solve this problem,menapace2020learningpropose Adaptive Clustering of Images under Domain Shift (ACIDS), a method that uses an information-theoretic loss(ji2019invariant)for clustering and batch normalization alignment(li2016revisiting)for target adaptation. However, it has two major drawbacks. First, it assumes that we have full access to the source model parameters to initialize the target model before clustering, limiting its use in privacy-sensitive situations where access to the source model is restricted. Second, it requires batch normalization, a specific architectural design of the source model that may not be applicable in some recently proposed state-of-the-art models such as Vision Transformer(dosovitskiy2020image). In this paper, we consider a more practical problem that is a variant of UCDS (see Table1): in addition to the data privacy, we also consider model privacy. Target data owners have no direct access to the source model but can query it to obtain cluster labels during target adaptation. This requirement is important because, given full access to the model, target users or other adversaries may exploit it to recover the source data, jeopardizing source data privacychen2019data;luo2020large. To address this important and challenging problem, we propose Prototype-oriented Clustering with Distillation (PCD), a holistic method that consists of three stages. First, we construct a source clustering model from multiple-domain data. To achieve this, we use optimal transport(kantorovich2006translocation;COTFNT)to align the distributions of data and prototypes, as well as a mutual-information maximization to assist the learning of the feature encoder and prototypes(krause2010discriminative;shi2012information;liang2020we). Second, we use the target cluster assignments provided by the source model to distill the knowledge to the target model while simultaneously clustering the target data. Finally, we perform clustering on the target data alone to further refine the target model. Figure1illustrates the schematic diagram of our approach. PCD achieves the following benefits. Our approach can be directly applied to the inference-as-a-service paradigm, which is becoming increasingly popular(soifer2019deep). Many providers currently serve users with API services without sharing direct access to their models. Our method also protects the privacy of both the data and model in the source domains, which is especially critical in practical applications such as healthcare. Moreover, we no longer require the source and target models to share the same architecture, allowing for more flexibility in the training process. Unlike source data owners, target users may have limited resources and cannot afford to train large models. Our main contributions include:1)We propose a generalized approach for tackling the problem of data-and-model private unsupervised clustering under domain shift. PCD integrates a prototype-oriented clustering algorithm and knowledge distillation into a unified method. Our clustering algorithm synergistically combines optimal transport with the mutual-information objective for prototype and data alignment.2)We verify the effectiveness and general applicability of the proposed method in practical settings: model transfer as well as limited-data and cluster-imbalanced scenarios.3)We provide comprehensive study and experiments on multiple datasets and demonstrate consistent gains over the baselines.
2302.13959v2	Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets	Increasingly larger datasets have become a standard ingredient to advancing the state-of-the-art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to apply, because of the multifacetedness of the harmful data and elusiveness of filtering rules that would generalize across multiple tasks. We study the fitness of task-agnostic self-influence scores of training examples for data cleaning, analyze their efficacy in capturing naturally occurring outliers, and investigate to what extent self-influence based data cleaning can improve downstream performance in machine translation, question answering and text classification, building up on recent approaches to self-influence calculation and automated curriculum learning.	Deep learning on increasingly larger and diverse data sources brought impressive advances in natural language processing (NLP), however, data quality might be the major bottleneck to unlock further gains(Kumaret al.,2020). NLP data are usually acquired via large-scale weakly-labeled data scraping or crowd-sourcing labels from non-expert human annotators, which are both error-prone(Bowman and Dahl,2021). At the same time, ambiguous training data are also known to hurt models’ performance through overfitting or memorization in overparameterized networks(Zhanget al.,2017). Finally, not all data are equally easy to learn and overly complex instances may hinder learning as well. Below, we refer to all of those cases – label noise, out-of-distribution, ambiguous or difficult-to-learn examples – by an umbrella termoutliers. Two key questions of our work are: How can outliers be detected and how should they be dealt with? Defining outliers and how they may (harmfully) influence model predictions in a task-agnostic way is hard and so, until recently, mostly task-dependent heuristics have been employedWanget al.(2018). More principled approaches define an impact of a training instance via the concept ofinfluence functions(henceforth IFs)(Cook and Weisberg,1980), which quantify the effect on the loss on a test pointzwhen removing an individual training pointx. For example,Koh and Liang (2017)used access to gradients and their fast products with the loss HessianPearlmutter (1994)to approximate the loss change atzthat would occur hadxbeen infinitesimally upweighted in the training set. IFs have been used for debugging of machine learning models(Hanet al.,2020), data poisoning attacks(Kohet al.,2022)and detecting dataset errors(Schioppaet al.,2021; Konget al.,2022). There, it has been conjectured and empirically tested that filtering highlyself-influential(z=x) points, i.e., the ones that would cause a large loss delta on themselves (suggesting that they are “unsupported” by other data points and need to be memorized), does lead to improvements in synthetic and real scenarios. To deepen this line of work, we also operationalize IFs to detect outliers with self-influence scores, and formulate our first research question being: RQ1: When are self-influence scores effective for detecting outliers? The above improvements, however, contrast with the observations that IFs are sensitive to model and training hyperparameters in the general,z\neq x, case due to violation of the convexity assumption by IFs in deep learning:Basuet al.(2021)showed that depth and width of the network, its architecture, training regularization and the stochastic approximations inside IF have strong effects on the IF accuracy and stability (measured on retrieved influentialxs for a fixedz), which are aggravated with the network size.K and Søgaard (2021)further found that IFs are sensitive to the parameter initialization, ordering of the training data and batch size. Both papers thus doubted that IF scores of training instances would be reliable for practical purposes, and that retraining after removing or fixing the flagged instances would lead to improvements. Very recentlySchioppaet al.(2023)gave a theoretical perspective on IFs instability. Since, at a minimum, for self-influence to point at outliers in any objective and verifiable sense, they should exhibit certain stability, this leads us to the second research question of our work: RQ2: How stable are self-influence scores? Unlike general influence, the self-influence stability has not been covered by previous studies. A standard approach to reducing the harms caused by outliers is to filter them out(Khayrallah and Koehn,2018; Peskovet al.,2019). However, coming up with a filtering rule that would generalize across multiple tasks is not straightforward. Most scalar-based (incl. self-influence) filtering schemes would prescribe setting a threshold cut-off value to delineate outliers from the rest data. This may be reasonable for datasets where the to-be-filtered data portion has no apparent signal, however, in more realistic scenarios many outliers are at least somewhat useful. Applying threshold filtering in such situation may lead to performance decrease as a portion of useful signal would be lost. For example, memorizingsomeoutliers (e.g. under-represented training examples) can in fact improve accuracy(Feldman and Zhang,2020). Motivated by this shortcoming, in this paper we explore alternatives to filtering where it is possible to make use of outliers and in particular considerautomated curriculum learning(AutoCL). AutoCL covers a range of algorithms, where not only the training data are presented to a neural network in a different order than random sampling, but also where this order is adapted alongside main training based on learning progressGraveset al.(2017); Kreutzeret al.(2021). This is particularly useful when dealing with outliers, as we can learn (via the self-influence proxy) to ignore the outlying data samples and prioritize the most helpful ones, without having to choose apriori the cut-off threshold. Thus, the final research question we address is:RQ3: Does AutoCL with self-influence scores bring gains compared to filtering? We study the stability ofself-influence scores, which are task-agnostic and, if stable, would be an attractive candidate to serve as the data cleaning foundation. We further analyze the efficacy of capturing naturally occurring outliers by IFs and investigate to what extent self-influence can improve performance in NLP tasks with Transformer architectures, building up on recent improvements in IF approximation accuracy and scalability with the Arnoldi iteration based IFs (ABIF)(Schioppaet al.,2021), and AutoCL(Kreutzeret al.,2021). In more detail, our contributions are: [leftmargin=*] Stability of self-influence scores.We start by measuring how stable self-influence scores are, since this is a prerequisite for both successful data filtering and data scheduling. To this end, in §3, we study correlation and overlap of data rankings by self-influence scores across different model states, i.e., different final states the training converges to as a function of varying batch size, random seeds of data sampling, data ordering and IF calculation. We also explore the correlation between model prediction stability (defined below asmodel churn) and IF’s sensitivity to architecture. We find that, unlike the general (z\neq x) IF scores, the self-influence (z=x) scoresarestable with respect to training and model hyperparameters, and across architecture variations, but care should be exercised in transferring findings between architectures of different capacity. Effectiveness of self-influence scores.In §4, we employ a suite of different in-distribution and out-of-distribution (o.o.d. ) evaluation setups and show that filtering out highly self-influential examples is more effective for the o.o.d. setup. We hypothesize that self-influence capturing general outliers prevents learning systematic noise patterns that would otherwise artificially inflate performance in the in-distribution evaluation setup, making it harder to improve upon with filtering. Furthermore, we investigate what is captured by influence scores using both natural outliers and synthetic label noise, showing that natural data can be spread among high and low influential samples, thus the common top-X% filtering strategies can be ineffective. Data filtering automation.The fixed percentage filtering can also be costly to tune or inaccurate, while attempts to automate it using abrupt changes at the top of the ranking have a low recall rate(Lamet al.,2022). To remedy, in §5we employ bandit AutoCL to dynamically detect, during training, the harmful or the useful training data quantiles to feed from at each step. The possible bandit actions are derived from the self-influence ranking and further split into a fixed number of discrete buckets. As a result, AutoCL adjusts on-the-fly the ratio of high or low influence examples to train on. This is more general than threshold filtering, which is a particular (static and hard-weighted) case of general (dynamic and soft-weighted) schedules.
2308.08982v1	Evaluation of really good grammatical error correction	Although rarely stated, in practice, Grammatical Error Correction (GEC) encompasses various models with distinct objectives, ranging from grammatical error detection to improving fluency. Traditional evaluation methods fail to fully capture the full range of system capabilities and objectives. Reference-based evaluations suffer from limitations in capturing the wide variety of possible correction and the biases introduced during reference creation and is prone to favor fixing local errors over overall text improvement. The emergence of large language models (LLMs) has further highlighted the shortcomings of these evaluation strategies, emphasizing the need for a paradigm shift in evaluation methodology. In the current study, we perform a comprehensive evaluation of various GEC systems using a recently published dataset of Swedish learner texts. The evaluation is performed using established evaluation metrics as well as human judges. We find that GPT-3 in a few-shot setting by far outperforms previous grammatical error correction systems for Swedish, a language comprising only 0.11% of its training data. We also found that current evaluation methods contain undesirable biases that a human evaluation is able to reveal. We suggest using human post-editing of GEC system outputs to analyze the amount of change required to reach native-level human performance on the task, and provide a dataset annotated with human post-edits and assessments of grammaticality, fluency and meaning preservation of GEC system outputs.	Grammatical Error Correction (GEC) is typically used in an extended sense of correcting language at multiple levels, including spelling errors, grammatical errors, word choice and idiom usage. In the literature on evaluating GEC systems, one is rarely explicit about the purpose of the system. FollowingSakaguchiet al.(2016), we see two somewhat different objectives: Error detection and correction, where grammaticality has priority over fluency. The goal is to point out individual language errors, which could ideally be fixed one by one, resulting in an acceptable text that is as close as possible to the original. General text improvement, where fluency is on equal footing with grammaticality. The goal is to produce a text which is as close as possible to what a highly proficient writer would have produced, assuming a perfect understanding of the intended message of the original text. The distinction between the two objectives is less clear for writers at high proficiency levels, where changing an occasional spelling or grammar mistake typically results in a high-quality text. For a less proficient writer, a text may contain so many overlapping problems that it is difficult to identify local changes that together result in a high-quality text. If a GEC system is allowed to work directly at the level of general text improvement, its task may become significantly simpler. The choice of objective has practical implications for how to evaluate the result. Traditional methods for GEC evaluation are reference-based, where either the GEC system output is compared to a human-created reference(e.g. Napoleset al.,2015), or the sets of edit operations produced by the GEC system is compared to those needed to transform the original text to the human reference(Bryantet al.,2017). One important problem with reference-based evaluations is that there is typically a large and varied set of possible ways to express the same information. It is generally infeasible to approximate the full set of possibilities, although providing multiple references is a common approach in the machine translation community to alleviate this problem(e.g. Qin and Specia,2015). Results are also highly dependent on the way the references were created.Freitaget al.(2020)show that biases due to “translationese” effects in the creation of references negatively affect the accuracy of the resulting evaluations, where interference from the source language may affect the translation to become less idiomatic in the target language. They obtained higher agreement between the automatic reference-based evaluations and human judgments by first asking human annotators to maximally paraphrase the reference sentences, to encourage diversity among the multiple references. The references used for GEC evaluations suffer from the same bias, and often annotators are explicitly instructed to stay as close to the original text as possible(Volodinaet al.,2019, Section 6.1). We are aware of no GEC evaluation data which, in the style ofFreitaget al.(2020), aims for a high amount of diversity in the references. This has the effect of biasing existing automatic evaluations against systems that perform paraphrasing rather than conservatively fixing individual errors. In the related field of text summarization,Goyalet al.(2022)found that automatic evaluation metrics severely underestimate the performance of large language models, further strengthening our suspicion that such powerful models necessitate a paradigm shift in evaluation methodology. In this work, we perform a comprehensive manual analysis of the output of multiple GEC systems, and point towards analysis of human post-edits as the most promising way of evaluating really good GEC systems.
2312.03940v2	PECANN: Parallel Efficient Clustering with Graph-Based Approximate Nearest Neighbor Search	This paper studies density-based clustering of point sets. These methods use dense regions of points to detect clusters of arbitrary shapes. In particular, we study variants of density peaks clustering, a popular type of algorithm that has been shown to work well in practice. Our goal is to cluster large high-dimensional datasets, which are prevalent in practice. Prior solutions are either sequential, and cannot scale to large data, or are specialized for low-dimensional data.   This paper unifies the different variants of density peaks clustering into a single framework, PECANN, by abstracting out several key steps common to this class of algorithms. One such key step is to find nearest neighbors that satisfy a predicate function, and one of the main contributions of this paper is an efficient way to do this predicate search using graph-based approximate nearest neighbor search (ANNS). To provide ample parallelism, we propose a doubling search technique that enables points to find an approximate nearest neighbor satisfying the predicate in a small number of rounds. Our technique can be applied to many existing graph-based ANNS algorithms, which can all be plugged into PECANN.   We implement five clustering algorithms with PECANN and evaluate them on synthetic and real-world datasets with up to 1.28 million points and up to 1024 dimensions on a 30-core machine with two-way hyper-threading. Compared to the state-of-the-art FASTDP algorithm for high-dimensional density peaks clustering, which is sequential, our best algorithm is 45x-734x faster while achieving competitive ARI scores. Compared to the state-of-the-art parallel DPC-based algorithm, which is optimized for low dimensions, we show that PECANN is two orders of magnitude faster. As far as we know, our work is the first to evaluate DPC variants on large high-dimensional real-world image and text embedding datasets.	Clustering is the task of grouping similar objects into clusters and is a fundamental task in data analysis and unsupervised machine learning(Jain1999;Aggarwal2013;berkhin2006survey). For example, clustering algorithms can be used to identify different types of tissues in medical imaging(Yang02), analyze social networks(MishraSST07), and identify weather regimes in climatology(Coe21). They are also widely used as a data processing subroutine in other machine learning tasks(Coleman79;Wu22;Lin19;Marco13). One popular type of clustering is density-based clustering, where clusters are defined as dense regions of points in space. Recently, density-based clustering algorithms have received a lot of attention(Ester96;Agrawal98;Ankerst99;Januzaj04;Rodriguez14;Wang97;Hinneburg98;Hanmanthu18;Sheikholeslami00)because they can discover clusters of arbitrary shapes and detect outliers (unlike popular algorithms such ask-means, which can only detect spherical clusters). Density peaks clustering (DPC)(Rodriguez14)is a popular density-based clustering technique for spatial data (i.e., point sets) that has proven very effective at clustering challenging datasets with non-spherical clusters. Due to DPC’s success, many DPC variants have been proposed in the literature (e.g.,(sddp;chen2020fast;sieranoja2019fast;yaohui2017adaptive;sun2021density;xie2016robust;yin2022improved;du2018density;su2018bpec;hou2019enhancing;geng2018recome)). However, existing DPC variants are sequential and/or tailored to low-dimensional data, and so cannot scale to the large, high-dimensional datasets that are common in practice. This paper addresses this gap by proposing a novel framework called PECANN:ParallelEfficientClustering withApproximateNearestNeighbors. PECANN contains implementations for a variety of different DPC density techniques that both scale to large datasets (via efficient parallel implementations) and run on high dimensional data (via approximate nearest neighbor search). Before going into more details on our contributions, we review the main steps of DPC variants and discuss existing bottlenecks. The three key steps of DPC variants are as follows:{mdframed} Compute the density of each pointx. Construct a tree by connecting each pointxto its closest neighbor with higher density thanx. Remove edges in the tree according to a pruning heuristic. Each resulting connected component is a separate cluster. Stepais computed differently based on the variant, but all variants use a function that depends on either thek-nearest neighbors ofxor the points within a given distance fromx. Efficient implementations of this step rely on nearest neighbor queries or range queries. In low dimensions, these queries can be answered efficiently using spatial trees, such askd-trees. However,kd-trees are inefficient in high dimensions due to the curse of dimensionality(weber1998quantitative). Stepbagain requires finding nearest neighbors, but with the constraint that only neighbors with higher density are considered. Stepccan easily be computed using any connected components algorithm. Stepsaandbform the bottleneck of the computation, and take quadratic work in the worst case, while Stepccan be done in (near) linear work. Note that different clusterings can be generated by reusing the tree from Stepband simply re-running Stepcusing different pruning heuristics. The tree from Stepbcan be viewed as a cluster hierarchy (or dendrogram) that contains clusterings at different resolutions. Existing papers on DPC variants mainly focus on their own proposed variant, and as far as we know, there is no unified framework for implementing and comparing DPC variants and evaluating them on the same datasets. Furthermore, most DPC papers focus on clustering low-dimensional data, but many datasets in practice are high dimensional (d>100). The PECANN framework unifies a broad class of DPC variants by abstracting out these three steps and providing efficient parallel implementations for different variants of each step. For Stepa, we leverage graph-based approximate nearest neighbor search (ANNS) algorithms, which are fast and accurate in high dimensions(ANNScaling;wang2021comprehensive). For Stepb, we adapt graph-based ANNS algorithms to find higher density neighbors by iteratively doubling the number of nearest neighbors returned until finding one that has higher density. The doubling search guarantees that the algorithm finishes in a logarithmic number of rounds, making it highly parallel. For Stepsaandb, PECANN supports the following graph-based ANNS algorithms:\algnameVamana(jayaram2019diskann),\algnamepyNNDescent(pynndescent), and\algnameHCNNG(munoz2019hcnng). For Stepc, we use a concurrent union-find algorithm(Jayanti21)to achieve high parallelism. Prior work(sieranoja2019fast)has explored using graph-based ANNS for high-dimensional clustering, but their algorithm is not parallel and they only consider one DPC variant and one underlying ANNS algorithm. In addition, we provide theoretical work and span bounds111Theworkis the number of operations and thespan(or parallel time) is the length of the longest sequential dependence of a computation.of PECANN that depend on the complexity of the underlying ANNS algorithm. PECANN is implemented in C++, using the ParlayLib(Blelloch20)and ParlayANN(ANNScaling)libraries, and provides Python bindings as well. We use PECANN to implement five DPC variants and evaluate them on a variety of synthetic and real-world data sets with up to1.28million points and up to1024dimensions. We find that using a density function that is the inverse of the distance to thek^{\text{th}}nearest neighbor, combined with the\algnameVamana algorithm for ANNS, gives the best overall performance. On a 30-core machine with two-way hyper-threading, this best algorithm in PECANN achieves 37.7–854.3x speedup over a parallel brute force approach, and 45–734x speedup over\algnamefastdp(sieranoja2019fast), the state-of-the-art DPC-based algorithm for high dimensions, while achieving similar accuracy in terms of ARI score.\algnamefastdp is sequential, but even if we assume that it achieves a perfect speedup of 60x, PECANN still achieves a speedup of 0.76–12.24x. Compared to the state-of-the-art parallel density peaks clustering algorithm bydpc, which is optimized for low dimensions, our best algorithm achieves 320x speedup while achieving a higher ARI score on the MNIST dataset (in high dimensions, this algorithm is slower than brute force and failed to run on larger datasets). Our contributions are summarized below. We introduce the PECANN framework that unifies existingk-nearest neighbor-based DPC variants and supports parallel implementations of them that scale to large high-dimensional datasets. We extend graph-based ANNS algorithms with a parallel doubling-search method for finding higher density neighbors. We provide fast parallel implementations for five DPC variants. We perform comprehensive experiments on a 30-core machine with two-way hyper-threading showing that PECANN outperforms the state-of-the-art DPC-based algorithm for high dimensions by 45–734x. As far as we know, we are the first to compare different variants of DPC on large high-dimensional real-world image and text embedding datasets.
2302.02089v1	MOMA:Distill from Self-Supervised Teachers	Contrastive Learning and Masked Image Modelling have demonstrated exceptional performance on self-supervised representation learning, where Momentum Contrast (i.e., MoCo) and Masked AutoEncoder (i.e., MAE) are the state-of-the-art, respectively. In this work, we propose MOMA to distill from pre-trained MoCo and MAE in a self-supervised manner to collaborate the knowledge from both paradigms. We introduce three different mechanisms of knowledge transfer in the propsoed MOMA framework. : (1) Distill pre-trained MoCo to MAE. (2) Distill pre-trained MAE to MoCo (3) Distill pre-trained MoCo and MAE to a random initialized student. During the distillation, the teacher and the student are fed with original inputs and masked inputs, respectively. The learning is enabled by aligning the normalized representations from the teacher and the projected representations from the student. This simple design leads to efficient computation with extremely high mask ratio and dramatically reduced training epochs, and does not require extra considerations on the distillation target. The experiments show MOMA delivers compact student models with comparable performance to existing state-of-the-art methods, combining the power of both self-supervised learning paradigms. It presents competitive results against different benchmarks in computer vision. We hope our method provides an insight on transferring and adapting the knowledge from large-scale pre-trained models in a computationally efficient way.	Self-supervised learning (SSL)(Heet al.,2020)(Heet al.,2022)has shown impressive potential in various vision tasks and applications, owing to increasingly available data and advancing hardware. SSL extracts semantically rich information from large-scale unlabelled data and delivers a foundation model (e.g.,(Baoet al.,2021),(Devlinet al.,2018)) whose representations can be transferred for downstream tasks. Among the blossom of self-supervised learning methods, there are two dominant branches: contrastive learning and masked image modelling. Contrastive learning (e.g.,(Chenet al.,2020b),(Heet al.,2020)) enables the unsupervised learning by maximizing the agreement between two different augmented views from the same input. The key is to introduce reliable and challenging data augmentations that encourages semantically meaningful representations. Contrastive learning approaches have demonstrated exceptional results in the past few years, which even surpassed supervised learning algorithms. Recently, masked image modelling (e.g.,(Xieet al.,2022),(Heet al.,2022)) has become another main paradigm for learning self-supervised vision representations. The idea of masked image modelling stems from the success of masked language pre-training (e.g.,(Devlinet al.,2018),(Brownet al.,2020)) in Natural Language Processing. The objective is to reconstruct original images from partially masked inputs. Masked image modelling presents high efficiency under a high mask ratio and achieves superior performance than contrastive learning across various benchmarks (e.g.,(Denget al.,2009)(Linet al.,2014)). However, both contrastive learning and masked image modelling suffer from their own limitations. Contrastive learning relies heavily on data augmentations and requires additional techniques such as memory bank(Wuet al.,2018), momentum encoder(Heet al.,2020), and stop-gradient(Chen and He,2021). In(Chenet al.,2021), the authors also pointed out the necessity to freeze the patch embedding layer when training with vision transformers(Dosovitskiyet al.,2020). Additionally, the quality of negative samples(Robinsonet al.,2020)is also critical for contrastive learning. As for masked image modelling, it optimizes a pixel-level objective, which gains low-level representation and knowledge from the images. Therefore, such pre-training lacks of high-level representation, especially the semantic meanings behind the images. Recent studies(Chunget al.,2021)(Huanget al.,2022)(Mishraet al.,2022)(Zhouet al.,2022)(Yaoet al.,2022)attempt to combine the power of contrastive learning and masked modelling, yielding promising results. They suggest that both paradigms are complementary with each other and can deliver stronger representations when they are combined into a unified framework. Furthermore, integrating two paradigms into one framework introduces higher computational cost, which requires extensive resources (e.g., hundreds of GPU hours, enormous memory capacity, and excessive storage requirements). It is also not energy-efficient to training different frameworks from the scratch as they all tend to require large training epochs but the resulting difference in the performance is often negligible. In this work, we introduceMOMA, which integrates knowledge from pre-trained contrastive learning (i.e.,MOco) and masked image modelling (i.e.,MAsked autoencoder) through knowldge distillaiton(Hintonet al.,2015). There are three options presented in MOMA: (1) Distil from pre-trained MoCo to pre-trained MAE. (2) Distil from pre-trained MAE to pre-trained MoCo. (3) Distil from both pre-trained MoCo and MAE to a random initialized student model. We feed the original image to the teacher model and pass masked or intensively augmented samples to the student model. The learning objective is straightforward, which aligns the representations from normalized teacher outputs and reconstructed student outputs. This design leads to a simple and efficient framework for combining both contrastive learning and masked modelling. MOMA can accept an extremely high mask ratio during training, which leads to lower computational cost and faster training speed. Instead of training from the scratch, MOMA fully uses the pre-trained checkpoints from existing state-of-the-art paradigms. It enables MOMA to achieve excellent performance within only limited number of training epochs, which saves computation,energy and achieves competitive performance across different tasks. Additionally, it does not require a sophisticated design for knowledge distillation objectives as it directly aligns the representations from teacher and student. Finally, MOMA makes it possible to extract a more compact and lightweight model that fuses the power of different self-supervised learning paradigms. The proposed work enables new framework and mechanisms to utilize large-scale self-supervised models effectively and perform transfer in an energy-efficient manner.
2310.00092v1	Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality	Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.	Large Language Models (LLMs) have demonstrated impressive zero-shot and few-shot learning abilities in natural language understanding and generationbrown2020language. With human alignments like reinforcement learning from human feedback (RLHF), these models become better at following human instructionsouyang2022training; with instruction prompting and providing external resourcesnakano2021webgpt, they can be used as agents to autonomously choose toolsschick2023toolformer, communicate with other agentsshen2023hugginggpt, and show superior ability in decision-making and task execution. However, the seamless integration of these models within VR has remained a challenging frontier, hindered by efficiency, accuracy, and the complexities associated with interactions and manipulations in 3D spaces. Firstly, as a simulated three-dimensional interaction environment that mimics the real world, the VR environment has enormous possibilities in the way that the user can interact with entities (objects in the virtual scene) and manipulate their properties; secondly, the game engines that execute the user instructions has a pre-defined set of atomic operations for entity attribute modifications, causing it non-trivial to map or classify the user instruction to the most proper configuration in the engine; lastly, the accuracy of VR hardware (i.e., the voice recognition SDK,Wit.ai) and the efficiency in 3D graphics rendering (i.e., the uv rendering pipeline) limits the number of operations we can perform while not exceeding user’s comfortable response time to receive the feedback of the executed tasks. In this paper, we focus on two main challenges for deploying agent LLMs in VR: efficiency and accuracy. While improving and balancing these metrics, we plan to define how agent LLMs operate within the virtual environments, and then build an interactive tool to provide users with a more practical experience in developing their customized virtual scene. Hence, we propose the Voice2Action framework, created upon a rich taxonomy of text input commands, ranging from simple object selection and state manipulation to more complex operations involving animation, scripted sequences, and environment configuration modification. By hierarchical instruction prompting and entity extraction, Voice2Action can accurately interpret users’ textual instructions by incorporating environmental feedback. To provide empirical validation, we conduct experiments and ablation studies in an urban city planning virtual environment. We build a synthetic dataset generated by thetext-davinci-003model fromOpenAI APIwith the self-instructwang2022selfframework, where we use a pre-defined canonical instruction subset as the seed tasks, and manually filter out the unsatisfactory generated instruction-execution pair. The results indicate a marked increase and well-balanced execution efficiency and accuracy. To summarize, our contributions to operating agent LLMs in VR are: We define a hierarchical set of canonical instructions in VR for language models to perform decision-making actions. We improve efficiency and accuracy by incorporating different-purposed LLMs to divide and conquer the execution tasks, and error prevention from environment feedback. We build and open source the Voice2Action framework111Code is available athttps://github.com/yang-su2000/VR-Multimodal-Interactionto stimulate further research for applying agent LLMs in customizable 3D interactive environments.
2304.14483v1	Adversary Aware Continual Learning	Class incremental learning approaches are useful as they help the model to learn new information (classes) sequentially, while also retaining the previously acquired information (classes). However, it has been shown that such approaches are extremely vulnerable to the adversarial backdoor attacks, where an intelligent adversary can introduce small amount of misinformation to the model in the form of imperceptible backdoor pattern during training to cause deliberate forgetting of a specific task or class at test time. In this work, we propose a novel defensive framework to counter such an insidious attack where, we use the attacker's primary strength-hiding the backdoor pattern by making it imperceptible to humans-against it, and propose to learn a perceptible (stronger) pattern (also during the training) that can overpower the attacker's imperceptible (weaker) pattern. We demonstrate the effectiveness of the proposed defensive mechanism through various commonly used Replay-based (both generative and exact replay-based) class incremental learning algorithms using continual learning benchmark variants of CIFAR-10, CIFAR-100, and MNIST datasets. Most noteworthy, our proposed defensive framework does not assume that the attacker's target task and target class is known to the defender. The defender is also unaware of the shape, size, and location of the attacker's pattern. We show that our proposed defensive framework considerably improves the performance of class incremental learning algorithms with no knowledge of the attacker's target task, attacker's target class, and attacker's imperceptible pattern. We term our defensive framework as Adversary Aware Continual Learning (AACL).	Continual learning (CL) represents a setting where a model is asked to learn from sequential data with evolving distributions[de2019continual]. To be useful, a continual learning model has to strike a balance between two opposing characteristics: i) stability, which refers to model’s ability to retain previously learned but still relevant knowledge, and ii) exhibit plasticity, which refers to the model’s ability to acquire new knowledge and adapt itself to a possibly drifting or changing distributions. Traditionally the main challenge for CL models is to maintain stability, i.e. the CL models face difficulty in retaining the previously acquired knowledge while they are asked to learn new knowledge, a common phenomenon referred to as catastrophic forgetting[mccloskey1989catastrophic]. Therefore, much of the work in continual learning has focused on avoiding catastrophic forgetting while maintaining this delicate balance between stability and plasticity. While several CL approaches have been proposed to avoid the problem of catastrophic forgetting, but recently it has been found that these approaches are extremely vulnerable to adversarial backdoor attacks[umer2021adversarial,umer2020targeted,umer2022false], where an intelligent adversary can easily insert miniature amount of misinformation in the training data to deliberately or intentionally disturb the balance between stability and plasticity acquired by the CL model. More specifically, the goal of such an attack is to artificially increase the forgetting of the CL model on a explicitly targeted previously learned task. In this work, we propose a novel defensive framework to ensure robustness in CL models to imperceptible misinformation inserted via adversarial backdoor attack. Our defensive framework utilizes a small amount of defensive (decoy) samples to inhibit the impact of the malicious samples. The defensive (decoy) samples also contain a pattern similar to adversarial backdoor malicious samples but this pattern is: i) perceptible (stronger); and ii) different than the attacker’s unknown imperceptible (weak) pattern. We use the intensity of a pattern, i.e., perceptibility of a pattern to determine the strength of the pattern. The more stronger pattern is more perceptible and vice-versa. Specifically, as the malicious samples (falsely labeled samples containing attacker’s imperceptible pattern) are appended to the training data by the adversary, we as a defender also provide additional defensive samples during training. The goal here is two-fold: i) to force the CL model to learn to not only correctly classify the clean samples of each task but also to correctly classify the defensive samples, i.e., the samples with the defensive pattern, ii) to weaken the impact of the attacker’s imperceptible pattern in the presence of the defender’s perceptible pattern. Once the training is done, the defensive pattern is applied to all the test samples at test time including those that contain the unknown attacker’s malicious pattern. In the presence of stronger defensive pattern, the CL model ignores the weaker attacker’s pattern and correctly classify all the test samples including the malicious ones. In other words, the defensive samples serve to inoculate the CL model against the malicious samples. We referred to such learning as theAdversary Aware Continual Learning (AACL). We note that on the surface AACL may seem similar to well known state of the art adversarial training (AT)[goodfellow2014explaining]defense to defend against adversarial examples[szegedy2013intriguing,papernot2016practical,papernot2016transferability]. However, in section 5 we compare AACL with adversarial training and show that AACL is not only different but also more efficient than adversarial training. Our paper is organized as follows; section 2 briefly describes the class-incremental learning and adversarial threats to class incremental learning, section 3 discusses the existing backdoor defenses and their limitations in class incremental learning setting, section 4 describes the proposed AACL defensive mechanism against the adversarial backdoor attacks to class incremental learning models, section 5 compares the AACL framework against the adversarial training, and section 6 constitutes the experiment and results followed by conclusions section.
2310.09358v1	When are Bandits Robust to Misspecification?	Parametric feature-based reward models are widely employed by algorithms for decision making settings such as bandits and contextual bandits. The typical assumption under which they are analysed is realizability, i.e., that the true rewards of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true rewards are (potentially significantly) misspecified with respect to the model class. For parameterized bandits and contextual bandits, we identify sufficient conditions, depending on the problem instance and model class, under which classic algorithms such as $\epsilon$-greedy and LinUCB enjoy sublinear (in the time horizon) regret guarantees under even grossly misspecified rewards. This is in contrast to existing worst-case results for misspecified bandits which show regret bounds that scale linearly with time, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecification.	Sequential optimization over a set of decisions, e.g., actions in a multi-armed bandit and policies in an MDP, is often carried out by assuming a parametric model for the payoff of a decision, which is learnt with time. Well-known instantiations of this approach are algorithms for structured multi-armed bandits, e.g., linear banditsrusmevichientong2010linearlyand generalized linear banditsfilippi2010parametric, linear contextual banditschu2011contextual, and value function approximation-based methods for Markov Decision Processessutton2018reinforcement. Algorithms that make decisions based on an estimated reward model are known to enjoy attractive regret guarantees when the true rewards encountered areperfectly realizableby the model, see, e.g.,oful;filippi2010parametric;jin2020provably. However, it is more likely than not that a parametric class of models is, at best, only an approximation of reality, succinctly expressed by the aphorism ‘all models are wrong, but some are useful’box1976science. This is quite likely true in the context of sequential decision-making – after all, even if the rewards of, say, arms in a multi-armed bandit, are estimated with (significantly) large error, one may still hope to discern the optimal arm if its (erroneous) estimate ends up dominating those of the other arms. This begs a natural question—While the task of reward estimation can be fraught with error under misspecified models, when can the task of optimal action learning remain immune to it? We initiate a rigorous study of the extent to which sequential decision-making algorithms based on estimating reward models can be robust to misspecification in the model. In particular, we are interested in characterizing the interplay between the (true) arms’ rewards of a bandit and the reward model class assumed by the algorithm, and how it governs whether the algorithm can still learn to play optimally if the true rewards are not realizable by the reward model. In this respect, our specific contributions are as follows: For misspecified linear bandits, we identify a novel family of instances (reward vectors, in dimension the number of arms), which we term the robust observation region. Reward vectors in this region are characterized by an invariance property of the greedy action that they induce after being projected, with respect to any weighted Euclidean norm, onto the linear feature subspace. This region, depending upon the feature subspace, can be non-trivially large, and need not be confined to within a small deviation from the subspace. We prove that for any instance with the arms’ rewards in the robust observation region, both (i) the\epsilon-greedy algorithm, with least-squares parameter estimation and an exploration rate of1/\sqrt{t}in each roundt, and (ii) the LinUCB (or OFUL) algorithm, achieveO(\sqrt{T})regret in the time horizonT. We extend the notion of robust reward instances to the problem of linear contextual bandits, for which we provide a generalization of the robust observation region. We show that both the\epsilon-greedy and LinUCB algorithms for linear contextual bandits getO(\sqrt{T})regret whenever the true, misspecified, reward vector belongs to this robust observation region. We stress that our results pertain to the original algorithms (i.e., not modified to be misspecification-aware in any manner). It is our novel analytical approach that shows that they achieve nontrivial sublinear regret, even under arbitrarily large misspecification error111The term ‘misspecification error’ is to be understood as the distance of the arms’ reward vector to the model reward subspace.. This is in contrast to, and incomparable with, existing results that argue that, in the ‘worst case’ across all reward vectors that are a constant distance away from the feature subspace, any algorithm must incur regret that scales linearly with the time horizon, e.g.,(pmlr-v119-lattimore20a, Thm. F.1). Our results lend credence, in a strong theoretical sense, to the observation that reinforcement learning algorithms based on presumably approximate value function models are often able to learn (near-) optimal behavior in practice across challenging benchmarksmnih2013playing;lillicrap2015continuous. They also shine light on the precise structure of bandit problems that makes robustness possible in the face of significant misspecification. The key concepts and results of this paper can be understood using a simple toy example of a misspecified 2-armed (non-contextual) linear bandit. Assume that the vector of mean rewards of the arms (the “instance”) is\bm{\mu}=\begin{bmatrix}\mu_{1},\mu_{2}\end{bmatrix}^{\top}=\begin{bmatrix}20,% 3\end{bmatrix}^{\top}. This instance can be thought of as an element in the\Real^{2}plane (marked by\timesin Fig.1). Suppose one attempts to learn this bandit via a 1-dimensional linear reward model in which the arms’ features are assumed to be\phi_{1}=3,\phi_{2}=1. It follows that (i) any (2-armed) bandit instance in this linear model is of the form\bm{\Phi}\theta, where\theta\in\Realand\bm{\Phi}=\begin{bmatrix}\phi_{1},\phi_{2}\end{bmatrix}^{\top}=\begin{bmatrix}% 3,1\end{bmatrix}^{\top}\in\Real^{2\times 1}, and corresponds to an element in the range space of\bm{\Phi}, and (ii) the instance\bm{\mu}is misspecified as it is off the subspace222Thel_{\infty}misspecification error (deviation from subspace) of\bm{\mu}is2.75for this example.. For ease of exposition, consider that there is no noise in the rewards observed by pulling arms. In this case, the ordinary least squares estimate of\theta, computed at timetfromn_{1}observations of arm1andn_{2}observations of arm2, is\hat{\theta}_{t}=\frac{n_{1}\phi_{1}\mu_{1}+n_{2}\phi_{2}\mu_{2}}{n_{1}\phi_{1% }^{2}+n_{2}\phi_{2}^{2}}. A key observation is that\hat{\theta}_{t}can be written as a convex combination of\mu_{1}/\phi_{1}=6.7and\mu_{2}/\phi_{2}=3:\hat{\theta}_{t}=\frac{n_{1}\phi_{1}^{2}}{n_{1}\phi_{1}^{2}+n_{2}\phi_{2}^{2}}% \Big{(}\mu_{1}/\phi_{1}\Big{)}+\frac{n_{2}\phi_{2}^{2}}{n_{1}\phi_{1}^{2}+n_{2% }\phi_{2}^{2}}\Big{(}\mu_{2}/\phi_{2}\Big{)}, regardless of the past sampling distribution of the arms. The corresponding parametric estimated rewards\bm{\Phi}\hat{\theta}_{t}, must thus lie in the set\{[3\theta,\theta]^{\top}:\theta\in[3,6.7]\}, which appears as the hypotenuse of the right triangle with vertex(20,3)in Fig.1. Note that if a greedy rule is applied to play all subsequent actions (A_{t+1}=\argmax_{i=1,2}\phi_{i}^{\top}\hat{\theta}_{t}), then the action will be1, since the point\bm{\Phi}\hat{\theta}_{t}is always ‘below’ the standard diagonal\mu_{1}=\mu_{2}(the black line in Fig.1). Since action1is optimal for the (true) rewards\bm{\mu}, the algorithm will never incur regret in the future. The instance above has a misspecification error significantly smaller than the reward gap (2.75<17). One can also find instances at the other extreme, e.g.,\bm{\tilde{\mu}}=[20,18]^{\top}(marked by\circin Fig.1) for which the misspecification error is much larger than the gap (8.5>2), that remain robust in the sense of regret. Such instances fall outside the scope of existing work on misspecified banditszhang2023interplay, and we address them in our work. The remainder of the paper generalizes this observation for stochastic arm rewards and algorithms that incorporate some form of exploration (\epsilon-greedy and optimism-based) and explicitly characterizes the set of all true reward instances for which no-regret learning is possible. Existing work on misspecified bandits contributes results that can be put into two categories. The first category is negative results of a worst-case form, for instance: there exist reward instances for linear bandits for which (i) the model misspecification error (l_{\infty}distance of the reward vector from the model subspace\text{Range}(\bm{\Phi})) is at most\rho>0and (ii) the LinUCB algorithm suffers regret\Omega(\rho T)inTrounds. Results of this nature are well established in the misspecified bandit and contextual bandit literature, including but not limited to the works ofghosh2017misspecified;pmlr-v119-lattimore20a;zanette2020learning.pmlr-v119-lattimore20apresent a frequentist analysis that shows that even with the knowledge of the misspecification error, a modified form of LinUCB fails to give sub-linear regret in the misspecified contextual setting.zanette2020learningextend a similar analysis to learning Markov Decision Processes. The second type of result is along a positive direction: to develop conditions, and associated algorithms, under which misspecified bandits can give sub-linear regret. The work ofpmlr-v216-liu23canalyzes the LinUCB algorithm when the sub-optimality gaps of the arms bound the misspecification. They show that when the misspecification is of low order, the algorithm enjoys sub-linear regret. Under a similar condition,zhang2023interplaywere able to extend the study to the contextual setting. They propose a phased arm elimination algorithm, which performs similarly to SupLinUCBchu2011contextualbut requires knowledge of the sub-optimality gap. Our results, in this positive spirit, give more fine-grained structural conditions instead of the coarse notion of misspecification error to ensure that algorithms such as\epsilon-greedy and LinUCB can achieve robust learning without any additional modifications. A detailed comparison of our results withzhang2023interplayappears at the end of Sec.2and a more detailed survey of previous work in the appendix.
2309.04162v1	GLS-CSC: A Simple but Effective Strategy to Mitigate Chinese STM Models' Over-Reliance on Superficial Clue	Pre-trained models have achieved success in Chinese Short Text Matching (STM) tasks, but they often rely on superficial clues, leading to a lack of robust predictions. To address this issue, it is crucial to analyze and mitigate the influence of superficial clues on STM models. Our study aims to investigate their over-reliance on the edit distance feature, commonly used to measure the semantic similarity of Chinese text pairs, which can be considered a superficial clue. To mitigate STM models' over-reliance on superficial clues, we propose a novel resampling training strategy called Gradually Learn Samples Containing Superficial Clue (GLS-CSC). Through comprehensive evaluations of In-Domain (I.D.), Robustness (Rob.), and Out-Of-Domain (O.O.D.) test sets, we demonstrate that GLS-CSC outperforms existing methods in terms of enhancing the robustness and generalization of Chinese STM models. Moreover, we conduct a detailed analysis of existing methods and reveal their commonality.	"The Short Text Matching (STM) task holds significant importance in Natural Language Processing (NLP), aiming to determine the semantic similarity of sentence pairs. The STM task finds widespread applications in areas such as information retrieval, question-answering, and dialogue systems. In recent years, the advent of pre-trained models such as BERTDevlinet al.(2018), ERNIESunet al.(2019), and RoBERTaLiuet al.(2019)established the state-of-the-art models. However, previous studiesZhanget al.(2019b); McCoyet al.(2019); Naiket al.(2018); Nieet al.(2020); Huanget al.(2020); Morriset al.(2020); Zhanget al.(2020); Jia and Liang (2017)revealed that pre-trained models often rely on superficial clues, resulting in their limited performance on Robustness (Rob.) and Out-of-domain (O.O.D.) test sets. Recent studiesSchusteret al.(2019)have highlighted that pre-trained models can easily capture strong correlations as superficial clues due to the biased distribution of the training set. While pre-trained models may achieve satisfactory performance on In-Domain (I.D.) data by leveraging these spurious clues, they remain vulnerable to adversarial samples that exploit superficial cluesLaiet al.(2021); Poliaket al.(2018); Kavumbaet al.(2019); Wanget al.(2021). Fig.1illustrates an instance where the STM model predicts semantic matched by focusing on similar textual literals (highlighted in blue) while disregarding semantic differences between “cat” and “cows” (highlighted in red). Therefore, it becomes imperative to analyze and mitigate the impact of superficial clues on STM models. For English STM tasks, the concept of word overlap, as a superficial cue that can be easily captured by STM modelsGururanganet al.(2018); Duet al.(2022), has garnered significant attention. Word overlap simply examines the presence or absence of words, without taking into account the influence of word order. However, for the Chinese language, extensive linguistics researchJames (1985); LaPolla (1995); Zhao and Cao (2016)consistently demonstrates its sequential nature, where word order strongly influences sentence meaning. For instance, in Chinese, although ""喜欢"" (like) and ""欢喜"" (joy) words completely overlap, they have completely different semantics and parts of speech. Compared with word overlap, the edit distance can further consider the influence of word order. Moreover, due to the inherent characteristics of STM tasks, that the divergence in sentence lengths is not excessively significant, it mitigates the drawback of edit distance’s sensitivity to disparities in sentence length. To sum up, for Chinese STM tasks, we employ the Levenshtein distance111https://en.wikipedia.org/wiki/Levenshtein_distanceas a proxy for the superficial clue to assess STM models’ behavior. In our work, we quantitatively demonstrate that STM models tend to predict semantic matches when the edit distance is relatively small and that STM models tend to predict semantic mismatches when the edit distance is relatively large. Such phenomena suggest that edit distance can serve as a proxy for the superficial clue captured by STM models. For mitigating STM models’ over-reliance on the superficial clue, we propose a simple and effectiveGLS-CSCstrategy:GraduallyLearnSamplesContainingSuperficialClue. Similar to curriculum learningBengioet al.(2009), GLS-CSC involves resampling the order of training samples. Furthermore, we conduct our experiments on two Chinese STM datasets (LCQMC{}_{train}and CCKS{}_{train}). Following the advice of previous workGeirhoset al.(2020), we apply multiple O.O.D. real-world test sets to evaluate STM models’ performance. Our experimental results show that our GLS-CSC strategy can mitigate STM models’ over-reliance on the superficial clue effectively, and improves their robustness and generalization significantly compared to strong baselines. Notably, for our GLS-CSC strategy, employing edit distance as a proxy has demonstrated notable advantages over employing word overlap. Additionally, we conduct a detailed analysis of existing methods and observe that while they can enhance performance on adversarial data, they inadvertently compromise performance on a specific subset of data."
2310.10418v2	Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms	Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying visual-grounded commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a new approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code are released at https://seungjuhan.me/normlens.	Reasoning aboutcommonsense norms111One line of developmental moral psychology tradition argues moral and social conventional norms present salient distinctionsTuriel(1983). Nevertheless, recent studies point out that these two concepts are inherently interconnected without meaningful distinctionsStich(2018). Additionally, other recent studies identify that what counts as moral or socially acceptable is highly provincialLevine et al.(2021). In this work, we consider a wide range of socio-moral judgments for our inclusive definition ofcommonsense norms.highly depends on the context in which actions are performedPyatkin et al. (2022); Jin et al. (2022); Ziems et al. (2023). While an actionreading a bookis generally considered positive, the action is deemed to bewrongin the context ofdriving a carbecause the attention should be focused on the road. Understanding thedefeasible commonsense norms— norms that could be further strengthened or attenuated based on the context — are crucial, and prior worksHendrycks et al. (2021); Jiang et al. (2021); Forbes et al. (2020)have primarily focused on the defeasible norms based solely on text inputs. However, real-world scenarios often lack explicit contextual information described in language. Consider the situations depicted in Figure1: when humans see the first image, the action ofreading a bookwill be considered to bewrong. Conversely, when looking at the second image, the same action will be considered to beokayas reading a book together while sitting on the couch is viewed positively. When humans make judgments, they perceive the visual scene, make adjustments to reflect the visual defeasible cues, and then make intuitive judgments. It is a more natural process to go directly from visual scene to judgment, but this is very understudied. In this work, we study model capacity forvisually grounded reasoning about defeasible commonsense normsthat align with humans. To this end, we introduceNormLens, a dataset consisting of 10K human annotations about 2K multimodal situations. Our dataset covers diverse situations about defeasible commonsense norms (§2). Each situation consists of a visual context and an associated action, and five human annotators make moral judgments about the situation and provide explanations for the judgments. To construct a truly multimodal benchmark centered around defeasible commonsense norms, we employ a data collection pipeline that is based on human-AI collaboration (see Figure3). The starting point is image-description pairs sourced from existing vision-language datasets — SherlockHessel et al. (2022), COCO captionsLin et al. (2014), and Localized NarrativesPont-Tuset et al. (2020)dataset. Then, we utilize language models (LMs) to generate a set of multimodal situations conditioned on input descriptions such that: (1) the generated action ismorally appropriategiven the context provided by the input image description, and (2) in contrast, the generated action ismorally inappropriateunder the generated situation (§2.1). Finally, for each multimodal situation, we employ human annotation to collect moral judgments and explanations (§2.2). An important consideration in constructing our benchmark is the subjective nature of moral judgmentsTalat et al. (2022), which can lead to disagreements among individuals when facing a single situation. For instance, in the last image of Figure2, one human annotator deemsit is rude to read a book during a concert, while others findit is okayorreading a book is impractical during a concert. To consider this inherent characteristic of moral reasoning task, we organize our benchmark by splitting the dataset into two different parts (NormLens{}^{HA}andNormLens{}^{MA}) based on the degree of agreement among human annotators (§2.3). We design two tests based onNormLensto study how well models’ predictions align with humans in this context (§3). Given a multimodal situation, a model is asked to (1) provide a moral judgment about the situation, and (2) offer a plausible explanation for its judgment. Experimental results demonstrate that these tests are challenging even for state-of-the-art large pretrained models (§4). In particular, models struggle to account for defeasible visual contexts, and also often fail to identify cases where humans agree that the action is impossible to perform. Finally, we investigate a method for improving model agreement with human judgment without relying on additional human annotations (§5). We begin by utilizing image-description pairs once more, seeding image descriptions into the LM to generate 90K instances of actions with judgments and explanations. Then, we construct multimodal situations by combining the generated actions and images that are paired with provided descriptions. Subsequently, we fine-tune models using these generated examples, and find that fine-tuned models exhibit better alignments with humans, achieving the highest improvement of 31.5% compared to the counterpart in the judgment task forNormLens{}^{HA}. In summary, our main contributions are: [leftmargin=*,topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex] NormLens, a new dataset/benchmark of 10K human annotations covering 2K multimodal situations about commonsense norms. Two new tasks posed over the corpus: making judgments and explaining judgments. Experimental results demonstrating that while these two tasks remain challenging for models, that multimodal models can be improved with a newly proposed text-only distillation step.
2303.04187v1	Stabilized training of joint energy-based models and their practical applications	"The recently proposed Joint Energy-based Model (JEM) interprets discriminatively trained classifier $p(y|x)$ as an energy model, which is also trained as a generative model describing the distribution of the input observations $p(x)$. The JEM training relies on ""positive examples"" (i.e. examples from the training data set) as well as on ""negative examples"", which are samples from the modeled distribution $p(x)$ generated by means of Stochastic Gradient Langevin Dynamics (SGLD). Unfortunately, SGLD often fails to deliver negative samples of sufficient quality during the standard JEM training, which causes a very unbalanced contribution from the positive and negative examples when calculating gradients for JEM updates. As a consequence, the standard JEM training is quite unstable requiring careful tuning of hyper-parameters and frequent restarts when the training starts diverging. This makes it difficult to apply JEM to different neural network architectures, modalities, and tasks. In this work, we propose a training procedure that stabilizes SGLD-based JEM training (ST-JEM) by balancing the contribution from the positive and negative examples. We also propose to add an additional ""regularization"" term to the training objective -- MI between the input observations $x$ and output labels $y$ -- which encourages the JEM classifier to make more certain decisions about output labels. We demonstrate the effectiveness of our approach on the CIFAR10 and CIFAR100 tasks. We also consider the task of classifying phonemes in a speech signal, for which we were not able to train JEM without the proposed stabilization. We show that a convincing speech can be generated from the trained model. Alternatively, corrupted speech can be de-noised by bringing it closer to the modeled speech distribution using a few SGLD iterations. We also propose and discuss additional applications of the trained model."	One of the most common Machine Learning tasks is to classify input data points into chosen categories which typically is accomplished by a discriminatively trained classifier. Having an enormous amount of data and a powerful machine learning model of suitable architecture with a huge number of learnable parameters is usually enough to get close to state-of-the-art performance. The alternative approach of training a generative model and then inferring the posterior probability over the possible categories can outperform discriminative models only in the low-resource settings and usually fails to be competitive due to its restricted modeling power otherwise. The utility of explicit generative models then lies in the access to the likelihood of the input data useful for e.g. detecting outliers while implicit generative models are evaluated based on the capability to generate realistic and diverse input, especially when applied to images. The promise of generative models to avoid expensive labeling of unlabeled data (for which a discriminative classifier has no use) is shadowed by the recent success of self-supervised techniques that take advantage of self-contained information in the time sequence or leverage known input data manipulations (e.g. shift and resize of images) to introduce labels used to train a discriminative model. Self-supervised models are usually used as pre-trained models to extract embeddings that work well in the downstream task. Recently,Grathwohlet al.(2019)showed that every discriminatively trained classifier can be seen as an energy-based model modeling the joint distribution between the input dataxand the labely. In fact, when the discriminator is trained in a standard way, we are only training the model to provide us with a good estimate ofp(y|x)whilep(x)is not being optimized at all. In order to optimizep(x), authors used Stochastic Gradient Langevin Dynamics (SGLD) as it was described inWelling and Teh (2011)to sample111These samples are sometimes called negative samples as opposed to train data being called positive samples.from the modeled distribution and called the resulting model Joint Energy-based Model (JEM). Authors demonstrated that the longer training time and slight performance degradation (compared to its strictly discriminative counterpart) is compensated by the possibility to generate either category-conditional or unconditional samples, robustness against adversarial attacks, improved calibration, and out-of-distribution detection. Unfortunately, it is difficult to take these models and easily apply them to new tasks as the training often diverges, and restarting the training from the last saved epoch seems to be the only reliable solution to reach a stable optimum. This prevents the community from conducting a deeper exploration of these models. As a response, VERA (Duvenaudet al.(2021)) introduced a stable way of training JEMs. In this work, the authors demonstrate that they are able to speed up the training by introducing an auxiliary model (generator) and they are capable of producing high-quality images based on that generator.
2304.02738v1	Core Challenges in Embodied Vision-Language Planning	Recent advances in the areas of Multimodal Machine Learning and Artificial Intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Robotics. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly leverage computer vision and natural language for interaction in physical environments. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the current and new algorithmic approaches, metrics, simulators, and datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalisability and furthers real-world deployment.	With recent progress in the fields of Artificial Intelligence (AI) and Robotics, intelligent agents are envisaged to interact with humans in shared environments. Such agents include any entities that can make decisions and take actions autonomously and are expected to understand semantic concepts in those environments, using, e.g., visual, haptic, auditory, or textual information perceived via sensorsWooldridge and Jennings (1995); Castelfranchi (1998). With the goal of developing intelligent agents equipped with these sensory and reasoning capabilities, Embodied AI (EAI), as a field, has become popular for studying the particular set of AI problems surrounding agents situated in a physical environment: recently, the number of papers and datasets for the tasks that require the agents to use both vision and language understanding has increased markedlyDaset al.(2018); Gordonet al.(2018); Andersonet al.(2018); Krantzet al.(2020); Thomasonet al.(2019); Nguyen and Daumé III (2019); Majumdaret al.(2020); Liet al.(2020). In this article, we conduct a survey of recent works on these types of problems, which we refer to asEmbodied Vision-Language Planning(EVLP) tasks. In this article, we aim to provide a bird’s-eye view of current research on EVLP problems, addressing their main challenges and future directions. Our main contributions are as follows: (i) We formally define the field of Embodied Vision-Language Planning and we propose a taxonomy that both unifies a set of related tasks in EVLP and serves as a basis for categorising new tasks; (ii) we survey recent EVLP tasks, compare their task properties, highlight modelling approaches used in those tasks, and analyse the datasets, simulators, and metrics used to evaluate the approaches on those tasks; finally, (iii) we identify open challenges that afflict existing works in the EVLP family, with an emphasis towards encouraging unseen generalisation and deploying algorithms to the real world. We refer readers to our full journal article for further detailsFranciset al.(2022b). We discuss a broad set of problems, related to an embodied agent’s ability to make planning decisions in physical environments. Formally, letSandAdenote sets of states and actions;VandLdenote sets of vision and language inputs available to the agent. A planning problem is defined by the tuple\Phi=\{S,A,s_{ini},s_{goal}\}, wheres_{ini},s_{goal}\in Sdenote initial and goal states, respectively. A solution\psi\in\Psi_{\Phi}to planning problem\Phiis a sequence of actions to take in each state, starting from an initial state to reach a goal state,\psi=[s_{ini},a_{0},...,s_{t},a_{t},...,a_{T},s_{goal}], wheret\in Tis a finite time-step in episode lengthTand\Psi_{\Phi}is a set of possible solutions to\Phi. Given a particular EVLP problem\Phi, states_{t}\in Sat time steptcan be defined in terms of vision and language inputs up to the current time step, such that,s_{t}=\{(v_{0},l_{0}),(v_{1},l_{1}),\dots,(v_{t},l_{t}),\dots,(v_{T},l_{T})\}, wherev_{t}\in Vandl_{t}\in L. The agent’s objective is to minimize the difference between an admissible solution\psi\in\Psi_{\Phi}and its predicted one\bar{\psi}. This definition broadly captures the crux of EVLP problems. Customized definitions are needed for specific tasks, where additional constraints or assumptions are added to focus on particular subareas of this general problem. We propose a taxonomy of EVLP research, illustrated inFigure 1, around which the rest of the paper is organized. The taxonomy subdivides the field into three branches; tasks, approaches, and evaluation methods. The Tasks branch proposes a framework to classify existing tasks and to serve as a basis for distinguishing new ones. The Approaches branch touches on the learning paradigms, common architectures used for the different tasks, as well as common tricks used to improve performance. The right-most branch of the taxonomy discusses task Evaluation Methodology, which is subdivided into two parts: metrics and environments. The metrics subsection references many of the common metrics, used throughout EVLP tasks, while the environments subsection presents the different simulators and datasets currently used.
2309.15462v2	DTC: Deep Tracking Control	Legged locomotion is a complex control problem that requires both accuracy and robustness to cope with real-world challenges. Legged systems have traditionally been controlled using trajectory optimization with inverse dynamics. Such hierarchical model-based methods are appealing due to intuitive cost function tuning, accurate planning, generalization, and most importantly, the insightful understanding gained from more than one decade of extensive research. However, model mismatch and violation of assumptions are common sources of faulty operation. Simulation-based reinforcement learning, on the other hand, results in locomotion policies with unprecedented robustness and recovery skills. Yet, all learning algorithms struggle with sparse rewards emerging from environments where valid footholds are rare, such as gaps or stepping stones. In this work, we propose a hybrid control architecture that combines the advantages of both worlds to simultaneously achieve greater robustness, foot-placement accuracy, and terrain generalization. Our approach utilizes a model-based planner to roll out a reference motion during training. A deep neural network policy is trained in simulation, aiming to track the optimized footholds. We evaluate the accuracy of our locomotion pipeline on sparse terrains, where pure data-driven methods are prone to fail. Furthermore, we demonstrate superior robustness in the presence of slippery or deformable ground when compared to model-based counterparts. Finally, we show that our proposed tracking controller generalizes across different trajectory optimization methods not seen during training. In conclusion, our work unites the predictive capabilities and optimality guarantees of online planning with the inherent robustness attributed to offline learning.	TO is a commonly deployed instance of optimal control for designing motions of legged systems and has a long history of successful applications in rough environments since the early 2010s[1,2]. These methods require a model of the robot’s kinematics and dynamics during runtime, along with a parametrization of the terrain. Until recently, most approaches have used simple models such as single rigid body[3]or inverted pendulum dynamics[4,5], or have ignored the dynamic effects altogether[6]. Research has shifted towards more complex formulations, including centroidal[7]or full-body dynamics[8]. The resulting trajectories are tracked by awhole-body control(WBC) module, which operates at the control frequency and utilizes full-body dynamics[9]. Despite the diversity and agility of the resulting motions, there remains a considerable gap between simulation and reality due to unrealistic assumptions. Most problematic assumptions include perfect state estimation, occlusion-free vision, known contact states, zero foot-slip, and perfect realization of the planned motions. Sophisticated hand-engineered state machines are required to detect and respond to various special cases not accounted for in the modeling process. Nevertheless, highly dynamic jumping maneuvers performed by Boston Dynamics’ bipedal robot Atlas demonstrate the potential power oftrajectory optimization(TO). RL has emerged as a powerful tool in recent years for synthesizing robust legged locomotion. Unlike model-based control,reinforcement learning(RL) does not rely on explicit models. Instead, behaviors are learned, most often in simulation, through random interactions of agents with the environment. The result is a closed-loop control policy, typically represented by a deep neural network, that maps raw observations to actions. Handcrafted state-machines become obsolete because all relevant corner cases are eventually visited during training. End-to-end policies, trained from user commands to joint target positions, have been deployed successfully on quadrupedal robots such as ANYmal[10,11]. More advanced teacher-student structures have substantially improved the robustness, enabling legged robots to overcome obstacles through touch[12]and perception[13]. Although locomotion across gaps and stepping stones is theoretically possible, good exploration strategies are required to learn from the emerging sparse reward signals. So far, these terrains could only be handled by specialized policies, which intentionally overfit to one particular scenario[14]or a selection of similar terrain types[15,16,17,18]. Despite promising results, distilling a unifying locomotion policy may be difficult and has only been shown with limited success[19]. Some of the shortcomings that appear inRLcan be mitigated using optimization-based methods. While the problem of sparse gradients still exists, two important advantages can be exploited: First, cost-function and constraint gradients can be computed with a small number of samples. Second, poor local optima can be avoided by pre-computing footholds[5,8], pre-segmenting the terrain into steppable areas[20,7], or by smoothing out the entire gradient landscape[21]. Another advantage ofTOis its ability to plan actions ahead and predict future interactions with the environment. If model assumptions are generic enough, this allows for great generalization across diverse terrain geometries[21,7]. The sparse gradient problem has been addressed extensively in the learning community. A notable line of research has focused on learning a specific task while imitating expert behavior. The expert provides a direct demonstration for solving the task[22,23], or is used to impose a style while discovering the task[24,25,26]. These approaches require collecting expert data, commonly done offline, either through re-targeted motion capture data[24,25,26]or aTOtechnique[22,23]. The reward function can now be formulated to be dense, meaning that agents can collect non-trivial rewards even if they do not initially solve the task. Nonetheless, the goal is not to preserve the expert’s accuracy but rather to lower the sample and reward complexity by leveraging existing knowledge. To further decrease the gap between the expert and the policy performance, we speculate that the latter should have insight into the expert’s intentions. This requires online generation of expert data, which can be conveniently achieved using any model-based controller. Unfortunately, rolling out trajectories is often orders of magnitude more expensive than a complete learning iteration. To circumvent this problem, one possible alternative is to approximate the expert with a generative model, for instance, by sampling footholds from a uniform distribution[15,16], or from a neural network[27,28,17]. However, for the former group, it might be challenging to capture the distribution of an actual model-based controller, while the latter group still does not solve the exploration problem itself. In this work, we propose to guide exploration through the solution ofTO. As such data will be available both on- and offline, we refer to it as “reference” and not expert motion. We utilize a hierarchical structure introduced in deep loco[28], where a high-level planner proposes footholds at a lower rate, and a low-level controller follows the footholds at a higher rate. Instead of using a neural network to generate the foothold plan, we leverageTO. Moreover, we do not only use the target footholds as an indicator for a rough high-level direction but as a demonstration of optimal foot placement. The idea of combining model-based and model-free approaches is not new in the literature. For instance, supervised[29]and unsupervised[30,31]learning has been used to warm-start nonlinear solvers.RLhas been used to imitate[23,22]or correct[32]motions obtained by solvingTOproblems. Conversely, model-based methods have been used to check the feasibility of learned high-level commands[27]or to track learned acceleration profiles[33]. Compared to[32], we do not learn corrective joint torques around an existingWBC, but instead, learn the mapping from reference signals to joint positions in an end-to-end fashion. To generate the reference data, we rely on an efficientTOmethod calledterrain-aware motion generation for legged systems(TAMOLS)[21]. It optimizes over footholds and base pose simultaneously, thereby enabling the robot to operate at its kinematic limits. We let the policy observe only a small subset of the solution, namely planar footholds, desired joint positions, and the contact schedule. We found that these observations are more robust under the common pitfalls of model-based control, while still providing enough information to solve the locomotion task. In addition, we limit computational costs arising from solving the optimization problems by utilizing a variable update rate. During deployment, the optimizer runs at the fastest possible rate to account for model uncertainties and external disturbances. Our approach incorporates elements introduced in[14], such as time-based rewards and position-based goal tracking. However, we reward desired foothold positions at planned touch-down instead of rewarding a desired base pose at an arbitrarily chosen time. Finally, we use an asymmetric actor-critic structure similar to[22], where we provide privileged ground truth information to the value function and noisified measurements to the network policy. We trained more than4000robots in parallel for two weeks on challenging ground covering a surface area of more than76000\,\mathrm{m^{2}}. Throughout the entire training process, we generated and learned from about23years of optimized trajectories. The combination of offline training and online re-planing results in accurate, agile, and robust locomotion. As showcased in Fig.1and movie 1, with our hybrid control pipeline, ANYmal[34]can skillfully traverse parkours with high precision, and confidently overcome uncertain environments with high robustness. Without the need for any post-training, the tracking policy can be deployed zero-shot with differentTOmethods at different update rates. Moreover, movie 2 demonstrates successful deployment in search-and-rescue scenarios, which demand both accurate foot placement and robust recovery skills. The contributions of our work are therefore twofold: Firstly, we enable the deployment of model-based planners in rough and uncertain real-world environments. Secondly, we create a single unifying locomotion policy that generalizes beyond the limitations imposed by state-of-the-artRLmethods.
2311.14387v3	Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling	In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an {\em exponential rate}. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow {\em polynomial rate}. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) {\em provably fail} in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when applied to linearly non-separable datasets and deep neural networks.	In modern machine learning,††† Corresponding Authors.models are often over-parameterized in the sense that they can easily interpolate all training data, giving rise to a loss landscape with many global minima. Although all these minima yield zero training loss, their generalization ability can vary significantly. Intriguingly, it is often observed that Stochastic Gradient Descent (SGD) and its variants consistently converge to solutions with favorable generalization properties even without needing any explicit regularization(40;62). This phenomenon implies that the “implicit bias” of SGD plays a crucial role in ensuring the efficacy of deep learning; therefore, revealing the underlying mechanism is of paramount importance. (44)investigated implicit bias of GD for classifying linearly separable data with linear models. They showed that GD trained with exponentially-tailed loss functions can implicitly maximize the\ell_{2}-margin during its convergence process, ultimately locating a max-margin solution. This discovery offers valuable insights into the superior generalization performance often observed with GD, as larger margins are generally associated with better generalization(4;2). However, the rate at which GD maximizes the margin has been shown to be merely\mathcal{O}(1/\log t)(44). This naturally leads to the question: can we design a better gradient-based algorithm to accelerate the margin maximization. In the pursuit of this,(37;19)has demonstrated that employing GD with aggressively loss-scaled step sizes can achieve polynomial rates in margin maximization. Notably,(19)specifically established that the rate of NGD is\mathcal{O}(1/t). Building on this,(21)further introduced a momentum-based gradient method by applying Nesterov acceleration to the dual formulation of this problem, which achieves a remarkable margin-maximization rate of\mathcal{O}(\log t/t^{2})and(50)further improved it to\mathcal{O}(1/t^{2}), currently standing as the state-of-the-art algorithm for this problem. Our Contributions.In this paper, we begin by introducing a toy dataset to elucidate the causes of inefficiency in GD/NGD and to clarify the underlying intuition for accelerating margin maximization. Subsequently, we demonstrate that these insights are applicable to a broader range of scenarios. [leftmargin=2em] We reveal that the rate of directional convergence and margin maximization is governed by the centripetal velocity–the component orthogonal to the max-margin direction. We show that under mild conditions on data distribution, NGD and GD will inevitably be trapped in a region where the centripetal velocity is diminished, thereby explaining the inefficiency of GD/NGD. Specifically, we establish that the aforementioned margin-maximization rates:\mathcal{O}(1/\log t)for GD and\mathcal{O}(1/t)for NGD also serve aslower bounds. Based on the above observations, we propose to speed up the margin maximization by maintaining a non-degenerate centripetal velocity. We show that there exists a favorable region, where the centripetal velocity is uniformly lower-bounded and moreover, we can reposition parameters into this region via a simplenorm rescaling. Leveraging these properties, we introduce an algorithm called Progressive Rescaling Gradient Descent (PRGD). Notably, we prove that PRGD can achieve both directional convergence and margin maximization at anexponentialrate\mathcal{O}(e^{-\Omega(t)}). This stands in stark contrast to all existing algorithms, which maximize the margin at a slow polynomial rate. Lastly, we validate our theoretical findings through both synthetic and real-world experiments. In particular, when applying PRGD to linearly non-separable datasets and homogenized deep neural networks—beyond the scope of our theory—we still observe consistent test performance improvements. In Table1, we summarize our main theoretical results and compare them with existing ones.
2310.00178v1	Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm	Contextual biasing refers to the problem of biasing the automatic speech recognition (ASR) systems towards rare entities that are relevant to the specific user or application scenarios. We propose algorithms for contextual biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During beam search, we boost the score of a token extension if it extends matching into a set of biasing phrases. Our method simulates the classical approaches often implemented in the weighted finite state transducer (WFST) framework, but avoids the FST language altogether, with careful considerations on memory footprint and efficiency on tensor processing units (TPUs) by vectorization. Without introducing additional model parameters, our method achieves significant word error rate (WER) reductions on biasing test sets by itself, and yields further performance gain when combined with a model-based biasing method.	Recent years have seen a tremendous explosion in voice user interfaces (VUIs), like voice search, assistant, and control applications. The success of VUI-based applications depends on the ability of the underlying Automatic Speech Recognition (ASR) system to properly transcribe phrases that are contextually relevant to the speaker, the listener, or both. Examples of contextually-relevant phrases include names of the speaker’s contacts and geographically-close points of interest. Contextually-relevant phrases are inherently hard to recognize because they represent instances of domain shift. For example, generally, it is much more likely for a single user to speak the name of one of their contacts than for any given contact name to occur in a given training data set; indeed, a given name or phrase may not appear at all in an ASR system’s training set in the case of unorthodox spellings (Ke$ha) or novel words (COVID-19). Further, contextually-relevant phrases may not be known until inference time, e.g., as the user of a voice assistant can add contact names any time before speaking. ASRcontextual biasingis a set of techniques which enables ASR systems to recognize contextually-relevant words without retraining. Contextual biasing can generally be grouped into model-based and inference-based approaches. Model-based methods typically incorporate a biasing component into an end-to-end (E2E) ASR system(Graves_12a;Chorowski15a;Chan_16a), which takes in biasing contexts as additional input to the E2E model. An attention mechanism(Vaswani_17a)is typically used to condition the model outputs on biasing contexts(munkhdalai2021fast;chang2021automatic;han2022improving)(see Sec3for more discussions). The more classical inference-based approach, dating back to the pre-E2E era, injects biasing contexts to boost decoding scores for the words or phrases in the biasing contexts to increase the probability of recognizing those words(apetar;Hall2015CompositionbasedOR). A compact search graph, based on Weighted Finite State Transducers (WFSTs,mohri2002weighted), is built to encompass the set of biasing phrases, and incorporated into the normal search graph which then transduces acoustic model outputs to word-level hypotheses. Weights are distributed along edges of the biasing search graph, so that when the acoustic model output extends the matching of the phrases, a bonus score is added to the hypothesis to help it survive beam search and increase its likelihood of becoming the top hypothesis. The approach was later extended to E2E models(zhao2019shallowfusion)where bonuses are incorporated at subword level. While E2E ASR systems have greatly simplified modeling and deployment, and that most components are readily implemented on GPU or TPU to enjoy parallel processing, FST-based biasing poses significant challenges for an efficient TPU-based implementation, due to their inherent sparse nature (adjacency matrices for FSTs are typically very sparse). In this work, we propose a TPU-friendly implementation of search-based biasing, leveraging the equivalence between the biasing FST and the efficient matching algorithm bykmp_siam, with careful considerations on memory complexity and efficiency through vectorization. Our algorithms can be incorporated into the beam search of any ASR system, in both the on-the-fly rescoring and shallow fusion manner. On large voice search datasets, our method achieves significant word error rate (WER) reductions on biasing test sets by itself, without introducing additional model parameters. And when plugged into a model-based biasing method, namely neural associative memory (NAM,munkhdalai2021fast), our method leads to further improved biasing accuracy. Our method enables learning with the discrete structure of ASR biasing, and can be potentially useful for other sequence transduction tasks.
2310.15976v2	Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization	signSGD is popular in nonconvex optimization due to its communication efficiency. Yet, existing analyses of signSGD rely on assuming that data are sampled with replacement in each iteration, contradicting the practical implementation where data are randomly reshuffled and sequentially fed into the algorithm. We bridge this gap by proving the first convergence result of signSGD with random reshuffling (SignRR) for nonconvex optimization. Given the dataset size $n$, the number of epochs of data passes $T$, and the variance bound of a stochastic gradient $\sigma^2$, we show that SignRR has the same convergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD \citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, which leverage variance-reduced gradients and momentum updates respectively, both converging at $O(\log (nT)/\sqrt{nT} + \log (nT)\sqrt{n}/\sqrt{T})$. In contrast with the analysis of signSGD, our results do not require an extremely large batch size in each iteration to be of the same order as the total number of iterations \citep{bernstein2018signsgd} or the signs of stochastic and true gradients match element-wise with a minimum probability of 1/2 \citep{safaryan2021stochastic}. We also extend our algorithms to cases where data are distributed across different machines, yielding dist-SignRVR and dist-SignRVM, both converging at $O(\log (n_0T)/\sqrt{n_0T} + \log (n_0T)\sqrt{n_0}/\sqrt{T})$, where $n_0$ is the dataset size of a single machine. We back up our theoretical findings through experiments on simulated and real-world problems, verifying that randomly reshuffled sign methods match or surpass existing baselines.	We study the following optimization problem where each individual functionf_{i}is smooth and nonconvex, and can be viewed as the loss function on different training data points in diverse machine learning applications. In general, solving the above nonconvex problem is NP hard(Hillar and Lim,2013). An alternative objective is to find an\epsilon-approximate stationary point\xb\in\RR^{d}of (1) such that\|\nabla f(\xb)\|_{2}\leq\epsilonfor some user-defined precision parameter\epsilon>0. To this end, gradient descent (GD) is typically used(Nesterov,2003). Nonetheless, GD is inefficient in large-scale, high-dimensional machine learning problems. In these cases, the dataset size (n) and the model size/dimension (d) are both considerable, causing standard GD methods to exhibit slow convergence and necessitate increased memory. Consequently, it becomes both challenging and crucial to develop efficient optimization algorithms to alleviate the computational burden caused by extensive datasets and enable model training from compressed information. To address the complexities caused by large datasets, numerous methods have been proposed(Reddi et al.,2016; Allen-Zhu and Hazan,2016; Lei et al.,2017; Nguyen et al.,2017a,b; Fang et al.,2018; Zhou et al.,2020), most of which are based on variants of stochastic gradient descent(Robbins and Monro,1951; Bottou,2009,2012, SGD), where the learning parameter\xb_{t}at time steptis updated based on the gradient of one or a mini-batch of individual functions that are sampled uniformly with replacement. Similarly, the challenges presented by extensive, deep models can be mitigated by using low-precision gradients(Seide et al.,2014; Zhang et al.,2017; Lin et al.,2017), referred to as gradient compression(Alistarh et al.,2017; Wen et al.,2017; Wang et al.,2018; Khirirat et al.,2018). The prime approach for gradient compression uses only the gradient’s sign in updates(Seide et al.,2014; Strom,2015; Carlson et al.,2015; Wen et al.,2017; Liu et al.,2018; Balles and Hennig,2018; Bernstein et al.,2018,2019), and recent theoretical developments have revealed the effectiveness of sign-based SGD in accelerating learning and compressing gradients into one-bit signals, especially for deep neural networks(Bernstein et al.,2018,2019; Safaryan and Richtárik,2021). In large-scale machine learning tasks, datasets and models are often distributed across multiple machines. This introduces communication complexities during the training process between local workers, which compute gradients, and a global server, which aggregates the information to update the model. Sign-based SGD approaches greatly reduce such communication costs and lessen the memory demands for storing substantial models(Seide et al.,2014; Zhang et al.,2017; Lin et al.,2017; Bernstein et al.,2019; Safaryan and Richtárik,2021). While recent studies of Sign-based Stochastic Gradient Descent (signSGD) approaches have made theoretical advancements(Bernstein et al.,2018,2019; Safaryan and Richtárik,2021; Chzhen and Schechtman,2023), prevailing analyses of sign-based SGD methods rely on the assumption of data being independently sampled with replacement at each iteration of the training. This contradicts the practical implementation in most large-scale machine learning problems, which typically utilize without-replacement SGD or Random Reshuffling (RR). Specifically, RR shuffles the dataset at the beginning of each epoch to acquire a random permutation of the index set[n], and updates the model by sequentially reading data from this permutation. The sequential passage of the dataset makes RR easier to implement and more computationally efficient than SGD(Bottou,2012; Recht and Ré,2012,2013; Sun,2019; Gürbüzbalaban et al.,2021). Despite its empirical superiority over SGD, proving RR’s faster convergence theoretically is challenging due to statistical dependencies among iterates. It is not until recently that RR is shown to converge faster than SGD in both convex problems(Haochen and Sra,2019; Nagaraj et al.,2019; Safran and Shamir,2020; Gürbüzbalaban et al.,2021)and nonconvex problems(Meng et al.,2019; Li et al.,2019). However, the convergence analysis of signSGD with random reshuffling remains unaddressed, which leaves the following questions open: Does without-replacement signSGD converge? Is the convergence rate comparable to that of with-replacement signSGD? In this work, we provide the first convergence analysis of algorithms using sign-based updates and random reshuffling-based data sampling schemes. The primary challenges in our analyses stem from two key factors: (1) the introduction of biased gradient estimators by the sign-based compressor, and (2) the complex statistical dependency between iterations fostered by the without-replacement sampling scheme. Our main contributionsare summarized as follows. [leftmargin=*,nosep] We study signSGD with random reshuffling, viz.,SignRR, for solving (1), which updates the parameter using the sign of the stochastic gradient and reads the dataset in a sequential way. These features makeSignRRboth computationally efficient and memory efficient. We show thatSignRRconverges at the rate ofO(\log(nT)/\sqrt{nT}+\|\sigma\|_{1}), wherenis the dataset size,Tis the number of passes of the dataset, and\sigma=(\sigma_{1},\ldots,\sigma_{d})is the component-wise upper bound of the variance of the stochastic gradient vector. This result matches the convergence rate of signSGD(Bernstein et al.,2018; Safaryan and Richtárik,2021)when using constant stepsize and the variance of the stochastic gradient noise is small. To the best of our knowledge, this is the first convergence result of sign-based SGD under the random reshuffling sampling scheme. We then proposeSignRVRusing the idea of stochastic variance reduced gradient(Johnson and Zhang,2013). We prove thatSignRVRconverges at the rate ofO(\log(nT)/\sqrt{nT}+\log(nT)\sqrt{n}/\sqrt{T})even when the variance bound of the stochastic gradient is large. It is worth noting that our result does not require additional assumptions such as that the batch size in each iteration is of the same order as the total number of iterations(Bernstein et al.,2018)or that the probability of signs of the true and stochastic gradients matching each other elementwisely is at least1/2(Safaryan and Richtárik,2021)111This is referred to as the success probability bounds (SPB) assumption inSafaryan and Richtárik (2021).. Under the random reshuffling scheme,SignRVRis the first sign-based algorithm that guarantees the convergence for nonconvex functions without relying on a large mini-batch. To enhance the empirical performance, we introduce a momentum update to our proposed algorithm, resulting inSignRVM, which enjoys the same convergence rate asSignRVR. However, our result does not depend on additional conditions such as the bounded variance or an unimodal symmetric stochastic gradient noise, which are necessary for Signum(Bernstein et al.,2018). Furthermore, Signum mandates a unique warmup stage where the sign of the stochastic gradient is used rather than the momentum sign. Their analysis is contingent upon an accurate estimate of the warmup phase length, which can be challenging. Our algorithm mitigates these issues, providing a more efficient and accessible solution. We also extend bothSignRVRandSignRVMto the distributed setting where data are stored across multiple worker machines. The parameter updates rely on the majority vote for the sign of the stochastic gradient. We show that the resulting algorithmsdist-SignRVRanddist-SignRVMconverge at rateO(\log(n_{0}T)/\sqrt{n_{0}T}+\log(n_{0}T)\sqrt{n_{0}}/\sqrt{T}), wheren_{0}is the dataset size on a single machine. In contrast to SSDM(Safaryan and Richtárik,2021)and SPAESIGNSGD(Jin et al.,2023)in the same distributed setting, our results do not rely on the bounded variance or the SPB assumption, which can be difficult to validate in practical scenarios. To substantiate our theoretical findings, we conducted experiments on both simulated and real-world datasets. The experimental results align well with our theoretical predictions. Specifically, in the centralized setting, we found that: (i) SignRR’s performance is comparable to that of signSGD; (ii) SignRVR surpasses SignRR and other baselines due to reduced variance; (iii)SignRVMconsistently attains the best convergence result among all sign-based methods, and matches the performance of SGD, RR, and Adam. In the distributed setting, we also observed that: (i)dist-SignRVRoutperforms signSGD with majority vote. (ii)dist-SignRVMoutshinesdist-SignRVR, SIGNUM with majority vote, and SSDM in its performance. We denote[k]as the set\{0,1,\ldots,k-1\},k\in\NN^{+}.\lVert\xb\rVert_{1}=\sum_{i=1}^{n}|\xb_{i}|and\|\xb\|_{2}=\sqrt{\xb^{\top}\xb}are the\ell_{1}-norm and the\ell_{2}-norm of a vector\xb\in\RR^{d}respectively. For a functionf(T),O(f(T))is used to hide constant factors with respect toT. For an eventA, the indicator function is defined as\ind\{A\}=1, whenAhappens and\ind\{A\}=0otherwise. We use[\xb]_{j}to denote thej-th coordinate of\xb.
2307.05152v2	Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders	Experimental particle physics demands a sophisticated trigger and acquisition system capable to efficiently retain the collisions of interest for further investigation. Heterogeneous computing with the employment of FPGA cards may emerge as a trending technology for the triggering strategy of the upcoming high-luminosity program of the Large Hadron Collider at CERN. In this context, we present two machine-learning algorithms for selecting events where neutral long-lived particles decay within the detector volume studying their accuracy and inference time when accelerated on commercially available Xilinx FPGA accelerator cards. The inference time is also confronted with a CPU- and GPU-based hardware setup. The proposed new algorithms are proven efficient for the considered benchmark physics scenario and their accuracy is found to not degrade when accelerated on the FPGA cards. The results indicate that all tested architectures fit within the latency requirements of a second-level trigger farm and that exploiting accelerator technologies for real-time processing of particle-physics collisions is a promising research field that deserves additional investigations, in particular with machine-learning models with a large number of trainable parameters.	A crucial aspect of particle physics experiments at colliders is the trigger and data acquisition system. In fact, efficiently collecting the products of the collisions resulting in interesting physics processes is a challenging task, for both the complexity and sparsity of the detector data to be analysed and the stringent latency requirements imposed by the high frequency of the occurring collisions. Both the ATLAS and CMS experiments[1,2], being the two multi-purpose particle-physics detectors with cylindrical geometry currently running at the Large Hadron Collider (LHC) at CERN[3], employ a two-tier trigger system for selecting the products of the proton-proton collisions, so-called events, for storage and analyses[4,5]. The initial 40 MHz rate of proton-proton collisions produced by the LHC is first reduced to\mathcal{O}(100\mbox{ kHz})by a hardware-based Level-1 (L1) trigger system, and then further reduced down to\mathcal{O}(1\mbox{ kHz})by a software High Level Trigger (HLT). Triggering events is therefore an optimisation problem: how to maximise the variety and richness of the physics program with the limitations in terms of latency, throughput, data transfer, and storage capabilities. The selection at L1 must occur with a latency of\mathcal{O}(10^{-1}\div 10^{0}\,\mu\mbox{s})and is obtained by using low-resolution detector information. The selection at the HLT, instead, is based on software running on a commercial CPU-based farm, and, with access to more granular detector information, needs to occur with typical latency times between\mathcal{O}(10^{-1}\div 10^{0}\,\mbox{s}). With the upcoming high-luminosity phase of the LHC (HL-LHC)[6], the design of the trigger and data acquisition system needs to cope with the higher occupancy and the higher number of readout channels of the upgraded detectors. The advancement in single-processor computing performance is not adequate, and more modern solutions of heterogeneous computing may offer an interesting avenue of exploration[7,8]. In particular the works presented in[9,10]suggest that FPGA-accelerated inference of machine-learning algorithms is a promising option for particle physice experiments, requiring minimal modifications to the current computing models. In this context, we study the possibility to implement algorithms based on deep neural networks for the event selection at the HLT, and to use commercial accelerator boards based on FPGA processors to improve the performance in terms of processing time and throughput. FPGAs are reconfigurable hardware architectures which can be adapted for specific tasks and are traditionally programmed using hardware description languages like VHDL or Verilog. In recent years several tools and libraries were developed to facilitate the implementation and deployment of both traditional and machine learning algorithms on FPGAs. The Xilinx[11]company for example has released Vitis-AI x[12], being an AI-inference development platform for AMD devices, boards, and Alveo data center acceleration cards. Similarly, Intel has developed the FPGA AI Suite based on OpenVINO[13]. In this work we construct and characterize deep neural networks targeting the selection of events where neutral long-lived particles decay within the detector volume. We present the design and the results of the implementation in a working engineering pipeline that starts from the pre-processing of the input data, to the training of the deep neural network-based model, to the optimization and deployment on two Xilinx FPGA accelerators, the Alveo U50 and the Alveo U250, all based on the use of publicly available libraries. Two approaches based on a deep convolutional neural network and on an autoencoder are developed and presented. A comparison of the performances of the deployed algorithms in CPU, GPU and FPGA accelerators is also shown. We stress the complementarity of this approach, also in terms of development and maintenance of the needed libraries, with respect to the ongoing work of deploying neural networks on FPGA boards with a latency compatible with the selection occurring at L1, where a dedicated software library,hls4ml, is being developed[14,15], and dedicated implementations have been recently proposed[16]. The paper is organized as follows. In Section2we describe the physics benchmark and the dataset. In Section3we introduce the trigger strategies we have tested, and the associated algorithms: a convolutional neural network (CNN) and an autoencoder (AE) architecture. In Section4we present and discuss the results. Finally, we provide our concluding remarks in Section5. The dataset used for the presented results is made available in Zenodo at the link in Ref.[17]. The codes for constructing the algorithms, converting the models and evaluating their performances are available on request by contacting the authors.
2401.10935v1	SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents	Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a visual GUI agent -- SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -- the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding dataset that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. The model, data and code are available at https://github.com/njucckevin/SeeClick.	Developing autonomous agents to assist humans on computing devices has been a persistent goal for artificial intelligence(Shiet al.,2017; Liet al.,2020a; Furutaet al.,2023; Zhouet al.,2023). These Graphical User Interface (GUI) agent systems aim to mimic human interactions in solving complex tasks, thereby enhancing efficiency and reducing manual effort, with examples like Siri and Copilot. Recent advances in Large Language Models (LLMs) such as GPT-4(OpenAI,2023)and LLaMA(Touvronet al.,2023)have significantly propelled the evolution of GUI agents(Guret al.,2023a; Zhouet al.,2023). These agents interact with the environment through extracted structured texts, e.g., HTML from web pages, then elicit LLM for planning, reasoning, and action gereration(Kimet al.,2023; Zhenget al.,2023). However, GUI agents depend on structured text face three inherent limitations: (1) Structured text is not always accessible, especially for iOS or desktop applications where acquiring such information is challenging(Shawet al.,2023); (2) The verbose nature of structured text serves as an inefficient context for LLMs, while also omitting crucial information such as layout, images, and icons(Denget al.,2023); (3) The variety of structured text - including HTML, DOM, and Android VH - necessitates the curation of task-specific observation and action spaces(Kimet al.,2023; Zhouet al.,2023). These entrenched deficiencies in text-based approaches call for an alternative solution. In this paper, we propose a visual GUI agent built on Large Vision-Language Models (LVLMs) - SeeClick. Inspired by human interaction with GUIs, SeeClick performs low-level actions like clicking or typing directly by observing interface screenshots. This methodology bypasses the need for interacting with cumbersome structured text, empowering SeeClick as a universal visual agent suitable for various GUI platforms. Building such visual agents involves a foundational challenge: GUI grounding - the capacity to accurately locate screen elements based on instructions, which is absent in current LVLMs. To tackle this challenge, SeeClick enhances LVLM with a GUI grounding pre-training strategy. We devise a method to automate the curation of web grounding data and adapt public mobile UI datasets to obtain mobile grounding data. SeeClick employs the above-curated dataset for continual pre-training of the LVLM, enabling it to accurately locate elements such as text, widgets, and icons in various GUI environments. Given GUI grounding is a fundamental yet underexplored capacity for GUI agents, we createdScreenSpot, the first realistic GUI grounding evaluation benchmark across various GUI platforms.ScreenSpotcontains over 600 screenshots and 1200 instructions from iOS, Android, macOS, Windows, and web environments, and specifically includes both text-based elements and a variety of widgets and icons. Evaluation results confirm SeeClick’s superiority over current LVLMs, validating the effectiveness of GUI grounding pre-training. Finally, we adapted SeeClick to three downstream agent tasks: MiniWob(Shiet al.,2017), AITW(Rawleset al.,2023), and Mind2Web(Denget al.,2023). As a purely visual-based agent, SeeClick achieves impressive performance across three tasks. Notably, SeeClick outperforms the visual baseline Pix2Act with only 0.3% training data on MiniWob. Moreover, experimental results on three tasks consistently support our finding that improvement in GUI grounding directly correlates with enhanced agent task performance. This paper makes the following contributions: We develop a unified visual GUI agent SeeClick, which directly performs clicking and typing actions based on interface screenshots across diverse GUI platforms. We prospectively explore GUI grounding for visual GUI agents, and enhanced SeeClick with our proposed GUI grounding pre-training strategy. We create a realistic GUI grounding benchmarkScreenSpot, encompassing more than 1200 instructions from various GUI platforms. Experimental results onScreenSpotand three agent tasks demonstrate that enhancing agents’ grounding capacity is key to improving performance in downstream agent tasks.
2304.12180v2	Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies	Unrolled computation graphs are prevalent throughout machine learning but present challenges to automatic differentiation (AD) gradient estimation methods when their loss functions exhibit extreme local sensitivtiy, discontinuity, or blackbox characteristics. In such scenarios, online evolution strategies methods are a more capable alternative, while being more parallelizable than vanilla evolution strategies (ES) by interleaving partial unrolls and gradient updates. In this work, we propose a general class of unbiased online evolution strategies methods. We analytically and empirically characterize the variance of this class of gradient estimators and identify the one with the least variance, which we term Noise-Reuse Evolution Strategies (NRES). Experimentally, we show NRES results in faster convergence than existing AD and ES methods in terms of wall-clock time and number of unroll steps across a variety of applications, including learning dynamical systems, meta-training learned optimizers, and reinforcement learning.	First-order optimization methods are a foundational tool in machine learning. With many such methods (e.g., SGD, Adam) available in existing software, ML training often amounts to specifying a computation graph of learnable parameters and computing some notion of gradients to pass into an off-the-shelf optimizer. Here,unrolled computation graphs(UCGs), where the same learnable parameters are repeatedly applied to transition a dynamical system’s inner state, have found their use in various applications such as recurrent neural networks(Hochreiter and Schmidhuber,1997; Cho et al.,2014), meta-training learned optimizers(Metz et al.,2019; Harrison et al.,2022), hyperpameter tuning(Maclaurin et al.,2015; Franceschi et al.,2017), dataset distillation(Wang et al.,2018; Cazenavette et al.,2022), and reinforcement learning(Sutton et al.,1999; Schulman et al.,2015). While a large number of automatic differentiation (AD) methods exist to estimate gradients in UCGs(Baydin et al.,2018), they often perform poorly over loss landscapes with extreme local sensitivity and cannot handle black-box computation dynamics or discontinuous losses(Parmas et al.,2018; Metz et al.,2019,2021). To handle these shortcomings, evolution strategies (ES) have become a popular alternative to produce gradient estimates in UCGs(Salimans et al.,2017). ES methods convolve the (potentially pathological or discontinuous) loss surface with a Gaussian distribution in the learnable parameter space, making it smoother and infinitely differentiable. Unfortunately, vanilla ES methods cannot be applied online444Onlinehere means a method can produce gradient estimates using onlya truncation windowof an unrolled computation graphinstead of the full graph, thus allowing the interleaving of partial unrolls and gradient updates.— the computation must reach the end of the graph to produce a gradient update, thus incurring large update latency for long UCGs. To address this, a recently proposed approach, Persistent Evolution Strategies(Vicol et al.,2021)(\PES), samples a new Gaussian noise in every truncation unroll and accumulates the past sampled noises to get rid of the estimation bias in its online application. In this work, we investigate the coupling of the noise sampling frequency and the gradient estimation frequency in\PES. By decoupling these two values, we arrive at a more general class of unbiased, online ES gradient estimators. Through a variance characterization of these estimators, we find that the one which provably has the lowest variance in fact reuses the same noise for the entire time horizon (instead of over a single truncation window as in\PES). We name this methodNoise-Reuse Evolution Strategies(\NRES). In addition to being simple to implement,\NRESconverges faster than\PESacross a wide variety of applications due to its reduced variance. Overall, we make the following contributions: [leftmargin=*] We propose a class of unbiased online evolution strategies gradient estimators for unrolled computation graphs that generalize Persistent Evolution Strategies(Vicol et al.,2021). We analytically and empirically characterize the variance of this class of estimators and identify the lowest-variance estimator which we name Noise-Reuse Evolution Strategies (\NRES). We identify the connection between\NRESand the existing offline ES method\FullESand show that\NRESis a better alternative to\FullESboth in terms of parallelizability and variance. We demonstrate that\NREScan provide optimization convergence speedups (up to 5-60\times) over AD/ES baselines in terms of wall-clock time and number of unroll steps in applications of 1) learning dynamical systems, 2) meta-training learned optimizers, and 3) reinforcement learning.
2310.14747v3	MCC-KD: Multi-CoT Consistent Knowledge Distillation	Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution datasets but also highlight its robust generalization ability on out-of-distribution datasets.	Recently, large language models (LLMs) such as ChatGPT have exhibited impressive emergent capabilities, showcasing their competence in various tasks, including those demanding complex reasoning. While directly providing answers without generating intermediate steps may lead to errors and limited interpretability, chain of thought (CoT)(chain-of-thought)prompting enables LLMs to break down reasoning tasks into a series of intermediate steps, guiding the model to generate the subsequent steps before arriving at the final answer. The effectiveness of CoT prompting has been demonstrated on diverse reasoning taskskojima2022large. Despite the effectiveness of CoT prompting, recent studies(chain-of-thought;teaching-small2022;fu2023specializing)have shown that these reasoning capabilities only manifest in language models with over 100 billion parameters, such as PaLM (540B)(chowdhery2022palm)and GPT-3 (175B)(gpt-3). These LLMs with massive parameter sizes require significant computational resources during both training and inference, which restrict their deployment on resource-limited platforms. While LLMs could be accessed through API calls, there are still several challenges to overcome, including network instability, difficulty in customizing the models, and privacy concerns. Therefore, an alternative approach is to deploy smaller language models such as LLaMA-7B/13B(llama)and FlanT5-XL/XXL(chung2022scaling), which have fewer than 13 billion parameters. Through knowledge distillation (KD)(knowledge-distilling), the reasoning capabilities can be transferred from LLMs to these smaller models. However, traditional KD techniques require the teacher model to provide output logits or hidden layer features, which cannot be readily applied to LLMs due to the limited accessibility of their internals. One potential solution is to leverage rationales generated by LLMs to train smaller models, thereby acquiring their reasoning abilities(ho2022reasoning-teacher;teaching-small2022;fu2023specializing). However, these methods for rationale distillation also face several challenges. Firstly, the limited diversity in reasoning paths may lead to a dilemma where the student model simply mimics the superficial style of the teacher model’s outputs(false-promise)or overfits the training data, resulting in limited generalization capabilities. Secondly, despite the existence of multiple rationales leading to the same answer for each given question (as depicted in Figure1), these methods neglect the consistency among different rationales in reaching the predicted answer when training the student model. Such oversights can undermine the stability of student models during training and impair their generalization capabilities. To address these challenges, we propose Multi-CoT Consistent Knowledge Distillation (MCC-KD), a novel solution that incorporates two pivotal characteristics. Firstly, this approach leverages multiple diverse rationales for each given question and aims to improve their consistency in predicting the answer. This improvement is expected to enhance the stability and generalizability of the student models. Secondly, we introduce a similarity-based method to facilitate the selection of diverse rationales. MCC-KD draws inspiration from real-world teaching scenarios, where presenting multiple distinct solutions to one problem benefits the student’s learning process. With these inherent advantages, MCC-KD enables the smaller models to acquire reasoning capabilities from larger models through effective knowledge distillation. We conduct extensive experiments with LLaMA(llama)and FlanT5(chung2022scaling)on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results demonstrate the effectiveness and superiority of MCC-KD over previous CoT-based knowledge distillation methods. For example, MCC-KD achieves an accuracy improvement from point 38.01 to 41.58 on the GSM8K(cobbe2021training)dataset with LLaMA-7B. Moreover, the generalization experiments reveal that MCC-KD achieves a substantial accuracy improvement, raising the performance from point 47.69 to 49.52 on the out-of-distribution dataset ASDiv(miao-etal-2020-diverse)using FlanT5-XXL. These findings provide compelling evidence of the effectiveness and robustness of MCC-KD.
2304.06305v1	Boosting Convolutional Neural Networks with Middle Spectrum Grouped Convolution	"This paper proposes a novel module called middle spectrum grouped convolution (MSGC) for efficient deep convolutional neural networks (DCNNs) with the mechanism of grouped convolution. It explores the broad ""middle spectrum"" area between channel pruning and conventional grouped convolution. Compared with channel pruning, MSGC can retain most of the information from the input feature maps due to the group mechanism; compared with grouped convolution, MSGC benefits from the learnability, the core of channel pruning, for constructing its group topology, leading to better channel division. The middle spectrum area is unfolded along four dimensions: group-wise, layer-wise, sample-wise, and attention-wise, making it possible to reveal more powerful and interpretable structures. As a result, the proposed module acts as a booster that can reduce the computational cost of the host backbones for general image recognition with even improved predictive accuracy. For example, in the experiments on ImageNet dataset for image classification, MSGC can reduce the multiply-accumulates (MACs) of ResNet-18 and ResNet-50 by half but still increase the Top-1 accuracy by more than 1%. With 35% reduction of MACs, MSGC can also increase the Top-1 accuracy of the MobileNetV2 backbone. Results on MS COCO dataset for object detection show similar observations. Our code and trained models are available at https://github.com/hellozhuo/msgc."	DCNN has revolutionized the computer vision community in many applications, from preliminary tasks like salient object detection[63]and edge detection[54], to semantically more sophisticated tasks like image classification[37], object detection[44], and human pose estimation[4]. The increasing prediction accuracy is usually at the cost of considerable consumed energies, with large computational cost by deep models[59]. How to reduce the computational cost of DCNNs without sacrificing accuracy has been a pressing topic, especially in the era of edge computing, since deep models are moving to resource constrained devices like smart phones and IoTs. In recent years, numerous efforts have been made in the community to tackle this issue, such as network pruning[32,78], compact and lighter network design[22,52], network quantization[74,55],etc. Among these attempts, network pruning[32,77,46,21,16]and grouped convolution[5,53,70,28,56]have attracted tremendous research interests. The former aims to prune the unnecessary redundant parts of deep models to make them lighter and more efficient to run, while the latter focuses on constructing compact structures by splitting computational operations into groups. It is not surprising that many research works considered either of these two methods alone, since they are structurally independent. In this paper, we give a novel view by regarding them as two poles of a network designing spectrum (Fig.1), inside which we find there are a big variety of structural possibilities that incorporate these two paradigms. Based on that, we further build our module that can effectively reveal powerful structures within the spectrum, outperforming both the previous channel pruning and grouped convolution based counterparts in both accuracy and computational cost. To make our motivation clearer, we start by giving a brief introduction on both methods below. Without loss of generality, supposing a convolutional layer takes the input tensor\mathbf{X}withCchannels and generates the output tensor\mathbf{Y}, the convolutional operation can be formulated as: where[,]represents the concatenation operation,Gis the number of groups,f^{i}is a standard convolutional function that generates1/Gpart of\mathbf{Y}, and\pi^{i}is a selecting function extracting a sub-tensor from\mathbf{X}with possibly fewer channels. WhenG=1and\pi\equiv\mathbf{1}\in{\{1\}}^{1\times C}, which means all the input channels are kept as they are, the formulation reduces to the standard convolution. As the network often contains feature redundancy, it might be unnecessary to keep all the input channels. It is essentially the design spirit of many channel pruning methods, which focus on tuning\pi. The grouped convolution is formulated when we set1<G\leq Cand the reduction of feature redundancy is neatly organized in a predefined way. For example,\{\pi^{i}(\mathbf{X})|i=1,2,..,G\}are a series of regularly partitioned segments of the input tensor in the channel extent. This derived a lot of classical approaches in recent years like Deep Roots[28], ResNeXt[70], ShuffleNet[76], IGC series[75,69,56], UGConvs[80], ChannelNets[11], SCGNet[73], sharing grouped convolution[5], and the extreme case of depthwise separable convolution used in the Inception modules[57,58], MobileNet[22], and Xception[8], whereG=C. One main focus of these methods is to design the correlation between groups to make the convolution complementary[56](means that each output channel has at least one path to any of the input channels in the connection topology). Such encouragement of inter-group communication plays an important role in breaking the intrinsic order of channels[26]to maximally preserve the prediction accuracy. However, we may rethink the relationship between the grouped convolution and channel pruning methods, since both aim to reduce the network redundancy from the channel extent. The design strategies behind them help us to derive the method proposed in the paper. The two frameworks are illustrated inFig.2. On the one hand, channel pruning[77,81,20,21,46]attempts to learn the most important feature maps that contribute to the final prediction accuracy. Suchlearnabilityhelps channel pruning to identify the unimportant channels that can be pruned without degrading the network performance significantly. However, since it is not easy to guarantee the correctness of such identification, or due to the bounded representation capacity by a pruning rate, there might always be some channels that contain specific useful and meaningful information than other channels. This is evidenced by the fact that a certain pruning rate usually causes a performance drop. On the other hand, grouped convolution preserves all the input channels that may to some extent, avoid the above issue of information loss. In effect, different from channel pruning, grouped convolution hypothesizes that the grouped network can still learn enough information to give comparable prediction accuracy than the original network, by regularly sparsifying the connection but keeping all the input channels intact. In this way, the efforts on finding the most important channels, can be circumvented. In other words, grouped convolution is powered bychannel preservation. Generally, learning to prune and learning to group are manners that both lead to efficient and accurate networks. Previous works tend to consider them separately, which restrict their methods to go beyond. We believe there is a better balance between information preservation and learnability that can be achieved by integrating both worlds toward building more powerful network structures. To achieve this, we proposed our module named MSGC (middle spectrum grouped convolution), which enables the learning process to find a structure in between, by unfolding the spectrum along four dimensions: Group-wise: injecting learnability for each group to learn how to segment channels; Layer-wise: allowing layer-dependent pruning ratios; Sample-wise: dynamically building grouped connection topology for individual samples (middle ofFig.2); Attention-wise: decoupling channel gating and attention for individual groups. We conduct extensive experiments including image classification and object detection on large-scale datasets on the ResNet[18], MobileNetV2[52], and DenseNet[27]backbones, which consistently verify the superiority of MSGC compared with prior state-of-the-art methods. Those methods include both existing pruning-based and grouped convolution-based ones. Notably, we achieve not only the MAC reduction, but enhanced accuracy as well, due to the flexibility of forming groups. For example, on the ImageNet[9]dataset for image classification, MSGC reduces computational cost of the ResNet backbones by 50% and the MobileNetV2 backbone by 35% with even improved accuracy. On the MS COCO[42]dataset for object detection, MSGC can also effectively slim the backbones with negligible performance drop. In addition, MSGC can also be used to simply improve the prediction accuracy of the original networks with a relatively small pruning rate. In this case, the main role of MSGC transfers from an “computation booster” to a strong “accuracy booster”. The rest of this article is organized as follows. InSec.II, we review the related works. Following that, we illustrate our method in detail inSec.III. A comprehensive experimental comparison with state-of-the-art methods is provided inSec.IV, with detailed ablation studies. Finally, we conclude our paper inSec.V.
2311.03278v1	Discretizing Numerical Attributes: An Analysis of Human Perceptions	Machine learning (ML) has employed various discretization methods to partition numerical attributes into intervals. However, an effective discretization technique remains elusive in many ML applications, such as association rule mining. Moreover, the existing discretization techniques do not reflect best the impact of the independent numerical factor on the dependent numerical target factor. This research aims to establish a benchmark approach for numerical attribute partitioning. We conduct an extensive analysis of human perceptions of partitioning a numerical attribute and compare these perceptions with the results obtained from our two proposed measures. We also examine the perceptions of experts in data science, statistics, and engineering by employing numerical data visualization techniques. The analysis of collected responses reveals that $68.7\%$ of human responses approximately closely align with the values generated by our proposed measures. Based on these findings, our proposed measures may be used as one of the methods for discretizing the numerical attributes.	Various types of variables are available in real-world data. However, discrete values have explicit roles in statistics, machine learning (ML), and data mining. Presently, there is no benchmark approach to find the optimum partitions for discretizing complex real-world datasets. Generally, if a factor impacts another factor, in that case, humans can easily perceive the compartments or partitions because the human brain can easily perceive the differences between the factors and detect the partitions. However, it is not easy for a human or even an expert to find the appropriate compartments in complex real-world datasets. In state-of-the-art, to find the optimum partitions of the numerical values, various discretization techniques have also been presented in the literature[23,13,22]. However, the existing discretization techniques do not reflect best the impact of the independent numerical factor on the dependent numerical target factor. Moreover, no existing discretization approach uses numerical attributes as influencing and response factors. To find the cut-points for the cases of two-partitioning and three-partitioning, we have proposed two measuresLeast Squared Ordinate-Directed Impact Measure(LSQM) andLeast Absolute-Difference Ordinate-Directed Impact Measure(LADM)[18]. These measures provide a simple way to find partitions of numerical attributes that reflect best the impact of one independent numerical attribute on a dependent numerical attribute. In these measures, we use numerical attributes as influencing and response factors to distinguish them from the existing approaches. In this paper, the outcome ofLSQMandLADMmeasures are compared with the human-perceived cut-points to assess the accuracy of the measures. We use numerical attributes as influencing and response factors to distinguish them from the existing approaches. A series of graphs with different data points are used to collect the human responses. Here, data scientists, ML experts and other non-expert persons are referred to as humans. The idea of this research emerged from the research on partial conditionalization[8,9], association rule mining (ARM)[31,29]and numerical association rule mining (NARM)[32,19,20]. These papers discuss the discretization process as an essential step for NARM. Moreover, research on discretizing the numerical attributes is an essential step in frequent itemset mining, especially for quantitative association rule mining (QARM)[32]. In the same sequence, we have also presented a tool named Grand report[27]and a framework[30]for unifying ARM, statistical reasoning, and online analytical processing. These paper strengthens the generalization of ARM by finding the partitions of numerical attributes that reflect best the impact of one independent numerical attribute on a dependent numerical attribute. Our vision is to develop an ecosystem to generalize the ML approaches by significantly improving the ARM from different dimensions. The paper is organized as follows. In Sect.2, we delve into the discussion of related work concerning discretization and its connection with human perception. This section aims to provide a comprehensive overview of prior research and studies that have explored the topic from different angles. In Sect.3, we explain the motivation for conducting this study. Sect.4describes theLSQMandLADMmeasures. Then, we describe the design of the experiment in Sect.5. In Sect.6, we present the analysis and results. The conclusion and future work are given in Sect.7.
2308.07498v1	DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation	VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions. It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose DREAMWALKER -- a world model based VLN-CE agent. The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation. DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, DREAMWALKER is able to make strategic planning through large amounts of ``mental experiments.'' Moreover, the imagined future scenarios reflect our agent's intention, making its decision-making process more transparent. Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.	For decades, the AI community has strived to develop intelligent robots that can understand human instructions and carry them out. As a small step towards this long-held goal, vision-language navigation (VLN)[6]— the task of entailing autonomous agents to navigate innever-before-seen3D environments with language instructions — gained growing attention. In the standard VLN setting, agent’s movement is constrained to a small set of pre-defined sparse locations. As pointed out by[47], such over-simplified, discrete task setup involves many unrealistic assumptions such as known topology, perfect localization, and deterministic transition. To better reflect the challenges of real world navigation, Krantz\etal[47]update the discrete VLN to a continuous ver- sion – VLN-CE (VLN in continuous environments), where the agent is free to traverse any unobstructed location with low-level actions. VLN-CE proved much more challenging than its discrete counterpart: the performance gap between the state-of-the-arts in the two settings is more than 20%, in terms of episode success rate. The main challenge posed by VLN-CE lies in the demand of strategic planning in conti- nuous environments with low-level actions. As a direct response, we developed aworld modelbased VLN-CE agent, calledDreamwalker. Previous studies in cognitive science[17,37,36]suggest that humans build a mental model of the local surrounding, based on our limited senses. This internal world model summarizes our knowle- dge about the environment and serves as the basis for many high-level meta-skills,\eg, reasoning, planning, decision-making, and interpretation. The world model theory is one source of the idea of model-based Reinforcement Learning (RL)[76]and promotes many recent advances in robot con- trol[72,65,61,94]. Keeping this grand idea in head, we letDreamwalkerexplicitly abstract crucial characteristics of its continuous surrounding environment to adiscrete,struc-turedrepresentation (Fig.1). This allowsDreamwalkerto “imagine” a lot of future possible navigation plans and eva- luate the corresponding consequences entirely in the mind, before taking actual low-level actions in the real world. In this way,Dreamwalkertakes the challenge of VLN-CE head-on: mental planning with discrete world model enables efficient navigation behavior in continuous environments. Technically, the world model is built upon agent’s past experiences and can make predictions about the future. It con- tains two parts:i)Anenvironment graph(EG) is constructed as a composition of selected or predicted waypoints and their typological relations. EG collects agent’s temporary know- ledge about its surrounding.ii)A learnablescene synthesizer(SS) predicts future observations from a waypoint with mul- tiple steps. SS embeds agent’s stable knowledge about envi- ronments, such as general room layout rules and transition dynamics, into its network parameters. Based on the world model,Dreamwalkersynthesizes various future navigation trajectories, and assesses their progress towards the final target location. Then, the best mental plan is found by Monte Carlo Tree Search[43]and executed in the continuous world with low-level actions. With the navigation proceeds, EG is further updated for making a new round of mental planning. Notably, ourDreamwalkersignificantly distinguishes itself from prior VLN-CE solutions[70,30,45,46]in the following aspects:i)Recent advanced solutions are essentially model-free methods. While in principle a representation of the environment could beimplicitlylearned through model-free RL, the reinforcement signal may be too weak to quickly learn such a representation and how to make use of it. In contrast, our agent plans its actions within an explicit, and abstract model of the continuous environment.ii)Existing agents navigate by greedily and reactively choosing between a small set of nearby waypoints, based on their hidden state which compresses past observations. They tend to be shortsighted, due to the absence of reliable strategies for capturing information for achieving the future[23]. YetDreamwalkercan use the world model to anticipate the impacts of possible actions and plan strategic behavior.iii)The future scenarios created by the world model explain the intention ofDreamwalkerin a way that human can understand, making its behaviors more interpretable[9,87]. Extensive experiments on VLN-CE dataset[47]confirm that ourDreamwalkergains promising performance with the appealing ability of real-time behavioral interpretation. This work is expected to foster future research in developing more strategic, robust, and interpretable VLN-CE agents.
2308.11485v1	Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features	Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir	Content-Based Image Retrieval (CBIR) is a fundamental task in multimedia and computer vision which has undergone a continuous evolution since its early years(Smeulderset al.,2000), moving from the use of engineered features like SIFT to CNNs(Zhenget al.,2017; Liet al.,2021). It has been applied to many different specialized domains like artworks and cultural heritage(Companioni-Brito,Claudiaet al.,2018; Baldratiet al.,2022), commerce(Zhanet al.,2021; Donget al.,2021), surveillance(Ahmadet al.,2018), nature(Ionescuet al.,2019,2020). In the basic form, the query is composed of only an image, of which features are computed and compared with the ones extracted by a database of images. We can extend CBIR systems to improve their effectiveness by adding additional information to the query image. For example, interactive image retrieval systems extend CBIR systems by adding some form of user feedback, e.g. to provide some measure of relevance(Banerjeeet al.,2018). In composed image retrieval, the visual query is extended to an image-language pair(Liuet al.,2021)where a short textual description, typically expressed in natural language, may request constraints and desired changes or add specifications on some attributes of the retrieved results(Jandialet al.,2022).Figure1illustrates two examples of this task. In both queries, a user selects a reference image and then provides additional requests in the form of text, e.g. asking to change details, texture, color, or shape features of the reference image. Composed image retrieval systems find applications in various domains such as web search, e-commerce, and surveillance. However, developing solutions for this task can be challenging due to the need for incorporating feedback and user intent while addressing the semantic gap between image and text content. Very recently, researchers proved that deep neural networks combining visual and language modalities like CLIP(Radfordet al.,2021), ALIGN(Jiaet al.,2021), and the more recent method proposed in(Chenget al.,2021), trained using an image-caption alignment objective on large-scale internet data, can obtain impressive zero-shot transfer on a myriad of downstream tasks like image classification, text-based image retrieval, and object detection. In this work, we show that features obtained from vision and language pretrained (VLP) models – we employed CLIP-based features – can be effectively used to implement a composed image retrieval system where user feedback is provided as natural language input to provide additional (or contrasting) requirements concerning those embedded in the visual features of the image used to query the system. Firstly, we apply the system to the fashion domain, performing experiments on the challenging FashionIQ dataset(Wuet al.,2021). Then, to study the generalization capabilities to a broader image domain, we perform experiments on the newly introduced CIRR dataset(Liuet al.,2021). Experiments show that the proposed approach obtains state-of-the-art results on both datasets. To summarize, we highlight our main contributions as follows: We propose a novel task-oriented fine-tuning scheme for adapting vision-language models to the composed image retrieval task. The aim of such a task-oriented adaptation scheme is to reduce the mismatch between the large-scale pre-training and the downstream task. We propose a novel two-stage approach that combines task-oriented fine-tuning with the training of a Combiner network which can perform a fine-grained merging of the multimodal features. This two-stage approach achieves state-of-the-art results on two standard and challenging datasets: FashionIQ and CIRR. We address the issue of using the CLIP model with images having a high aspect ratio since the CLIP visual encoder can input only square pictures. We propose a novel preprocess pipeline suited for image retrieval tasks that helps to reduce content information loss compared to the standard CLIP preprocess pipeline. To provide further insight into the workings of our proposed system, we perform several qualitative experiments. The first experiment aims to demonstrate how our approach affects the feature distribution in the embedding spaces and the impact of pairwise feature distances on retrieval performance. Additionally, we report visualization experiments utilizing the gradCAM technique(Selvarajuet al.,2019)to gain a deeper understanding of the image portions that are most significant during retrieval.
2302.05593v1	ReMIX: Regret Minimization for Monotonic Value Function Factorization in Multiagent Reinforcement Learning	Value function factorization methods have become a dominant approach for cooperative multiagent reinforcement learning under a centralized training and decentralized execution paradigm. By factorizing the optimal joint action-value function using a monotonic mixing function of agents' utilities, these algorithms ensure the consistency between joint and local action selections for decentralized decision-making. Nevertheless, the use of monotonic mixing functions also induces representational limitations. Finding the optimal projection of an unrestricted mixing function onto monotonic function classes is still an open problem. To this end, we propose ReMIX, formulating this optimal projection problem for value function factorization as a regret minimization over the projection weights of different state-action values. Such an optimization problem can be relaxed and solved using the Lagrangian multiplier method to obtain the close-form optimal projection weights. By minimizing the resulting policy regret, we can narrow the gap between the optimal and the restricted monotonic mixing functions, thus obtaining an improved monotonic value function factorization. Our experimental results on Predator-Prey and StarCraft Multiagent Challenge environments demonstrate the effectiveness of our method, indicating the better capabilities of handling environments with non-monotonic value functions.	Reinforcement learning has demonstrated great potential in solving challenging real-world problems, from autonomous driving[4,10]to robotics and planning[21,18,11]. In many scenarios, these tasks involve multiple agents within the same environment and thus require multiagent reinforcement learning (MARL)[32,12,1,34]to coordinate agents and learn desired behaviors from their experiences. Due to practical communication constraints and the need to cope with vast joint action space, MARL algorithms often leverage fully decentralized policies but learn them in a centralized fashion with access to additional information during training. Value function factorization methods, e.g., QMIX[26], QPLEX[33], Qatten[36], FOP[37], and DOP[35], have been a dominant approach for such centralized training and decentralized execution (CTDE) MARL[15]. By factorizing the optimal joint action value function using a monotonic mixing function of per-agent utilities, these algorithms ensure the consistency between joint and local action selections for decentralized decision-making. Superior performance has been reported in many MARL tasks, such as the StarCraft Multiagent Challenge (SMAC)[27]. It is known that value function factorization can be viewed as an operator[6], which first computes the optimal joint action value functions as targets and then projects them onto the space representable by monotonic function classes. The projected monotonic mixing functions enable efficient maximization yet allow decentralized decision-making. However, it also poses representational limitations. For instance, QMIX leverages a universal approximator for non-linear monotonic mixing functions. It prevents QMIX from efficiently representing joint action value functions where agents’ orderings of their action choices depend on each other[20]. Later, the authors in the paper[25]proposed an improved projection using Weighted QMIX (WQMIX). It assigns higher weights to the values of optimal joint actions than the suboptimal ones, resulting in a better projection that more accurately represents these optimal values. However, WQMIX relies purely on a heuristic design – such as Centrally-Weighted (CW) and Optimistically-Weighted (OW) – where such weight term is a constant. Finding an optimal projection onto the monotonic function class is still an open problem. To this end, we propose\NAME, formulating the optimal projection problem for value function factorization as a regret minimization over the projection weights of different state-action values. Specifically, we construct an optimal policy following the optimal joint action-value function and a restricted policy using its projection onto monotonic mixing functions. A policy regret is then defined as the difference between the expected discounted reward of the optimal policy and that of the restricted policy. By minimizing such policy regret through an upper bound, we can narrow the gap between the optimal and restricted policies and thus force the projected monotonic value function to approach the optimal one during learning, leading to an optimal monotonic factorization with minimum regret. We note that while policy regret minimization has been employed to formulate various optimizations in reinforcement learning, such as optimal prioritized experience replay[19]and loss function design[13], to the best of our knowledge, this is the first proposal for optimizing value function factorization in MARL through policy regret minimization. We show that the proposed regret minimization can be solved via the Lagrangian method[2]considering an upper bound. By examining a weighted Bellman equation involving monotonic mixing functions and per-agent critics, we leverage the implicit function theorem[16]and derive Karush–Kuhn–Tucker (KKT)[7]conditions to find the optimal projection weights in closed form. Our results highlight the key principles contributing to optimal monotonic value function factorization. The optimal projection weights can be interpreted to consist of four components: Bellman error, value underestimates, the gradient of the monotonic mixing function, and the on-policiness of available transitions. We note that the first two terms relating to Bellman error and value underestimates are consistent with the weighting heuristics proposed in WQMIX, thus providing a quantitative justification and recovering WQMIX as a special case. More importantly, our analysis reveals that an optimal value function factorization should also depend on the gradient of the monotonic mixing function and the positive impact of more current transitions. Following the theoretical results, we provide a tractable approximation of the optimal projection weights and propose a MARL algorithm of\NAMEwith regret-minimizing monotonic value function factorization. We validate the effectiveness of\NAMEin Predator-Prey[3]and SMAC. Compared with state-of-the-art factorization-based MARL algorithms (e.g., WQMIX, QPlex, FOP, DOP),\NAMEis shown to better cope with environments with non-monotonic value functions, resulting in improved convergence and superior empirical performance. The main contributions of our work are as follows: We propose a novel method,\NAME, formulating the optimal value function factorization as a policy regret minimization and solving the weights of the optimal projection in closed form. The theoretical results and tractable weight approximations of\NAMEenable cooperative MARL algorithms with improved value function factorization. Experiment results of\NAMEin Predator-Prey and SMAC environments demonstrate superior convergence and empirical performance over state-of-the-art factorization-based methods. We further perform ablation studies to demonstrate the contribution of each component in our design.
2310.04148v1	Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning	The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at \url{https://github.com/ydchen0806/dbMiM}.	Neuron segmentation is a crucial task for neuroscientists that allows for the analysis of the distribution and morphology of neurons, providing valuable insights into the connectomics researchSheridanet al.(2022); Krasowskiet al.(2017). Electron microscopy (EM) is the mainstream method for accurately identifying neural structures, but the dense nature of neurons and the presence of artifacts and deformations in EM images make the labeling process costly and decrease the credibility of existing annotation dataDenget al.(2022); Zhouet al.(2019); Chenet al.(2023a). Therefore, fully supervised neuron segmentation methods meet great challenges, especially when applied to large scale EM data. Self-supervised methods have emerged as a solution to the limitations of fully supervised methods, which can be roughly divided into two categories: contrastive learning-based approach and mask image model (MIM)-based approach. The former requires a large number of positive and negative samplesChenet al.(2020); Grillet al.(2020); Youet al.(2022); Chenet al.(2023b)and relies heavily on data augmentationCaronet al.(2021), making it a high-cost option for 3D biomedical images. The latter aims to learn useful structural information in images by masking and recovering certain voxels, which has been recently applied to pretraining biomedical images, showing improvements in downstream tasksZhouet al.(2022); Tanget al.(2022); Huanget al.(2022a). However, the highly localized and structured nature of EM data, as well as the existence of considerable noise, make it inefficient to directly employ the existing MIM in extracting useful information for neuron segmentation. It has also been observed that the masking ratio and masking strategy of MIM are highly sensitive and the optimal ones vary greatly across different datasets. Adjusting these configurations to train large models requires significant manual efforts and resources. In this paper, targeting the neuron segmentation task, we propose a novel decision-based MIM relying on multi-agent reinforcement learning (MARL)Littman (1994)for automatically selecting the appropriate masking ratio and masking strategy, which consists of a target network and a policy network. Our approach partitions the input EM volume into patches and treats each patch as a basic control unit. The overall multi-agent task is modeled as a search process for patch masking strategies, where the action space for each patch is to either keep the original voxels or mask them. The feedback of the target network, in the form of the reconstruction loss, serves as the team reward signal for guiding the policy network to adaptively learn masking strategies that are beneficial for the pretraining taskFoersteret al.(2016). To improve the stability of training and achieve optimal joint decision-making for the entire volume, all agent networks share parameters and are trained in parallel. Furthermore, we introduce the HOG feature as an additional reconstruction loss to enable the target network to learn more structure information. Finally, following a UNETR decoder designHatamizadehet al.(2022), we add a segmentation head to the pretrained target network in the finetuning stage. Experimental results in Figure1show that our decision-based MIM achieves clearer reconstruction results than the original MAEHeet al.(2022)in the pretraining phase. Overall, our main contribution lies in the following aspects: 1) We propose an efficient self-supervised method, named decision-based MIM, for EM neuron segmentation using unlabeled EM data. To the best of our knowledge, it is the first effort that large-scale transformer pretraining is conducted on this task. 2) We propose a MARL-based approach for searching the optimal masking ratio and masking strategy by treating each patch as an agent with a shared policy, effectively exploring the search space and capturing dependencies between voxels. 3) We introduce the HOG feature as an additional reconstruction loss of our decision-based MIM, improving the convergence speed of network training and the performance of the downstream segmentation task. 4) We comprehensively demonstrate the effectiveness of our proposed method on two representative EM datasets, especially against alternative self-supervised methods on the task of neuron segmentation.
2306.05045v1	Spain on Fire: A novel wildfire risk assessment model based on image satellite processing and atmospheric information	Each year, wildfires destroy larger areas of Spain, threatening numerous ecosystems. Humans cause 90% of them (negligence or provoked) and the behaviour of individuals is unpredictable. However, atmospheric and environmental variables affect the spread of wildfires, and they can be analysed by using deep learning. In order to mitigate the damage of these events we proposed the novel Wildfire Assessment Model (WAM). Our aim is to anticipate the economic and ecological impact of a wildfire, assisting managers resource allocation and decision making for dangerous regions in Spain, Castilla y Le\'on and Andaluc\'ia. The WAM uses a residual-style convolutional network architecture to perform regression over atmospheric variables and the greenness index, computing necessary resources, the control and extinction time, and the expected burnt surface area. It is first pre-trained with self-supervision over 100,000 examples of unlabelled data with a masked patch prediction objective and fine-tuned using 311 samples of wildfires. The pretraining allows the model to understand situations, outclassing baselines with a 1,4%, 3,7% and 9% improvement estimating human, heavy and aerial resources; 21% and 10,2% in expected extinction and control time; and 18,8% in expected burnt area. Using the WAM we provide an example assessment map of Castilla y Le\'on, visualizing the expected resources over an entire region.	Forests cover 30% of terrestrial ecosystems, representing a total of 4.06 billion hectares[mansoor2022elevation]and are home to 80% of amphibians, 75% of birds and 68% of mammals worldwide[vie2009wildlife]. At the environmental level, in addition to affecting biodiversity (animal and plant), forests are also an important factor in soil transformation, vegetation succession, soil degradation, and air quality, among others[rodrigues2014modeling]. Wildfires threaten to disturb these ecosystems with increasing frequency and damage, a worrying byproduct of climate change[senande2022spatial,jones2022global]among many other factors. Fires can also directly affect humans destroying buildings, burning crops, causing the death of animals, or directly have an effect on the health of the population due to the direct action (burns) or indirect action (smoke inhalation) of fire[shi2022characterization]. In recent years the Copernicus Sentinel-3 mission recorded 16000 wildfires throughout the world in August 2018 and 79000 in the same period of 2019, which is a large increase in a single year[fernandez2016copernicus]. In Spain, between January and August, the burnt area in 2022 was 247.667 hectares, an area almost five times larger compared to the previous year, 51.571 hectares. In addition, the number of large fires has increased from 16 fires in 2021 to 51 between January and August 2022. Thus, the highest figure since 2012, when 34 large fires occurred, had already been reached111https://www.epdata.es/datos/incendios-forestales-datos-estadisticas-cifras/267. Some European countries show an increasing number of wildfires also in the burnt area, as stated by European Forestry Fire Information System (EFFIS). For example, Romania, Italy and France have increased the number of fires and burnt area, showing double the increase in 2022 compared to the annual average between 2006 and 2021222https://effis.jrc.ec.europa.eu/apps/effis.statistics/estimates. The growing in severity over the years is concerning. Quality tools are needed to help control and manage resources in order to minimize the damage they cause. Fire causes that can be aggregated into two groups: natural or anthropogenic. The latter can be divided into two types:accidental, due to human negligence; orprovoked, whether caused by arsonists or pyromaniacs[short2022empirical]. In most cases the cause of wildfires is unknown, but if the origin is known human causes account for more than 90% of the total number of wildfires. This makes fire prediction very difficult, since human behaviour is still unpredictable[tiefenbacher2012approaches,menezes2022lightning,pozo2022assessing]. For this reason, an accurate fire prediction model can be considered unfeasible as it would have to rely on individual human behaviour, i.e., when a person is going to commit a reckless act that triggers a fire or when a person is going to decide to start a fire. However, the severity of a fire is tightly related to the existing environmental and vegetation state conditions before its occurrence. Therefore, severity could be estimated observing these conditions[cansler2022previous,bonannella2022characterization]. Atmospheric and environmental variables that influence fire intensity are usually georeferenced variables with a latitude and longitude. Their similarity to images makes this modality analyzable with Computer Vision (CV) techniques such as Convolutional Neural Networks (CNN). These architectures have outstanding ability to identify patterns in data, which enhances the performance of earlier systems based on machine learning models. Due to their strong performance in a variety of tasks, such as computer vision or natural language processing, this advantage has made them a benchmark in deep learning (DL)[lecun2015deep]. However, most state-of-the-art articles use machine learning techniques, and the few that use DL do not explore options such as CNN. One of the possible reasons for not exploring this type of technique is the lack of data, since most datasets contain a small number of samples. For this reason, we have decided to explore the option of creating a pretrained autoencoder (AE) that is capable of learning the patterns and understanding the atmospheric and environmental variables used. Later, we transfer the encoder to a regression task to predict the variables related to fire management. The article can be divided into five main modules, as shown in Figure1. The main contribution of this manuscript is the creation of a regression model based on Deep Learning techniques for wildfire management, called Wildfire Assessment Model (WAM), pretrained with atmospheric and environmental variables of the area and finetuned with a very small data set, 445 samples. However, this is not the only contribution presented in this manuscript: The approach used to create the input data is novel and provides more information than traditional approaches. WAM model creates a Deep Learning baseline for comparisons for future work in the field. A new AE model that uses categories instead of the original sample, extracting information on how the meteorological and environmental variables work. This manuscript has been structured as follows: Section2summarizes the most relevant work in the state of the art, with a special focus on fire severity and burnt area prediction problems; Section3, describes in detail the atmospheric and environmental variables used, as well as the labels that are intended to be predicted; Section4, describes the proposed methodology for this problem, how we have prepared the samples that are the input to our models, the encoder model and the regression model, the baselines against which we compare ourselves and the final visualization; Section5presents the experimental results, and Section6presents the main conclusions and possible lines of future work.
2308.02190v1	Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition	Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of EMO-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotion-relevant features from corpus-specific ones. On the other hand, our dual-level emotion alignment introduces an adaptive threshold pseudo-labeling to select confident target samples for class-level alignment, and performs corpus-level alignment to jointly guide model for learning class-discriminative corpus-invariant features across corpora. Extensive experimental results demonstrate the superior performance of EMO-DNA over the state-of-the-art methods in several cross-corpus scenarios. Source code is available at https://github.com/Jiaxin-Ye/Emo-DNA.	Speech emotion recognition (SER) aims to automatically recognize human emotions from speech signals(Schuller,2018), which has attracted much attention in human-computer interaction (HCI), mental diagnostic tools, in-car board systems, etc(Ayadi et al.,2011; Wu et al.,2023). However, in these practical applications, significant discrepancies exist between training and real-world corpora, arising from different languages, cultures, speakers, contents, data scales, etc. These discrepancies across corpora lead to significant idiosyncratic variations impeding the generalization of current SER technology. Therefore, SER faces enormous challenges when being applied to cross-corpus scenarios, which require a strong generalization of inferring speech emotion from a well-labeled corpus to an unlabeled one. To solve these challenges, many researchers have explored unsupervised domain adaptation (UDA)(Wen et al.,2022; Gao et al.,2022; Zhang et al.,2022)to match global distributions of source and target data, which aims to learn domain-invariant representations. Recent advances in UDA have proven effective in addressing the challenge of corpus111Here, we use the term “corpus” to refer to the “domain” in the UDA. They are exchangeable throughout this paper.misalignment for the cross-corpus SER(Deng et al.,2017). One key idea to achieve corpus alignment is to reduce distribution shifts between the source and target corpus, which is commonly implemented through statistic divergence alignment and adversarial learning(Liu et al.,2022). Statistic divergence alignment methods(Liu et al.,2020b; Lu et al.,2022)utilize various divergence measures, such as maximum mean discrepancy (MMD)(Rozantsev et al.,2019), to minimize domain discrepancy in a latent feature space. Luet al.(Lu et al.,2022)propose a hierarchical alignment framework based on Multi-Kernel MMD (MK-MMD) criterion to learn corpus-invariant features. Instead of introducing statistic divergence measures, adversarial learning methods(Latif et al.,2022; Gao et al.,2022; Feng et al.,2022)adaptively learn a measure of divergence under the guidance of adversarial domain discriminators. Fenget al.(Feng et al.,2022)present a noise representation learning framework that leveraged adversarial training to retain emotion-relevant information. While these dominant UDA methods can effectively align the distributions across corpora at feature level and improve the generalization ability of models to some extent, they still suffer from the following two dilemmas in cross-corpus SER. 1)False alignmentrefers to falsely aligning features of different attributes (e.g., emotion, corpus, language, and speaker) during domain adaptation, leading to performance degradation. Due to the nonlinear manifold structures underlying data distributions(Cai et al.,2019), the emotion-relevant and corpus-specific features might be highly entangled in corpus-invariant feature space. Most previous efforts focus on matching global marginal distributions between source and target corpora(Luo et al.,2019), which fail to decouple corpus information and preserve emotion information from the aligned distribution. This may lead to the false alignment problem, i.e., aligning emotion-irrelevant features (e.g., corpus-specific ones) with emotion-relevant ones222Here, we use the term “emotion-relevant features” to refer to the features are emotion-discriminative. The “corpus-specific features” denotes features that contain specific information related to the corpus itself and are corpus-discriminative.. 2)Class confusionrefers to ignoring class consistency when aligning data distributions of different domains, leading to performance degradation. Although matching global source and target data distributions can pull the features of source and target corpora closer, it also mixes features of different classes. This can cause class confusion since the class-discriminative information from the source corpus cannot be easily transferred to the target corpus. In other words, if the model fails to maintain class consistency during corpus alignment, it ultimately results in the erroneous mapping of target corpus features (i.e., negative transfer). In this paper, we propose a novelEmotionDecoupling aNdAlignment learning framework (Emo-DNA) to tackle the aforementioned issues for cross-corpus SER. For Emo-DNA, we hope that feature decoupling and alignment are two key techniques to cross-corpus SER, like the complementary paired duplexes in deoxyribonucleic acid. On one hand, for contrastive emotion decoupling, we first leverage two encoders to learn emotion-relevant and corpus-specific features of each corpus, guided by source emotion labels and corpora labels, respectively. Then we propose a novel contrastive decoupling loss based on emotion-relevant and corpus-specific prototypes, which encourages decoupling emotion features from corpus-specific features by pushing emotion prototypes away from the corpus-specific ones. On the other hand, dual-level emotion alignment is based on class-level and corpus-level to achieve effective alignment. We leverage an adaptive threshold pseudo-labeling to select confident samples of the target corpus, and then devise a contrastive class alignment loss based on pseudo-labeled target samples and the source samples to encourage class alignment across corpora by maximizing the mutual information of representations of the same class. We further introduce an explicit corpus alignment loss to minimize corpus discrepancy across corpora considering the significant domain shifts between different corpora. During the training process, the contrastive emotion decoupling enforces the features to be emotion-relevant for the corpus alignment, and dual-level emotion alignment encourages the features to be emotion-relevant corpus-invariant with class discrimination. Decoupling and alignment promote progressive learning with each other and enable the model to achieve robust adaptability in the target corpus, establishing new state-of-the-art performance on several benchmark datasets. The main contributions of this paper are summarized as follows: [topsep=0pt] We propose a novel UDA framework for cross-corpus SER, called Emo-DNA, which decouples emotion-relevant features from highly coupled distorted feature space and learns the emotion-relevant corpus-invariant features. To the best of our knowledge, Emo-DNA makes the first attempt at identifying the false alignment and class confusion problems as the key limiting factors for cross-corpus SER. We propose a contrastive emotion decoupling module to strengthen the separability of emotion-relevant features from corpus-specific ones in a contrastive learning framework. We propose a dual-level emotion alignment module based on the class- and corpus-level to jointly guide the model to learn class-discriminative corpus-invariant features. Extensive experimental results demonstrate effectiveness and superiority of Emo-DNA over the state-of-the-art methods in several cross-corpus scenarios.
2309.02552v1	Data Aggregation for Hierarchical Clustering	Hierarchical Agglomerative Clustering (HAC) is likely the earliest and most flexible clustering method, because it can be used with many distances, similarities, and various linkage strategies. It is often used when the number of clusters the data set forms is unknown and some sort of hierarchy in the data is plausible. Most algorithms for HAC operate on a full distance matrix, and therefore require quadratic memory. The standard algorithm also has cubic runtime to produce a full hierarchy. Both memory and runtime are especially problematic in the context of embedded or otherwise very resource-constrained systems. In this section, we present how data aggregation with BETULA, a numerically stable version of the well known BIRCH data aggregation algorithm, can be used to make HAC viable on systems with constrained resources with only small losses on clustering quality, and hence allow exploratory data analysis of very large data sets.	Hierarchical Agglomerative Clustering (HAC) is a popular clustering method that is especially useful if a hierarchy of clusters exists in the data set. Initially, each data entry is seen as a cluster of one. In each hierarchy level the two clusters with the least distance (c.f. Section2) between them are combined until the whole data set is in one cluster. Another commonly used name, Simple Agglomerative Hierarchical Nesting (SAHN), reflects this easy-to-understand core idea. The standard algorithm used for HAC, known as AGNES[Kaufman/Rousseeuw/90a], requires the pairwise distances between all data points to be stored in a distance matrix, and when merging clusters, two columns and rows in this matrix are combined using the Lance-Williams equations[Lance/Williams/66a,Lance/Williams/67a]. AGNES can be utilized with different primary distance functions, but also with different cluster distances (commonly called linkages), see Section2. Hierarchical Agglomerative Clustering, like many other clustering methods, is a rather resource-hungry process commonly implemented using{O\mathopen{}(N^{2})\mathclose{}}memory and{O\mathopen{}(N^{2})\mathclose{}}to{O\mathopen{}(N^{3})\mathclose{}}time, depending on the exact algorithm implemented. One possibility to reduce the resource demands for big data or when using small embedded systems is data aggregation. The BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)[Zhang/etal/96a,Zhang/etal/97a]algorithm is a well-known data aggregation technique for clustering. BIRCH is a multi-step clustering algorithm that aggregates the data into a tree structure known as CF-tree before the actual clustering. We will first review some fundamentals of hierarchical clustering, and then discuss an improved version of BIRCH, called BETULA[Lang/Schubert/2020a,Lang/Schubert/2021a], that avoids some numerical problems in the original BIRCH. We then show how it can be used to accelerate HAC for big data, and reduce its memory requirements.
2311.14749v1	Compositional Zero-shot Learning via Progressive Language-based Observations	"Compositional zero-shot learning aims to recognize unseen state-object compositions by leveraging known primitives (state and object) during training. However, effectively modeling interactions between primitives and generalizing knowledge to novel compositions remains a perennial challenge. There are two key factors: object-conditioned and state-conditioned variance, i.e., the appearance of states (or objects) can vary significantly when combined with different objects (or states). For instance, the state ""old"" can signify a vintage design for a ""car"" or an advanced age for a ""cat"". In this paper, we argue that these variances can be mitigated by predicting composition categories based on pre-observed primitive. To this end, we propose Progressive Language-based Observations (PLO), which can dynamically determine a better observation order of primitives. These observations comprise a series of concepts or languages that allow the model to understand image content in a step-by-step manner. Specifically, PLO adopts pre-trained vision-language models (VLMs) to empower the model with observation capabilities. We further devise two variants: 1) PLO-VLM: a two-step method, where a pre-observing classifier dynamically determines the observation order of two primitives. 2) PLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to craft composition-specific prompts for step-by-step observing. Extensive ablations on three challenging datasets demonstrate the superiority of PLO compared with state-of-the-art methods, affirming its abilities in compositional recognition."	“There’s more than one way to skin a cat.” What enables us humans to recognize new concepts we have never encountered before? It all comes down to our capacity to generalize learned knowledge to unseen domains. For instance, when presented with concepts “green apple” and “yellow banana”, we can instantly recognize and imagine the concept “green banana” by combining state “green” with object “banana”. Inspired by this innate cognitive ability of humans, Compositional Zero-Shot Learning (CZSL) emerges to tackle the challenge of recognizingunseenstate-object compositions (e.g., “green banana”) by leveraging visible primitives (i.e., state and object) in compositional concepts during training and applying the knowledge during inference[mancini2021open,naeem2021learning,li2022siamese]. Effectively modeling the interactions between state and object primitives, as well as extrapolating the understanding of seen compositions to unseen ones, poses major challenges in CZSL. Concretely, it revolves around two critical factors: 1)Object-conditioned Variance: Wherein the visual representations of the same state category can vary considerably when different objects are involved. As depicted in Figure1(a), considering the state “old” in the context of modifying a “car” and a “cat”, it may refer to a vintage design with classic curves and retro elements for the “car”, evoking a sense of nostalgia and history, whereas for the “cat”, it denotes the senior age of feline with features like grey fur, reflecting the passage of time and aging process. 2)State-conditioned Variance: It pertains to the variations in the appearance of an object when combined with different states. As shown in Figure1(a), for composition “peeled banana”, the “banana” exhibits a smooth texture and a pale appearance, as the outer peel is removed. In contrast, for the composition “sliced banana”, the “banana” takes on a sliced appearance with visible segments. Previous approaches in CZSL often construct separate classifiers for recognizing states and objects simultaneously, overlooking their intrinsic relationship. Recent efforts have made strides in addressing thefirst factorby adopting a two-stage method with an object-then-state order[wang2023learning,huo2022procc,kim2023hierarchical]. Prioritizing the prediction of the object primitive allows the model to capture salient visual cues (e.g., shapes), thereby enhancing the overall comprehension of compositions. Subsequently, armed with the knowledge of the object primitives, the CZSL model sequentially refines its understanding by classifying the state primitives conditioned on guided object features. Nonetheless, we argue thatthere is more than one way to skin a cat, and the human cognition process will progressively collect different observations for specific compositions, in asimple to complexmanner. In certain cases, such as the composition “ripe banana” in Figure1(b), the object itself, “banana”, possesses highly salient visual cues that make it easily recognizable due to its curving shape and vibrant yellow color. Once we establish that it is a “banana”, we can then further analyze its state and recognize it as a “ripe banana” by observing additional visual cues,e.g., the presence of brown spots on the yellow skin. In contrast, for compositions like “mashed banana”, possess distinct visual features primarily related to the state “mashed” rather than the object. The mushy texture becomes the prominent aspect that captures our attention. Consequently, through further analysis of extra visual features,e.g., yellow and sticky material, we refine our recognition and discern it as a “mashed banana”. In this paper, inspired by the human-like cognition process, we propose a novel approach,ProgressiveLanguage-basedObservations (PLO) for CZSL. Specifically, PLO dynamically determines the order of progressive observations in the form of language, building upon the pre-trained vision-language models (VLMs),e.g., CLIP[radford2021learning]. These observations comprise a series of languages that allow the model to observe the image’s content step by step. Due to training with image-text pairs, these VLMs endow the model withobservingcapability by measuring the similarity between the two modalities within the same space. For dynamic progressive observation, we propose two variants: PLO-VLM and PLO-LLM. InPLO-VLM, we introduce a two-step observation strategy that adopts a pre-observing classifier based on VLM to dynamically determine the order of primitive-containing languages based on image features. Subsequently, leveraging the observed primitive knowledge (semantic features from primitive prompts), we integrate this information via a cross-modal attention module for category prediction of the remaining primitive. InPLO-LLM, we further extend to a multi-step observation scheme, employing large language models (LLMs),e.g., GPT[brown2020language]to design composition-specific prompts (e.g., “yellow, mushy substance” in Figure1(b)) for each composition category, thus obtaining a composition-specific observation order. This method allows us to selectively extract features at each observation step, boosting the model’s ability to understand and recognize composition categories more effectively. Three popular and challenging CZSL datasets MIT-States[isola2015discovering], UT-Zappos[naeem2021learning], and C-GQA[yu2014fine]are used for evaluation. Extensive results show that our PLO exceeds the current state-of-the-art CZSL methods with significant gains in both closed-world and open-world settings. In summary, the main contributions of our work are three-fold: We propose the novel PLO for CZSL. To the best of our knowledge, it is the first work to dynamically allocate the order of observations using language, enabling effective prediction of unseen state-object compositions. We introduce two model variants: 1) PLO-VLM: It uses a pre-observing classifier to determine observation order based on image features. 2) PLO-LLM: It employs LLMs to design composition-specific observation prompts. Extensive results on multiple datasets demonstrate that PLO outperforms existing methods, showcasing its effectiveness in recognizingunseenstate-object compositions.
2303.06064v1	Non-invasive Waveform Analysis for Emergency Triage via Simulated Hemorrhage: An Experimental Study using Novel Dynamic Lower Body Negative Pressure Model	The extent to which advanced waveform analysis of non-invasive physiological signals can diagnose levels of hypovolemia remains insufficiently explored. The present study explores the discriminative ability of a deep learning (DL) framework to classify levels of ongoing hypovolemia, simulated via novel dynamic lower body negative pressure (LBNP) model among healthy volunteers. We used a dynamic LBNP protocol as opposed to the traditional model, where LBNP is applied in a predictable step-wise, progressively descending manner. This dynamic LBNP version assists in circumventing the problem posed in terms of time dependency, as in real-life pre-hospital settings, intravascular blood volume may fluctuate due to volume resuscitation. A supervised DL-based framework for ternary classification was realized by segmenting the underlying noninvasive signal and labeling segments with corresponding LBNP target levels. The proposed DL model with two inputs was trained with respective time-frequency representations extracted on waveform segments to classify each of them into blood volume loss: Class 1 (mild); Class 2 (moderate); or Class 3 (severe). At the outset, the latent space derived at the end of the DL model via late fusion among both inputs assists in enhanced classification performance. When evaluated in a 3-fold cross-validation setup with stratified subjects, the experimental findings demonstrated PPG to be a potential surrogate for variations in blood volume with average classification performance, AUROC: 0.8861, AUPRC: 0.8141, $F1$-score:72.16%, Sensitivity:79.06 %, and Specificity:89.21 %. Our proposed DL algorithm on PPG signal demonstrates the possibility of capturing the complex interplay in physiological responses related to both bleeding and fluid resuscitation using this challenging LBNP setup.	Hemorrhage with blood volume loss is one of the leading potentially preventable causes of death in trauma patients(convertino2022advanced,). Hypotension is a late sign during blood volume loss due to associated physiological compensatory mechanisms. For this reason, early diagnosis of ongoing mild to moderate hemorrhage is difficult, especially in young and healthy subjects. Even invasive arterial blood pressure (ABP), exhibits poor sensitivity due to human compensatory responseschen2020estimating. The other vital signs, including heart rate and blood oxygen saturation, also have low specificity and sensitivity for estimating blood volume loss. Researchers have resorted to exploring various models that can artificially simulate hemorrhage. One such model is LBNPcooke2004lower;convertino2001lower;convertino2022ai;convertino2011use;van2018support. In this model of hypovolemia, healthy volunteers are placed in an air-tight chamber to which different levels of negative pressure is applied. This retains blood in the veins of the lower extremities and pelvis, creating graded central hypovolemia. Different LBNP-levels correspond to different levels of hypovolemia. Most studies to datecooke2004lower;convertino2011use;van2018support;ji2013heart, have applied LBNP in a predictable stepwise, progressive descending manner based on the hypothesis that”as the time elapses there is a substantial steady and linear loss of blood among the test subjects”. When testing algorithms for classifying levels of LBNP and degree of hypovolemia, this predictability based on time of LBNP can pose a problem. For instance, in a real-life pre-hospital emergency setting, volume resuscitation may be administrated during ongoing bleeding. We therefore propose an experimental setup with added degree of randomness in LBNP levels to avoid complete predictability by time. For the same reason, we also introduce unequal duration at each LBNP level. Hence, our proposed experimental setup is an attempt to emulate the patient with bleeding and fluid resuscitation as may be the case in pre-hospital treatment. This experimental model is more robust in the classification of the entire dynamic LBNP trajectory for the simulated hemorrhage. To our knowledge, no such reliable artificial intelligence method currently exists to predict the different likelihoods among the entire trajectory of applied LBNP, and thus assist to infer the stage of hemodynamic instability independent of time. Recently, studies on artificial intelligence (AI) based algorithms have indicated that continuous analyses of noninvasive arterial waveform analysis (AWFA) reflect the information pertaining to the compensatory mechanisms compared to other standard vital signsconvertino2022ai;chew2013haemodynamic;convertino2013estimation. Thus, making the earlier diagnosis of hemorrhage possible by the detection of hypovolemia prior to overt hemodynamic decompensationconvertino2016compensatory. Hence, the design of such AI-driven predictive algorithms holds the potential to reduce morbidity and mortality among patients with hemorrhagedavies2020ability;summers2009validation;hatib2018machine. In the initial screening of trauma patients, assessment is often restricted to electrocardiogram (ECG), non-invasive photoplethysmography (PPG; giving arterial oxygen saturation), and blood pressure; the first two being continuous waveforms, the latter with intermittent values. However, the PPG signal is generally considered a potential measure for variations in blood volume because of its ability to detect intravascular volume changesconvertino2022advanced;chen2020estimating;scully2012using. Prior studies have reported that PPG-based amplitude-derived features have the potential to measure dynamic blood volume lossselvaraj2011early;shamir1999pulse;cannesson2007respiratory. Pulse-arrival-timeliang2018hypertension;mukkamala2015toward(based on both ECG and PPG) also known as pulse transit time is used in arterial wave propagation theory for blood loss estimationdjupedal2022effects. Current early hemorrhage detection studies based on machine learning (ML) approaches rely on AWFA that mostly employs morphological changes in the features of PPG signalschen2020estimating;convertino2011use;elgendi2018toward;chen2020development;pinsky2020parsimony. However AWFA coupled with ML techniques and the aforementioned techniques involves complex feature extraction to capture the subtle information for the compensatory mechanisms in the arterial waveforms. Following are the limitations involved in the cumbersome feature extraction for PPG morphological theory and artery wave propagation theory: (i) In artery wave propagation theory, the fiducial points of each heartbeat in both ECG and PPG need to be extracted correctlyelgendi2013systolic;elgendi2014detection. (ii) This further adds the need to have proper sync among the two modalities and also both signals have to be of high quality. (iii) It is inevitable to have optimal filteringelgendi2016optimal. Hence, the morphological features are quite sensitive to signal quality, movement (placement) of the sensors, towards skin properties, and hence hinder the performancemejia2022comparison. Unlike the analysis of non-invasive signals in the time-domain, which involves beat-to-beat quantification within a sole respiratory cycle, a sequence of breaths (5-10 typically) is quantified in the spectral analysisscully2012using;pybus2019realfor estimating blood volume loss. Prior studiesji2013heart;scully2012usingthat coupled LBNP experiment setup with AI have efficiently used time-frequency (T-F) spectral methods for the assessment of blood volume loss in awake, spontaneously breathing subjects. The present study also focuses on the assessment of two non-invasive signals viz., ECG and PPG using high- resolution transient signatures based on T-F spectral analysis to detect progressive hypovolemia in awake spontaneously breathing subjects. The present study aimed to (i) determine to what extent non-invasive ECG and PPG waveforms when coupled with ML (more specifically DL) predictive analytics can classify the degree of hypovolemia in healthy volunteers undergoing LBNP with added randomness both in level and duration of each LBNP-level to reduce the effect of time, (ii) to compare the diagnostic capability of efficient T-F representation schemes with classical feature extraction methods.
2401.01493v1	Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework	Remote Sensing Target Fine-grained Classification (TFGC) is of great significance in both military and civilian fields. Due to location differences, growth in data size, and centralized server storage constraints, these data are usually stored under different databases across regions/countries. However, privacy laws and national security concerns constrain researchers from accessing these sensitive remote sensing images for further analysis. Additionally, low-resource remote sensing devices encounter challenges in terms of communication overhead and efficiency when dealing with the ever-increasing data and model scales. To solve the above challenges, this paper proposes a novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed PRFL. The proposed framework allows each client to learn global and local knowledge to enhance the local representation of private data in environments with extreme statistical heterogeneity (non. Independent and Identically Distributed, IID). Thus, it provides highly customized models to clients with differentiated data distributions. Moreover, the framework minimizes communication overhead and improves efficiency while ensuring satisfactory performance, thereby enhancing robustness and practical applicability under resource-scarce conditions. We demonstrate the effectiveness of the proposed PRFL on the classical TFGC task by leveraging four public datasets.	Remote sensing involves detecting and monitoring the characteristics of an area by measuring its radiation from afar, typically through satellites or aircraft(Di and Yu,2023). This techniques is crucial for both military and civilian uses, as it allows for accurate analysis or images from a distance(Yanget al.,2023; Yiet al.,2022,2023). Target fine-grained classification (TFGC) is a key task in remote sensing image analysis, focusing on identifying subtle differences among similar object categories. Unlike broad category classification, TFGC works to differentiate closely related subcategories, such as various types of ships(Guoet al.,2023)or aircraft(Zhaoet al.,2023), which is challenging due to the similarity of their interclass samples and the diversity of features in the intraclass samples. Existing related works based on advanced Deep Learning (DL) techniques mainly focus on improving the classification performance via introducing well-designed modules and learning strategies to optimize feature extraction capability(Yiet al.,2022; Nurhasanahet al.,2023; Nieet al.,2022; Xionget al.,2022; Yiet al.,2023). However, these works empirically default to the idea that all data are stored in an idealized central server while ignoring the potential obstacles in real-world applications: privacy and resources. Privacy concerns, along with national and regional security issues, mean that remote sensing data are often considered highly sensitive. This sensitivity can make it challenging to share and work with this data across borders(Zhuet al.,2023; Zhanget al.,2023d; Büyüktaşet al.,2023). Regarding resources, edge devices in various locations typically capture remote sensing data. These devices prioritize local storage, but the growing volume of data strains the storage capacity of devices with limited resourcesLeiet al.(2023); Büyüktaşet al.(2023). Moreover, transferring this data to a central server within a country or region presents additional communication challenges(Chenet al.,2023a). These constraints make it difficult for organizations to access a full range of remote sensing data. Consequently, this limits their ability to conduct effective analysis and obtain reliable insights from decision-making systems. Federated Learning (FL)(McMahanet al.,2017)is a promising learning paradigm that enables multiple clients to train a Machine Learning (ML) model without revealing any private data, such as images and their metadata. It is increasingly popular in fields like healthcare(Dasaradharami Reddyet al.,2023; Liet al.,2023), personalised recommendations(Imranet al.,2023; Zhanget al.,2023c,a), weather analysis(Chenet al.,2023a,b), and remote sensing data analysis(Zhaiet al.,2023; Zhanget al.,2023d). Vanilla FL focuses on developing a standard model by frequently updating and sharing the local model parameters among all participants(McMahanet al.,2017). However, most existing FL methods encounter challenges stemming from the presence of statistical heterogeneity, characterized non-independent and identically distributed (non-IID) data. This circumstance may markedly hamper the overall performance of the system. This challenge is intensified in federated remote sensing analysis due to the subtle differences between categories and the often skewed distribution of data among those categories. These issues make it difficult to train a global model that performs well across all devices. Finding ways to improve performance amid such pronounced statistical heterogeneity remains an unresolved issue. Shallow neural networks struggle to capture the intricate representations in remote sensing images. This difficulty arises because image clarity can be compromised by various factors, including noise and irrelevant objects. Researchers have proposed using bigger and more complex network architectures to enhance performance, despite the higher computational costs involved(Xuet al.,2023; Yoonet al.,2023). Nonetheless, these complex models create significant challenges within a FL system. Both the server and the clients must exchange model updates frequently during training. Using deeper or wider networks means sending more parameters back and forth, which increases communication demands. Such high communication overhead is particularly problematic for remote devices that often have limited resources. To address the above issues, this paper proposes a novelPrivacy-PReserving TFGCFramework based on FederatedLearning, dubbedPRFL. The proposed framework targets cross-regional or international remote sensing TFGC tasks with distributed, heterogeneous data. It enables databases to collaboratively train models personalized to their own data, using knowledge from other devices, without sharing raw data. WithinPRFL, we implement a Synchronized Bidirectional Knowledge Distillation (SynKD) strategy, which provides each client with a custom local model that learns from both global and local knowledge without compromising privacy. Additionally, we propose a parameters decomposition method that compresses the full local inference model into low-rank matrices. By applying information entropy-based constraints, we filter out unnecessary parameters. This significantly cuts down communication costs between the server and clients while preserving performance. Consequently, the system trainsmits fewer parameters, making it well-suited for resource-limted remote sensing. We quantitatively evaluate the performance of the proposedPRFLand typical FL algorithms based on four publicly available remote sensing target fine-grained classification datasets. The main contributions of this work are summarized in four-fold: We propose a privacy-preserving framework for distributed remote sensing target fine-grained classification tasks. This is the first solution to this problem and addresses the weak performance caused by the notorious heterogeneity while ensuring low communication costs. We propose a simple yet effective knowledge distillation mechanism inPRFL. This mechanism encourages the student model on each client to learn both global and local knowledge while ensuring privacy, thereby tailoring a highly personalized model for each client solving a specific data distribution. We propose dynamic parameter decomposition to reduce communication overhead and improve efficiency by significantly decreasing transmission parameters while maintaining excellent performance. Extensive experiments on four publicly available real-world TFGC datasets demonstrate thatPRFLoutperforms state-of-the-art (SOTA) FL algorithms and provides en efficient distributed learning strategy for low-resource scenarios.
2311.05109v1	Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks	Quantized networks use less computational and memory resources and are suitable for deployment on edge devices. While quantization-aware training QAT is the well-studied approach to quantize the networks at low precision, most research focuses on over-parameterized networks for classification with limited studies on popular and edge device friendly single-shot object detection and semantic segmentation methods like YOLO. Moreover, majority of QAT methods rely on Straight-through Estimator (STE) approximation which suffers from an oscillation phenomenon resulting in sub-optimal network quantization. In this paper, we show that it is difficult to achieve extremely low precision (4-bit and lower) for efficient YOLO models even with SOTA QAT methods due to oscillation issue and existing methods to overcome this problem are not effective on these models. To mitigate the effect of oscillation, we first propose Exponentially Moving Average (EMA) based update to the QAT model. Further, we propose a simple QAT correction method, namely QC, that takes only a single epoch of training after standard QAT procedure to correct the error induced by oscillating weights and activations resulting in a more accurate quantized model. With extensive evaluation on COCO dataset using various YOLO5 and YOLO7 variants, we show that our correction method improves quantized YOLO networks consistently on both object detection and segmentation tasks at low-precision (4-bit and 3-bit).	Deep neural networks have achieved remarkable success in various applications, including image classification, object detection, and semantic segmentation. However, deploying these models on edge devices such as mobile phones, smart cameras, and drones poses a significant challenge due to their limited computational and memory resources. These devices typically have limited battery life, storage capacity, and processing power, making it challenging to execute complex neural networks. To overcome these challenges, researchers have developed techniques for optimizing neural networks to reduce their computational and memory requirements while maintaining their accuracy. One such line of research isqat, which reduces the number of bits used to represent the network parameters, and activations resulting in smaller model sizes and faster inference times. Existingqat[6,liu2022nonuniform,16,13]methods have made remarkable progress in quantizing neural networks at ultra-low precision with the effectiveness ofStraight Through Estimator(ste)approximation still being a point of study. Previous works[7,24]have proposed smooth approximation of rounding function to avoid the usesteapproximation butsteis still considered to be the de-facto method for approximating gradient of quantization function during propagation due to its simplicity. Furthermore, recent works[18,5]have shown oscillation issue affects quantization performance of efficient network architecture at low-precision due tosteapproximation inqat. Apart from that, the majority ofqatliterature focuses on image classification, and quantization performance achieved on such classification tasks does not necessarily translate onto downstream tasks such as object detection, and semantic segmentation. In this paper, we focus on the more challenging task of quantizing the single-shot efficient detection networks such asYOLO5[22]andYOLO7[23]at low-precision (3-bits and 4-bits). Furthermore, we show that the oscillation issue is even more prevalent on these networks and the gap between full-precision and quantized performance is far from what is usually observed inqatliterature. We also show that apart from latent weights, learnable scale factors for both weights and activations are also affected by the oscillation issue inYOLOmodels and latent weights around quantization boundaries are sometimes closer to optimality than quantization levels. This indicates that per-tensor quantization worsens the issue of oscillation. To deal with the issues of oscillations inYOLO, we proposeExponential Moving Average (ema)inqat, that smoothens out the effect of oscillations andQuantization Correction (qc), that corrects the error induced due to oscillation after each quantized layer as a post-hoc step after performingqat. By mitigating side-effects of oscillations, these two methods in combination achieve state-of-the-art quantization results at 3-bit and 4-bit onYOLO5andYOLO7for both object detection and semantic segmentation on extremely challengingCOCOdataset. Below we summarize the contributions of this paper: [leftmargin=*] We show that quantization on most recent efficientYOLOmodels such asYOLO5andYOLO7is extremely challenging even with state-of-the-artqatmethods due to oscillation issue. Our analysis finds that the oscillation phenomenon does not only affect latent weights but also affects the training of learnable scale factors for both weights and activations. We propose two simple methods namelyemaandqc, that can be used in combination with anyqattechnique to reduce the side-effects of oscillations duringqaton efficient networks. With extensive experiments onCOCOdataset for both object detection and semantic segmentation tasks, we show that our methods in combination consistently improve quantizedYOLO5andYOLO7variants and establish a state-of-the-art at ultra-low precision (4-bits and 3-bits).
2306.00212v1	Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning	We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror descent and the dual variable via projected gradient step and we prove that it enjoys sublinear rate $ O((|X|+|Y|) L \sqrt{T(|A|+|B|)}))$ for both regret and constraint violation after playing $T$ episodes of the game. Here, $L$ is the horizon of each episode, $(|X|,|A|)$ and $(|Y|,|B|)$ are the state/action space sizes of the min-player and the max-player, respectively. To the best of our knowledge, we provide the first provably efficient online safe reinforcement learning algorithm in constrained Markov games.	Safe Reinforcement Learning (RL) studies how a single agent learns to maximize its expected total reward subject to safety-concerned constraints by interacting with an unknown environment over time(Garcıa and Fernández,2015; Thomas,2015; Amodeiet al.,2016). The constrained Markov decision processes (MDPs) provide a standard class of constraint critical environment models(Altman,1999)that are utilized in autonomous robots(Feyzabadi,2017; Fisacet al.,2018), personalized medicine(Girard,2018), online advertising(Boutilier and Lu,2016), and financial management(Abeet al.,2010). General constrained MDPs for two or more agents are often formulated as constrained Markov games (MGs) in which agents compete under constraints(Altman and Shwartz,2000; Altmanet al.,2005,2008), providing an effective model for safe multi-agent RL(Nguyenet al.,2014; Shalev-Shwartzet al.,2016; Zhanget al.,2021). Considerable recent progress has been made in single-agent safe RL, especially for solving constrained MDP problems with constraint satisfaction guarantees(Efroniet al.,2020; Brantleyet al.,2020; Baiet al.,2020a; Dinget al.,2021; Chenet al.,2021; Singhet al.,2022; Dinget al.,2022b). In these references, Lagrangian-based methods have been combined with the optimistic exploration to address exploration-exploitation trade-off under constraints. These constrained MDP learning algorithms are sample-efficient (in achieving both low regret and low constraint violation) and they effectively enhance classical RL methods to attain safety requirements. However, most of these algorithms are limited to the single-agent setting and it is an open question how to balance the exploration-exploitation trade-off under constraints for multiple agents. Another motivation for our work comes from recent advances on the efficient competitive RL algorithms in MGs(Weiet al.,2017; Bai and Jin,2020; Baiet al.,2020b; Xieet al.,2020). In this work, we take initial steps towards developing provably efficient safe multi-agent RL algorithms. We examine perhaps the most basic safe multi-agent RL setup that involves a two-player zero-sum constrained MG with independent state transitions(Altman and Shwartz,2000; Altmanet al.,2005,2008; Singh and Hemachandra,2014). This problem represents a generalization of constrained MDPs to the two-player case with coupled constraints. In such a constrained MG, two players follow their own state transitions independently, take actions simultaneously, and observe the reward and utility functions while competing against each other by maximizing/minimizing the reward while both are restrained by the constraint regarding some utility for safety reasons. The decision-coupling that arises from the constraint is often encountered in multi-agent systems(Rosen,1965; Li and Marden,2014; Kulkarni,2011,2017; De Nijs,2019). More specifically, we aim to design an online RL algorithm for solving episodic two-player zero-sum constrained MGs. Here, two players do not know the transition models and have no access to a generative model, but can play the game for multiple episodes using arbitrary policies. The goal is to find an approximate constrained Nash equilibrium of the game in hindsight, a generalization of Nash equilibrium to characterize violating constraints if any unilateral deviations occur. We utilize a notion of regret to quantify the approximation error of the constrained Nash equilibrium and employ a constraint dissatisfaction (which results from violation of any utility constraints) to evaluate the constraint violation. Contribution. We develop the first provably efficient algorithm for a constrained Markov game (MG) withO(\sqrt{T})regret andO(\sqrt{T})constraint violation. Specifically, we introduce an episodic constrained MG with unknown independent transition functions and decision-couplings that come from both adversarial reward functions and coupled stochastic constraints on utility functions. We use the occupancy measure approach to formulate such a MG as a constrained saddle-point problem with an explicit constraint. We extend the Lagrange method in constrained optimization to deal with the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. We develop an upper confidence reinforcement learning algorithm – an\underline{\text{U}}pper\underline{\text{C}}onfidence\underline{\text{B}}ound\underline{\text{C}}onstrained\underline{\text{SA}}ddle-\underline{\text{P}}oint\underline{\text{O}}ptimization (UCB-CSAPO) algorithm – to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via optimistic mirror descent and the dual variable via projected gradient step and we prove that it enjoys sublinear rateO((|X|+|Y|)L\sqrt{T(|A|+|B|)}))for both regret and constraint violation after playingTepisodes. Here,Lis the horizon of each episode,(|X|,|A|)and(|Y|,|B|)are the state/action space sizes of the min-player and max-player, respectively. Related Work. We briefly review the most-related work; see Appendix6for details. Our work is closely related to safe multi-agent RL in constrained MGs. The Nash equilibrium for constrained MGs have been studied inAltman and Shwartz (2000); Gómez-Ramırezet al.(2003); Altmanet al.(2005); Alvarez-Mena and Hernández-Lerma (2006); Altmanet al.(2007,2008); Altman and Solan (2009); Singh and Hemachandra (2014)using the notion ofconstrained Nash equilibrium(which generalizes the concept ofgeneralized Nash equilibriumin static games(Arrow and Debreu,1954)to MGs); see more studies inYaji and Bhatnagar (2015); Zhang (2019); Wei (2020,2021); Zhang and Zou (2021). These results are not applicable to the RL setting that assumes unknown models. Recently, asymptotic convergence in learning constrained MGs was examined inHakami and Dehghan (2015); Jianget al.(2020)but sample efficiency and exploration were not fully addressed, except for a concurrent work on learning correlated equilibria(Chenet al.,2022b). Our work fills this gap by adding built-in exploration mechanisms under constraints and proving the first non-asymptotic convergence for learning constrained Nash equilibria. Our work is also pertinent to a rich RL literature on learning constrained MDPs(Zheng and Ratliff,2020; Qiuet al.,2020; Kalagarlaet al.,2020; Baiet al.,2020a; Chowet al.,2017; Tessleret al.,2019; Dinget al.,2020,2021,2022b; Wachi and Sui,2020; Efroniet al.,2020; Brantleyet al.,2020; Chenet al.,2021; Liuet al.,2021a; Yinget al.,2022; Liuet al.,2021b; Baiet al.,2022; Zhao and You,2021; Liet al.,2021; Chenet al.,2022a). While these results provide provably efficient algorithms regarding regret and constraint satisfaction in the single-agent setting, they are not applicable to our multi-agent game being played under constraints, because of thenon-convexitynauture of constrained multi-agent policy optimization and thenon-stationaryenvironment each agent is facing. An extended line of work on constrained MDPs focuses on cooperative multi-agent learning under constraints and most efforts study the case where multiple agents have independent MDPs with a coupled budget/resource constraint(Meuleauet al.,1998; Boutilier and Lu,2016; Weiet al.,2018; de Nijs and Stuckey,2020; Gagrani and Nayyar,2020). All these results assume knowing transition models or system dynamics. Only a few studies considered the shared MDP case(Diddigiet al.,2019; Luet al.,2020; Parnikaet al.,2021; Guet al.,2021), but they lack theoretical guarantees and do not handle exploration. In contrast, our work focuses on the MG setting with unknown models and attacks the exploration challenge directly.
2308.06248v1	FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods	The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.	Even though deep learning models have achieved breakthrough results in computer vision, their inner workings remain largely opaque. As a result, deep networks sometimes receive only limited user trust and cannot be applied blindly in safety-critical domains. To overcome this issue, a growing interest in the field of explainable artificial intelligence (XAI) has emerged, attempting to explain the inner workings of deep neural models in a human-comprehensible way. However, since there are generally no ground-truth explanations, evaluating XAI methods remains an open challenge. In fact, a third of XAI papers lack sound quantitative evaluation[38], while other work has limited comparability[31]or problematic evaluation protocols[23,38]. To overcome the issue of missing ground-truth explanations, automatic evaluations are often done via proxy tasks that adhere to the idea of performing image interventions by removing certain input features to then measure the resulting impact on the model output[21,55,23]. As image interventions are non-trivial to perform on existing vision datasets, they are usually applied on a pixel level,\eg, masking out single pixels[21,55,27,52,23]. However, this and related approaches share several downsides. First, performing interventions, as well as evaluating explanations on a pixel level, is disconnected from the downstream task of providinghuman-understandable explanations since humans perceive images in concepts rather than pixels. Second, existing automatic evaluation protocols are developed for specific explanation types,\eg, pixel-level attribution maps, and thus, cannot be extended to other explanation types like prototypes[12]. Third, by performing unrealistic interventions in image space,\eg, masking out pixels, they introduce domain shifts compared to the training distribution[23,28,10], which can cause the model to behave unexpectedly, and thus negatively affects the evaluation. In this work, we address the above and more challenges to contribute an important step toward a more rigorousquantitativeevaluation of XAI methods by proposing a thorough, dedicated evaluation/analysis tool. We do so by building afully controllable, synthetic classification dataset consisting of renderings of artificial bird species. This approach to analyzing XAI methods is analogous to controlled laboratory research, where we have full control over all variables, eliminating the potential influence of irrelevant factors, and therefore, providing clearer evidence of the observed behavior[5]. Our proposed dataset allows us to make the following main contributions:(1)We cover a wide range of dimensions of explainability by considering acollection of different evaluation protocols.(2)We allow to automatically compare various explanation types in asharedframework.(3)We avoid the out-of-domain issue of previous image-space interventions by introducingsemantically meaningful interventionsat training time.(4)We reduce the gap between the downstream task of human comprehension and XAI evaluation by proposingmetrics that operate at a semantically meaningful part levelrather than the semantically less meaningful pixel level.(5)Weautomaticallyanalyze thecoherenceof explanations.(6)We analyze 24 different combinations of existing XAI methods and neural models, highlighting their strengths and weaknesses as well as identifying new insights that may be of general interest to the XAI community.
2310.16253v1	ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair	"With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics. To address this issue, we introduce ""ConDefects"", a novel dataset of real faults meticulously curated to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for in fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk."	The advancement of Large Language Models (LLMs) has opened up vast potential and garnered significant interest for their application in software engineering, especially in fault localization and program repair(Fanet al.,2023). As the reliance on LLMs intensifies, it becomes imperative to maintain the integrity of LLM-based research. A significant challenge in this field arises from the benchmarks used to evaluate the performance of these LLM-based solutions. Existing widely-adopted datasets with program bugs, such as Defects4J(Justet al.,2014), ManyBugs(Le Goueset al.,2015), IntroClass(Smithet al.,2015), and CodeNet(Puriet al.,2021), have been instrumental in shaping research in fault localization and program repair. However, the code in these datasets was produced before the surge in LLM popularity, and has a large possibility to have been incorporated into the training data of prevalent LLMs(Aiyappaet al.,2023), leading to the data leakage threat. This data leakage issue can lead to an overestimation of the LLM’s capabilities, presenting performance metrics that are overly optimistic and misleading(Huet al.,2022; Samalaet al.,2020). To address this gap, we present ConDefects, a meticulously curated dataset of bugs and their patches sourced from submissions on the AtCoder platform. ConDefects has 1,254 Java faulty programs and 1,625 Python faulty programs that were produced between October 2021 and September 2023. ConDefects has three unique features:1)each faulty program is labelled with the faulty line number and is paired with a repaired version, making the dataset suitable for fault localisation and automatic program repair tasks.2)The dataset has a time window selection feature, allowing researchers to select code samples based on their creation period so that researchers have the flexibility to evaluate the effectiveness of different LLMs according to their training data cut-off date.3)The dataset also has a feature that offers users to select coding tasks with different difficulty levels.4)We provide a user-friendly interface to enable test case execution and coverage collection, to further support fault localisation and program repair related tasks. ConDefects is publicly available and can be accessed through Github:https://github.com/appmlk/ConDefects.
2304.00898v1	Tunable Convolutions with Parametric Multi-Loss Optimization	Behavior of neural networks is irremediably determined by the specific loss and data used during training. However it is often desirable to tune the model at inference time based on external factors such as preferences of the user or dynamic characteristics of the data. This is especially important to balance the perception-distortion trade-off of ill-posed image-to-image translation tasks. In this work, we propose to optimize a parametric tunable convolutional layer, which includes a number of different kernels, using a parametric multi-loss, which includes an equal number of objectives. Our key insight is to use a shared set of parameters to dynamically interpolate both the objectives and the kernels. During training, these parameters are sampled at random to explicitly optimize all possible combinations of objectives and consequently disentangle their effect into the corresponding kernels. During inference, these parameters become interactive inputs of the model hence enabling reliable and consistent control over the model behavior. Extensive experimental results demonstrate that our tunable convolutions effectively work as a drop-in replacement for traditional convolutions in existing neural networks at virtually no extra computational cost, outperforming state-of-the-art control strategies in a wide range of applications; including image denoising, deblurring, super-resolution, and style transfer.	Neural networks are commonly trained by optimizing a set of learnable weights against a pre-defined loss function, often composed of multiple competing objectives which are delicately balanced together to capture complex behaviors from the data. Specifically, in vision, and in image restoration in particular, many problems are ill-posed,i.e. admit a potentially infinite number of valid solutions[15]. Thus, selecting an appropriate loss function is necessary to constrain neural networks to a specific inference behavior[64,39]. However, any individual and fixed loss defined empirically before training is inherently incapable of generating optimal results for any possible input[43]. A classic example is the difficulty in finding a good balance for the perception-distortion trade-off[64,5], as shown in the illustrative example of Fig.1. The solution to this problem is to design a mechanism to reliably control (i.e.tune) neural networks at inference time. This comes with several advantages, namely providing a flexible behavior without the need to retrain the model, correcting failure cases on the fly, and balancing competing objectives according to user preference. Existing approaches to control neural networks, commonly based on weights[55,54]or feature[53,63]modulation, are fundamentally limited to consider only two objectives, and furthermore require the addition of a new set of layers or parameters for every additional loss considered. Different approaches, specific to image restoration tasks, first train a network conditioned on the true degradation parameter of the image,e.g. noise standard deviation or blur size, and then, at inference time, propose to interact with these parameters to modulate the effects of the restoration[17,50,24]. However this leads the network to an undefined state when asked to operate in regimes corresponding to combinations of input and parameters unseen during training[27]. In this work, we introduce a novel framework to reliably and consistently tune model behavior at inference time. We propose a parametric dynamic layer, called tunable convolution, consisting inpindividual kernels (and biases) which we optimize using a parametric dynamic multi-loss, consisting inpindividual objectives. Different parameters can be used to obtain different combinations of kernels and objectives by linear interpolation. The key insight of our work is to establish an explicit link between thepkernels and objectives using asharedset ofpparameters. Specifically, during training, these parameters are randomly sampled to explicitly optimize the complete loss landscape identified by all combinations of thepobjectives. As a result, during inference, each individual objective is disentangled into a different kernel, and thus its influence can be controlled by interacting with the corresponding parameter of the tunable convolution. In contrast to previous approaches, our strategy is capable of handling an arbitrary number of objectives, and by explicitly optimizing all their intermediate combinations, it allows to tune the overall network behavior in a predictable and intuitive fashion. Furthermore, our tunable layer can be used as a drop-in replacement for standard layers in existing neural networks with negligible difference in computational cost. In summary the main contributions of our work are: [noitemsep] A novel plug-and-play tunable convolution capable to reliably control neural networks through the use of interactive parameters; A unique parametric multi-loss optimization strategy dictating how tunable convolution should be optimized to disentangle the different objectives into the different tunable kernels; Extensive experimental validation across several image-to-image translation tasks demonstrating state-of-the-art performance for tunable inference.
2302.12666v1	Modelling Temporal Document Sequences for Clinical ICD Coding	Past studies on the ICD coding problem focus on predicting clinical codes primarily based on the discharge summary. This covers only a small fraction of the notes generated during each hospital stay and leaves potential for improving performance by analysing all the available clinical notes. We propose a hierarchical transformer architecture that uses text across the entire sequence of clinical notes in each hospital stay for ICD coding, and incorporates embeddings for text metadata such as their position, time, and type of note. While using all clinical notes increases the quantity of data substantially, superconvergence can be used to reduce training costs. We evaluate the model on the MIMIC-III dataset. Our model exceeds the prior state-of-the-art when using only discharge summaries as input, and achieves further performance improvements when all clinical notes are used as input.	ICD (International Classification of Diseasesicd) coding refers to the task where medical professionals classify clinical diagnoses and medical procedures associated with each patient using standardised taxonomies, which in turn supports billing, service planning and research. The process is manual and laborious in natureicderrors, however there is potential to automate it by identifying relevant information from clinical notes, which are already captured in EHR systems. With this in mind, researchers have started to explore whether machine learning models can succeed at this taskcaml. The current research on the ICD coding task focuses on the extraction of codes from the discharge summary. This document is commonly written at the end of a hospital stay and provides a textual description of the important diagnoses and procedures for a given patient, making it particularly helpful for the task. However, many other clinical notes are also created during the hospital stay, which can provide important details or useful additional context that may be missing from the discharge summary itself. Analysing the full sequence of notes would allow models to make more accurate decisions and make the problem more similar to a real-life setting, where clinicians have to consider all information about a patient for ICD coding, rather than information only in a single document. In this work we study how the inclusion of clinical notes across the entire hospital stay can affect performance on the ICD coding task. We propose theHierarchicalTransformers forDocumentSequences (HTDS) model, which is an adaptation of the hierarchical transformer modelhibertfor temporal modelling of document sequences. The model takes text and metadata (such as the time and type of note) from a sequence of multiple documents as input and achieves improved performance when additional clinical notes are used for modelling. We compare different prioritisation criteria for selecting which notes to use as input and how to best represent the sequence information. Methods related to superconvergence are applied to speed up the model training process in order to handle the increased size of the data that needs to be processed. Our experiments show that the inclusion of additional clinical notes indeed improves model accuracy and leads to better predictions. We evaluate our models against the MIMIC-III-50mimictest set. When considering only the discharge summaries of each hospital stay as input, our model exceeds the current state-of-the-art performance in terms of Micro-F1. When considering all clinical notes as input, further performance improvements across all metrics of interest are observed, exceeding the state-of-the-art performance in Micro-F1, Micro-AUC, Macro-AUC, and Precision@5 scores.
2311.10789v1	Stratified-NMF for Heterogeneous Data	Non-negative matrix factorization (NMF) is an important technique for obtaining low dimensional representations of datasets. However, classical NMF does not take into account data that is collected at different times or in different locations, which may exhibit heterogeneity. We resolve this problem by solving a modified NMF objective, Stratified-NMF, that simultaneously learns strata-dependent statistics and a shared topics matrix. We develop multiplicative update rules for this novel objective and prove convergence of the objective. Then, we experiment on synthetic data to demonstrate the efficiency and accuracy of the method. Lastly, we apply our method to three real world datasets and empirically investigate their learned features.	Non-negative matrix factorization (NMF) is a classical unsupervised machine learning method used in dimensionality reduction and topic modeling[11,27,23]. NMF best addresses data which could be grouped into topics. For example, one could break down a collection of books by topics and then further break down the topics by words associated to them[5,25]. In a more general setting, we refer to books, topics, and words assamples,features, andvariables, respectively. This is modeled by where||\cdot||_{F}denotes the Frobenius norm,Ais the data matrix of samples by variables,His the topics matrix which associates topics to variables, andWis a matrix which associates samples with topics. These names come from the fact that each sample is approximated by a linear combination of the rows inHwith coefficients from the rows ofW. The foundational work by Seung and Lee showed that the standard NMF objective can be optimized efficiently with a multiplicative update[22,13]. Additionally,WandHare chosen to be low rank so that the method learns to compress the data into features. The low rank, efficient multiplicative updates allow this method to scale to large datasets. Additionally, NMF is sought after for its sparse and interpretable feature learning[4]. A potential drawback of the standard NMF model is that it does not directly account for stratified data, e.g. data drawn from multiple sources[1]. Data collection methods, along with geographic or time differences, can introduce heterogeneity into the dataset. One common solution is to stratify data in order to obtain accurate results for subgroups of the data[8,17,21]. In this paper, we augment the NMF objective to account for stratified data. We consider the setting where multiple groups, or strata, share a common topics dictionary with positive, strata dependent shifts. In this work, strata will be given in the problem, but can be obtained via meta-data or other common, external information. For example, articles written in various regions or across time periods differ in dialect or semantic progression. Although NMF may attribute the words “pop” and “soda” to the topic “beverage”, it will be unable to explain the regional differences in the use of this word. Using Stratified-NMF, we can learn the strata dependent shifts and obtain a topic dictionary which measures the underlying commonalities between strata. In this work, we develop a novel extension of NMF called Stratified-NMF, which is able to account for heterogeneous data. We also derive multiplicative updates and show that the Stratified-NMF objective is non-increasing under our multiplicative update rules. Next we analyze our method on four datasets, spanning synthetic data, image data, census data, and natural language text. We experimentally validate the convergence properties of the method and empirically demonstrate the interpretability of Stratified-NMF. Code for our experiments is also publicly available222Code can be found at https://github.com/chapman20j/Stratified-NMF.
2310.01330v1	Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association	Reporting bias arises when people assume that some knowledge is universally understood and hence, do not necessitate explicit elaboration. In this paper, we focus on the wide existence of reporting bias in visual-language datasets, embodied as the object-attribute association, which can subsequentially degrade models trained on them. To mitigate this bias, we propose a bimodal augmentation (BiAug) approach through object-attribute decoupling to flexibly synthesize visual-language examples with a rich array of object-attribute pairing and construct cross-modal hard negatives. We employ large language models (LLMs) in conjunction with a grounding object detector to extract target objects. Subsequently, the LLM generates a detailed attribute description for each object and produces a corresponding hard negative counterpart. An inpainting model is then used to create images based on these detailed object descriptions. By doing so, the synthesized examples explicitly complement omitted objects and attributes to learn, and the hard negative pairs steer the model to distinguish object attributes. Our experiments demonstrated that BiAug is superior in object-attribute understanding. In addition, BiAug also improves the performance on zero-shot retrieval tasks on general benchmarks like MSCOCO and Flickr30K. BiAug refines the way of collecting text-image datasets. Mitigating the reporting bias helps models achieve a deeper understanding of visual-language phenomena, expanding beyond mere frequent patterns to encompass the richness and diversity of real-world scenarios.	Reporting bias denotes the inclination of individuals to under-report the information they have accessed(Gordon and Van Durme,2013). This bias often arises when people assume that certain information, typically commonsense knowledge, is universally understood and, therefore, does not necessitate explicit elaboration, leading to the omission of some foundational details. Reporting bias rarely hinders human communication because individuals can infer the missing information from context and their own knowledge. However, it could be a crucial challenge in vision–language (VL) datasets because VL models do not inherently possess the ability to grasp commonsense knowledge, making them susceptible to misinterpretations when faced with reporting bias. In standard VL datasets, images are accompanied by descriptive captions. Considering captions are typically collected by either automatic web crawling(Schuhmannet al.,2022), human annotating(Linet al.,2014), or even generated by LLMs(Fanet al.,2023), the reporting bias issue would widely appear in existing large-scale VL datasets. For instance, Figure1(a) presents two examples highlighting reporting bias. The first example showcases two images both labeled with the caption‘A dog runs with a tennis ball in its mouth’, omitting details such as the dog’s color and whether the background is grass or snow. Similarly, the second example provides two images captioned as‘salmon dish on the table’, but one displays sliced salmon and the other a whole fish. Despite each caption being accurate for its respective image, a VL model trained on such data may struggle to discern nuances likeblack vs. brown dog,snow vs. grass, orsliced vs. salmon fish. Hence, mitigating the reporting bias in VL datasets is crucial for enhancing the performance of VL models trained on them. This need arises from several concerns: Biased captions, which might be perceived as lacking objects or attributes, can be associated with multiple images that are dissimilar. Such imprecise pairings can compromise the training quality of VL models because they do not naturally have the capability to grasp commonsense knowledge to discern the difference. Reporting bias skews the VL model towards frequently occurring patterns. For instance, with reporting bias, a search for‘a flag’might predominantly yield images of a USA flag, ignoring the broader spectrum of flags. This bias hinders the model’s efficacy in distinguishing nuanced object–attribute combinations. We introduce a novel bimodal data augmentation framework, denoted asBiAug, that strategically disentangles object–attribute association for this problem. As demonstrated in Figure1(b), given a caption-image pairing, BiAug is designed to: Synthesizeboth new captions and corresponding images. In this process, the caption’s object receives additional descriptive detail, and the image’s object undergoes a corresponding edit. Through thedisentanglement of object–attribute association, BiAug crafts bimodal hard negative examples that emphasize a particular attribute. Given that the object and attribute are decoupled, BiAug possesses the flexibility to produce samples with arich array of object–attribute pairings. This feature helps diminish the over-representation of recurrent patterns. We utilize BiAug to augment existing datasets and to evaluate BiAug by comparing models trained on the augmented dataset and the original source dataset, respectively. Our investigations span a variety of benchmarks. Primarily, VL models trained with BiAug consistently surpass baseline models on compositionality benchmarks. These benchmarks gauge a model’s aptitude for grasping intricate commonsense knowledge. In addition, our trials on general text-image retrieval benchmarks also indicate that BiAug outperforms the baseline, which could be empirical evidence of mitigating the noise caused by reporting bias. BiAug refines the way of collecting text-image datasets. Mitigating the reporting bias ensures that models can achieve a deeper understanding of vision–language phenomena, expanding beyond mere frequent patterns to encompass the richness and diversity of real-world scenarios.
2401.12471v1	Zero Shot Open-ended Video Inference	Zero-shot open-ended inference on untrimmed videos poses a significant challenge, especially when no annotated data is utilized to navigate the inference direction. In this work, we aim to address this underexplored domain by introducing an adaptable framework that efficiently combines both the frozen vision-language (VL) model and off-the-shelf large language model (LLM) for conducting zero-shot open-ended inference tasks without requiring any additional training or fine-tuning. Our comprehensive experiments span various video action datasets for goal inference and action recognition tasks. The results demonstrate the framework's superior performance in goal inference compared to conventional vision-language models in open-ended and close-ended scenarios. Notably, the proposed framework exhibits the capability to generalize effectively to action recognition tasks, underscoring its versatility and potential contributions to advancing the video-based zero-shot understanding.	Video action understanding tasks have gained attention in the field of computer vision due to the explosive growth of digital video content. Given that the field of vision-language (VL) foundational models is in a constant state of evolution, continuously enhancing its ability to facilitate few-shot or zero-shot generalization, thereby opening up a multitude of practical applications in the realm of multimodal visual understanding and natural inference. Furthermore, large language models (LLMs) have achieved remarkable potency and versatility across various tasks, further expanding their significance. The recent literature establishes the significance of incorporating LLMs with the VL models to amplify their capacity for tackling even more complex visual understanding tasks such as goal inference. We proposed a modular inference framework for video understanding as simplified illustrated inFig.1, calledZERO:ZERo-Shot andOpen-ended, which is capable of zero-shot inference as well as open-ended inference. We harness the capabilities of VL models and off-the-shelf available LLMs, seamlessly integrating them for enhanced performance in video comprehension tasks without any extensive resource-consuming training or fine-tuning. Different from conventional classification model that required list of options, we aim to develop a framework that capable to handle open-ended inference accurately. As depicted inFig.2, our modular framework comprises three adaptable components: frozen visual descriptor, frozen LLM, and evidence selector. Firstly, the visual descriptor takes the lead in visual understanding by generating frame-level captions for the video, converting visual content into textual representation. Then, the LLM capitalizes on this textual information by generating more relevant textual hypotheses for the given inference task and execution steps of the hypothesis as a script[schank1975scripts]or the program of the hypothesis. Subsequently, we input the rich textual execution steps of the scripts and the video frames into the evidence selector to recognize the most relevant frames for the given inference task. Inference task-specific captions are generated based on the selected relevant frames and lastly, the LLM performs final inference with the new evidence. We evaluate the proposed framework across different video datasets with open-ended inferences as well as close-ended. Specifically, we access the framework in video goal inference and extend to video action recognition task. The experiment results manifest the efficacy of the proposed framework in zero-shot open-ended and close-ended inference tasks. We do not rely on interval annotation data which typically provided through human effort. Instead, we exclusively utilize untrimmed videos as the direct input for inference. It presents a significant challenge especially in goal inference task when attempting to establish connections between the observed action sequences and infer the actor’s intentions. In general, the VL models excel in describing visual content but encounter limitations in reasoning and handling lengthy video data which requires multi-hop reasoning in some cases. In contrast, LLMs exhibit certain extend of reasoning capabilities but lack of visual perception. Therefore, we integrate VL models with LLM to compensate for their respective weaknesses. In summary, our contributions are as following: We propose an adaptable modular framework for video zero-shot open-ended inference as well as close-ended. We demonstrate new possibilities for zero-shot video inference without any fine-tuning by just using pre-trained visual-language model and large language model, meanwhile it is generic to various inference tasks. We present an extensive ablation study and providing insights into the significance of each component and its associated settings within our proposed framework.
2302.10430v1	Interval Type-2 Fuzzy Neural Networks for Multi-Label Classification	Prediction of multi-dimensional labels plays an important role in machine learning problems. We found that the classical binary labels could not reflect the contents and their relationships in an instance. Hence, we propose a multi-label classification model based on interval type-2 fuzzy logic. In the proposed model, we use a deep neural network to predict the type-1 fuzzy membership of an instance and another one to predict the fuzzifiers of the membership to generate interval type-2 fuzzy memberships. We also propose a loss function to measure the similarities between binary labels in datasets and interval type-2 fuzzy memberships generated by our model. The experiments validate that our approach outperforms baselines on multi-label classification benchmarks.	Multi-label classification (MLC) aims at predicting multiple labels by exploring the dependencies among labels. It has been widely used in natural language processing[10]and computer vision[40].Classical MLC models make binary predictions. That is, they use 0 and 1 to indicate the categorizations. However, intuitively, human has fuzzy discrimination on an instance(Fig.1).When categorizing these four images in Fig.1, human observers may consider the semantic relationships between flowers or women in the images and hence they may give fuzzy estimations of their categories. For image (a), it apparently belongs to flower. For (b), someone may consider the woman is the main content while others may consider woman and flower are of the same importance. For (c), we human can make sure that the image belongs to the woman category and the flower is just a decoration. However, the flower occupies a considerably large area in the image. A MLC model may also consider that it belongs to flower category in certain degree. For (d), human observers’ opinions may be contradictory again because someone may consider the woman holds the flower in front of her face so she probably want to show a special species of flower, while someone may consider that the flower is just an ordinary one and the main content of the image is still the woman.Almost in all datasets, instances are directly labeled by 0 or 1, which cannot effectively represent humans’ fuzzy judgment. Interval type-2 fuzzy logic is capable of representing different human opinions on categorization. It is the motivation that we explore how to build a fuzzy-logic-based model for MLC. In uni-label classification, probability and type-1 fuzzy logic are a little bit confusing. Softmax is widely used in neural networks for uni-label classification. The output of softmax can be explained as the probability of an instance belonging to a category. Someone can “illegally” explain the probabilities as type-1 fuzzy memberships. If we are working on uni-label classification, there is few numerical difference between these two explanations. For example, we can explain Fig.1as this. The probability of the image (a) belonging to flower category is maximum, so the image is belonging to flower category. The membership of the image (b) belonging to woman category is maximum. If we must make a decision on which category it belongs to, we have to categorize the image into woman category. However, if we are interested in multi-label classifications, there is a key difference between probabilities and fuzzy logic. We should examine the joint probability (Fig.2). Instead, we can explain the outputs of a neural network as type-1 fuzzy memberships. Although we give a reasonable explanation to the outputs, type-1 fuzzy logic is not fuzzy[36]. Hence, interval type-2 fuzzy logic is used here. We believe that the more categories an instance belongs to, the fuzzier its memberships are. Therefore, we use the number of its categories to control the fuzziness of its memberships (Fig.2). By introducing interval type-2 fuzzy logic, we need design the loss function of the classifier to measure the dissimilarities between interval type-2 fuzzy labels and binary labels. The overall scheme of our model is illustrated in Fig.3. The following of this paper is organized as follows. In SectionII, related works are briefly reviewed. The preliminaries are given in SectionIII. The formulation of our models is shown in SectionIV. In SectionV, we report the experimental results of our method. The conclusive remarks are given in SectionVI.
2305.15723v1	Learning across Data Owners with Joint Differential Privacy	In this paper, we study the setting in which data owners train machine learning models collaboratively under a privacy notion called joint differential privacy [Kearns et al., 2018]. In this setting, the model trained for each data owner $j$ uses $j$'s data without privacy consideration and other owners' data with differential privacy guarantees. This setting was initiated in [Jain et al., 2021] with a focus on linear regressions. In this paper, we study this setting for stochastic convex optimization (SCO). We present an algorithm that is a variant of DP-SGD [Song et al., 2013; Abadi et al., 2016] and provides theoretical bounds on its population loss. We compare our algorithm to several baselines and discuss for what parameter setups our algorithm is more preferred. We also empirically study joint differential privacy in the multi-class classification problem over two public datasets. Our empirical findings are well-connected to the insights from our theoretical results.	"In recent years, there are growing attention to ensuring privacy when user data owners share data for jointly training machine learning models. These owners could be single users who have their personalized models trained on their own devices, or online platforms, who are prevented from sharing data with other platforms due to user requirements or data privacy laws and regulations (e.g. the Digital Markets Act (Article 5(2))(European Union,2022)aims to regulate how each Big Tech firm shares data collected from different service platforms). In many of these scenarios, the model trained for each data owner is only used by itself. And therefore, it is only necessary to ensure that its data are used under privacy guarantees when training models for other owners, but not the model for itself. In differential privacy research, this is precisely captured by a notion called ""joint differential privacy""Kearnset al.(2014), which was initially developed for applications in implementing approximate equilibrium in game theory. Informally, joint differential privacy requires that when one data owner switches to reporting a neighboring dataset, the output distribution of other data owners’ outcomes, machine learning models in this paper’s context, should not change by much. Machine learning with joint differential privacy was initiated inJainet al.(2021), focusing on linear regressions. Their main result is an alternating minimization algorithm that alternates between users updating individual predictor components non-privately and users jointly updating the common embedding component with differential privacy. In this work, we consider stochastic convex optimization (SCO), the basis of many machine learning tasks, under joint differential privacy (joint-DP). Consider a set ofnowners, and each ownerj\in[n]containsrusers who jointly hold a dataset ofmrecords\mathcal{S}_{j}=\{z_{ij}\}_{i\in[m]}wherez_{ij}\in\mathcal{Z}\subseteq\mathbb{R}^{d}are drawn i.i.d. from a distribution\mathcal{P}_{j}over the domain\mathcal{Z}. For concreteness, let us assume that each user of the same owner holds the same numberm/rof data points in the dataset\mathcal{S}_{j}. Let\mathcal{S}=\{\mathcal{S}_{j}\}_{j\in[n]}denote the collection of all owners’ data. To characterize the personalized model for each owner and shared information across different owners, we assume the set of parameters is divided into two parts: each ownerj’s personalized parametersx_{j}, and common parametersuthat are shared by all owners in the system. We let\mathcal{X}\subset\mathbb{R}^{k}be the domain for each owner’s parametersx_{j}, and\mathcal{U}\subset\mathbb{R}^{\ell}the domain of the parametersushared by all owners. DenoteD_{\mathcal{X}}=\mathsf{diam}(\mathcal{X})andD_{\mathcal{U}}=\mathsf{diam}(\mathcal{U})the diameters of these domains. Given anL-Lipschitz loss functionh(x,u,z):\mathcal{X}\times\mathcal{U}\times\mathcal{Z}\rightarrow\mathbb{R}that is convex in(x,u)for anyz\in\mathcal{Z}. We define the excess population loss function of each ownerjas and the excess population loss function of all owners as wherex=[x_{1},x_{2},\cdots,x_{n}]\in\mathbb{R}^{nk}is the concatenation of thenpersonalized parameters. Intuitively, the excess population loss functionfis the average excess population loss function of all the owners. We aim to learn the personalized and common parameters\{x_{j}^{*}\}_{j\in[n]},u^{*}to minimize the excess population functionfwhile satisfying joint differential privacy across different owners. Intuitively, the parameterx_{j}^{*}\in\mathcal{X}corresponds to each ownerj’s best personalization, and the parameteru^{*}\in\mathcal{U}corresponds to the commonalities among different owners. To capture the shared information among the users, we assume that the minimizer(x_{j}^{*},u^{*})to each ownerj’s population lossf_{j}(x_{j},u)shares the sameu^{*}for anyj\in[n]. In practical applications, it is often instructive to consider the case where\ell\gg k, i.e., the amount of shared information is much larger than the amount of personalized information, but we do not make this assumption and the statement of our results hold for all regimes of\ellandk. To satisfy the joint-DP guarantees, we require that each ownerj’s data is protected from any other ownerj^{\prime}in the learning of the personalized and shared parameters, while ownerjcan learn and compute non-privately using her own data. In particular, this means that different users of thesameowner can learn and compute non-privately using each others’ data, but user’s data are protected from any user belonging to adifferentowner. As inJainet al.(2021), we operate in the billboard model where apart from thenowners introduced above, there is additionally a computing server. The server runs a differentially private algorithm on sensitive information from the owners, and broadcasts the output to all the owners. Each ownerj\in[n]can then use the broadcasted output in a computation that solely relies on her data. The output of this computation is not made available to other owner. The billboard model is that it trivially satisfies joint differential privacy. We consider the standard setting of user-level joint-DP. This means that the replacement or not of a single user’s data in an owner’s dataset should be indistinguishable to every other owner in the system (seeDefinition2.1for a more formal definition of indistinguishability and differential privacy). Our main result is an algorithm for user-level joint-DP with optimal excess population loss. In particular, we prove the following theorem. For any0<\delta<1/\mathsf{poly}(n)and10\geq\varepsilon>0, there is an algorithm for the problem above achieving user-level(\varepsilon,\delta)joint-DP with excess population loss whereR=\sqrt{nD_{\mathcal{X}}^{2}+D_{\mathcal{U}}^{2}}. The formal statement of the theorem will be given inTheorem3.1. In Section4, we conduct empirical studies on the multi-class calssification problem over two public datasets: MNISTLeCunet al.(1998)and FashionMNISTXiaoet al.(2017). We compare joint-DP models to full-DP model and per-silo models (users learning individually), in terms of test accuracy. The comparison results are consistent with the discussion we get from comparing our theoretical bounds in Section3.2. The most related paper isJainet al.(2021), which studies a very similar joint differential privacy setup as our paper on linear regressions. There’s a minor difference in the setting in which our paper distinguishes between the notion of data owners and users. And their paper’s setting is equivalent to the case when each data owner is a single user in our paper. Stochastic convex optimization under (normal) differential privacy has been studied extensivelyBassilyet al.(2014,2019); Feldmanet al.(2020), and there is a long line of work on this. It is still a recent hotspot for the DP optimization community. Some of the representative research topics in this direction include studying the non-Euclidean geometriesAsiet al.(2021); Bassilyet al.(2021b,a); Gopiet al.(2022), improving the gradient computation complexity to achieve optimal privacy-utility tradeoffKulkarniet al.(2021); Carmonet al.(2023), getting dimension-independent bounds with additional assumptionsSonget al.(2021); Liet al.(2022), dealing with heavy-tailed dataWanget al.(2020); Jinet al.(2022); Kamathet al.(2022)and so on. The privacy notion “joint differential privacy” was initiated inKearnset al.(2014)and it is followed up inRogers and Roth (2014); Hsuet al.(2014); Kannanet al.(2018); Cummingset al.(2015). Their main focus is to implement various types of equilibria. Recent studies on user-level (full) differential privacyLevyet al.(2021); Esfandiariet al.(2021); Ghaziet al.(2023)have introduced intriguing techniques that could possibly be applied in the joint-DP setting we explore in this paper. In these studies, each owner holds only one user’s data (r=1), and only shared parametersuare considered. Specifically, in scenarios where the functions are smooth, the results inLevyet al.(2021)can be adapted to achieve an excess population loss of\tilde{O}(RL(\frac{1}{\sqrt{mn}}+\frac{\ell}{\epsilon n\sqrt{mr}})). While this bound offers improved dependence onm/r, it exhibits worse dependence on the dimension\ell. Further details can be found in the Appendix."
2308.10781v1	Mixed-Integer Projections for Automated Data Correction of EMRs Improve Predictions of Sepsis among Hospitalized Patients	"Machine learning (ML) models are increasingly pivotal in automating clinical decisions. Yet, a glaring oversight in prior research has been the lack of proper processing of Electronic Medical Record (EMR) data in the clinical context for errors and outliers. Addressing this oversight, we introduce an innovative projections-based method that seamlessly integrates clinical expertise as domain constraints, generating important meta-data that can be used in ML workflows. In particular, by using high-dimensional mixed-integer programs that capture physiological and biological constraints on patient vitals and lab values, we can harness the power of mathematical ""projections"" for the EMR data to correct patient data. Consequently, we measure the distance of corrected data from the constraints defining a healthy range of patient data, resulting in a unique predictive metric we term as ""trust-scores"". These scores provide insight into the patient's health status and significantly boost the performance of ML classifiers in real-life clinical settings. We validate the impact of our framework in the context of early detection of sepsis using ML. We show an AUROC of 0.865 and a precision of 0.922, that surpasses conventional ML models without such projections."	Complex decision-making in time-critical domains, such as the Intensive Care Unit (ICU), involves careful and meticulous attention to information from several data sources including the medical history and clinical tests. Trained clinicians comprehend the limitations of each data type in a given clinical situation and evaluate the entirety of this data, considering any conflicting information. Thus, in some sense, they continually navigate a multidimensional feature space grounded in physical and biological constraints. There has been a massive interest in harnessing machine learning (ML) methods to augment clinical decision-making. Machine learning models are often trained on retrospective patient data stored in Electronic Medical Records (EMR). However, the integrity and veracity of clinical data frequently come into question, as EMRs contain missing or nonsensical data from errors arising from various sources and contributed by various care providers through a patient’s hospital stay. While clinicians can interpret and amend these discrepancies based on patient history, ML models do not have the clinical context to be able to make similar corrections. Therefore, in this work, we study the problem of inducing more robustness in ML models utilizing a projections-based method to automatically correct potential errors in the EMRs, to enhance situational awareness and interoperability. A plethora of research so far has introduced methods for outlier detection and missing data imputation for medical records. These rely on statistical tests like inter-quartile range tests[1], mean-variance tests[2], or K-Nearest Neighbour algorithms[2]to detect outliers. Outliers are typically rectified using supervised learning techniques like regression models[31,28,1]. We take a completely different approach to detect outliers by harnessing clinical domain knowledge as opposed to purely statistical, data-driven methods. We translate complex biological and physiological constraints on vitals and laboratory values into mathematical constraints and project the EMR data onto these constraints. Projecting data onto the space defined by feasible domain constraints allows us to bring outliers back into the feasible range of values, consequently bolstering the reliability of subsequent machine-learning tasks. We also design a novel set of features, that we calltrust scores, using the distance between a patient’s current clinical data and the space defined by constraints on the normal range of values for healthy patients. These features are significantly predictive of how sick a patient is, as they gauge the health of a patient by measuring deviations from the norm. We present in this paper our projections-centric data processing framework, termedTrust-MAPS, for a high-fidelity machine learning pipeline. We validate our framework by applying it to thePhysioNetComputing in Cardiology Challenge 2019[29]for the early prediction of sepsis using EMR data. Sepsis is one of the most challenging to diagnose and manage among all complex diseases, requiring continuous monitoring and intensive care. It is characterized by organ failure due to the body’s dysregulated response to a severe infection. Accurate and prompt diagnosis of sepsis is of the essence and requires careful integration of data from different sources about a patient’s medical state. It is a leading cause of morbidity globally and is the leading cause of death in intensive care units (ICUs) in the U.S.[3,26]. Given that delayed sepsis treatment magnifies mortality risks to 8% each hour[21], its timely identification is paramount. While several machine learning models have been developed for sepsis prediction[16], their inability to recognize the inherent biases and systematic errors in the training data gives rise to significant false alarms and fatigue among clinicians[8]. We show that Trust-MAPS can improve the performance of sepsis prediction models when compared to models trained without it, achieving an AUROC of 0.865 and a precision of 0.922 on thePhysionetChallenge dataset. The key idea of using projections is very general, and can be used for more advanced predictive pipelines (e.g., using clinical notes or other sources of information). We first present a summary of our proposed projections pipeline in Section2, then summarize our experimental data analysis and results in Section3. Next, we discuss the implications of these results in Section4to highlight their utility in the context of machine learning for healthcare data. In Section6, we describe our complete sepsis prediction algorithmic pipeline comprising (i) translating known cross-vital dependencies and clinical domain knowledge for critically ill ICU patients into high-dimensional mathematical constraints, (ii) data processing by imputing missing data and unifying data dimensions with indefinite hospital stay lengths, (iii) correcting outliers by computing projections onto clinically feasible domain constraints, (iv) augmenting the data with variable-wise trust scores by calculating the distance to the space defined by constraints on healthy patient data, (v) unsupervised clustering based on clinical data and training ML models separately over patient clusters. We give an overview of our algorithmic pipeline at a high level in Figure2(and a detailed overview in Figure9in the Appendix, with a depiction of data dimensions).
2308.04249v1	MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion	Reconstructing visual stimuli from brain recordings has been a meaningful and challenging task. Especially, the achievement of precise and controllable image reconstruction bears great significance in propelling the progress and utilization of brain-computer interfaces. Despite the advancements in complex image reconstruction techniques, the challenge persists in achieving a cohesive alignment of both semantic (concepts and objects) and structure (position, orientation, and size) with the image stimuli. To address the aforementioned issue, we propose a two-stage image reconstruction model called MindDiffuser. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into Stable Diffusion, which yields a preliminary image that contains semantic information. In Stage 2, we utilize the CLIP visual feature decoded from fMRI as supervisory information, and continually adjust the two feature vectors decoded in Stage 1 through backpropagation to align the structural information. The results of both qualitative and quantitative analyses demonstrate that our model has surpassed the current state-of-the-art models on Natural Scenes Dataset (NSD). The subsequent experimental findings corroborate the neurobiological plausibility of the model, as evidenced by the interpretability of the multimodal feature employed, which align with the corresponding brain responses.	The human visual system possesses the exceptional ability to efficiently and robustly perceive and comprehend complex visual stimuli in the real world, which is unparalleled by current artificial intelligence systems. Understanding these brain activities and reconstructing[39]the corresponding stimuli is a critical step towards the ultimate goal of deciphering the workings of the human brain, despite its immense difficulty. With the advancement of sophisticated image generation methods and the increase in the volume of neuroimaging data, researchers are increasingly focusing on image reconstruction. Recently, researches have revealed that deep learning frameworks exhibit a certain level of consistency with the hierarchical encoding-decoding process of the human visual system[37,30,44]. As a result, numerous studies have extensively employed deep neural networks (DNN) for reconstructing natural images. Based on the structure of the previous image reconstruction models, we categorize them intooptimized modelsandgenerative models. The optimized model is represented by DGN[46]proposed by Shen et al., which utilizes image feature extracted from a DNN as a constraint, and optimizes the latent space of the image generator to achieve similarity with the decoded DNN feature. While this method allows for alignment of the structural information of the reconstructed images with the corresponding ones in pixel space, the absence of image priori in latent space means that optimization starting from Gaussian noise can result in indistinct outcomes and a lack of clear semantic information. The generative reconstruction models involve decoding fMRI into the latent space of models such as VAE[26], GAN[20], and Diffusion model[47], and leveraging their powerful generation capabilities to reconstruct images that are semantically similar to the original. While this paradigm enables rapid generation of realistic and semantically rich reconstruction images, the outcomes are always lacking in control over structural information. Building upon the preceding discussions, we make the following contributions in this work:(1) We present an image reconstruction model (MindDiffuser) that integrates the strengths of the two aforementioned paradigms and effectively addresses their respective limitations, resulting in semantically similar and structurally aligned reconstruction outcomes. A series of detailed quantitative comparisons demonstrate that our model has surpassed the current state-of-the-art models.(2) Our proposed two-stage image reconstruction model leverages semantic feature incorporating and structural feature aligning to achieve a remarkably powerful image reconstruction capability, as evidenced by the upper bound showcased in Figure1. Additionally, our model can serve as a module and be combined with future advancements in more effective brain signal decoders to achieve higher-quality visual reconstruction results.(3) Our experiments demonstrate that MindDiffuser can adapt to inter-subject variability without additional modifications. Furthermore, the visualization of feature decoding precosses provide evidence of the model’s rationality and interpretability in neuroscience.
2312.16215v1	SUNDIAL: 3D Satellite Understanding through Direct, Ambient, and Complex Lighting Decomposition	3D modeling from satellite imagery is essential in areas of environmental science, urban planning, agriculture, and disaster response. However, traditional 3D modeling techniques face unique challenges in the remote sensing context, including limited multi-view baselines over extensive regions, varying direct, ambient, and complex illumination conditions, and time-varying scene changes across captures. In this work, we introduce SUNDIAL, a comprehensive approach to 3D reconstruction of satellite imagery using neural radiance fields. We jointly learn satellite scene geometry, illumination components, and sun direction in this single-model approach, and propose a secondary shadow ray casting technique to 1) improve scene geometry using oblique sun angles to render shadows, 2) enable physically-based disentanglement of scene albedo and illumination, and 3) determine the components of illumination from direct, ambient (sky), and complex sources. To achieve this, we incorporate lighting cues and geometric priors from remote sensing literature in a neural rendering approach, modeling physical properties of satellite scenes such as shadows, scattered sky illumination, and complex illumination and shading of vegetation and water. We evaluate the performance of SUNDIAL against existing NeRF-based techniques for satellite scene modeling and demonstrate improved scene and lighting disentanglement, novel view and lighting rendering, and geometry and sun direction estimation on challenging scenes with small baselines, sparse inputs, and variable illumination.	Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press. This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version. All manuscripts must be in English. Please refer to the author guidelines on the CVPR 2023 web page for a discussion of the policy on dual submissions. Papers, excluding the references section, must be no longer than eight pages in length. The references section will not be included in the page count, and there is no limit on the length of the references section. For example, a paper of eight pages with two pages of references would have a total length of 10 pages.There will be no extra page charges for CVPR 2023. Overlength papers will simply not be reviewed. This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide. Note that thisLATEXguide already sets figure captions and references in a smaller font. The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts. The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven. TheLATEXstyle defines a printed ruler which should be present in the version submitted for review. The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution. If you are preparing a document using a non-LATEXdocument preparation system, please arrange for an equivalent ruler to appear on the final output pages. The presence or absence of the ruler should not change the appearance of any other content on the page. The camera-ready copy should not contain a ruler. (LATEXusers may use options of cvpr.sty to switch between different versions.) Reviewers: note that the ruler measurements do not align well with lines in the paper — this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly. Just use fractional references (e.g., this line is087.5), although in most cases one would expect that the approximate location will be adequate. Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the “*****” you see in this document). If you are using theLATEXtemplate,make sure to update paper ID in the appropriate place in the tex file. Please number all of your sections and displayed equations as in these examples: and It is important for readers to be able to refer to any particular equation. Just because you did not refer to it in the text does not mean some future reader might not need to refer to it. It is cumbersome to have to use circumlocutions like “the equation second from the top of page 3 column 1”. (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers). All authors will benefit from reading Mermin’s description of how to write mathematics:http://www.pamitc.org/documents/mermin.pdf. Many authors misunderstand the concept of anonymizing for blind review. Blind review does not mean that one must remove citations to one’s own work—in fact it is often impossible to review a paper unless the previous citations are known and available. Blind review means that you do not use the words “my” or “our” when citing previous work. That is all. (But see below for tech reports.) Saying “this builds on the work of Lucy Smith [1]” does not say that you are Lucy Smith; it says that you are building on her work. If you are Smith and Jones, do not say “as we show in [7]”, say “as Smith and Jones show in [7]” and at the end of the paper, include reference 7 as you would any other cited work. An example of a bad paper just asking to be rejected: An analysis of the frobnicatable foo filter. In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods. Why the previous paper was accepted without this analysis is beyond me.Removed for blind review An example of an acceptable paper: An analysis of the frobnicatable foo filter. In this paper we present a performance analysis of the paper of Smithet al. [1], and show it to be inferior to all previously known methods. Why the previous paper was accepted without this analysis is beyond me.Smith, L and Jones, C. “The frobnicatable foo filter, a fundamental contribution to human knowledge”. Nature 381(12), 1-213. If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work. In such cases, include the anonymized parallel submission[Authors14]as supplemental material and cite it as [1] Authors. “The frobnicatable foo filter”, F&G 2014 Submission ID 324, Supplied as supplemental materialfg324.pdf. Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report. For conference submissions, the paper must stand on its own, and notrequirethe reviewer to go to a tech report for further details. Thus, you may say in the body of the paper “further details may be found in[Authors14b]”. Then submit the tech report as supplemental material. Again, you may not assume the reviewers will read this material. Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution. For example, let’s say it’s 1969, you have solved a key problem on the Apollo lander, and you believe that the CVPR70 audience would like to hear about your solution. The work is a development of your celebrated 1968 paper entitled “Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties”, by Zeuset al. You can handle this paper like any other. Do not write “We show how to improve our previous work [Anonymous, 1968]. This time we tested the algorithm on a lunar lander [name of lander removed for blind review]”. That would be silly, and would immediately identify the authors. Instead write the following: We describe a system for zero-g frobnication. This system is new because it handles the following cases: A, B. Previous systems [Zeus et al. 1968] did not handle case B properly. Ours handles it by including a foo term in the bar integral.…The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don’t you know. It displayed the following behaviours, which show how well we solved cases A and B: … As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors. A reviewer might think it likely that the new paper was written by Zeuset al., but cannot make any decision based on that guess. He or she would have to be sure that no other authors could have been contracted to solve problem B. FAQQ:Are acknowledgements OK?A:No. Leave them for the final copy.Q:How do I cite my results reported in open challenges?A:To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper. For your results, however, you should not identify yourself and should not mention your participation in the challenge. Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results. Compare the following:$conf_a$conf_{a}$\mathit{conf}_a$\mathit{conf}_{a}See TheTEXbook, p165. The space aftere.g., meaning “for example”, should not be a sentence-ending space. Soe.g. is correct,e.g.is not. The provided\egmacro takes care of this. When citing a multi-author paper, you may save space by using “et alia”, shortened to “et al.” (not “et. al.” as “et” is a complete word). If you use the\etalmacro provided, then you need not worry about double periods when used at the end of a sentence as in Alpheret al. However, use it only when there are three or more authors. Thus, the following is correct: “Frobnication has been trendy lately. It was introduced by Alpher[Alpher02], and subsequently developed by Alpher and Fotheringham-Smythe[Alpher03], and Alpheret al.[Alpher04].” This is incorrect: “… subsequently developed by Alpheret al.[Alpher03]…” because reference[Alpher03]has just two authors.
2309.11983v3	Variational Connectionist Temporal Classification for Order-Preserving Sequence Modeling	Connectionist temporal classification (CTC) is commonly adopted for sequence modeling tasks like speech recognition, where it is necessary to preserve order between the input and target sequences. However, CTC is only applied to deterministic sequence models, where the latent space is discontinuous and sparse, which in turn makes them less capable of handling data variability when compared to variational models. In this paper, we integrate CTC with a variational model and derive loss functions that can be used to train more generalizable sequence models that preserve order. Specifically, we derive two versions of the novel variational CTC based on two reasonable assumptions, the first being that the variational latent variables at each time step are conditionally independent; and the second being that these latent variables are Markovian. We show that both loss functions allow direct optimization of the variational lower bound for the model log-likelihood, and present computationally tractable forms for implementing them.	Many real-world applications such as speech recognition[13,21], speech synthesis[16], handwriting recognition[6], etc., involve sequence modeling that estimates the mapping function between two sequences. When the input sequence and the target sequence have the same length (i.e., they arealigned), recurrent neural networks (RNNs) generally show great promise in modeling the mapping function[9]. However, in a number of sequence modeling tasks, the input and output sequences are of different lengths (e.g., speech recognition, neural machine translation, etc.), and the use of conventional RNNs in these tasks may be suboptimal. To address these tasks, three main approaches including the use of attention-based encoder-decoders (AEDs)[2,17,19], connectionist temporal classification (CTC) loss[12], and RNN-transducers (RNN-Ts)[11]have been largely developed. However, not all of them are equally suited to all sequence modeling tasks. For example, the order of the input and output sequences will always be preserved for tasks such as speech recognition and handwriting recognition[22], but AEDs cannot guarantee this order preservation constraint will always be met (unlike CTC). Similarly, both CTC and RNN-Ts share a common limitation of having discontinuous and sparse latent spaces. This becomes a problem when the test data is mapped to unexplored area of the latent space. Although variational modeling has been successfully applied in various domains (e.g., data generation) to address the discontinuity and sparsity issues[18,23], so far, it has not been incorporated with CTC. The challenge mainly arises from approximating the joint distribution of the latent variable sequence across all time steps, rendering the standard form of the variational lower bound and the model log-likelihood intractable. In this paper, we present a novel approach to order-preserving sequence modeling, where CTC is incorporated with variational modeling. The proposed approach aims to leverage both the strength of CTC, which guarantees to preserve order, and that of variational modeling, which does not produce discontinuous and sparse latent spaces. To tackle the intractability of the variational lower bound, we show that one of two pragmatic assumptions can be made. Specifically, the first assumption is that the variational latent variables at each time step are conditionally independent, while the second assumes the temporal dependency amongst the latent variables is Markovian. Through mathematical derivations, both assumptions lead to tractable loss functions, allowing direct optimization of the variational lower bound of the model log-likelihood. Errors caused by the discontinuity and sparsity of the latent space can thus be mitigated.
2305.18436v2	Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming	$K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments, we observe that our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-the-art.	Clustering remains a common unsupervised learning technique, for which the basic objective is to assign similar data points to the same group. Given data in the Euclidean space, a widely used clustering method isK-means clustering, which quantifies “similarity” in terms of distances between the given data and learned clustering centers, known as centroids(MacQueen,1967). In order to divide the data pointsX_{1},\dots,X_{n}\in\mathbb{R}^{p}intoKgroups,K-means clustering aims to minimize the following cost function: where\beta_{k}is the centroid of thek-th cluster fork\in[K]:=\{1,\dots,K\}. It is well-known that exactly solving problem (1) is NP hard in the worst-case(Dasgupta,2007; Aloiseet al.,2009), so computationally tractable approximation algorithms and relaxed formulations have been extensively studied in the literature. Notable examples include Lloyd’s algorithm(Lloyd,1982), spectral clustering(von Luxburg,2007; Nget al.,2001), nonnegative matrix factorization (NMF)(Heet al.,2011; Kuanget al.,2015; Wang and Zhang,2012), and semidefinite programming (SDP)(Peng and Wei,2007; Mixonet al.,2017; Royer,2017; Fei and Chen,2018; Giraud and Verzelen,2018). Among those popular relaxations, the SDP approach enjoys the strongest statistical guarantees under the standard Gaussian mixture model in that it achieves an information-theoretic sharp threshold for the purpose for exact recovery of the true cluster partition(Chen and Yang,2021). Unfortunately, the SDP and its strong statistical guarantees remain completely inaccessible to real-world datasets, owing to the prohibitively high costs of solving the resulting SDP relaxation. Givenndata points, the SDP is a matrix optimization problem, over a densen\times nmembership matrixZ, that is constrained to be both positive semidefiniteZ\succeq 0as well as elementwise nonnegativeZ\geq 0. Even ignoring the constraints, a basic but fundamental difficulty is the need to store and optimize over then^{2}individual elements of the matrix. Even a small dataset withn\approx 1000, such as the banknote authentication dataset(Dua and Graff,2017), translates into an SDP withn^{2}\approx 10^{6}optimization variables, which is right at the very limit of state-of-the-art SDP solvers like SDPNAL+(Yanget al.,2015). On the other hand, NMF remains one of the simplest and practically useful approaches to clustering due to its scalability(Heet al.,2011; Kuanget al.,2015). When the clustering problem at hand exhibits an appropriate low-dimensional structure, NMF gains significant computational savings by imposing elementwise nonnegativity over ann\times rlow-rank factor matrixU\geq 0, in order to imply positive semidefinitenessZ\succeq 0and elementwise nonnegativityZ\geq 0over then\times nmembership matrixZ=UU^{T}. While highly scalable, there remains unfortunately very little statistical underpinning behind NMF-based algorithms. Our contributions.In this paper, we propose an efficient, large-scale, NMF-like algorithm for theK-means clustering problem, thatmeanwhileenjoys the same sharp exact recovery guarantees provided by SDP relaxations. We are motivated by the fact that the three classical approaches toK-means clustering, namely spectral clustering, NMF, and SDP, can all be interpreted as techniques for solving slightly different relaxations of the same underlyingK-means mixed integer linear program (MILP); see our exposition in Section2. This gives us hope to break the existing computational and statistical bottleneck by investigating the intersection of these three classical approaches. At its core, our proposed algorithm is a primal-dual gradient descent-ascent algorithm to optimize over a nonnegative factor matrix, inside an augmented Lagrangian method (ALM) solution of the SDP. The resulting iterations closely resemble the projected gradient descent algorithms widely used for NMF and spectral clustering in the existing literature; in fact, we show that the latter can be recovered from our algorithm by relaxing suitable constraints. We prove that the new algorithm enjoys local linear convergence within a primal-dual neighborhood of the SDP solution, which is unique whenever the centroids satisfy a well-separation condition from(Chen and Yang,2021). In practice, we observe that the algorithm converges globally at a linear rate. As shown in Figure1, our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-the-art. The main novelty of our algorithm is the use of projected gradient descent to solve the difficult primal update inside the ALM. Indeed, this primal update had been the main critical challenge faced by prior work; while a similar nonnegative low-rank ALM was previously proposed byKuliset al.(2007), their inability to solve the primal update to high accuracy resulted in a substantial slow-down to the overall algorithm, which behaved more like aninexactALM. In contrast, our projected gradient descent method is able to solve the primal update at a rapid linear rate to machine precision (see Theorem1), so our overall algorithm is able to enjoy the rapid primal-dual linear convergence that is predicted and justified by classical theory forexactALMs. As shown in Figure4in AppendixA, our algorithm is the first ALM that can solve the SDP relaxation to arbitrarily high accuracy. Organization.The rest of the paper is organized as follows. In Section2, we present some background on several equivalent and relaxed formulations of theK-means clustering problem. In Section3, we introduce a nonnegative low-rank SDP for solving the Burer–Monteiro formulation ofK-means problem. In Section4, we establish the linear convergence guarantee for our proposed primal-dual gradient descent-ascent algorithm in the exact recovery regime. Numerical experiments are reported in Section5. Proof details are deferred to the Supplementary Material.
2312.13509v1	MR-STGN: Multi-Residual Spatio Temporal Graph Network Using Attention Fusion for Patient Action Assessment	Accurate assessment of patient actions plays a crucial role in healthcare as it contributes significantly to disease progression monitoring and treatment effectiveness. However, traditional approaches to assess patient actions often rely on manual observation and scoring, which are subjective and time-consuming. In this paper, we propose an automated approach for patient action assessment using a Multi-Residual Spatio Temporal Graph Network (MR-STGN) that incorporates both angular and positional 3D skeletons. The MR-STGN is specifically designed to capture the spatio-temporal dynamics of patient actions. It achieves this by integrating information from multiple residual layers, with each layer extracting features at distinct levels of abstraction. Furthermore, we integrate an attention fusion mechanism into the network, which facilitates the adaptive weighting of various features. This empowers the model to concentrate on the most pertinent aspects of the patient's movements, offering precise instructions regarding specific body parts or movements that require attention. Ablation studies are conducted to analyze the impact of individual components within the proposed model. We evaluate our model on the UI-PRMD dataset demonstrating its performance in accurately predicting real-time patient action scores, surpassing state-of-the-art methods.	Physical rehabilitation is a critical component of patient care, particularly for individuals with disabilities or injuries[10]. However, ensuring patients are performing exercises correctly and adhering to their rehabilitation program can be challenging for healthcare providers[11]. Recent advances in technology have opened up the possibility of using machine learning algorithms to automatically assess patients’ performance of physical rehabilitation exercises. However, there are several challenges to overcome in order to achieve reliable and accurate assessment, including sensor placement, data variability, and patient heterogeneity. Various technologies, including inertial measurement units (IMUs), computer vision, and machine learning algorithms, have been explored for automatically assessing physical rehabilitation exercises[6]. IMUs are small sensors attached to a patient’s body that measure movement and orientation, while computer vision techniques track joint positions and movements from video footage. Machine learning algorithms[4], like artificial neural networks, help recognizing exercise patterns and providing personalized feedback on patient performance using sensor data or video footage. While these approaches show promise in accurately identifying exercises and detecting errors, challenges remain in developing reliable and robust algorithms that can handle data variability and patient heterogeneity. More extensive validation studies in clinical settings are necessary to ensure the safety and effectiveness of these automated systems. Moreover, these approaches consider the exercise evaluation as a binary classification task, distinguishing exercises as either correct or incorrect. Nevertheless, these investigations were constrained in their capacity to furnish comprehensive feedback for individual exercise performances, as depicted in Fig.1. Deep learning approaches[15,23,13], including CNNs, RNNs, and GCNs, have received attention in the physical rehabilitation field for their ability to automatically assess patients’ exercise performance by analyzing sensor data or video footage. The use of Graph convolutional networks (GCNs), which operates on data represented as graphs, offers a unique advantage in capturing spatiotemporal characteristics of exercises and modeling complex interactions between body parts while respecting the natural topological structure of the human body skeleton. However, these approaches mostly require time-consuming processing and need to be improved for better precision. Furthermore, they do not offer a reliable and complete feedback, which can be very helpful for patient exercise improvement. Inspired by Graph Convolutional Networks (GCNs), we propose a novel Multi-Residual Spatio Temporal Graph Network (MR-STGN) that employs both information from positional and angular 3D skeletons to capture the spatial and temporal dependencies of human body parts. GCNs have shown promising results in modeling graph-structured data, and we leverage their benefits in our proposed model. The use of 3D skeletons allows us to capture the positional and angular information of human body parts, which is crucial in assessing patient actions during rehabilitation exercises. We employ a multi-residual framework to learn deeper representations and mitigate the problem of vanishing gradients in deep neural networks. Additionally, our proposed model utilizes a graph attention mechanism that selectively attends to relevant nodes and edges in the graph, improving the model’s discriminative power. Furthermore, our model provides clear guidance on which body parts or movements to focus on, leading to improved assessment quality. The real-time performance of the MR-STGAN model makes it suitable for healthcare applications requiring timely feedback, such as rehabilitation. The main contributions of this paper are summarized as follows: (1) A novel Multi-Residual Spatio Temporal Graph Network (MRSTGN) that incorporates both angular and positional 3D skeletons for patient action assessment; (2) A multi-residual architecture that captures the spatio-temporal dynamics of patient actions by integrating information from multiple residual layers, each of which extracts features at different levels of abstraction; (3) An attention fusion mechanism that selectively combines the information from the angular and positional skeletons, allowing the network to adaptively weight the importance of different features and providing a more comprehensive and accurate assessment of patient action; (4) Guidance: The system provides clear guidance on which body parts or movements to focus on and has shown high performance on physical rehabilitation benshmarks; (5) High performance: The efficiency of the proposed model is shown through extensive experimentation on physical rehabilitation datasets UI-PRMD.
2305.07039v2	Value Iteration Networks with Gated Summarization Module	In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments on 2D grid world path-finding problems and the Atari Mr. Pac-man environment, demonstrating that GS-VIN outperforms the baseline in terms of single-step accuracy, planning success rate, and overall performance across different map sizes. Additionally, we provide an analysis of the relationship between input size, kernel size, and the number of iterations in VI-based models, which is applicable to a majority of VI-based models and offers valuable insights for researchers and industrial deployment.	In path-finding problems, intelligent agents need to find the optimal path from a starting point to an endpoint in a given environment. These problems have significant implications in various practical applications such as autonomous driving, robot navigation, and gaming AI. To address these issues, researchers have proposed many different algorithms and models, such as the Dijkstra’s algorithm[dijkstra], A* algorithm[Astar], etc. However, these algorithms usually require global environmental information and have low computational efficiency in complex environments. To address the computational efficiency issues of traditional path-finding algorithms in complex environments, Tamar et al. proposed Value Iteration Networks (VIN) in 2016[VIN]. VIN is an end-to-end trainable neural network that combines the concepts of Value Iteration (VI)[Bellman1957]and Convolutional Neural Networks (CNN)[LeCun1998]. The core idea of VIN is to embed the Value Iteration algorithm as a planning module (also called the value iteration module) into the neural network architecture to perform active planning. This design allows VIN to directly learn the optimal policy from the input’s raw information through active global planning without explicit state space modeling or pre-defined feature representations. According to literature[VIN], it has been demonstrated that VIN exhibits superior performance and generalization capabilities when compared to conventional reactive neural networks. Although VIN has achieved some success, it still faces many challenges. Recent research based on VIN has addressed many of these issues from different perspectives, such as improving network overestimation[DVIN], enhancing VIN’s generalization capabilities[TVIN], and enabling networks to handle larger map inputs through multi-sampling of the input[AVIN]. However, in these studies, to the best of our knowledge, researchers often overlook the utilization of convolutional layers within the VI module, primarily employing conventional3\times 3kernel sizes[DVIN,TVIN,AVIN,UVIN,Nardelli2018ValuePN,Pflueger2018RoverIRLIR,sykora2020multi]. This decision results in a prevalent issue among such networks: when confronted with larger inputs, the network can only achieve global planning through an increased number of iterations. Consequently, this necessitates a deeper network depth, which ultimately leads to the excessive accumulation of single-iteration computation errors and training instability stemming from the network’s depth and the backpropagation algorithm[rumelhart1986learning], such as vanishing gradients and exploding gradients[hochreiter2001gradient]. Previous research[VIRN]has provided preliminary investigations into the aforementioned problems. In this paper, we propose a more comprehensive approach, namely Value Iteration Networks with Gated Summarization Module (GS-VIN), building upon the previous research. The GS-VIN model includes two key enhancements: (1) adopting an adaptive iteration strategy in the value iteration (VI) module to reduce the number of iterations required, and (2) introducing a gated summarization(GS) module to robustly summarize the iterative process, thereby mitigating the impact of accumulated errors. These improvements aim to achieve more precise planning in larger and more complex environments. The proposed improvements can be summarized as follows: An adaptive iteration strategy in the VI module actively uses larger convolution kernels in conjunction with fewer iteration times. This approach not only reduces network depth, lowering the risk of gradient explosion and vanishing, and increasing training stability, but also ensures the accuracy of the planning process. Recognizing the importance of considering the future prediction process when making decisions, rather than merely focusing on the ultimate future outcome, we propose a new gated summarization module. In larger inputs, the VI module requires more iterations, making the final iteration results more unreliable due to the accumulation of single-iteration errors. Therefore, our proposed module employs gating mechanisms to achieve a similar effect to attention mechanisms[mnih2014recurrent]. With this module, the network can temporally and spatially sample the entire planning process within the VI module, rather than solely relying on the global planning output. This approach endows the model with the capacity to abstract and summarize the planning process, ultimately mitigating the impact of cumulative errors on the network’s output. We conduct experiments on 2D grid world path-finding problems and the Atari Mr. Pac-man environment. For the 2D grid world path-finding problems, we demonstrate that GS-VIN outperforms the baseline in single-step accuracy and planning success rate across different map sizes. We also analyze the relationship between input size, convolution kernel size, and iteration times in the VI module of VI-based models based on 2D grid world task. In the Mr. Pac-man environment, the experiments show that GS-VIN maintains superior accuracy performance in larger and more complex environments compared to other competitors. The main contributions of this paper are as follows: We propose the GS-VIN network, GS-VIN employs an adaptive iteration strategy in the value iteration module, and additionally incorporates a gated summarization module to mitigate error accumulation. These characteristics enable the network model to emphasize the entire planning process from local to global, rather than solely relying on the final global planning outcome, while simultaneously reducing the number of iterations in the VI module. Ultimately, this enhances the model’s ability to summarize and improve the planning process. We present a novel heuristic function to reveal the relationship among input size, convolution kernel size, and iteration number in adaptive iteration strategy, and conduct extensive experiments to investigate the impact of different combinations of convolution kernel size and iteration number on network performance. The experiment results indicate that the adaptive iteration strategy can be applied to VI-based network models, and the heuristic function we propose has an important reference value for VI-based network models. We conduct experiments on 2D grid world path-finding problems and Atari’s Mr. Pac-man environment, comparing GS-VIN with the method in[VIRN]and other methods. We demonstrate that GS-VIN outperforms the baseline in terms of overall performance. The contribution of this paper builds upon the direction proposed in prior work[VIRN]and further investigates it. For a detailed comparison between this study and the work presented in[VIRN], please refer to sectionIII. The structure of this paper is as follows: In sectionII, we introduce some background knowledge related to this research. In sectionIII, we review recent related work on VIN. In sectionIV, we provide a detailed description of our method. In sectionV, we present experiments and discuss the results. Finally, in sectionVI, we summarize our work and mention future work.
2301.11745v1	Side Auth: Synthesizing Virtual Sensors for Authentication	While the embedded security research community aims to protect systems by reducing analog sensor side channels, our work argues that sensor side channels can be beneficial to defenders. This work introduces the general problem of synthesizing virtual sensors from existing circuits to authenticate physical sensors' measurands. We investigate how to apply this approach and present a preliminary analytical framework and definitions for sensor side channels. To illustrate the general concept, we provide a proof-of-concept case study to synthesize a virtual inertial measurement unit from a camera motion side channel. Our work also provides an example of applying this technique to protect facial recognition against silicon mask spoofing attacks. Finally, we discuss downstream problems of how to ensure that side channels benefit the defender, but not the adversary, during authentication.	Sensor side channels enable an adversary to violate integrity of sensor outputs by influencing or controlling the sensor with transduction attacks(Yanet al.,2020; Giechaskiel and Rasmussen,2019), or to eavesdrop on sensitive information and compromise confidentiality by exploiting flaws in sensor and system designs(Michalevskyet al.,2014; Boltonet al.,2021; Anandet al.,2021; Simon and Anderson,2013).For example, the eavesdropping example PIN Skimmer(Simon and Anderson,2013)shows that adversaries can infer smartphone touchscreen inputs by exploiting side channel motion information captured by smartphone cameras.While the security research community invested significant effort identifying and mitigating analog sensor side channels, our work argues that it can bebeneficial to embrace, understand, and control analog sensor side channels instead of simply eliminating them.This is motivated by our observation that such side channel information may also be used for authentication. For example, extensive research has been conducted on using dedicated motion sensors to capture smartphone touch dynamics for continuous implicit user authentication(Tehet al.,2016). Relating it to PIN Skimmer, a natural question arises as to whether cameras support such authentication when dedicated motion sensors are not available.We thus propose and investigate the problem of how to utilize sensor side channels for defensive purposes such as multimodal authentication by synthesizing virtual sensors from them. Side channels are inherent to analog sensors’ physics. There exist a considerable number of potential sensor side channels besides those revealed by transduction and eavesdropping attacks. However, most of these side channels are deliberately “closed” in the design phase by employing mitigation mechanisms such as calibration and noise reduction. It is foreseeable that sensor and system designers will also try to mitigate newly discovered side channels. This work argues a different perspective and approach to embrace such sensor side channels. If these side channels can be used in a beneficial way, we envision future designs allowing mitigation mechanisms to be strategically disabled or downgraded when needed such as during authentication sessions. We provide a preliminary analytical framework for modeling analog sensor side channels and explaining the origins and characteristics of them. The framework categorizes sensor side channels according to their separability from intended signals and whether they have controllable mitigation mechanisms. Based on the framework, we define the problem of virtual sensor synthesis for multimodal measurand authentication and summarize three possible ways of applying this approach (Figure1). First, by verifying signatures of signal byproducts and asking the question “What is the probability that Alice generated both the measurands and byproducts?” Second, by verifying the person performing the measurement and asking “What is the probability that Alice generated the measurands if Bob was the measurer?” Third, by verifying the environment of the measurement process and asking “What is the probability that Alice generated the measurands if the measurement was taken in location B?” A proof-of-concept case study further concretizes the concepts and related considerations by studying a camera motion side channel that enables cameras to sense out-of-sight motion. This side channel is caused by mechanical connections between camera devices and adjacent objects in motion such as a hand holding the camera. We propose a methodology for synthesizing virtual inertial measurement units (IMUs) from this side channel that can extract both inter-frame low-frequency and intra-frame high-frequency motion information. The case study discusses this side channel’s potential application in helping facial recognition systems defend against 3D silicon mask spoofing attacks by verifying postural hand tremor motion of the person holding the camera device. Preliminary test with 4 people suggests the camera motion side channel help reduce false positive rates by up to 87.5%. It also shows that disabling video stabilization enables higher performance, emphasizing the benefits of strategically disabling side channel mitigation mechanisms. Finally, we discuss the possible issues of temporarily opening sensor side channels during authentication and the directions future works may take to address the issues. Our main contributions are summarized as follows: A new paradigm to embrace and harness analog sensor side channels for defensive purposes via active control or influence of sensor side channels. An analytical framework to enable definition and characterization of sensor side channels. The framework introduces the concept of virtual sensor synthesis for multimodal sensor measurand authentication. A case study and methodology of synthesizing virtual IMUs from camera motion side channels for enhancing performance of facial recognition systems. Virtual IMUs decrease false positive rates when facial recognition is subjected to emulated 3D silicon mask spoofing attacks.
2306.09177v1	Dis-AE: Multi-domain & Multi-task Generalisation on Real-World Clinical Data	Clinical data is often affected by clinically irrelevant factors such as discrepancies between measurement devices or differing processing methods between sites. In the field of machine learning (ML), these factors are known as domains and the distribution differences they cause in the data are known as domain shifts. ML models trained using data from one domain often perform poorly when applied to data from another domain, potentially leading to wrong predictions. As such, developing machine learning models that can generalise well across multiple domains is a challenging yet essential task in the successful application of ML in clinical practice. In this paper, we propose a novel disentangled autoencoder (Dis-AE) neural network architecture that can learn domain-invariant data representations for multi-label classification of medical measurements even when the data is influenced by multiple interacting domain shifts at once. The model utilises adversarial training to produce data representations from which the domain can no longer be determined. We evaluate the model's domain generalisation capabilities on synthetic datasets and full blood count (FBC) data from blood donors as well as primary and secondary care patients, showing that Dis-AE improves model generalisation on multiple domains simultaneously while preserving clinically relevant information.	Machine learning has promised to revolutionise healthcare for several yearssadegh-zadehDubioProAegro1990;handelmanEDoctorMachineLearning2018. Moreover, while there is an extensive literature describing high-performing machine learning models trained on immaculate benchmark datasetsmilletariVNetFullyConvolutional2016;hanMADGANUnsupervisedMedical2021;caoSwinUnetUnetlikePure2021, such promising approaches rarely make it into clinical practiceteodoridisWhyAIAdoption2022. Often, this is because of an unexpected drop in performance when deploying the model on unseen test data due todomain shiftrechtImageNetClassifiersGeneralize2019;moreno-torresUnifyingViewDataset2012, i.e. there is a change in the data distribution between the dataset a model is trained on (sourcedata) and that which it is deployed against (targetdata). Most common machine learning algorithms rely on an assumption that the source and target data are independent and identically distributed (i.i.d.)bishopPatternRecognitionMachine2006. However, with domain shift, this assumption no longer holds, and model performance can be significantly affected. For medical datasets, domain shift is widespread, resulting from differences in equipment and clinical practice between sitesliuSemisupervisedMetalearningDisentanglement2021;taoDeepLearningBased2019;thiagarajanUnderstandingBehaviorClinical2019;rolandDomainShiftsMachine2022, and models are vulnerable to associating clinically irrelevant features specific to the domain with their predictions, known asshortcut learninggeirhosShortcutLearningDeep2020, which may lead to poor performance on target data. For most medical applications, target data is rarely available prior to real-time deployment; thus, adomain adaptationapproach, where pre-trained models are fine-tuned on data from the target distribution is not feasible.Domain generalisationtechniques that focus on mitigating domain shift and improving model performance on unseen target datazhouDomainGeneralizationSurvey2022are more practical approaches. Various techniques have been developed in recent years:data augmentationotaloraStainingInvariantFeatures2019;zhouLearningGenerateNovel2020, where pre-processing is applied to source data in hopes of increasing data diversity;domain alignmentliDomainGeneralizationMedical2020;huDomainGeneralizationMultidomain2020, where domain shift is minimised by appropriate feature transformations which re-align the data before model training, andensemble learningdingDeepDomainGeneralization2018;xuExploitingLowRankStructure2014;wangDoFEDomainOrientedFeature2020;nozzaDeepLearningEnsemble2016, where an average of multiple models trained on different source distributions is taken. In medical datasets, particularly for tabular data, the extent and effect of unseen domains is often unknown, making alignment and augmentation impractical. For on-site deployment on non-specialised hardware, ensemble learning can often prove too expensive in terms of memory or computational timebifetNewEnsembleMethods2009, particularly for deep neural network models. Furthermore, in the sizeable domain generalisation literature, there is very little research into the problem of multiple interacting domains within any given dataset. In Figure1, we indicate such a separation for some example domain shifts we observe in real-world datasets. Authors in the literature often refer to ”domains” based solely on sample origin, rather than considering actual distribution-influencing effects causing domain shifts. In this paper, we propose a deep learning model architecture based on feature disentanglementpmlr-v37-ganin15to perform scalable domain generalisation for multiple interacting domains on tabular medical data. Such methods use multi-task learning to learn features useful for the desired task but penalise for learning features that can identify the sample’s domain label. Importantly, no prior knowledge of the expression of the domain shift is required. Our novel approach allows for multiple domains and tasks, both continuous and categorical, creating a truly disentangled embedding that can be used for multiple classification tasks. We test the model on both synthetic and real-life clinical datasets, with the goal of retaining high performance on data not used in training. The proposed architecture can be easily applied to other tabular data or images by adapting the corresponding model components.Our novel contributions presented in this paper are: We introduce a domain-instance grouping that goes beyond the current domain generalisation literature by identifying and separating effects that cause domain shift in the data (e.g. Figure1). We propose a novel disentangled autoencoder (Dis-AE) neural network model architecture to find a domain-agnostic lower-dimensional representation of the data. Dis-AE is easily scalable to multiple domains and classification tasks. We conduct experiments of Dis-AE, comparing it to a regular autoencoder on synthetic and real-life clinical data. We show that Dis-AE achieves high domain generalisation performance when using the same width and depth as the regular autoencoder.
2308.16622v1	Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering	As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.	Large Language Models (LLMs) hold the potential to change the way how we interact with data and technology. Especially models like GPT-3 and GPT-4 have shown proficient capabilities in solving textual assignments[1]and spawned a wave of subsequent models and the field ofprompt engineering. But the fast evolution and rapidly growing landscape of different LLMs make it challenging to keep track of their individual capabilities and to choose the best model and best prompt for the job. There exist efforts on generic LLM benchmarks (e.g.[2]). However, despite these advancements, the application and (automated) assessment of LLMs in the context of knowledge graph engineering (KGE) and the Semantic Web is still a highly under-explored area. In response to this gap, this paper proposes a first LLM KGE benchmarking frameworkLLM-KG-Bench111Repository:https://github.com/AKSW/LLM-KG-Benchordoi:10.5281/zenodo.8251944that follows our vision of an automated and continuous evaluation platform for different tasks in KGE scenarios. A test of the framework is presented by comparing three LLMs for three exemplary KGE tasks.
